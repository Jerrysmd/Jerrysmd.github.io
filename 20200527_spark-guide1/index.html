<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Spark Guide, Part Ⅰ - JerrysBlog</title><meta name=Description content="Article description."><meta property="og:title" content="Spark Guide, Part Ⅰ"><meta property="og:description" content="Article description."><meta property="og:type" content="article"><meta property="og:url" content="https://jerrysmd.github.io/20200527_spark-guide1/"><meta property="og:image" content="https://jerrysmd.github.io/logo.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2020-05-27T17:20:52+08:00"><meta property="article:modified_time" content="2020-05-27T17:20:52+08:00"><meta property="og:site_name" content="JerrysBlog"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://jerrysmd.github.io/logo.png"><meta name=twitter:title content="Spark Guide, Part Ⅰ"><meta name=twitter:description content="Article description."><meta name=application-name content="JerrysBlog"><meta name=apple-mobile-web-app-title content="JerrysBlog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel="shortcut icon" type=image/x-icon href=/favicon.ico><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jerrysmd.github.io/20200527_spark-guide1/><link rel=prev href=https://jerrysmd.github.io/20200526_app-architecture-development/><link rel=next href=https://jerrysmd.github.io/20200615_hive-key-point/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Spark Guide, Part Ⅰ","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jerrysmd.github.io\/20200527_spark-guide1\/"},"image":[{"@type":"ImageObject","url":"https:\/\/jerrysmd.github.io\/images\/Apple-Devices-Preview.png","width":3200,"height":2048}],"genre":"posts","keywords":"Distribution, Spark","wordcount":1518,"url":"https:\/\/jerrysmd.github.io\/20200527_spark-guide1\/","datePublished":"2020-05-27T17:20:52+08:00","dateModified":"2020-05-27T17:20:52+08:00","publisher":{"@type":"Organization","name":"xxxx","logo":{"@type":"ImageObject","url":"https:\/\/jerrysmd.github.io\/images\/avatar.png","width":528,"height":560}},"author":{"@type":"Person","name":" "},"description":"Article description."}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=JerrysBlog><span class=header-title-pre><i class='far fa-edit fa-fw' aria-hidden=true></i></span>JERRYSBLOG</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/>Home </a><a class=menu-item href=/categories/technology/>Technology </a><a class=menu-item href=/categories/life/>Life </a><a class=menu-item href=/tags/>Tags </a><a class=menu-item href=/about/>About </a><a class=menu-item href=https://github.com/Jerrysmd title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=JerrysBlog><span class=header-title-pre><i class='far fa-edit fa-fw' aria-hidden=true></i></span>JERRYSBLOG</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/ title>Home</a><a class=menu-item href=/categories/technology/ title>Technology</a><a class=menu-item href=/categories/life/ title>Life</a><a class=menu-item href=/tags/ title>Tags</a><a class=menu-item href=/about/ title>About</a><a class=menu-item href=https://github.com/Jerrysmd title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-adjust fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Spark Guide, Part Ⅰ</h1><div class=post-meta><div class=post-meta-line><span class=post-category>Category in
<a href=/categories/technology/><i class="far fa-folder fa-fw" aria-hidden=true></i>Technology</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2020-05-27>2020-05-27</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1518 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;8 minutes&nbsp;<span id=/20200527_spark-guide1/ class=leancloud_visitors data-flag-title="Spark Guide, Part Ⅰ">
<i class="far fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;views
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#spark-introduction>Spark Introduction</a></li><li><a href=#spark-cluster>Spark Cluster</a></li><li><a href=#running-process>Running Process</a></li><li><a href=#rdd>RDD</a></li><li><a href=#transformation-operator>Transformation Operator</a></li><li><a href=#action-operator>Action Operator</a></li><li><a href=#data-type-in-rdd>Data Type in RDD</a></li><li><a href=#practice>Practice</a></li><li><a href=#rdd-feature>RDD Feature</a></li><li><a href=#spark-running-process>Spark Running Process</a></li></ul></nav></div></div><div class=content id=content><p>Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.</p><h2 id=spark-introduction>Spark Introduction</h2><p>1 Spark Component</p><ul><li>Spark提供了批处理（RDDs），结构化查询（DataFrame），流计算（SparkStreaming），机器学习（MLib），图计算（GraphX）等组件</li><li>这些组件均是依托于通用的计算引擎RDDs而构建出，所以spark-core的RDDs是整个Spark的基础</li></ul><p><figure><a class=lightgallery href=/posts/picture/sparkStructure.png title=sparkStructure data-thumbnail=/posts/picture/sparkStructure.png data-sub-html="<h2>sparkStructure</h2><p>sparkStructure</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/picture/sparkStructure.png data-srcset="/posts/picture/sparkStructure.png, /posts/picture/sparkStructure.png 1.5x, /posts/picture/sparkStructure.png 2x" data-sizes=auto alt=/posts/picture/sparkStructure.png></a><figcaption class=image-caption>sparkStructure</figcaption></figure></p><p>2 Spark & Hadoop</p><table><thead><tr><th></th><th>Hadoop</th><th>Spark</th></tr></thead><tbody><tr><td>类型</td><td>基础平台，包含计算，存储，调度</td><td>分布式计算工具（主要代替Hadoop的计算功能）</td></tr><tr><td>场景</td><td>大规模数据集上的批处理</td><td>迭代计算，交互式计算，流计算</td></tr><tr><td>延迟</td><td>大</td><td>小</td></tr><tr><td>易用性</td><td>API较为底层，算法适应性差</td><td>API较为顶层，方便使用</td></tr><tr><td>价格</td><td>性能要求低，便宜</td><td>对内存要求高，相对较贵</td></tr></tbody></table><h2 id=spark-cluster>Spark Cluster</h2><p>1 Cluster relation</p><p><figure><a class=lightgallery href=/posts/picture/clusterManager.png title=clusterManager data-thumbnail=/posts/picture/clusterManager.png data-sub-html="<h2>clusterManager</h2><p>clusterManager</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/picture/clusterManager.png data-srcset="/posts/picture/clusterManager.png, /posts/picture/clusterManager.png 1.5x, /posts/picture/clusterManager.png 2x" data-sizes=auto alt=/posts/picture/clusterManager.png></a><figcaption class=image-caption>clusterManager</figcaption></figure></p><ul><li>Driver：该进程调用Spark程序的main方法，并且启动SparkContext</li><li>Cluster Manager：该进程负责和外部集群工具打交道，申请或释放集群资源</li><li>Worker：该进程是一个守护进程，负责启动和管理Executor</li><li>Executor：该进程是一个JVM虚拟机，负责运行Spark Task</li></ul><p>运行一个Spark程序大致经历如下几个步骤：</p><ol><li>启动Driver，创建SparkContext</li><li>Client提交程序给Drive，Drive向Cluster Manager申请集群资源</li><li>资源申请完毕，在Worker中启动Executor</li><li>Driver将程序转化为Tasks，分发给Executor执行</li></ol><p>2 Build Cluster</p><ol><li>Download Spark</li><li>Upload</li><li>Config</li><li>HistoryServer</li><li>Distribute: scp -r spark node02: $PWD</li><li>Start</li></ol><p>3 High Availability</p><p>对于 Spark Standalone 集群来说，当Worker调度出现问题时，会自动的弹性容错，将出错的Task调度到其他Worker执行。</p><p>但对于Master来说，是会出现单点失败的，为了避免可能出现的单点失败问题，Spark提供了两种方式满足高可用</p><ol><li>使用Zookeeper实现Master的主备切换(Zookeeper是一个分布式强一致性的协调服务，Zookeeper最基本的一个保证是：如果多个节点同时创建一个ZNode)只有一个能够成功创建，这个做法的本质使用的是Zookeeper的ZAB协议，能够在分布式环境下达成一致。</li><li>使用文件系统做主备切换</li></ol><h2 id=running-process>Running Process</h2><p>1 Spark-Shell Run</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=o>(</span><span class=s>&#34;/data/wordcount.txt&#34;</span><span class=o>)</span> <span class=c1>//Hadoop默认读取hdfs路径：hdfs:///data/wordcount.txt
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>rdd2</span> <span class=k>=</span> <span class=n>rddflatMap</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>rdd3</span> <span class=k>=</span> <span class=n>rdd2</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>,</span><span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>rdd4</span> <span class=k>=</span> <span class=n>rdd3</span><span class=o>.</span><span class=n>reduceByKey</span><span class=o>((</span><span class=n>curr</span><span class=o>,</span><span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>rdd4</span><span class=o>.</span><span class=n>collect</span><span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><p><figure><a class=lightgallery href=/posts/picture/sparkDemoRunProcess.png title=sparkDemoRunProcess data-thumbnail=/posts/picture/sparkDemoRunProcess.png data-sub-html="<h2>sparkRunProcess</h2><p>sparkDemoRunProcess</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/picture/sparkDemoRunProcess.png data-srcset="/posts/picture/sparkDemoRunProcess.png, /posts/picture/sparkDemoRunProcess.png 1.5x, /posts/picture/sparkDemoRunProcess.png 2x" data-sizes=auto alt=/posts/picture/sparkDemoRunProcess.png></a><figcaption class=image-caption>sparkRunProcess</figcaption></figure></p><p>2 Local IDEA Run</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>def</span> <span class=n>main</span><span class=o>(</span><span class=n>args</span><span class=k>:</span><span class=kt>Arrary</span><span class=o>[</span><span class=kt>String</span><span class=o>])</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// 创建SparkContext
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>conf</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkConf</span><span class=o>().</span><span class=n>setMaster</span><span class=o>(</span><span class=s>&#34;local[6]&#34;</span><span class=o>).</span><span class=n>setAppName</span><span class=o>(</span><span class=s>&#34;word_count&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>sc</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkContext</span><span class=o>(</span><span class=n>conf</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//2. 加载文件
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//	准备文件
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//	2.读取文件
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>testFile</span><span class=o>(</span><span class=n>path</span> <span class=k>=</span> <span class=s>&#34;dataset/wordcount.txt&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//3. 处理
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//	拆分为多个单词
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd2</span> <span class=k>=</span> <span class=n>rddflatMap</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=c1>//	2.把每个单词指定一个词频
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd3</span> <span class=k>=</span> <span class=n>rdd2</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>,</span><span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=c1>//	3.聚合
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd4</span> <span class=k>=</span> <span class=n>rdd3</span><span class=o>.</span><span class=n>reduceByKey</span><span class=o>((</span><span class=n>curr</span><span class=o>,</span><span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//4.得到结果
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>result</span> <span class=k>=</span> <span class=n>rdd4</span><span class=o>.</span><span class=n>collect</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span><span class=o>.</span><span class=n>foreach</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>println</span><span class=o>(</span><span class=n>item</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><p>3 Submit Run</p><ol><li>修改代码<ol><li>去掉master设置，并修改文件路径</li></ol></li><li>Maven打包上传</li><li>在集群中运行</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>bin/spark -submit --class cn.demo.spark.rdd.WordCount --master spark://node01:7077 ~/original -spark-0.0.jar
</span></span></code></pre></td></tr></table></div></div><h2 id=rdd>RDD</h2><p>1 Cause of creation</p><p><strong>在RDD出现之前，MapReduce是比较主流的</strong></p><p>但多个MapReduce任务之间没有基于内存的数据共享方式，只能通过磁盘来进行共享，这种方式明显比较低效。</p><p><strong>RDD如何解决迭代计算非常低效的问题</strong></p><p>在Spark中，最终Job3从逻辑上的计算过程是：Job3 = (Job1.map).filter，整个过程是共享内存的，而不需要中间结果存放在可靠的分布式文件系统中。</p><p>2 Resilient Distributed Datasets</p><p>分布式</p><blockquote><p>RDD支持分区，可以运行在集群中</p></blockquote><p>弹性</p><blockquote><ul><li>RDD支持高效的容错</li><li>RDD中的数据即可以缓存在内存中，也可以缓存在磁盘中，也可以缓存在外部存储中</li></ul></blockquote><p>数据集</p><blockquote><ul><li>RDD可以不保存具体数据，只保留创建自己的必备信息，例如依赖和计算函数</li><li>RDD也可以缓存起来，相当于存储具体数据</li></ul></blockquote><p>3 Feature</p><ol><li>RDD是数据集</li></ol><blockquote><p>RDD不仅是数据集，也是编程模型</p><p>RDD的算子大致分为两类：</p><ol><li>Transformation转化操作，例如：map、flatMap、filter等</li><li>Action动作操作，例如：reduce、collect、show等</li></ol></blockquote><ol><li>RDD是编程模型</li><li>RDD相互之间有依赖关系</li><li>RDD是可以分区的</li><li><strong>RDD是只读的</strong></li></ol><blockquote><p>RDD需要容错，可以惰性求值，可以移动计算，所以很难支持修改，显著降低问题的复杂度。</p></blockquote><p><figure><a class=lightgallery href=/posts/picture/sparkRdd.png title=sparkRdd data-thumbnail=/posts/picture/sparkRdd.png data-sub-html="<h2>sparkRdd</h2><p>sparkRdd</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/picture/sparkRdd.png data-srcset="/posts/picture/sparkRdd.png, /posts/picture/sparkRdd.png 1.5x, /posts/picture/sparkRdd.png 2x" data-sizes=auto alt=/posts/picture/sparkRdd.png></a><figcaption class=image-caption>sparkRdd</figcaption></figure></p><p>4 sparkContext</p><p>SparkContext是spark功能的主要入口。其代表与spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。每个JVM里只能存在一个处于激活状态的SparkContext，在创建新的SparkContext之前必须调用stop()来关闭之前的SparkContext。</p><p>每一个Spark应用都是一个SparkContext实例，可以理解为一个SparkContext就是一个spark application的生命周期，一旦SparkContext创建之后，就可以用这个SparkContext来创建RDD、累加器、广播变量，并且可以通过SparkContext访问Spark的服务，运行任务。spark context设置内部服务，并建立与spark执行环境的连接。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>sparkContext</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>//  Spark Context 编写
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// 		创建SparkConf
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>conf</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkConf</span><span class=o>().</span><span class=n>setMaster</span><span class=o>(</span><span class=s>&#34;local[6]&#34;</span><span class=o>).</span><span class=n>setAppName</span><span class=o>(</span><span class=s>&#34;spark_context&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=c1>//		2.创建SparkContext
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>sc</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkContext</span><span class=o>(</span><span class=n>conf</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//SparkContext身为大入口API，应该能够创建RDD，并且设置参数，设置Jar包
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//sc...
</span></span></span><span class=line><span class=cl><span class=c1></span>    
</span></span><span class=line><span class=cl>    <span class=c1>//2. 关闭SparkContext，释放集群资源
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><p>5 Creation Way</p><p>三种RDD的创建方式</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl> <span class=n>通过本地集合创建RDD</span>
</span></span><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>rddCreationLocal</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>conf</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkConf</span><span class=o>().</span><span class=n>setMaster</span><span class=o>(</span><span class=s>&#34;local[6]&#34;</span><span class=o>).</span><span class=n>setAppName</span><span class=o>(</span><span class=s>&#34;spark_context&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>sc</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkContext</span><span class=o>(</span><span class=n>conf</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=s>&#34;Hello1&#34;</span><span class=o>,</span> <span class=s>&#34;Hello2&#34;</span><span class=o>,</span> <span class=s>&#34;Hello3&#34;</span><span class=o>),</span> <span class=mi>2</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>rdd2</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>makeRDD</span><span class=o>(</span><span class=n>seq</span><span class=o>,</span> <span class=mi>2</span><span class=o>)</span> <span class=c1>// parallelize和makeRDD区别：parallelize可以不指定分区数
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=o>}</span>
</span></span><span class=line><span class=cl><span class=mf>2.</span> <span class=n>通过外部数据创建RDD</span>
</span></span><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>rddCreationFiles</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=o>(</span><span class=s>&#34;/.../...&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=c1>//testFile: 传入* hdfs://   file://   /.../...(这种方式分为在集群还是本地执行，在集群中读的是hdfs，本地读本地文件)
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//2.是否支持分区：支持，在hdfs中由hdfs文件的block决定
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//3.支持什么平台：支持aws和阿里云...
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=o>}</span>
</span></span><span class=line><span class=cl><span class=mf>3.</span> <span class=n>通过RDD衍生新的RDD</span>
</span></span><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>rddCreationFromRDD</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=mi>1</span><span class=o>,</span><span class=mi>2</span><span class=o>,</span><span class=mi>3</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=c1>//通过在rdd上执行算子操作，会生成新的rdd
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//非原地计算：str.substr 返回新的字符串，非原地计算。字符串不可变，RDD也不可变
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd2</span><span class=k>:</span> <span class=kt>RDD</span><span class=o>[</span><span class=kt>Int</span><span class=o>]</span> <span class=k>=</span> <span class=n>rddmap</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=transformation-operator>Transformation Operator</h2><p>map()</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>mapTest</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>//创建RDD
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=mi>1</span><span class=o>,</span><span class=mi>2</span><span class=o>,</span><span class=mi>3</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=c1>//2.执行map操作
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd2</span> <span class=k>=</span> <span class=n>rdd1</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span> <span class=o>*</span> <span class=mi>10</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=c1>//3.得到结果
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>result</span> <span class=k>=</span> <span class=n>rdd2</span><span class=o>.</span><span class=n>collect</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span><span class=o>.</span><span class=n>foreach</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>println</span><span class=o>(</span><span class=n>item</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><p>flatmap()</p><ol><li>把rdd中的数据转化成数组或集合形式</li><li>把集合展开</li><li>生成了多条数据</li></ol><p>flatmap是一对多</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>flatMapTest</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=s>&#34;Hello a&#34;</span><span class=o>,</span><span class=s>&#34;Hello b&#34;</span><span class=o>,</span><span class=s>&#34;Hello c&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>rdd2</span> <span class=k>=</span> <span class=n>rddf1</span><span class=o>.</span><span class=n>latMap</span><span class=o>(</span> <span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>result</span> <span class=k>=</span> <span class=n>rdd2</span><span class=o>.</span><span class=n>collect</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span><span class=o>.</span><span class=n>foreach</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>println</span><span class=o>(</span><span class=n>item</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><p>reducebykey()</p><p>reduceByKey第一步先按照key分组，然后对每一组进行聚合，得到结果。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>reduceBykeyTest</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>//创建RDD
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=s>&#34;Hello a&#34;</span><span class=o>,</span><span class=s>&#34;Hello b&#34;</span><span class=o>,</span><span class=s>&#34;Hello c&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=c1>//2.处理数据
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>rdd2</span> <span class=k>=</span> <span class=n>rdd1</span><span class=o>.</span><span class=n>flatMap</span><span class=o>(</span> <span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>map</span><span class=o>(</span> <span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>,</span><span class=mi>1</span><span class=o>)</span> <span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>reduceByKey</span><span class=o>(</span> <span class=o>(</span><span class=n>curr</span><span class=o>,</span> <span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span><span class=c1>//curr是当前的总值，agg是单个item的值
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//3.得到结果
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>result</span> <span class=k>=</span> <span class=n>rdd2</span><span class=o>.</span><span class=n>collect</span><span class=o>()</span>
</span></span><span class=line><span class=cl>    <span class=n>result</span><span class=o>.</span><span class=n>foreach</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>println</span><span class=o>(</span><span class=n>item</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=c1>//4.关闭sc
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span> 
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><p>Q&A</p><ol><li>数据量过大，如何处理？</li></ol><blockquote><p>集群中处理，利用集群多台计算机来并行处理</p></blockquote><ol start=2><li>如何放在集群中运行?</li></ol><blockquote><p><figure><a class=lightgallery href=/posts/picture/sparkPutFile2Cluster.png title=sparkPutFile2Cluster data-thumbnail=/posts/picture/sparkPutFile2Cluster.png data-sub-html="<h2>sparkPutFile2Cluster</h2><p>sparkPutFile2Cluster</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/picture/sparkPutFile2Cluster.png data-srcset="/posts/picture/sparkPutFile2Cluster.png, /posts/picture/sparkPutFile2Cluster.png 1.5x, /posts/picture/sparkPutFile2Cluster.png 2x" data-sizes=auto alt=/posts/picture/sparkPutFile2Cluster.png></a><figcaption class=image-caption>sparkPutFile2Cluster</figcaption></figure></p><p>并行计算就是同时使用多个计算资源解决一个问题，有四个要点</p><ul><li>解决的问题可以分解为多个可以并发计算的部分</li><li>每个部分可以在不同处理器上被同时执行</li><li><strong>需要一个共享内存的机制</strong></li><li>需要一个总体上的协作机制来进行调度</li></ul></blockquote><ol start=3><li>如果放在集群中，如何对整个计算任务进行分解？</li></ol><blockquote><p><figure><a class=lightgallery href=/posts/picture/sparkFile2Cluster2.png title=sparkFile2Cluster2 data-thumbnail=/posts/picture/sparkFile2Cluster2.png data-sub-html="<h2>sparkFile2Cluster2</h2><p>sparkFile2Cluster2</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/posts/picture/sparkFile2Cluster2.png data-srcset="/posts/picture/sparkFile2Cluster2.png, /posts/picture/sparkFile2Cluster2.png 1.5x, /posts/picture/sparkFile2Cluster2.png 2x" data-sizes=auto alt=/posts/picture/sparkFile2Cluster2.png></a><figcaption class=image-caption>sparkFile2Cluster2</figcaption></figure></p><p><strong>概述</strong></p><ul><li>对于HDFS中的文件，是分为不同的Block</li><li>在进行计算的时候，就可以按照Block来划分，每一个Block对应一个不同的计算单元</li></ul><p><strong>扩展</strong></p><ul><li>RDD并没有真实的存放数据，数据是从HDFS中读取的，在计算的过程中读取即可</li><li>RDD至少是需要可以<strong>分片</strong>的，因为HDFS中的文件就是分片的，RDD可以分片也意味着可以并行计算</li></ul></blockquote><ol start=4><li>移动数据不如移动计算是一个基础的优化，如何做到？</li></ol><blockquote><p>每一个计算单元需要记录其存储单元的位置，尽量调度过去</p></blockquote><ol start=5><li>集群中运行，需要多节点配合，出错的概率也更高，出错了怎么办？</li></ol><blockquote><p><strong>RDD1->RDD2->RDD3这个过程中，RDD2出错了，有两种解决办法</strong></p><ol><li>缓存RDD2的数据，直接恢复RDD2，类似HDFS的备份机制</li><li>记录RDD2的依赖关系，通过其父级的RDD来恢复RDD2，这种方式会少很多数据的交互和保存</li></ol><p><strong>如何通过父级RDD恢复？</strong></p><ol><li>记录RDD2的父亲是RDD1</li><li>记录RDD2的计算函数，例如RDD2 = RDD1.map(&mldr;)等计算函数</li><li>通过父级RDD和计算函数来恢复RDD2</li></ol></blockquote><ol start=6><li>任务特别复杂，流程特别长，有很多RDD之间有依赖关系，如何优化？</li></ol><blockquote><p>上面提到了可以使用依赖关系来进行<strong>容错</strong>，但是如果依赖关系特别长的时候，这种方式其实也比较低效，这个时候就应该使用另外一种方式，也就是记录数据集的状态</p><p><strong>在Spark中有两个手段可以做到</strong></p><ol><li>缓存</li><li>Checkpoint</li></ol></blockquote><p>map() & mapPartitions()</p><blockquote><p>mapPartitions 和 map 算子是一样的，只不过map是针对每一条数据进行转换，mapPartitions针对一整个分区的数据进行转换</p><p>所以</p><ol><li>map 的 func 参数是单条数据，mapPartitions 的 func 参数是一个集合(一个分区整个所有的数据)</li><li>map 的 func 返回值也是单条数据，mapPartition 的 func 返回值是一个集合</li><li>mapPartitionWithIndex 和 mapPartition 的区别是 func 中多分区数量参数</li></ol></blockquote><p>filter()</p><p>保留满足条件的元素</p><p>sample()</p><p>filter按照规律过滤，sample则是随机采样</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>def</span> <span class=n>sample</span><span class=o>(</span>
</span></span><span class=line><span class=cl>    <span class=n>withReplacement</span><span class=k>:</span> <span class=kt>Boolean</span><span class=o>,</span>	<span class=c1>//是否重复取样
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>fraction</span><span class=k>:</span> <span class=kt>Double</span><span class=o>,</span>			<span class=c1>//取样比例
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>seed</span><span class=k>:</span> <span class=kt>Long</span> <span class=o>=</span> <span class=nc>Utils</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>nextLong</span><span class=o>)</span><span class=k>:</span> <span class=kt>RDD</span><span class=o>[</span><span class=kt>T</span><span class=o>]</span> <span class=k>=</span> <span class=o>{...}</span>
</span></span></code></pre></td></tr></table></div></div><p>mapValues()</p><p>mapValue也是map，map作用于全部数据，mapValue作用于value</p><p>collection operation</p><p>交集：rdd1.intersection(rdd2)</p><p>并集：rdd1.union(rdd2)</p><p>差集：rdd1.subract(rdd2)</p><p>groupByKey()</p><p>聚合操作：</p><p>reduceByKey ->按照key分组，然后把每一组数据reduce。reduceByKey在map端combiner能减少IO，一个分区放多个数据。</p><p>groupByKey 运算结果的格式：（k，（value1，value2）），没有减少IO</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>((</span><span class=s>&#34;a&#34;</span><span class=o>,</span><span class=mi>1</span><span class=o>),(</span><span class=s>&#34;a&#34;</span><span class=o>,</span><span class=mi>1</span><span class=o>),(</span><span class=s>&#34;b&#34;</span><span class=o>,</span><span class=mi>1</span><span class=o>)))</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>groupByKey</span><span class=o>()</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>collect</span><span class=o>()</span>
</span></span><span class=line><span class=cl>  <span class=o>.</span><span class=n>foreach</span><span class=o>(</span><span class=n>println</span><span class=o>(</span><span class=k>_</span><span class=o>))</span>
</span></span></code></pre></td></tr></table></div></div><p>combineByKey()</p><p>接收三个参数：</p><p>转化数据的函数（初始函数，作用于第一条数据，用于开启整个计算）</p><p>在分区上进行聚合</p><p>把所有的分区的聚合结果聚合为最终结果</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>result</span> <span class=k>=</span> <span class=n>rdd</span><span class=o>.</span><span class=n>combineBykey</span><span class=o>(</span>
</span></span><span class=line><span class=cl>	<span class=n>createCombiner</span> <span class=k>=</span> <span class=n>curr</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>curr</span><span class=o>,</span><span class=mi>1</span><span class=o>),</span> 
</span></span><span class=line><span class=cl>    <span class=n>mergeValue</span> <span class=k>=</span> <span class=o>(</span><span class=n>curr</span><span class=k>:</span> <span class=o>(</span><span class=kt>Double</span><span class=o>,</span> <span class=kt>Int</span><span class=o>),</span> <span class=n>nextValue</span><span class=k>:</span> <span class=kt>Double</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>curr</span><span class=o>.</span><span class=n>_1</span> <span class=o>+</span> <span class=n>nextValue</span><span class=o>,</span> <span class=n>curr</span><span class=o>.</span><span class=n>_2</span> <span class=o>+</span> <span class=mi>1</span><span class=o>)),</span>
</span></span><span class=line><span class=cl>    <span class=n>mergeCombiners</span> <span class=k>=</span> <span class=o>(</span><span class=n>curr</span><span class=k>:</span> <span class=o>(</span><span class=kt>Double</span><span class=o>,</span><span class=kt>Int</span><span class=o>),</span> <span class=n>agg</span><span class=k>:</span> <span class=o>(</span><span class=kt>Double</span><span class=o>,</span> <span class=kt>Int</span><span class=o>))</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>curr</span><span class=o>.</span><span class=n>_1</span> <span class=o>+</span> <span class=n>agg</span><span class=o>.</span><span class=n>_1</span><span class=o>,</span> <span class=n>curr</span><span class=o>.</span><span class=n>_2</span> <span class=o>+</span> <span class=n>agg</span><span class=o>.</span><span class=n>_2</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>result</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=o>,</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>.</span><span class=n>_1</span> <span class=o>/</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>.</span><span class=n>_2</span><span class=o>))</span>
</span></span></code></pre></td></tr></table></div></div><p>foldByKey()</p><p>功能等同于reduceByKey()，增加了初始值。reduceByKey底层是combineByKey()，foldByKey()底层是aggregateByKey()。</p><p>aggregateByKey()</p><p>join()</p><p>按照相同的Key进行连接</p><p>sortBy()</p><p>排序：sortBy()，sortByKey()</p><p>coalesce()</p><p>一般涉及到分区操作的算子常见的有两个，repartition和coalesce，都可以调大或者调小分区数量</p><p>summary</p><p>所有的转化操作的算子都是惰性的，在执行时候不会调度运行求得结果，而只是生成了对应的RDD</p><p>只有在Action操作的时候，才会真的运行</p><h2 id=action-operator>Action Operator</h2><p>reduce((T, T) - U)</p><p>对整个结果集规约，最终生成一条数据，是整个数据集的总汇</p><blockquote><p><strong>reduceByKey和reduce有什么区别：</strong></p><ol><li><p>reduce是action算子，reduceByKey是一个转换算子</p></li><li><p>RDD里有一万条数据，大部分key是相同的，有10个不同的key生成10条数据</p><p>reduce生成1条数据</p></li><li><p>reduceByKey是按Key分组，然后把每组聚合</p><p>reduce是针对一整个数据集进行聚合</p></li><li><p>reduceByKey是对KV数据进行计算</p><p>reduce可针对所有类型数据</p></li></ol><p><strong>reduce算子是一个shuffle操作吗？</strong></p><ol><li><p>shuffle操作分为mapper和reducer，mapper将数据放入paritioner的函数计算，求得往哪个reducer里放</p></li><li><p>reduce操作没有mapper和reducer，因为reduce算子会作用于RDD中的每个分区，然后分区求得局部结果，最终汇总到Driver中求得最终结果</p></li><li><p>RDD有五大属性，partitioner在shuffle过程中使用</p><p>paritioner只有kv型的RDD才有</p></li></ol></blockquote><p>collect()</p><p>以数组的形式返回数据集中所有元素</p><p>countByKey()</p><p>count和countByKey</p><ul><li>countByKey结果：Map(Key -> Key的count)</li></ul><blockquote><p>调用Action会生成一个job，job会运行获取结果，所以在两个job中有大量的log</p><p>数据倾斜：解决数据倾斜的问题，需要先通过countByKey查看Key对应的数量</p></blockquote><p>first()</p><p>返回第一个元素</p><p>take(N)</p><p>返回前N个元素</p><p>takeSample(withReplacement, num)</p><p>类似于sample，区别这是action，直接返回结果</p><p>withReplacement：取数据有无放回</p><p>first()</p><p>first()速度相比其他方法最快</p><h2 id=data-type-in-rdd>Data Type in RDD</h2><p>RDD中存放的数据类型</p><ul><li>基本类型，String，对象</li><li>KV类型</li><li>数字类型</li></ul><h2 id=practice>Practice</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=nd>@Test</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>process</span><span class=o>()</span><span class=k>:</span> <span class=kt>Unit</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>//1. 创建sc对象
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>conf</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkConf</span><span class=o>().</span><span class=n>setMaster</span><span class=o>(</span><span class=s>&#34;local[6]&#34;</span><span class=o>).</span><span class=n>setAppName</span><span class=o>(</span><span class=s>&#34;practice&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>sc</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkContext</span><span class=o>(</span><span class=n>conf</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//2. 读取文件
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>//1,2010,1,1,0,4,NA,NA,NA,NA,-21,43,1021,-11,NW,1.79,0,0
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>source</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=o>(</span><span class=s>&#34;dataset/parctive.csv&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//3. 处理数据
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>resultRDD</span> <span class=k>=</span> <span class=n>source</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>((</span><span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34;,&#34;</span><span class=o>)(</span><span class=mi>1</span><span class=o>),</span> <span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34;,&#34;</span><span class=o>)(</span><span class=mi>2</span><span class=o>)),</span><span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34;,&#34;</span><span class=o>)(</span><span class=mi>6</span><span class=o>)))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=nc>StringUtils</span><span class=o>.</span><span class=n>isNotEmpty</span><span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>)</span> <span class=o>&amp;&amp;</span> <span class=o>!</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>.</span><span class=n>equalsIgnoreCase</span><span class=o>(</span><span class=s>&#34;NA&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=o>,</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>.</span><span class=n>toInt</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>reduceByKey</span><span class=o>((</span><span class=n>curr</span><span class=o>,</span><span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>.</span><span class=n>sortBy</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>ascending</span> <span class=k>=</span> <span class=kc>false</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//4. 获取结果
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>resultRDD</span><span class=o>.</span><span class=n>take</span><span class=o>(</span><span class=mi>10</span><span class=o>).</span><span class=n>foreach</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>println</span><span class=o>(</span><span class=n>item</span><span class=o>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1>//5. 关闭sc
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=rdd-feature>RDD Feature</h2><p>RDD&rsquo;s shuffle and partition</p><ol><li>RDD经常需要通过读取外部系统的数据来创建，外部存储系统往往是支持分片的。RDD需要支持分区，来和外部系统的分片一一对应</li><li>RDD的分区是一个并行计算的实现手段</li></ol><p>partition function</p><p>RDD使用分区来分布式处理，当使用RDD读取数据时，会尽量在屋里上靠近数据源。比如读HDFS或Cassandra时，会尽量的保持RDD的分区和数据源的分区数，分区模式一一对应</p><p>shuffle</p><p>从mapper端到reducer端</p><p>Spark支持宽依赖的转换，例如groupByKey和reduceByKey。在这些依赖项中，计算单个分区中的记录所需的数据可以来自于父数据集的许多分区中。要执行这些转换，具有相同key的所有元组必须最终位于同一分区中，由同一任务处理。为了满足这一要求，Spark产生一个shuffle，它在集群内部传输数据，并产生一个带有一组新分区的新stage。</p><ul><li>Hash base shuffle</li></ul><p>Reduce 找到每个Mapper中对应自己哈希桶拉取数据</p><p>缺点：过多占用资源占用</p><ul><li>Sort base shuffle</li></ul><p>先按照partition ID 排序， 后按照Key的HashCode排序</p><p>partition and shuffle relation</p><p>分区主要用来实现并行计算，和shuffle没什么关系，但数据处理时，例如reduceByKey，groupByKey等聚合操作，需要把Key相同的Value拉取到一起进行计算，这个时候因为这些Key的相同的Value可能会在不同的分区，所以理解分区才能理解shuffle的根本原理</p><p>shuffle feature</p><ul><li>只有KV型的RDD才会有Shuffle操作</li><li>早期版本spark的shuffle算法是 hash base shuffle，后来改为 sort base shuffle，更适合大吞吐量的场景</li></ul><p>check partition</p><p>指定分区数</p><ol><li>通过本地集合创建的时候指定分区数</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>conf</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkConf</span><span class=o>().</span><span class=n>setMaster</span><span class=o>(</span><span class=s>&#34;local[6]&#34;</span><span class=o>).</span><span class=n>setAppName</span><span class=o>(</span><span class=s>&#34;practice&#34;</span><span class=o>)</span><span class=c1>//创建App并开启6个分区
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>sc</span> <span class=k>=</span> <span class=k>new</span> <span class=nc>SparkContext</span><span class=o>(</span><span class=n>conf</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><ol start=2><li>通过读取文件创建的时候指定分区数</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>rdd1</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=mi>1</span><span class=o>,</span> <span class=mi>2</span><span class=o>,</span> <span class=mi>3</span><span class=o>,</span> <span class=mi>4</span><span class=o>,</span> <span class=mi>5</span><span class=o>,</span> <span class=mi>6</span><span class=o>,</span> <span class=mi>7</span><span class=o>),</span> <span class=mi>3</span><span class=o>)</span>	<span class=c1>//指定分区数3
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>rdd2</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>testFile</span><span class=o>(</span><span class=s>&#34;hdfs://node01:8020/data/test.txt&#34;</span><span class=o>,</span> <span class=mi>6</span><span class=o>)</span>	<span class=c1>//这里指定的是最小分区数6
</span></span></span></code></pre></td></tr></table></div></div><p>查看方法</p><ol><li>通过WebUI查看。端口：4040</li><li>通过partitions来查看。rdd1.partitions.size</li></ol><p>重分区</p><p>coalesce(num, true)</p><p>repartitions(num)</p><p>RDD Cache</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=c1>//1. 取出IP
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>countRDD</span> <span class=k>=</span> <span class=n>source</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>)(</span><span class=mi>0</span><span class=o>),</span> <span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=c1>//2. 数据清洗
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>cleanRDD</span> <span class=k>=</span> <span class=n>countRDD</span><span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=nc>StingUtils</span><span class=o>.</span><span class=n>isNotEmpty</span><span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=c1>//3. 统计ip的出现次数
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>aggRDD</span> <span class=k>=</span> <span class=n>cleanRDD</span><span class=o>.</span><span class=n>reduceBykey</span><span class=o>((</span><span class=n>curr</span><span class=o>,</span><span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1>//4. 统计出现最少的ip
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>leastIP</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>sortBy</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>ascending</span> <span class=k>=</span> <span class=kc>true</span><span class=o>).</span><span class=n>first</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=c1>//5. 统计出现最多的ip
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>mostIP</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>sortBy</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>ascending</span> <span class=k>=</span> <span class=kc>false</span><span class=o>).</span><span class=n>first</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>println</span><span class=o>(</span><span class=n>leastIP</span><span class=o>,</span> <span class=n>mostIP</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><p>第一次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey)</p><p>第二次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey)</p><p>转换算子的作用：生成RDD，以及RDD之间的依赖关系</p><p>Action算子的作用：生成job，执行job</p><p>全局执行了四个shuffle</p><p><strong>使用缓存的意义：</strong></p><ol><li>减少shuffle操作</li><li>容错，减少开销：rdd1->rdd2->rdd3，若rdd3算错会再次计算rdd1和rdd2整个流程。</li></ol><p><strong>缓存API:</strong></p><p>cache()或persist(null/level)</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=c1>//1. 处理
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>countRDD</span> <span class=k>=</span> <span class=n>source</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>)(</span><span class=mi>0</span><span class=o>),</span> <span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>cleanRDD</span> <span class=k>=</span> <span class=n>countRDD</span><span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=nc>StingUtils</span><span class=o>.</span><span class=n>isNotEmpty</span><span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>aggRDD</span> <span class=k>=</span> <span class=n>cleanRDD</span><span class=o>.</span><span class=n>reduceBykey</span><span class=o>((</span><span class=n>curr</span><span class=o>,</span><span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//2. cache
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>aggRDD</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>cache</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//3. 两个RDD的action操作
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>leastIP</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>sortBy</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>ascending</span> <span class=k>=</span> <span class=kc>true</span><span class=o>).</span><span class=n>first</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>mostIP</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>sortBy</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>ascending</span> <span class=k>=</span> <span class=kc>false</span><span class=o>).</span><span class=n>first</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>println</span><span class=o>(</span><span class=n>leastIP</span><span class=o>,</span> <span class=n>mostIP</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=c1>//1. 处理
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>countRDD</span> <span class=k>=</span> <span class=n>source</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>)(</span><span class=mi>0</span><span class=o>),</span> <span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>cleanRDD</span> <span class=k>=</span> <span class=n>countRDD</span><span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=nc>StingUtils</span><span class=o>.</span><span class=n>isNotEmpty</span><span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>aggRDD</span> <span class=k>=</span> <span class=n>cleanRDD</span><span class=o>.</span><span class=n>reduceBykey</span><span class=o>((</span><span class=n>curr</span><span class=o>,</span><span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//2. cache
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>aggRDD</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=n>storageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//3. 两个RDD的action操作
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>leastIP</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>sortBy</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>ascending</span> <span class=k>=</span> <span class=kc>true</span><span class=o>).</span><span class=n>first</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>mostIP</span> <span class=k>=</span> <span class=n>aggRDD</span><span class=o>.</span><span class=n>sortBy</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>ascending</span> <span class=k>=</span> <span class=kc>false</span><span class=o>).</span><span class=n>first</span><span class=o>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>println</span><span class=o>(</span><span class=n>leastIP</span><span class=o>,</span> <span class=n>mostIP</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>缓存级别：</strong></p><p>MEMORY_ONLY: CPU效率最高</p><p>MEMORY_ONLY_SER: 更加节省空间</p><p>Checkpoint</p><p>斩断RDD的依赖链，并且将数据存储在可靠的存储引擎中，例如HDFS</p><p>HDFS的NameNode中主要职责就是维护两个文件，一个是edits，另一个是fsimage。</p><p>edits中主要存放Editlog，FsImage保存了当前系统中所有目录和文件的信息，这个FsImage其实就是一个Checkpoint。</p><ol><li>每一次修改文件的时候，都会在Edits中添加一条记录。</li><li>在一定条件满足的情况下，把edits删掉添加一个新的FSimage，包含了系统当前最新的状态。好处：增加速度，提高稳定性</li></ol><blockquote><p><strong>Checkpoint和Cache的区别：</strong></p><p>Cache可以吧RDD计算出来放到内存中，但RDD的依赖链(相当于NameNode中的Edits日志)是不能丢的，若出现错误，只能重计算出来。</p><p>Checkpoint把结果存放在HDFS这类存储中，就变成了可靠的数据，如果出错了，则通过复制HDFS中的文件来实现容错。</p></blockquote><p><strong>如何使用：</strong></p><p>两步：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>conf</span> <span class=k>=</span> <span class=k>new</span><span class=o>.</span><span class=nc>SparkConf</span><span class=o>().</span><span class=n>setMaster</span><span class=o>(</span><span class=s>&#34;local[6]&#34;</span><span class=o>).</span><span class=n>setAppName</span><span class=o>(</span><span class=s>&#34;debug_string&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1>//1. setCheckPointDir：设置保存目录，也可以设置为HDFS上的目录
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>sc</span><span class=o>.</span><span class=n>setCheckpointDir</span><span class=o>(</span><span class=s>&#34;checkpoint&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>interimRDD</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>textFile</span><span class=o>(</span><span class=s>&#34;dataset/test.txt&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl>				<span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>)(</span><span class=mi>0</span><span class=o>),</span> <span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl>				<span class=o>.</span><span class=n>filter</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=nc>StringUtils</span><span class=o>.</span><span class=n>isNotBlank</span><span class=o>(</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=o>))</span>
</span></span><span class=line><span class=cl>				<span class=o>.</span><span class=n>reduceByKey</span><span class=o>((</span><span class=n>curr</span><span class=o>,</span> <span class=n>agg</span><span class=o>)</span> <span class=k>=&gt;</span> <span class=n>curr</span> <span class=o>+</span> <span class=n>agg</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>//2. setCheckPoint：是一个action操作，也就是说如果调用checkpoint，则会重新计算一下RDD，然后把结果存在HDFS或者本地目录中
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>interimRDD</span><span class=o>.</span><span class=n>checkpoint</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=n>interimRDD</span><span class=o>.</span><span class=n>collect</span><span class=o>().</span><span class=n>foreach</span><span class=o>(</span><span class=n>println</span><span class=o>(</span><span class=k>_</span><span class=o>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=spark-running-process>Spark Running Process</h2><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=c1>//1. 创建sc对象
</span></span></span><span class=line><span class=cl><span class=c1>//2. 创建数据集
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>textRDD</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=s>&#34;hadoop spark&#34;</span><span class=o>,</span> <span class=s>&#34;hadoop flume&#34;</span><span class=o>,</span> <span class=s>&#34;spark soo&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=c1>//3. 数据处理
</span></span></span><span class=line><span class=cl><span class=c1>//	1.拆词2.赋予初始词频3.聚合4.将结果转为字符串
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=k>val</span> <span class=n>splitRDD</span> <span class=k>=</span> <span class=n>textRDD</span><span class=o>.</span><span class=n>flatMap</span><span class=o>(</span><span class=k>_</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>tupleRDD</span> <span class=k>=</span> <span class=n>splitRDD</span><span class=o>.</span><span class=n>map</span><span class=o>((</span><span class=k>_</span><span class=o>,</span> <span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>reduceRDD</span> <span class=k>=</span> <span class=n>tupleRDD</span><span class=o>.</span><span class=n>reduceByKey</span><span class=o>(</span><span class=k>_</span> <span class=o>+</span> <span class=k>_</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>strRDD</span> <span class=k>=</span> <span class=n>reduceRDD</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=s>s&#34;</span><span class=si>${</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=si>}</span><span class=s>, </span><span class=si>${</span><span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=si>}</span><span class=s>&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1>//4. 结果获取
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>strRDD</span><span class=o>.</span><span class=n>collect</span><span class=o>().</span><span class=n>foreach</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>println</span><span class=o>(</span><span class=k>_</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=c1>//5. 关闭sc
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>sc</span><span class=o>.</span><span class=n>stop</span><span class=o>()</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>集群组成</strong></p><ul><li>Node1主节点:</li></ul><blockquote><blockquote><p><strong>Master Daemon</strong>：</p><p>负责管理Master节点， 协调资源的获取，以及连接Worker节点来运行Executor，是spark集群中的协调节点</p></blockquote></blockquote><ul><li>Node2:</li></ul><blockquote><blockquote><p><strong>Worker Daemon</strong>：</p><p>也称之为Slaves，是spark集群中的计算节点，用于和Master交互和并管理Driver， 当一个spark job 提交后，会创建sparkContext，worker会启动对应的Executor</p></blockquote><blockquote><p><strong>Driver</strong>:</p><p>ction算子操作获取的结果，会把结果存放在Driver中</p></blockquote><blockquote><p><strong>Executor Backend</strong>：</p><p>Worker用于控制Executor的启停，其实worker是通过 Executor Backend来进行控制的。 Executor Backend是一个进程（是一个JVM实例），持有一个Executor对象。</p><blockquote><p><strong>Executor</strong></p><blockquote><p>Task1 Task2 Task3</p></blockquote></blockquote></blockquote></blockquote><p><strong>逻辑执行图</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>textRDD</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=s>&#34;hadoop spark&#34;</span><span class=o>,</span> <span class=s>&#34;hadoop flume&#34;</span><span class=o>,</span> <span class=s>&#34;spark soo&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>splitRDD</span> <span class=k>=</span> <span class=n>textRDD</span><span class=o>.</span><span class=n>flatMap</span><span class=o>(</span><span class=k>_</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>tupleRDD</span> <span class=k>=</span> <span class=n>splitRDD</span><span class=o>.</span><span class=n>map</span><span class=o>((</span><span class=k>_</span><span class=o>,</span> <span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>reduceRDD</span> <span class=k>=</span> <span class=n>tupleRDD</span><span class=o>.</span><span class=n>reduceByKey</span><span class=o>(</span><span class=k>_</span> <span class=o>+</span> <span class=k>_</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>strRDD</span> <span class=k>=</span> <span class=n>reduceRDD</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=s>s&#34;</span><span class=si>${</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=si>}</span><span class=s>, </span><span class=si>${</span><span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=si>}</span><span class=s>&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>println</span><span class=o>(</span><span class=n>strRDD</span><span class=o>.</span><span class=n>toDebugString</span><span class=o>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=o>(</span><span class=mi>8</span><span class=o>)</span> <span class=nc>MapPartitionsRDD</span><span class=o>[</span><span class=err>4</span><span class=o>]</span> <span class=n>at</span> <span class=n>map</span> <span class=n>at</span> <span class=n>test</span><span class=o>.</span><span class=n>scala</span><span class=k>:</span><span class=err>12</span> <span class=err>[]</span>
</span></span><span class=line><span class=cl> <span class=kt>|</span>  <span class=kt>ShuffledRDD</span><span class=o>[</span><span class=err>3</span><span class=o>]</span> <span class=n>at</span> <span class=n>reduceByKey</span> <span class=n>at</span> <span class=n>test</span><span class=o>.</span><span class=n>scala</span><span class=k>:</span><span class=err>11</span> <span class=err>[]</span>
</span></span><span class=line><span class=cl> <span class=kt>+-</span><span class=o>(</span><span class=err>8</span><span class=o>)</span> <span class=kt>MapPartitionsRDD</span><span class=o>[</span><span class=err>2</span><span class=o>]</span> <span class=n>at</span> <span class=n>map</span> <span class=n>at</span> <span class=n>test</span><span class=o>.</span><span class=n>scala</span><span class=k>:</span><span class=err>10</span> <span class=err>[]</span>
</span></span><span class=line><span class=cl>    <span class=kt>|</span>  <span class=kt>MapPartitionsRDD</span><span class=o>[</span><span class=err>1</span><span class=o>]</span> <span class=n>at</span> <span class=n>flatMap</span> <span class=n>at</span> <span class=n>test</span><span class=o>.</span><span class=n>scala</span><span class=k>:</span><span class=err>9</span> <span class=err>[]</span>
</span></span><span class=line><span class=cl>    <span class=kt>|</span>  <span class=kt>ParallelCollectionRDD</span><span class=o>[</span><span class=err>0</span><span class=o>]</span> <span class=n>at</span> <span class=n>parallelize</span> <span class=n>at</span> <span class=n>test</span><span class=o>.</span><span class=n>scala</span><span class=k>:</span><span class=err>6</span> <span class=err>[]</span>
</span></span></code></pre></td></tr></table></div></div><p><figure><a class=lightgallery href=/20200527_spark-guide1/RDDlogic.png title=RDDlogic data-thumbnail=/20200527_spark-guide1/RDDlogic.png data-sub-html="<h2>RDDlogic</h2><p>RDDlogic</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20200527_spark-guide1/RDDlogic.png data-srcset="/20200527_spark-guide1/RDDlogic.png, /20200527_spark-guide1/RDDlogic.png 1.5x, /20200527_spark-guide1/RDDlogic.png 2x" data-sizes=auto alt=/20200527_spark-guide1/RDDlogic.png width=597 height=227></a><figcaption class=image-caption>RDDlogic</figcaption></figure></p><p><strong>物理执行图</strong></p><p>当触发Action执行的时候，这一组互相依赖的RDD要被处理，所以要转化为可运行的物理执行图，调度到集群中执行。</p><p>因为大部分RDD是不真正存放数据的，只是数据从中流转，所以不能直接在集群中运行RDD，要有一种pipeline的思想，需要将这组RDD转为Stage和Task，从而运行Task，优化整体执行速度。</p><p><figure><a class=lightgallery href=/20200527_spark-guide1/RDDphysic.png title=RDDphysic data-thumbnail=/20200527_spark-guide1/RDDphysic.png data-sub-html="<h2>RDDphysic</h2><p>RDDphysic</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20200527_spark-guide1/RDDphysic.png data-srcset="/20200527_spark-guide1/RDDphysic.png, /20200527_spark-guide1/RDDphysic.png 1.5x, /20200527_spark-guide1/RDDphysic.png 2x" data-sizes=auto alt=/20200527_spark-guide1/RDDphysic.png width=756 height=202></a><figcaption class=image-caption>RDDphysic</figcaption></figure></p><p>小结：</p><ul><li>① -> ① -> ① 在第一个stage中，每一个这样的执行流程是一个Task，也就是在同一个Stage中的所有RDD的对应分区，在同一个Task中执行</li><li>Stage的划分是由Shuffle操作来确定的，有Shuffle的地方，Stage断开</li></ul><p><strong>数据流动</strong></p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>sc</span> <span class=k>=</span> <span class=o>...</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>textRDD</span> <span class=k>=</span> <span class=n>sc</span><span class=o>.</span><span class=n>parallelize</span><span class=o>(</span><span class=nc>Seq</span><span class=o>(</span><span class=s>&#34;Hadoop Spark&#34;</span><span class=o>,</span> <span class=s>&#34;Hadoop Flume&#34;</span><span class=o>,</span> <span class=s>&#34;Spark Squad&#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>splitRDD</span> <span class=k>=</span> <span class=n>textRDD</span><span class=o>.</span><span class=n>flatMap</span><span class=o>(</span><span class=k>_</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34; &#34;</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>tupleRDD</span> <span class=k>=</span> <span class=n>splitRDD</span><span class=o>.</span><span class=n>map</span><span class=o>((</span><span class=k>_</span><span class=o>,</span><span class=mi>1</span><span class=o>))</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>reduceRDD</span> <span class=k>=</span> <span class=n>tupleRDD</span><span class=o>.</span><span class=n>reduceByKey</span><span class=o>(</span><span class=k>_</span> <span class=o>+</span> <span class=k>_</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=k>val</span> <span class=n>strRDD</span> <span class=k>=</span> <span class=n>reduceRDD</span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=s>s&#34;</span><span class=si>${</span><span class=n>item</span><span class=o>.</span><span class=n>_1</span><span class=o>,</span> <span class=n>$</span><span class=si>{</span><span class=n>item</span><span class=o>.</span><span class=n>_2</span><span class=si>}}</span><span class=s>&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=n>strRDD</span><span class=o>.</span><span class=n>collect</span><span class=o>.</span><span class=n>foreach</span><span class=o>(</span><span class=n>item</span> <span class=k>=&gt;</span> <span class=n>println</span><span class=o>(</span><span class=n>item</span><span class=o>))</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>Job和Stage的关系</strong></p><p>Job是一个最大的调度单位，DAGScheduler会首先创建一个Job的相关信息，然后去调度Job，但是没办法直接调度Job。</p><p>​ <strong>为什么Job需要切分</strong></p><ul><li><p>因为job的含义是对整个RDD求值，但RDD之间可能有一些宽依赖</p></li><li><p>如果遇到宽依赖的话，两个RDD之间需要进行数据拉取和复制</p><p>那么一个RDD就必须等待它所依赖的RDD所有分区先计算完成，然后再进行拉取</p></li><li><p>所以，一个Job是无法计算完整的RDD血统的</p></li></ul><p>​ <strong>Stage和Task的关系</strong></p><ol><li><p>Stage中的RDD之间是窄依赖：</p><p>窄依赖RDD理论上可以放在同一个Pipeline中执行的</p></li><li><p>RDD还有分区：</p><p>一个RDD只是一个概念，而真正存放和处理数据时，都是以分区作为单位的</p><p>Stage对应的是多个整体上的RDD，而真正的运行是需要针对RDD的分区来进行的</p></li><li><p>一个Task对应一个RDD的分区：</p><p>一个比Stage粒度更细的单元叫做Task，Stage是由Task组成的，之所以有Task这个概念，是因为Stage针对整个RDD，而计算的时候，要针对RDD的分区。</p></li><li><p>总结：</p><ul><li><p>Job>Stage>Task</p></li><li><p>一个Job由多个Stage组成(这个取决有多少个宽依赖)，一个Stage由多个Task组成（这个取决有多少个分区数量</p></li><li><p>而Stage中经常会有一组Task需要同时执行，所以针对每一个Task来进行调度太过频繁没有意义，所以每个Stage中的Task们会被收集起来，放入一个TaskSet集合中。</p></li><li><p>一个Stage有一个TaskSet</p></li><li><p>TaskSet中Task的个数由Stage中的最大分区数决定</p></li></ul></li></ol><p><figure><a class=lightgallery href=/20200527_spark-guide1/sparkFlow.png title=sparkFlow data-thumbnail=/20200527_spark-guide1/sparkFlow.png data-sub-html="<h2>sparkFlow</h2><p>sparkFlow</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20200527_spark-guide1/sparkFlow.png data-srcset="/20200527_spark-guide1/sparkFlow.png, /20200527_spark-guide1/sparkFlow.png 1.5x, /20200527_spark-guide1/sparkFlow.png 2x" data-sizes=auto alt=/20200527_spark-guide1/sparkFlow.png width=1486 height=927></a><figcaption class=image-caption>sparkFlow</figcaption></figure></p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/distribution/>Distribution</a>,&nbsp;<a href=/tags/spark/>Spark</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/20200526_app-architecture-development/ class=prev rel=prev title="The Development of App Architecture"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>The Development of App Architecture</a>
<a href=/20200615_hive-key-point/ class=next rel=next title="Hive Key Points">Hive Key Points<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2018 - 2022</span><span class=author itemprop=copyrightHolder>&nbsp;<a href=about target=_blank></a></span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:40},comment:{valine:{appId:"2jSYKh8PFUje3v7Ie3Nn5v1g-gzGzoHsz",appKey:"MyWn4P9G5N32q3oM5JDnlpdL",avatar:"retro",el:"#valine",emojiCDN:"https://img.t.sinajs.cn/t4/appstyle/expression/ext/normal/",enableQQ:!0,highlight:!0,lang:"en",meta:["nick","mail"],pageSize:10,placeholder:"Your comment ...",recordIP:!0,visitor:!0}},lightgallery:!0,search:{algoliaAppID:"BADKNNRXHD",algoliaIndex:"index",algoliaSearchKey:"7a8c2923330a44bdd9985698e3f28e0c",highlightTag:"em",maxResultLength:6,noResultsFound:"No results found",snippetLength:30,type:"algolia"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>