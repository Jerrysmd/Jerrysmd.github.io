<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on JerrysBlog</title>
    <link>https://jerrysmd.github.io/post/</link>
    <description>Recent content in Posts on JerrysBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 06 Jul 2022 10:35:10 +0800</lastBuildDate><atom:link href="https://jerrysmd.github.io/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>《Nonviolent Communication》</title>
      <link>https://jerrysmd.github.io/post/20220706_nonviolent-communication/</link>
      <pubDate>Wed, 06 Jul 2022 10:35:10 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20220706_nonviolent-communication/</guid>
      <description>
        
          
            &lt;p&gt;《Nonviolent Communication》is about connecting with ourselves and others from the heart. It’s about seeing the humanity in all of us. It’s about recognizing our commonalities and differences and finding ways to make life wonderful for all of us.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Memories of 2021</title>
      <link>https://jerrysmd.github.io/post/20211218_2021_summary_video/article/</link>
      <pubDate>Sat, 18 Dec 2021 10:01:20 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20211218_2021_summary_video/article/</guid>
      <description>
        
          
            &lt;p&gt;Photos and videos recorded in 2021.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Warehouse: Real-Time</title>
      <link>https://jerrysmd.github.io/post/20210308_dw_flink/</link>
      <pubDate>Mon, 08 Mar 2021 10:40:49 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20210308_dw_flink/</guid>
      <description>
        
          
            &lt;p&gt;Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tensorflow Identify Simple Captcha</title>
      <link>https://jerrysmd.github.io/post/20210204_tensorflow_captcha/article/</link>
      <pubDate>Thu, 04 Feb 2021 07:10:16 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20210204_tensorflow_captcha/article/</guid>
      <description>
        
          
            &lt;p&gt;CAPTCHA stands for &#39;Completely Automated Public Turing test to tell Computers and Humans Apart&#39;. It’s already possible to solve it with the rise of deep learning and computer vision.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Warehouse: Offline</title>
      <link>https://jerrysmd.github.io/post/20210108_dw_behavior_collection/article/</link>
      <pubDate>Fri, 08 Jan 2021 09:43:01 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20210108_dw_behavior_collection/article/</guid>
      <description>
        
          
            &lt;p&gt;Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Parquet Format</title>
      <link>https://jerrysmd.github.io/post/20201215_parquet_format/parquet_format/</link>
      <pubDate>Tue, 15 Dec 2020 10:47:59 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201215_parquet_format/parquet_format/</guid>
      <description>
        
          
            &lt;p&gt;Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types. This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Clickhouse Introduction</title>
      <link>https://jerrysmd.github.io/post/20201107_clickhouse_intro/clickhouse_intro/</link>
      <pubDate>Sat, 07 Nov 2020 14:55:59 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201107_clickhouse_intro/clickhouse_intro/</guid>
      <description>
        
          
            &lt;p&gt;A high performance columnar OLAP database management system for real-time analytics using SQL. ClickHouse can be customized with a new set of efficient columnar storage engines, and has realized rich functions such as data ordered storage, primary key indexing, sparse indexing, data sharding, data partitioning, TTL, and primary and backup replication.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hbase Optimize</title>
      <link>https://jerrysmd.github.io/post/20201026_hbase_optimize/article/</link>
      <pubDate>Mon, 26 Oct 2020 09:14:35 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201026_hbase_optimize/article/</guid>
      <description>
        
          
            &lt;p&gt;HBase is a high reliability, high performance, column-oriented, and scalable distributed database. However, the READ/write performance deteriorates when a large amount of concurrent data or existing data is generated. You can use the following methods to improve the HBase search speed.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hbase Rowkey Design</title>
      <link>https://jerrysmd.github.io/post/20201016_hbase-rowkey-design/hbase-rowkey-design/</link>
      <pubDate>Fri, 16 Oct 2020 10:30:42 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201016_hbase-rowkey-design/hbase-rowkey-design/</guid>
      <description>
        
          
            &lt;p&gt;Rows in HBase are sorted lexicographically by row key. This design optimizes for scans, allowing you to store related rows, or rows that will be read together, near each other. However, poorly designed row keys are a common source of hotspotting. Hotspotting occurs when a large amount of client traffic is directed at one node, or only a few nodes, of a cluster. This traffic may represent reads, writes, or other operations. The traffic overwhelms the single machine responsible for hosting that region, causing performance degradation and potentially leading to region unavailability. This can also have adverse effects on other regions hosted by the same region server as that host is unable to service the requested load. It is important to design data access patterns such that the cluster is fully and evenly utilized.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Elasticsearch Wildcard Search</title>
      <link>https://jerrysmd.github.io/post/20200911_es-wildcard-search/es-wildcard-search/</link>
      <pubDate>Fri, 11 Sep 2020 10:57:33 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200911_es-wildcard-search/es-wildcard-search/</guid>
      <description>
        
          
            &lt;p&gt;Elasticsearch is the distributed, RESTful search and analytics engine at the heart of the &lt;a href=&#34;https://www.elastic.co/products&#34;&gt;Elastic Stack&lt;/a&gt;. You can use Elasticsearch to store, search, and manage data for Logs，Metrics，A search backend，Application monitoring，Endpoint security.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark On Yarn: yarn-cluster, yarn-client</title>
      <link>https://jerrysmd.github.io/post/20200811_yarn-clusteryarn-client/article/</link>
      <pubDate>Tue, 11 Aug 2020 10:42:19 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200811_yarn-clusteryarn-client/article/</guid>
      <description>
        
          
            &lt;p&gt;YARN is a generic resource-management framework for distributed workloads; in other words, a cluster-level operating system. Although part of the Hadoop ecosystem, YARN can support a lot of varied compute-frameworks (such as Tez, and Spark) in addition to MapReduce.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅲ</title>
      <link>https://jerrysmd.github.io/post/20200803_sparkguide3/sparkguide3/</link>
      <pubDate>Mon, 03 Aug 2020 16:10:33 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200803_sparkguide3/sparkguide3/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅱ</title>
      <link>https://jerrysmd.github.io/post/20200707_sparkguide2/sparkguide2/</link>
      <pubDate>Tue, 07 Jul 2020 11:14:54 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200707_sparkguide2/sparkguide2/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hive Key Points</title>
      <link>https://jerrysmd.github.io/post/20200615_hivekeypoint/hivekeypoint/</link>
      <pubDate>Mon, 15 Jun 2020 09:26:00 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200615_hivekeypoint/hivekeypoint/</guid>
      <description>
        
          
            &lt;p&gt;Hive is a Hadoop-based data warehouse tool that maps structured data files into a database table and provides complete SQL query functionality that converts SQL statements into MapReduce tasks for execution. It is very suitable for statistical analysis of data warehouse.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅰ</title>
      <link>https://jerrysmd.github.io/post/20200527_sparkguide1/sparkguide1/</link>
      <pubDate>Wed, 27 May 2020 17:20:52 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200527_sparkguide1/sparkguide1/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The Development of App Architecture</title>
      <link>https://jerrysmd.github.io/post/20200526_apparchitecturedevelopment/article/</link>
      <pubDate>Tue, 26 May 2020 17:17:50 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200526_apparchitecturedevelopment/article/</guid>
      <description>
        
          
            &lt;p&gt;The features of large applications: high availability, high concurrency and big data. High availability: system need to provide service without interruption. High concurrency: still stable under the big access. Big data: store and manage big data well.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The Final Key Word In Java</title>
      <link>https://jerrysmd.github.io/post/20200526_finalkeywordinjava/finalkeywordinjava/</link>
      <pubDate>Tue, 26 May 2020 15:56:36 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200526_finalkeywordinjava/finalkeywordinjava/</guid>
      <description>
        
          
            &lt;p&gt;Final key word no doubt be mentioned most time in Java language. There are some points about final key word. First of all, final key word can modify three objects: first is modify variables, second is modify way, third is modify class.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Key points of ZooKeeper</title>
      <link>https://jerrysmd.github.io/post/20200519_zookeeper/zookeeper/</link>
      <pubDate>Tue, 19 May 2020 16:07:17 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200519_zookeeper/zookeeper/</guid>
      <description>
        
          
            &lt;p&gt;ZooKeeper is an open source distributed coordination framework. It is positioned to provide consistent services for distributed applications and is the administrator of the entire big data system. ZooKeeper will encapsulate key services that are complex and error-prone, and provide users with efficient, stable, and easy-to-use services.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Kafka &amp; Message Queue</title>
      <link>https://jerrysmd.github.io/post/20200427_kafka/kafka/</link>
      <pubDate>Mon, 27 Apr 2020 14:21:19 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200427_kafka/kafka/</guid>
      <description>
        
          
            &lt;p&gt;Apache Kafka aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a &amp;quot;message set&amp;quot; abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This &amp;quot;leads to larger network packets, larger sequential disk operations, contiguous memory blocks [...] which allows Kafka to turn a bursty stream of random message writes into linear writes.&amp;quot;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>HDFS &amp; NFS</title>
      <link>https://jerrysmd.github.io/post/20200326_hdfsnfs/hdfsnfs/</link>
      <pubDate>Thu, 26 Mar 2020 11:02:36 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200326_hdfsnfs/hdfsnfs/</guid>
      <description>
        
          
            &lt;p&gt;The major difference between the two is Replication/Fault Tolerance. HDFS was designed to survive failures. NFS does not have any fault tolerance built in. Other than fault tolerance, HDFS does support multiple replicas of files. This eliminates (or eases) the common bottleneck of many clients accessing a single file. Since files have multiple replicas, on different physical disks, reading performance scales better than NFS.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Scala Introduction</title>
      <link>https://jerrysmd.github.io/post/20200208_scalaintroduction/scalaintroduction/</link>
      <pubDate>Sat, 08 Feb 2020 10:16:17 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200208_scalaintroduction/scalaintroduction/</guid>
      <description>
        
          
            &lt;p&gt;Scala combines object-oriented and functional programming in one concise, high-level language. Scala&#39;s static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Static Keyword in Java</title>
      <link>https://jerrysmd.github.io/post/20200122_javastatic/javastatic/</link>
      <pubDate>Wed, 22 Jan 2020 11:20:11 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200122_javastatic/javastatic/</guid>
      <description>
        
          
            &lt;p&gt;The static keyword can be used for variables, methods, code blocks, and inner classes to indicate that a particular member belongs only to a class itself, and not to an object of that class.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Java Basic</title>
      <link>https://jerrysmd.github.io/post/20200101_javainterview/javainterview/</link>
      <pubDate>Wed, 01 Jan 2020 11:18:47 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200101_javainterview/javainterview/</guid>
      <description>
        
          
            &lt;p&gt;JVM, Garbage Collection, Multi - thread, Redis&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Leetcode</title>
      <link>https://jerrysmd.github.io/post/20200101_leetcode/leetcode/</link>
      <pubDate>Wed, 01 Jan 2020 10:57:11 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200101_leetcode/leetcode/</guid>
      <description>
        
          
            &lt;p&gt;Algorithms are used in every part of computer science. They form the field&#39;s backbone. In computer science, an algorithm gives the computer a specific set of instructions, which allows the computer to do everything, be it running a calculator or running a rocket.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Shallow Copy and Deep Copy in JAVA</title>
      <link>https://jerrysmd.github.io/post/20191229_copyinjava/copyinjava/</link>
      <pubDate>Sun, 29 Dec 2019 22:14:03 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191229_copyinjava/copyinjava/</guid>
      <description>
        
          
            &lt;p&gt;Cloning is a process of creating an exact copy of an existing object in the memory. In java, clone() method of java.lang.Object class is used for cloning process. This method creates an exact copy of an object on which it is called through field-by-field assignment and returns the reference of that object. Not all the objects in java are eligible for cloning process. The objects which implement Cloneable interface are only eligible for cloning process. Cloneable interface is a marker interface which is used to provide the marker to cloning process. Click here to see more info on clone() method in java.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Memories of 2019</title>
      <link>https://jerrysmd.github.io/post/20191218_2019_summary_video/article/</link>
      <pubDate>Wed, 18 Dec 2019 10:01:12 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191218_2019_summary_video/article/</guid>
      <description>
        
          
            &lt;p&gt;Photos and videos recorded in 2019.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Git Contribution</title>
      <link>https://jerrysmd.github.io/post/20191126_gitcontribution/gitcontribution/</link>
      <pubDate>Tue, 26 Nov 2019 11:22:47 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191126_gitcontribution/gitcontribution/</guid>
      <description>
        
          
            &lt;p&gt;Contribution graph shows activity from public repositories. You can choose to show activity from both public and private repositories, with specific details of your activity in private repositories anonymized.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Various data types in C</title>
      <link>https://jerrysmd.github.io/post/20191007_u32/u32/</link>
      <pubDate>Mon, 07 Oct 2019 09:11:47 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191007_u32/u32/</guid>
      <description>
        
          
            &lt;p&gt;C/C++ provides various data types that can be used in your programs.In general, you&#39;d commonly use: int for most variables and &amp;quot;countable&amp;quot; things (for loop counts, variables, events). char for characters and strings. float for general measurable things (seconds, distance, temperature). uint32 for bit manipulations, especially on 32-bit registers.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The size of structure in C</title>
      <link>https://jerrysmd.github.io/post/20190825_structsize/structsize/</link>
      <pubDate>Sun, 25 Aug 2019 14:38:34 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190825_structsize/structsize/</guid>
      <description>
        
          
            &lt;p&gt;The sizeof for a struct is not always equal to the sum of sizeof of each individual member. This is because of the padding added by the compiler to avoid alignment issues. Padding is only added when a structure member is followed by a member with a larger size or at the end of the structure.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The descriptions of typedef</title>
      <link>https://jerrysmd.github.io/post/20190717_typedef/typedef/</link>
      <pubDate>Wed, 17 Jul 2019 09:22:18 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190717_typedef/typedef/</guid>
      <description>
        
          
            &lt;p&gt;A typedef is a C keyword that defines a new name for a data type, including internal data types (int, char, etc.) and custom data types (struct, etc.). A typedef is itself a type of stored class keyword that cannot appear in the same expression as the keywords auto, extern, static, register, etc.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Conditional compilation directives</title>
      <link>https://jerrysmd.github.io/post/20190611_conditionalcompilation/article/</link>
      <pubDate>Tue, 11 Jun 2019 10:42:49 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190611_conditionalcompilation/article/</guid>
      <description>
        
          
            &lt;p&gt;Conditional compilation is static compilation of code based on the actual definition of macros (some kind of condition). Compilation conditions can be determined based on the value of an expression or whether a particular macro is defined. C language conditional compilation related precompilation instructions, including #define, #undef, #ifdef, #ifndef, #if, #elif, #else, #endif, defined.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Python Introduction</title>
      <link>https://jerrysmd.github.io/post/20190510_pythonintroduction/article/</link>
      <pubDate>Fri, 10 May 2019 09:34:11 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190510_pythonintroduction/article/</guid>
      <description>
        
          
            &lt;p&gt;Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. Python is often described as a &amp;quot;batteries included&amp;quot; language due to its comprehensive standard library.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>C Project performance optimization</title>
      <link>https://jerrysmd.github.io/post/20190409_cperformanceoptimization/article/</link>
      <pubDate>Tue, 09 Apr 2019 20:04:00 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190409_cperformanceoptimization/article/</guid>
      <description>
        
          
            &lt;p&gt;Performance optimization methods and ideas for large C projects. Performance optimization strategies for x86 projects that encounter performance bottlenecks when porting to low performance processors.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>GDB debug guide</title>
      <link>https://jerrysmd.github.io/post/20190304_gdb_debug/gdb_debug/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190304_gdb_debug/gdb_debug/</guid>
      <description>
        
          
            &lt;p&gt;GDB is a powerful program debugging tool based on command line under UNIX/LINUX operating system released by GNU Source Organization. For a C/C ++ programmer working on Linux, GDB is an essential tool.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>HugePages</title>
      <link>https://jerrysmd.github.io/post/20190203_hugepages/hugepages/</link>
      <pubDate>Sun, 03 Feb 2019 13:04:09 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190203_hugepages/hugepages/</guid>
      <description>
        
          
            &lt;p&gt;Reduce page tables by increasing the size of operating system pages to avoid fast table misses. Large page memory optimizer is designed for malloc mechanism, which means allocating large pages to increase TLB hit ratio.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Dynamic link library and static link library</title>
      <link>https://jerrysmd.github.io/post/20190125_dynamicstaticlinklibrary/article/</link>
      <pubDate>Fri, 25 Jan 2019 22:01:19 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190125_dynamicstaticlinklibrary/article/</guid>
      <description>
        
          
            &lt;p&gt;In development, we just need to know that lib is needed at compile time and DLL is needed at run time. If you want to compile source code, lib is all you need. If you want dynamically connected programs to run, you need only a DLL. This article will more clearly understand the difference, generation, use of the two.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The Advanced method of reading files</title>
      <link>https://jerrysmd.github.io/post/20181220_read_binary_file/read_binary_file/</link>
      <pubDate>Thu, 20 Dec 2018 18:39:37 -0700</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20181220_read_binary_file/read_binary_file/</guid>
      <description>
        
          
            &lt;p&gt;Read the binary file (any file will do; this article uses binary as an example) and read the entire contents of the binary file into a char* string. With fseek() and fread() functions to achieve file reading advanced methods.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>SQLcipher Guide</title>
      <link>https://jerrysmd.github.io/post/20181118_sqlcipher/sqlcipher/</link>
      <pubDate>Sun, 18 Nov 2018 21:58:51 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20181118_sqlcipher/sqlcipher/</guid>
      <description>
        
          
            &lt;p&gt;SQLCipher is based on SQLite, and thus, the majority of the accessible API is identical to the C/C++ interface for SQLite 3. However, SQLCipher does add a number of security specific extensions in the form of PRAGMAs, SQL Functions and C Functions.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>SQL Introduction</title>
      <link>https://jerrysmd.github.io/post/20181017_sqlintroduction/sqlintroduction/</link>
      <pubDate>Wed, 17 Oct 2018 11:52:36 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20181017_sqlintroduction/sqlintroduction/</guid>
      <description>
        
          
            &lt;p&gt;In computer programming, create, read, update, and delete (CRUD) are the four basic functions of persistent storage. Alternate words are sometimes used when defining the four basic functions of CRUD, such as retrieve instead of read, modify instead of update, or destroy instead of delete. CRUD is also sometimes used to describe user interface conventions that facilitate viewing, searching, and changing information, often using computer-based forms and reports.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Makefile Guide</title>
      <link>https://jerrysmd.github.io/post/20180915_makefile_cmakelist/makefile_cmakelist/</link>
      <pubDate>Sat, 15 Sep 2018 21:59:46 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180915_makefile_cmakelist/makefile_cmakelist/</guid>
      <description>
        
          
            &lt;p&gt;Makefiles define a set of rules that specify which files need to be compiled first, which files need to be compiled later, which files need to be recompiled, and even more complex functional operations, because makefiles are like Shell scripts that also execute operating system commands. One of the benefits of Makefiles is that they are &amp;quot;automatically compiled&amp;quot;. The entire project is automatically compiled, greatly improving the efficiency of software development.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hyperscan: high-performance multiple regex matching library</title>
      <link>https://jerrysmd.github.io/post/20180812_hyperscan/hyperscan/</link>
      <pubDate>Sun, 12 Aug 2018 05:42:44 -0700</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180812_hyperscan/hyperscan/</guid>
      <description>
        
          
            &lt;p&gt;Hyperscan is a high-performance regular expression matching library from Intel. It is based on the X86 platform based on PCRE prototype development. While supporting most of the syntax of PCRE, Hyperscan adds specific syntax and working modes to ensure its usefulness in real-world network scenarios.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The problem about file reading one more line</title>
      <link>https://jerrysmd.github.io/post/20180610_cgetline/cgetline/</link>
      <pubDate>Sun, 10 Jun 2018 21:58:13 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180610_cgetline/cgetline/</guid>
      <description>
        
          
            &lt;p&gt;Getline ()/get()/read() will read one more line. The cause may be a problem with the file itself or the getline() function. You can judge getLine ()/get()/read() while checking, and then process the data if you get it.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The realization of the MXML</title>
      <link>https://jerrysmd.github.io/post/20180507_mxml/mxml/</link>
      <pubDate>Mon, 07 May 2018 21:57:23 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180507_mxml/mxml/</guid>
      <description>
        
          
            &lt;p&gt;MXML (Minimal XML) is a small, fast and versatile library that reads a whole XML file and puts it in a DOM tree.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>C language realize HashMap</title>
      <link>https://jerrysmd.github.io/post/20180405_hashcode/hashcode/</link>
      <pubDate>Thu, 05 Apr 2018 21:56:51 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180405_hashcode/hashcode/</guid>
      <description>
        
          
            &lt;p&gt;Hash table is a very important data structure, which is useful in many application scenarios. This paper will simply analyze the principle of hash table, and use C language to achieve a complete HashMap.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
