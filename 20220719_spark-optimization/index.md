# Spark Performance Optimization


Spark SQL is the top active component in latest spark release. 46% of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in latest release.

<!--more-->

## Explain æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’

### è¯­æ³•

```scala
.explain(mode="xxx")
```

+ `explain(mode="simple")`: åªå±•ç¤ºç‰©ç†æ‰§è¡Œè®¡åˆ’
+ `explain(mode="extended")`: å±•ç¤ºç‰©ç†è®¡åˆ’å’Œé€»è¾‘æ‰§è¡Œè®¡åˆ’
+ `"codegen"`: å±•ç¤º codegen ç”Ÿæˆçš„å¯æ‰§è¡Œ Java ä»£ç 
+ `"cost"`: å±•ç¤ºä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ä»¥åŠç›¸å…³çš„ç»Ÿè®¡
+ `"formatted"`: åˆ†éš”è¾“å‡ºï¼Œè¾“å‡ºæ›´æ˜“è¯»çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’å¹¶å±•ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯

### æ‰§è¡Œè®¡åˆ’å¤„ç†æµç¨‹

![image-20220819153115834](image-20220819153115834.png "å¤„ç†æµç¨‹")

![image-20220819153308501](image-20220819153308501.png "æ ¸å¿ƒè¿‡ç¨‹")

**ğŸ”´Unresolved é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼š== Parsed Logical Plan ==**

+ Parser ç»„ä»¶æ£€æŸ¥ SQL è¯­æ³•æ˜¯å¦æœ‰é—®é¢˜ï¼Œç„¶åç”Ÿæˆ Unresolved çš„é€»è¾‘è®¡åˆ’ï¼Œä¸æ£€æŸ¥è¡¨åã€ä¸æ£€æŸ¥åˆ—æ˜

**ğŸŸ Resolved é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼š==Analyzed Logical Plan ==**

+ Spark ä¸­çš„ Catalog å­˜å‚¨åº“æ¥è§£æéªŒè¯è¯­ä¹‰ã€åˆ—åã€ç±»å‹ã€è¡¨åç­‰

ğŸŸ¡**ä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼š== Optimized Logical Plan ==**

+ Catalyst ä¼˜åŒ–å™¨æ ¹æ®å„ç§è§„åˆ™è¿›è¡Œä¼˜åŒ–

**ğŸŸ¢ç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼š== Physical Plan ==**

1. `HashAggregate` è¿ç®—ç¬¦è¡¨ç¤ºæ•°æ®èšåˆï¼Œä¸€èˆ¬ HashAggregate æ˜¯æˆå¯¹å‡ºç°ï¼Œç¬¬ä¸€ä¸ª HashAggregate æ˜¯å°†æ‰§è¡ŒèŠ‚ç‚¹æœ¬åœ°çš„æ•°æ®è¿›è¡Œå±€éƒ¨èšåˆï¼Œå¦ä¸€ä¸ª HashAggregate æ˜¯å°†å„ä¸ªåˆ†åŒºçš„æ•°æ®è¿›è¡Œèšåˆè®¡ç®—
2. `Exchange` è¿ç®—ç¬¦å…¶å®å°±æ˜¯ shuffleï¼Œè¡¨ç¤ºéœ€è¦åœ¨é›†ç¾¤ä¸Šç§»åŠ¨æ•°æ®ã€‚å¾ˆå¤šæ—¶å€™ HashAggregate ä¼šä»¥ Exchange åˆ†éš”å¼€
3. `Project`è¿ç®—ç¬¦æ˜¯ SQL ä¸­çš„é€‰æ‹©åˆ—ï¼Œselect name, age
4. `BroadcastHashJoin`è¡¨ç¤ºé€šè¿‡åŸºäºå¹¿æ’­æ–¹å¼è¿›è¡Œ HashJoin
5. `LocalTableScan` è¡¨ç¤ºå…¨è¡¨æ‰«ææœ¬åœ°çš„è¡¨

## èµ„æºè°ƒä¼˜

### èµ„æºè§„åˆ’

#### èµ„æºè®¾å®šè€ƒè™‘

##### æ€»ä½“åŸåˆ™

å•å°æœåŠ¡å™¨ 128G å†…å­˜ï¼Œ32çº¿ç¨‹ã€‚

å…ˆè®¾å®šå•ä¸ª Executor æ ¸æ•°ï¼Œæ ¹æ® Yarn é…ç½®å¾—å‡ºæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šçš„ Executor æ•°é‡ï¼Œ(Yarn æ€»æ ¸æ•° / æ¯ä¸ªexecutoræ ¸æ•°(é€šå¸¸ä¸º4)) = å•ä¸ªèŠ‚ç‚¹çš„executoræ•°é‡ï¼›

28 / 4 = 7 å•ä¸ªèŠ‚ç‚¹çš„executoræ•°é‡ï¼›

æ€»çš„ executor æ•° = å•èŠ‚ç‚¹executoræ•°é‡ * nmèŠ‚ç‚¹æ•°ã€‚

##### å…·ä½“æäº¤å‚æ•°

1. executor-cores

   æ¯ä¸ª executor çš„æœ€å¤§æ ¸æ•°ã€‚3 ~ 6 ä¹‹é—´æ¯”è¾ƒåˆç†ï¼Œé€šå¸¸ä¸º4

2. num-executors

   num-executors = æ¯ä¸ªèŠ‚ç‚¹çš„ executor æ•° * work èŠ‚ç‚¹æ•°ï¼›

   æ¯ä¸ª node çš„ executor æ•° = å•èŠ‚ç‚¹ yarn æ€»æ ¸æ•° / æ¯ä¸ª executor çš„æœ€å¤§ cpu æ ¸æ•°ï¼›

   32çº¿ç¨‹æœ‰28çº¿ç¨‹ç”¨åœ¨ Yarn ä¸Šï¼›

   é‚£ä¹ˆæ¯ä¸ª node çš„ executor æ•° = 28 / 4 = 7ï¼›

   å‡è®¾é›†ç¾¤èŠ‚ç‚¹ä¸º10ï¼›

   é‚£ä¹ˆ num-executors = 7 * 10 = 70ã€‚

3. executor-memoryâ­

   executor-memory = Yarn å†…å­˜ / å•ä¸ªèŠ‚ç‚¹çš„executoræ•°é‡ï¼›

   100G(æ€»128G, 100G ç»™ Yarn) / 7 = 14G; 

#### å†…å­˜è®¾ç½®

**ä¸€ä¸ª executor å†…éƒ¨**

![image-20220819173801820](image-20220819173801820.png "å †å†…å­˜æ¨¡å‹")

ğŸŸ¢ä¼°ç®— Other å†…å­˜ = è‡ªå®šä¹‰æ•°æ®ç»“æ„ * æ¯ä¸ª Executor æ ¸æ•°

ğŸ”µä¼°ç®— Storage å†…å­˜ = å¹¿æ’­å˜é‡ + cache/Executor æ•°é‡

ğŸŸ£ä¼°ç®— Executor å†…å­˜ = æ¯ä¸ª Executor æ ¸æ•° * (æ•°æ®é›†å¤§å°/å¹¶è¡Œåº¦)

* Sparksql å¹¶è¡Œåº¦é»˜è®¤ä¸º 200ï¼Œå¹¶è¡Œåº¦å³ä¸º Task æ•°é‡

### æŒä¹…åŒ–å’Œåºåˆ—åŒ–

#### RDD

##### Kryo åºåˆ—åŒ–ç¼“å­˜

ä½¿ç”¨ï¼š

1. sparkConf æŒ‡å®š kryo åºåˆ—åŒ–å™¨
2. sparkConf æ³¨å†Œæ ·ä¾‹ç±»

```scala
new SparkConf()
...
.set("spark.serializer","...kryoSerializer")
.registerKryoClasses(Array(classOf[...]))

result.persist(StorageLevel.MEMORY_ONLY_SER)
```

æµ‹è¯•ï¼š

> 2G çš„ HIVE å…ƒæ•°æ®ï¼Œä½¿ç”¨ RDD ç¼“å­˜ï¼Œå®Œæˆ 100% Fraction Cached éœ€è¦ 7 Gå·¦å³å†…å­˜ï¼Œä½¿ partition å¾ˆå®¹æ˜“æŒ‚æ‰ã€‚ä½¿ç”¨ Kryo åºåˆ—å™¨å®Œæˆ Cached éœ€è¦ 1 Gå†…å­˜ã€‚

#### DFã€DS

cache é»˜è®¤ä½¿ç”¨ `MEMORY_AND_DISK`ç¼“å­˜

{{< admonition note>}}

1. åºåˆ—åŒ–å™¨(Java, Kryo)æ˜¯é’ˆå¯¹ RDD è€Œè¨€çš„ï¼›è€Œ DFã€DS æ˜¯ç”± encoder é€‰æ‹©çš„ã€‚
2. encoder ç”± SparkSql è‡ªå·±å®ç°çš„ï¼Œä¹Ÿæœ‰å¯èƒ½ä½¿ç”¨ kryo çš„æ–¹å¼ã€‚
3. å¯¹ DFã€DSä½¿ç”¨åºåˆ—åŒ–å·®åˆ«ä¸å¤§ã€‚

{{< /admonition >}}

###  CPUä¼˜åŒ–

#### CPU ä½æ•ˆåŸå› 

##### å¹¶è¡Œåº¦

å¹¶è¡Œåº¦å°±æ˜¯ Task æ•°é‡ã€‚

ğŸŸ RDD å¹¶è¡Œåº¦å‚æ•°ï¼š

+ `spark.default.parallelism`

+ ä¸è®¾ç½®æ—¶ï¼Œé»˜è®¤ç”± joinã€reduceByKey å’Œ parallelize ç­‰è½¬æ¢å†³å®šã€‚

ğŸŸ¡SparkSQL å¹¶è¡Œåº¦å‚æ•°ï¼šä¸ RDD å¹¶è¡Œåº¦äº’ä¸å½±å“

+ `spark.sql.shuffle.partitions`

+ é»˜è®¤æ˜¯ 200ï¼Œåªèƒ½æ§åˆ¶ SparkSQLã€DataFrameã€Dataset åˆ†åŒºä¸ªæ•°ã€‚

##### å¹¶å‘åº¦

å¹¶å‘åº¦ï¼šåŒæ—¶æ‰§è¡Œçš„ Task æ•°é‡ã€‚

##### CPU ä½æ•ˆåŸå› 

1. å¹¶è¡Œåº¦è¾ƒä½ã€æ•°æ®åˆ†ç‰‡è¾ƒå¤§å®¹æ˜“å¯¼è‡´ CPU çº¿ç¨‹æŒ‚èµ·ï¼›
2. å¹¶è¡Œåº¦è¿‡é«˜ã€æ•°æ®è¿‡äºåˆ†æ•£ä¼šè®©è°ƒåº¦å¼€é”€æ›´å¤šï¼›

#### CPU èµ„æºè°ƒæ•´

`spark-submit --master yarn --deploy-mode client --driver-memory 1g --num-executors 3 --executor-cores 4 --executor-memory 6g --class com.jar`

ğŸŸ£å®˜æ–¹æ¨èå¹¶è¡Œåº¦ï¼ˆTask æ•°ï¼‰è®¾ç½®æˆå¹¶å‘åº¦ï¼ˆvcore æ•°ï¼‰çš„ 2 å€åˆ° 3 å€ã€‚

ä¾‹ï¼šå¦‚æœä»¥ç›®å‰çš„èµ„æºï¼ˆ3 ä¸ª executorï¼‰å»æäº¤ï¼Œæ¯ä¸ª executor æœ‰ä¸¤ä¸ªæ ¸ï¼Œæ€»å…± 6 ä¸ªæ ¸ï¼Œåˆ™å¹¶è¡Œåº¦è®¾ç½®ä¸º 12 ~ 18ã€‚

```scala
SparkConf()
...
.set("spark.sql.shuffle.partitions", "18")
```

## SparkSQL è¯­æ³•ä¼˜åŒ–

### åŸºäº RBO ä¼˜åŒ–

#### è°“è¯ä¸‹æ¨

```scala
//=============Inner on å·¦è¡¨=============
spark.sqlContext.sql(
  """
    |select
    | l.id,
    | l.name,
    | r.id,
    | r.name
    |from course l join student r
    | on l.id=r.id and l.dt=r.dt and l.dn=r.dn
    |on l.id<2
    |""".stripMargin)
//=============Inner where å·¦è¡¨=============
spark.sqlContext.sql(
  """
    |select
    | l.id,
    | l.name,
    | r.id,
    | r.name
    |from course l join student r
    | on l.id=r.id and l.dt=r.dt and l.dn=r.dn
    |where l.id<2
    |""".stripMargin)
```

**inner join**

+ æ— è®ºæ˜¯ ON è¿˜æ˜¯ WHEREï¼Œæ— è®ºæ¡ä»¶æ˜¯å³è¡¨è¿˜æ˜¯å·¦è¡¨ã€‚ä» logic plan -> Analyzed logical plan åˆ° **optimized logical plan**ï¼ŒsparkSQL éƒ½ä¼šä¼˜åŒ–**å…ˆè¿‡æ»¤æ•°æ®å†è¿›è¡Œ join** è¿æ¥ï¼Œè€Œä¸”å…¶ä¸­ä¸€è¡¨è¿‡æ»¤ï¼Œ**å¦ä¸€è¡¨ä¹Ÿä¼˜åŒ–æå‰è¿‡æ»¤**ï¼ˆæœ€ç»ˆè¦è¿‡æ»¤æ•°æ®ï¼Œå¦ä¸€è¡¨ä¹Ÿæ²¡æœ‰å­˜åœ¨çš„å¿…è¦ï¼‰

**left join**

+ |                 | æ¡ä»¶åœ¨ å·¦è¡¨ | æ¡ä»¶åœ¨ å³è¡¨ |
  | --------------- | ----------- | ----------- |
  | æ¡ä»¶åœ¨ on å    | åªä¸‹æ¨å³è¡¨  | åªä¸‹æ¨å³è¡¨  |
  | æ¡ä»¶åœ¨ where å | ä¸¤è¡¨éƒ½ä¸‹æ¨  | ä¸¤è¡¨éƒ½ä¸‹æ¨  |

+ æ³¨æ„ï¼šå¤–å…³è”æ—¶ï¼Œè¿‡æ»¤æ¡ä»¶åœ¨ on ä¸ whereï¼Œè¯­ä¹‰æ˜¯ä¸åŒçš„ï¼Œç»“æœä¹Ÿæ˜¯ä¸åŒçš„ã€‚

#### åˆ—è£å‰ª

æ‰«ææ•°æ®æºçš„æ—¶å€™ï¼Œåªè¯»å–é‚£äº›ä¸æŸ¥è¯¢ç›¸å…³çš„å­—æ®µã€‚

#### å¸¸é‡æ›¿æ¢

Catalyst ä¼šä½¿ç”¨ constantFolding è§„åˆ™ï¼Œè‡ªåŠ¨ç”¨è¡¨è¾¾å¼çš„ç»“æœè¿›è¡Œæ›¿æ¢ã€‚

### åŸºäº CBO ä¼˜åŒ–

#### Statistics æ”¶é›†

éœ€è¦å…ˆæ‰§è¡Œç‰¹å®šçš„ SQL è¯­å¥æ¥æ”¶é›†æ‰€éœ€çš„è¡¨å’Œåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

ğŸ”µ ç”Ÿæˆè¡¨çº§åˆ«ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ‰«è¡¨ï¼‰ï¼š

`ANALYZE TABLE è¡¨æ˜ COMPUTE STATISTICS`

#### ä½¿ç”¨ CBO

é€šè¿‡ `spark.sql.cbo.enabled` æ¥å¼€å¯ï¼Œé»˜è®¤æ˜¯ falseã€‚CBO ä¼˜åŒ–å™¨å¯ä»¥åŸºäºè¡¨å’Œåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œé€‰æ‹©å‡ºæœ€ä¼˜çš„æŸ¥è¯¢è®¡åˆ’ã€‚æ¯”å¦‚ï¼šBuild ä¾§é€‰æ‹©ã€ä¼˜åŒ– Join ç±»å‹ã€ä¼˜åŒ–å¤šè¡¨ Join é¡ºåºã€‚

### å¹¿æ’­ Join

#### é€šè¿‡å‚æ•°æŒ‡å®šè‡ªåŠ¨å¹¿æ’­

å¹¿æ’­ join é»˜è®¤å€¼ä¸º 10MBï¼Œç”± `spark.sql.autoBroadcastJoinThreshold`å‚æ•°æ§åˆ¶ã€‚

#### æŒ‡å®šå¹¿æ’­

1. sparkSQL åŠ  HINT æ–¹å¼
2. ä½¿ç”¨ function._ broadcast API 

### SMB Join

å¤§è¡¨ JOIN å¤§è¡¨ï¼Œè¿›è¡Œ SMBï¼ˆsort merge bucketï¼‰æ“ä½œï¼š

éœ€è¦è¿›è¡Œåˆ†æ¡¶ï¼Œé¦–å…ˆä¼šè¿›è¡Œæ’åºï¼Œç„¶åæ ¹æ® key å€¼åˆå¹¶ï¼ŒæŠŠç›¸åŒ key çš„æ•°æ®æ”¾åˆ°åŒä¸€ä¸ª bucket ä¸­ï¼ˆæŒ‰ç…§ key è¿›è¡Œ hashï¼‰ã€‚åˆ†æ¡¶çš„ç›®çš„å°±æ˜¯æŠŠå¤§è¡¨åŒ–æˆå°è¡¨ã€‚ç›¸åŒçš„ key çš„æ•°æ®éƒ½åœ¨åŒä¸€ä¸ªæ¡¶ä¸­ï¼Œå†è¿›è¡Œ join æ“ä½œï¼Œé‚£ä¹ˆåœ¨è”åˆçš„æ—¶å€™å°±ä¼šå¤§å¹…åº¦çš„å‡å°æ— å…³é¡¹çš„æ‰«æã€‚

## æ•°æ®å€¾æ–œ

### ç°è±¡

ç»å¤§å¤šæ•° Task ä»»åŠ¡è¿è¡Œé€Ÿåº¦å¾ˆå¿«ï¼Œä½†å‡ ä¸ª Task ä»»åŠ¡è¿è¡Œé€Ÿåº¦æå…¶ç¼“æ…¢ï¼Œæ…¢æ…¢çš„å¯èƒ½æ¥ç€æŠ¥å†…å­˜æº¢å‡ºçš„é—®é¢˜ã€‚

![image-20220822170107186](image-20220822170107186.png "Taskæ•°æ®å€¾æ–œ")

### åŸå› 

æ•°æ®å€¾æ–œå‘ç”Ÿåœ¨ shuffle ç±»çš„ç®—å­ï¼Œæ¯”å¦‚ distinctã€groupByKeyã€reduceByKeyã€aggregateByKeyã€joinã€cogroup ç­‰ï¼Œæ¶‰åŠåˆ°æ•°æ®é‡åˆ†åŒºï¼Œå¦‚æœå…¶ä¸­æŸä¸€ä¸ª key æ•°é‡ç‰¹åˆ«å¤§ï¼Œå°±å‘ç”Ÿäº†æ•°æ®å€¾æ–œã€‚éœ€è¦å…ˆå¯¹å¤§ Key è¿›è¡Œå®šä½ã€‚

### æ•°æ®å€¾æ–œå¤§ key å®šä½

ä½¿ç”¨æŠ½å–é‡‡æ ·æ–¹æ³•

```scala
val top10key = df
.select(keyColumn).sample(false, 0.1).rdd //æŠ½å– 10%
.map(k => (k, 1)).reduceByKey(_+_)
.map(k => (k._2, k._1)).sortByKey(false) //æŒ‰ç»Ÿè®¡çš„keyè¿›è¡Œæ’åº
.take(10)
```

### å•è¡¨æ•°æ®å€¾æ–œä¼˜åŒ–

#### å•è¡¨ä¼˜åŒ–

ä¸ºäº†å‡å°‘ shuffle ä»¥åŠ reduce ç«¯çš„å‹åŠ›ï¼ŒSparkSQL ä¼šåœ¨ map ç«¯ä¼šåšä¸€ä¸ª partial aggregateï¼ˆé¢„èšåˆæˆ–è€…åèšåˆï¼‰ï¼Œå³åœ¨ shuffle å‰å°†åŒä¸€åˆ†åŒºå†…æ‰€å±åŒ key çš„è®°å½•å…ˆè¿›è¡Œä¸€ä¸ªé¢„ç»“ç®—ï¼Œå†å°†ç»“æœè¿›è¡Œ shuffleï¼Œå‘é€åˆ° reduce ç«¯åšä¸€ä¸ªæ±‡æ€»ï¼Œç±»ä¼¼ MR çš„æå‰ Combinerï¼Œæ‰€ä»¥æ‰§è¡Œè®¡åˆ’ä¸­ Hashaggregate é€šå¸¸æˆå¯¹å‡ºç°ã€‚

#### äºŒæ¬¡èšåˆ

```hive
select 
	id,
	sum(course) total
from
(
	select 
		remove_random_prefix(random_courseid) courseid,
		course
	from
	(
    	select 
        	random_id,
        	sum(sellmoney) course
        from
        (
        	select
            	random_prefix(id, 6) random_id,
            	sellmoney
            from
            	doubleAggre
        )t1
        group by random_id
    )t2
)t3
group by id
```

```scala
def randomPrefixUDF(value, num):String = {
    new Random().nextInt(num).toString + "_" + value
}

def removeRandomPrefixUDF(value):String = {
    value.toString.split("_")(1)
}
```

### Joinæ•°æ®å€¾æ–œä¼˜åŒ–

#### å¹¿æ’­Join

##### é€šè¿‡å‚æ•°æŒ‡å®šè‡ªåŠ¨å¹¿æ’­

å¹¿æ’­ join é»˜è®¤å€¼ä¸º 10MBï¼Œç”± `spark.sql.autoBroadcastJoinThreshold`å‚æ•°æ§åˆ¶ã€‚

##### æŒ‡å®šå¹¿æ’­

1. sparkSQL åŠ  HINT æ–¹å¼
2. ä½¿ç”¨ function._ broadcast API 

#### æ‹†åˆ†å¤§ key æ‰“æ•£å¤§è¡¨ æ‰©å®¹å°è¡¨

> ä¸å•è¡¨æ•°æ®å€¾æ–œä¼˜åŒ–çš„äºŒæ¬¡èšåˆä¸åŒï¼Œjoin æ•°æ®å€¾æ–œè°ƒä¼˜è¦å¯¹ä¸¤è¡¨éƒ½è¿›è¡Œè°ƒæ•´ã€‚
>
> å› ä¸ºå¤§è¡¨ä¸ºäº†åˆ†åŒºåŠ å…¥äº†å‰ç¼€ï¼Œä¸ºäº†å’Œå°è¡¨åŒ¹é…ä¸Šï¼Œå°è¡¨ä¹Ÿåº”å»ºç«‹å¯¹åº”çš„å‰ç¼€ä¸ä¹‹åŒ¹é…ã€‚å¦‚ï¼šï¼ˆå‡è®¾æœ‰3ä¸ª taskï¼ŒæŠŠå¤§keyé‡æ–°æ‰“æ•£åˆ°æ‰€æœ‰taskä¸Šï¼‰

|                                                              |      |                 |                                                  |                                                              |       |                                     |
| ------------------------------------------------------------ | ---- | --------------- | ------------------------------------------------ | ------------------------------------------------------------ | ----- | ----------------------------------- |
| 1<br />1<br />1<br />1<br />1<br />1<br />1<br />1<br />2<br />3 | Join | 1<br />2<br />3 | æ‹†åˆ†å¤§ key<br />æ‰“æ•£å¤§è¡¨<br />æ‰©å®¹å°è¡¨<br />---> | 0_1<br />1_1<br />2_1<br />0_1<br />1_1<br />2_1<br />0_1<br />1_1 | Join  | 0_1<br />1_1<br />2_1<br />2<br />3 |
|                                                              |      |                 |                                                  |                                                              | Union |                                     |
|                                                              |      |                 |                                                  | 2<br />3                                                     | Join  | 1<br />2<br />3                     |



1. æ‹†åˆ†å€¾æ–œçš„ keyï¼šæ ¹æ® key è¿‡æ»¤å‡ºå€¾æ–œçš„æ•°æ®å’Œé™¤å€¾æ–œå¤–çš„å…¶ä»–æ•°æ®ï¼›
2. å°†åˆ‡æ–œçš„ key æ‰“æ•£ï¼šæ‰“æ•£æˆ task æ•°é‡çš„ä»½æ•°(æ¯”å¦‚æœ‰36ä¸ªtask)ï¼Œkey å€¼å‰åŠ (0 ~ 36)éšæœºæ•°ï¼›
3. å°è¡¨è¿›è¡Œæ‰©å®¹ï¼šæ‰©å¤§æˆ task æ•°é‡çš„ä»½æ•°ï¼Œkey å€¼ç”¨ flatmap ç”Ÿæˆ 36 ä»½ï¼Œ`i + "_" + key`
4. å€¾æ–œçš„å¤§ key ä¸æ‰©å®¹åçš„è¡¨è¿›è¡Œjoinï¼›
5. æ²¡æœ‰å€¾æ–œçš„ keyä¸åŸæ¥çš„è¡¨è¿›è¡Œjoinï¼›
6. å°†å€¾æ–œ key join åçš„ç»“æœä¸æ™®é€š key join åçš„ç»“æœï¼Œunionèµ·æ¥ã€‚

------

ğŸ‘‹æœªå®Œå¾…ç»­ğŸ‘‹

