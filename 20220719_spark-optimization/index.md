# Spark Performance Optimization


Spark SQL is the top active component in spark 3.0 release. Most of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in latest release.

<!--more-->

## Explain æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’

### è¯­æ³•

```scala
.explain(mode="xxx")
```

+ `explain(mode="simple")`: åªå±•ç¤ºç‰©ç†æ‰§è¡Œè®¡åˆ’
+ `explain(mode="extended")`: å±•ç¤ºç‰©ç†è®¡åˆ’å’Œé€»è¾‘æ‰§è¡Œè®¡åˆ’
+ `"codegen"`: å±•ç¤º codegen ç”Ÿæˆçš„å¯æ‰§è¡Œ Java ä»£ç 
+ `"cost"`: å±•ç¤ºä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ä»¥åŠç›¸å…³çš„ç»Ÿè®¡
+ `"formatted"`: åˆ†éš”è¾“å‡ºï¼Œè¾“å‡ºæ›´æ˜“è¯»çš„ç‰©ç†æ‰§è¡Œè®¡åˆ’å¹¶å±•ç¤ºæ¯ä¸ªèŠ‚ç‚¹çš„è¯¦ç»†ä¿¡æ¯

### æ‰§è¡Œè®¡åˆ’å¤„ç†æµç¨‹

![image-20220819153115834](image-20220819153115834.png "å¤„ç†æµç¨‹")

![image-20220819153308501](image-20220819153308501.png "æ ¸å¿ƒè¿‡ç¨‹")

**ğŸ”´Unresolved é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼š== Parsed Logical Plan ==**

+ Parser ç»„ä»¶æ£€æŸ¥ SQL è¯­æ³•æ˜¯å¦æœ‰é—®é¢˜ï¼Œç„¶åç”Ÿæˆ Unresolved çš„é€»è¾‘è®¡åˆ’ï¼Œä¸æ£€æŸ¥è¡¨åã€ä¸æ£€æŸ¥åˆ—æ˜

**ğŸŸ Resolved é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼š==Analyzed Logical Plan ==**

+ Spark ä¸­çš„ Catalog å­˜å‚¨åº“æ¥è§£æéªŒè¯è¯­ä¹‰ã€åˆ—åã€ç±»å‹ã€è¡¨åç­‰

ğŸŸ¡**ä¼˜åŒ–åçš„é€»è¾‘æ‰§è¡Œè®¡åˆ’ï¼š== Optimized Logical Plan ==**

+ Catalyst ä¼˜åŒ–å™¨æ ¹æ®å„ç§è§„åˆ™è¿›è¡Œä¼˜åŒ–

**ğŸŸ¢ç‰©ç†æ‰§è¡Œè®¡åˆ’ï¼š== Physical Plan ==**

1. `HashAggregate` è¿ç®—ç¬¦è¡¨ç¤ºæ•°æ®èšåˆï¼Œä¸€èˆ¬ HashAggregate æ˜¯æˆå¯¹å‡ºç°ï¼Œç¬¬ä¸€ä¸ª HashAggregate æ˜¯å°†æ‰§è¡ŒèŠ‚ç‚¹æœ¬åœ°çš„æ•°æ®è¿›è¡Œå±€éƒ¨èšåˆï¼Œå¦ä¸€ä¸ª HashAggregate æ˜¯å°†å„ä¸ªåˆ†åŒºçš„æ•°æ®è¿›è¡Œèšåˆè®¡ç®—
2. `Exchange` è¿ç®—ç¬¦å…¶å®å°±æ˜¯ shuffleï¼Œè¡¨ç¤ºéœ€è¦åœ¨é›†ç¾¤ä¸Šç§»åŠ¨æ•°æ®ã€‚å¾ˆå¤šæ—¶å€™ HashAggregate ä¼šä»¥ Exchange åˆ†éš”å¼€
3. `Project`è¿ç®—ç¬¦æ˜¯ SQL ä¸­çš„é€‰æ‹©åˆ—ï¼Œselect name, age
4. `BroadcastHashJoin`è¡¨ç¤ºé€šè¿‡åŸºäºå¹¿æ’­æ–¹å¼è¿›è¡Œ HashJoin
5. `LocalTableScan` è¡¨ç¤ºå…¨è¡¨æ‰«ææœ¬åœ°çš„è¡¨

## èµ„æºè°ƒä¼˜

### èµ„æºè§„åˆ’

#### èµ„æºè®¾å®šè€ƒè™‘

##### æ€»ä½“åŸåˆ™

å•å°æœåŠ¡å™¨ 128G å†…å­˜ï¼Œ32çº¿ç¨‹ã€‚

å…ˆè®¾å®šå•ä¸ª Executor æ ¸æ•°ï¼Œæ ¹æ® Yarn é…ç½®å¾—å‡ºæ¯ä¸ªèŠ‚ç‚¹æœ€å¤šçš„ Executor æ•°é‡ï¼Œ(Yarn æ€»æ ¸æ•° / æ¯ä¸ªexecutoræ ¸æ•°(é€šå¸¸ä¸º4)) = å•ä¸ªèŠ‚ç‚¹çš„executoræ•°é‡ï¼›

28 / 4 = 7 å•ä¸ªèŠ‚ç‚¹çš„executoræ•°é‡ï¼›

æ€»çš„ executor æ•° = å•èŠ‚ç‚¹executoræ•°é‡ * nmèŠ‚ç‚¹æ•°ã€‚

##### å…·ä½“æäº¤å‚æ•°

1. executor-cores

   æ¯ä¸ª executor çš„æœ€å¤§æ ¸æ•°ã€‚3 ~ 6 ä¹‹é—´æ¯”è¾ƒåˆç†ï¼Œé€šå¸¸ä¸º4

2. num-executors

   num-executors = æ¯ä¸ªèŠ‚ç‚¹çš„ executor æ•° * work èŠ‚ç‚¹æ•°ï¼›

   æ¯ä¸ª node çš„ executor æ•° = å•èŠ‚ç‚¹ yarn æ€»æ ¸æ•° / æ¯ä¸ª executor çš„æœ€å¤§ cpu æ ¸æ•°ï¼›

   32çº¿ç¨‹æœ‰28çº¿ç¨‹ç”¨åœ¨ Yarn ä¸Šï¼›

   é‚£ä¹ˆæ¯ä¸ª node çš„ executor æ•° = 28 / 4 = 7ï¼›

   å‡è®¾é›†ç¾¤èŠ‚ç‚¹ä¸º10ï¼›

   é‚£ä¹ˆ num-executors = 7 * 10 = 70ã€‚

3. executor-memoryâ­

   executor-memory = Yarn å†…å­˜ / å•ä¸ªèŠ‚ç‚¹çš„executoræ•°é‡ï¼›

   100G(æ€»128G, 100G ç»™ Yarn) / 7 = 14G; 

#### å†…å­˜è®¾ç½®

**ä¸€ä¸ª executor å†…éƒ¨**

![image-20220819173801820](image-20220819173801820.png "å †å†…å­˜æ¨¡å‹")

ğŸŸ¢ä¼°ç®— Other å†…å­˜ = è‡ªå®šä¹‰æ•°æ®ç»“æ„ * æ¯ä¸ª Executor æ ¸æ•°

ğŸ”µä¼°ç®— Storage å†…å­˜ = å¹¿æ’­å˜é‡ + cache/Executor æ•°é‡

ğŸŸ£ä¼°ç®— Executor å†…å­˜ = æ¯ä¸ª Executor æ ¸æ•° * (æ•°æ®é›†å¤§å°/å¹¶è¡Œåº¦)

* Sparksql å¹¶è¡Œåº¦é»˜è®¤ä¸º 200ï¼Œå¹¶è¡Œåº¦å³ä¸º Task æ•°é‡

### æŒä¹…åŒ–å’Œåºåˆ—åŒ–

#### RDD

##### Kryo åºåˆ—åŒ–ç¼“å­˜

ä½¿ç”¨ï¼š

1. sparkConf æŒ‡å®š kryo åºåˆ—åŒ–å™¨
2. sparkConf æ³¨å†Œæ ·ä¾‹ç±»

```scala
new SparkConf()
...
.set("spark.serializer","...kryoSerializer")
.registerKryoClasses(Array(classOf[...]))

result.persist(StorageLevel.MEMORY_ONLY_SER)
```

æµ‹è¯•ï¼š

> 2G çš„ HIVE å…ƒæ•°æ®ï¼Œä½¿ç”¨ RDD ç¼“å­˜ï¼Œå®Œæˆ 100% Fraction Cached éœ€è¦ 7 Gå·¦å³å†…å­˜ï¼Œä½¿ partition å¾ˆå®¹æ˜“æŒ‚æ‰ã€‚ä½¿ç”¨ Kryo åºåˆ—å™¨å®Œæˆ Cached éœ€è¦ 1 Gå†…å­˜ã€‚

#### DFã€DS

cache é»˜è®¤ä½¿ç”¨ `MEMORY_AND_DISK`ç¼“å­˜

{{< admonition note>}}

1. åºåˆ—åŒ–å™¨(Java, Kryo)æ˜¯é’ˆå¯¹ RDD è€Œè¨€çš„ï¼›è€Œ DFã€DS æ˜¯ç”± encoder é€‰æ‹©çš„ã€‚
2. encoder ç”± SparkSql è‡ªå·±å®ç°çš„ï¼Œä¹Ÿæœ‰å¯èƒ½ä½¿ç”¨ kryo çš„æ–¹å¼ã€‚
3. å¯¹ DFã€DSä½¿ç”¨åºåˆ—åŒ–å·®åˆ«ä¸å¤§ã€‚

{{< /admonition >}}

###  CPUä¼˜åŒ–

#### CPU ä½æ•ˆåŸå› 

##### å¹¶è¡Œåº¦

å¹¶è¡Œåº¦å°±æ˜¯ Task æ•°é‡ã€‚

ğŸŸ RDD å¹¶è¡Œåº¦å‚æ•°ï¼š

+ `spark.default.parallelism`

+ ä¸è®¾ç½®æ—¶ï¼Œé»˜è®¤ç”± joinã€reduceByKey å’Œ parallelize ç­‰è½¬æ¢å†³å®šã€‚

ğŸŸ¡SparkSQL å¹¶è¡Œåº¦å‚æ•°ï¼šä¸ RDD å¹¶è¡Œåº¦äº’ä¸å½±å“

+ `spark.sql.shuffle.partitions`

+ é»˜è®¤æ˜¯ 200ï¼Œåªèƒ½æ§åˆ¶ SparkSQLã€DataFrameã€Dataset åˆ†åŒºä¸ªæ•°ã€‚

##### å¹¶å‘åº¦

å¹¶å‘åº¦ï¼šåŒæ—¶æ‰§è¡Œçš„ Task æ•°é‡ã€‚

##### CPU ä½æ•ˆåŸå› 

1. å¹¶è¡Œåº¦è¾ƒä½ã€æ•°æ®åˆ†ç‰‡è¾ƒå¤§å®¹æ˜“å¯¼è‡´ CPU çº¿ç¨‹æŒ‚èµ·ï¼›
2. å¹¶è¡Œåº¦è¿‡é«˜ã€æ•°æ®è¿‡äºåˆ†æ•£ä¼šè®©è°ƒåº¦å¼€é”€æ›´å¤šï¼›

#### CPU èµ„æºè°ƒæ•´

`spark-submit --master yarn --deploy-mode client --driver-memory 1g --num-executors 3 --executor-cores 4 --executor-memory 6g --class com.jar`

ğŸŸ£å®˜æ–¹æ¨èå¹¶è¡Œåº¦ï¼ˆTask æ•°ï¼‰è®¾ç½®æˆå¹¶å‘åº¦ï¼ˆvcore æ•°ï¼‰çš„ 2 å€åˆ° 3 å€ã€‚

ä¾‹ï¼šå¦‚æœä»¥ç›®å‰çš„èµ„æºï¼ˆ3 ä¸ª executorï¼‰å»æäº¤ï¼Œæ¯ä¸ª executor æœ‰ä¸¤ä¸ªæ ¸ï¼Œæ€»å…± 6 ä¸ªæ ¸ï¼Œåˆ™å¹¶è¡Œåº¦è®¾ç½®ä¸º 12 ~ 18ã€‚

```scala
SparkConf()
...
.set("spark.sql.shuffle.partitions", "18")
```

## SparkSQL è¯­æ³•ä¼˜åŒ–

### åŸºäº RBO ä¼˜åŒ–

#### è°“è¯ä¸‹æ¨

```scala
//=============Inner on å·¦è¡¨=============
spark.sqlContext.sql(
  """
    |select
    | l.id,
    | l.name,
    | r.id,
    | r.name
    |from course l join student r
    | on l.id=r.id and l.dt=r.dt and l.dn=r.dn
    |on l.id<2
    |""".stripMargin)
//=============Inner where å·¦è¡¨=============
spark.sqlContext.sql(
  """
    |select
    | l.id,
    | l.name,
    | r.id,
    | r.name
    |from course l join student r
    | on l.id=r.id and l.dt=r.dt and l.dn=r.dn
    |where l.id<2
    |""".stripMargin)
```

**inner join**

+ æ— è®ºæ˜¯ ON è¿˜æ˜¯ WHEREï¼Œæ— è®ºæ¡ä»¶æ˜¯å³è¡¨è¿˜æ˜¯å·¦è¡¨ã€‚ä» logic plan -> Analyzed logical plan åˆ° **optimized logical plan**ï¼ŒsparkSQL éƒ½ä¼šä¼˜åŒ–**å…ˆè¿‡æ»¤æ•°æ®å†è¿›è¡Œ join** è¿æ¥ï¼Œè€Œä¸”å…¶ä¸­ä¸€è¡¨è¿‡æ»¤ï¼Œ**å¦ä¸€è¡¨ä¹Ÿä¼˜åŒ–æå‰è¿‡æ»¤**ï¼ˆæœ€ç»ˆè¦è¿‡æ»¤æ•°æ®ï¼Œå¦ä¸€è¡¨ä¹Ÿæ²¡æœ‰å­˜åœ¨çš„å¿…è¦ï¼‰

**left join**

+ |                 | æ¡ä»¶åœ¨ å·¦è¡¨ | æ¡ä»¶åœ¨ å³è¡¨ |
  | --------------- | ----------- | ----------- |
  | æ¡ä»¶åœ¨ on å    | åªä¸‹æ¨å³è¡¨  | åªä¸‹æ¨å³è¡¨  |
  | æ¡ä»¶åœ¨ where å | ä¸¤è¡¨éƒ½ä¸‹æ¨  | ä¸¤è¡¨éƒ½ä¸‹æ¨  |

+ æ³¨æ„ï¼šå¤–å…³è”æ—¶ï¼Œè¿‡æ»¤æ¡ä»¶åœ¨ on ä¸ whereï¼Œè¯­ä¹‰æ˜¯ä¸åŒçš„ï¼Œç»“æœä¹Ÿæ˜¯ä¸åŒçš„ã€‚

#### åˆ—è£å‰ª

æ‰«ææ•°æ®æºçš„æ—¶å€™ï¼Œåªè¯»å–é‚£äº›ä¸æŸ¥è¯¢ç›¸å…³çš„å­—æ®µã€‚

#### å¸¸é‡æ›¿æ¢

Catalyst ä¼šä½¿ç”¨ constantFolding è§„åˆ™ï¼Œè‡ªåŠ¨ç”¨è¡¨è¾¾å¼çš„ç»“æœè¿›è¡Œæ›¿æ¢ã€‚

### åŸºäº CBO ä¼˜åŒ–

#### Statistics æ”¶é›†

éœ€è¦å…ˆæ‰§è¡Œç‰¹å®šçš„ SQL è¯­å¥æ¥æ”¶é›†æ‰€éœ€çš„è¡¨å’Œåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ã€‚

ğŸ”µ ç”Ÿæˆè¡¨çº§åˆ«ç»Ÿè®¡ä¿¡æ¯ï¼ˆæ‰«è¡¨ï¼‰ï¼š

`ANALYZE TABLE è¡¨æ˜ COMPUTE STATISTICS`

#### ä½¿ç”¨ CBO

é€šè¿‡ `spark.sql.cbo.enabled` æ¥å¼€å¯ï¼Œé»˜è®¤æ˜¯ falseã€‚CBO ä¼˜åŒ–å™¨å¯ä»¥åŸºäºè¡¨å’Œåˆ—çš„ç»Ÿè®¡ä¿¡æ¯ï¼Œé€‰æ‹©å‡ºæœ€ä¼˜çš„æŸ¥è¯¢è®¡åˆ’ã€‚æ¯”å¦‚ï¼šBuild ä¾§é€‰æ‹©ã€ä¼˜åŒ– Join ç±»å‹ã€ä¼˜åŒ–å¤šè¡¨ Join é¡ºåºã€‚

### å¹¿æ’­ Join

#### é€šè¿‡å‚æ•°æŒ‡å®šè‡ªåŠ¨å¹¿æ’­

å¹¿æ’­ join é»˜è®¤å€¼ä¸º 10MBï¼Œç”± `spark.sql.autoBroadcastJoinThreshold`å‚æ•°æ§åˆ¶ã€‚

#### æŒ‡å®šå¹¿æ’­

1. sparkSQL åŠ  HINT æ–¹å¼
2. ä½¿ç”¨ function._ broadcast API 

### SMB Join

å¤§è¡¨ JOIN å¤§è¡¨ï¼Œè¿›è¡Œ SMBï¼ˆsort merge bucketï¼‰æ“ä½œï¼š

éœ€è¦è¿›è¡Œåˆ†æ¡¶ï¼Œé¦–å…ˆä¼šè¿›è¡Œæ’åºï¼Œç„¶åæ ¹æ® key å€¼åˆå¹¶ï¼ŒæŠŠç›¸åŒ key çš„æ•°æ®æ”¾åˆ°åŒä¸€ä¸ª bucket ä¸­ï¼ˆæŒ‰ç…§ key è¿›è¡Œ hashï¼‰ã€‚åˆ†æ¡¶çš„ç›®çš„å°±æ˜¯æŠŠå¤§è¡¨åŒ–æˆå°è¡¨ã€‚ç›¸åŒçš„ key çš„æ•°æ®éƒ½åœ¨åŒä¸€ä¸ªæ¡¶ä¸­ï¼Œå†è¿›è¡Œ join æ“ä½œï¼Œé‚£ä¹ˆåœ¨è”åˆçš„æ—¶å€™å°±ä¼šå¤§å¹…åº¦çš„å‡å°æ— å…³é¡¹çš„æ‰«æã€‚

## æ•°æ®å€¾æ–œ

### ç°è±¡

ç»å¤§å¤šæ•° Task ä»»åŠ¡è¿è¡Œé€Ÿåº¦å¾ˆå¿«ï¼Œä½†å‡ ä¸ª Task ä»»åŠ¡è¿è¡Œé€Ÿåº¦æå…¶ç¼“æ…¢ï¼Œæ…¢æ…¢çš„å¯èƒ½æ¥ç€æŠ¥å†…å­˜æº¢å‡ºçš„é—®é¢˜ã€‚

![image-20220822170107186](image-20220822170107186.png "Taskæ•°æ®å€¾æ–œ")

### åŸå› 

æ•°æ®å€¾æ–œå‘ç”Ÿåœ¨ shuffle ç±»çš„ç®—å­ï¼Œæ¯”å¦‚ distinctã€groupByKeyã€reduceByKeyã€aggregateByKeyã€joinã€cogroup ç­‰ï¼Œæ¶‰åŠåˆ°æ•°æ®é‡åˆ†åŒºï¼Œå¦‚æœå…¶ä¸­æŸä¸€ä¸ª key æ•°é‡ç‰¹åˆ«å¤§ï¼Œå°±å‘ç”Ÿäº†æ•°æ®å€¾æ–œã€‚éœ€è¦å…ˆå¯¹å¤§ Key è¿›è¡Œå®šä½ã€‚

### æ•°æ®å€¾æ–œå¤§ key å®šä½

ä½¿ç”¨æŠ½å–é‡‡æ ·æ–¹æ³•

```scala
val top10key = df
.select(keyColumn).sample(false, 0.1).rdd //æŠ½å– 10%
.map(k => (k, 1)).reduceByKey(_+_)
.map(k => (k._2, k._1)).sortByKey(false) //æŒ‰ç»Ÿè®¡çš„keyè¿›è¡Œæ’åº
.take(10)
```

### å•è¡¨æ•°æ®å€¾æ–œä¼˜åŒ–

#### å•è¡¨ä¼˜åŒ–

ä¸ºäº†å‡å°‘ shuffle ä»¥åŠ reduce ç«¯çš„å‹åŠ›ï¼ŒSparkSQL ä¼šåœ¨ map ç«¯ä¼šåšä¸€ä¸ª partial aggregateï¼ˆé¢„èšåˆæˆ–è€…åèšåˆï¼‰ï¼Œå³åœ¨ shuffle å‰å°†åŒä¸€åˆ†åŒºå†…æ‰€å±åŒ key çš„è®°å½•å…ˆè¿›è¡Œä¸€ä¸ªé¢„ç»“ç®—ï¼Œå†å°†ç»“æœè¿›è¡Œ shuffleï¼Œå‘é€åˆ° reduce ç«¯åšä¸€ä¸ªæ±‡æ€»ï¼Œç±»ä¼¼ MR çš„æå‰ Combinerï¼Œæ‰€ä»¥æ‰§è¡Œè®¡åˆ’ä¸­ Hashaggregate é€šå¸¸æˆå¯¹å‡ºç°ã€‚

#### äºŒæ¬¡èšåˆ

```hive
select 
	id,
	sum(course) total
from
(
	select 
		remove_random_prefix(random_courseid) courseid,
		course
	from
	(
    	select 
        	random_id,
        	sum(sellmoney) course
        from
        (
        	select
            	random_prefix(id, 6) random_id,
            	sellmoney
            from
            	doubleAggre
        )t1
        group by random_id
    )t2
)t3
group by id
```

```scala
def randomPrefixUDF(value, num):String = {
    new Random().nextInt(num).toString + "_" + value
}

def removeRandomPrefixUDF(value):String = {
    value.toString.split("_")(1)
}
```

### Joinæ•°æ®å€¾æ–œä¼˜åŒ–

#### å¹¿æ’­Join

##### é€šè¿‡å‚æ•°æŒ‡å®šè‡ªåŠ¨å¹¿æ’­

å¹¿æ’­ join é»˜è®¤å€¼ä¸º 10MBï¼Œç”± `spark.sql.autoBroadcastJoinThreshold`å‚æ•°æ§åˆ¶ã€‚

##### æŒ‡å®šå¹¿æ’­

1. sparkSQL åŠ  HINT æ–¹å¼
2. ä½¿ç”¨ function._ broadcast API 

#### æ‹†åˆ†å¤§ key æ‰“æ•£å¤§è¡¨ æ‰©å®¹å°è¡¨

> ä¸å•è¡¨æ•°æ®å€¾æ–œä¼˜åŒ–çš„äºŒæ¬¡èšåˆä¸åŒï¼Œjoin æ•°æ®å€¾æ–œè°ƒä¼˜è¦å¯¹ä¸¤è¡¨éƒ½è¿›è¡Œè°ƒæ•´ã€‚
>
> å› ä¸ºå¤§è¡¨ä¸ºäº†åˆ†åŒºåŠ å…¥äº†å‰ç¼€ï¼Œä¸ºäº†å’Œå°è¡¨åŒ¹é…ä¸Šï¼Œå°è¡¨ä¹Ÿåº”å»ºç«‹å¯¹åº”çš„å‰ç¼€ä¸ä¹‹åŒ¹é…ã€‚å¦‚ï¼šï¼ˆå‡è®¾æœ‰3ä¸ª taskï¼ŒæŠŠå¤§keyé‡æ–°æ‰“æ•£åˆ°æ‰€æœ‰taskä¸Šï¼‰

|                                                              |      |                 |                                                  |                                                              |       |                                     |
| :----------------------------------------------------------: | :--: | :-------------: | :----------------------------------------------: | :----------------------------------------------------------: | :---: | :---------------------------------: |
| 1<br />1<br />1<br />1<br />1<br />1<br />1<br />1<br />2<br />3 | Join | 1<br />2<br />3 | æ‹†åˆ†å¤§ key<br />æ‰“æ•£å¤§è¡¨<br />æ‰©å®¹å°è¡¨<br />---> | 0_1<br />1_1<br />2_1<br />0_1<br />1_1<br />2_1<br />0_1<br />1_1 | Join  | 0_1<br />1_1<br />2_1<br />2<br />3 |
|                                                              |      |                 |                                                  |                                                              | Union |                                     |
|                                                              |      |                 |                                                  |                           2<br />3                           | Join  |           1<br />2<br />3           |



1. æ‹†åˆ†å€¾æ–œçš„ keyï¼šæ ¹æ® key è¿‡æ»¤å‡ºå€¾æ–œçš„æ•°æ®å’Œé™¤å€¾æ–œå¤–çš„å…¶ä»–æ•°æ®ï¼›
2. å°†åˆ‡æ–œçš„ key æ‰“æ•£ï¼šæ‰“æ•£æˆ task æ•°é‡çš„ä»½æ•°(æ¯”å¦‚æœ‰36ä¸ªtask)ï¼Œkey å€¼å‰åŠ (0 ~ 36)éšæœºæ•°ï¼›
3. å°è¡¨è¿›è¡Œæ‰©å®¹ï¼šæ‰©å¤§æˆ task æ•°é‡çš„ä»½æ•°ï¼Œkey å€¼ç”¨ flatmap ç”Ÿæˆ 36 ä»½ï¼Œ`i + "_" + key`
4. å€¾æ–œçš„å¤§ key ä¸æ‰©å®¹åçš„è¡¨è¿›è¡Œjoinï¼›
5. æ²¡æœ‰å€¾æ–œçš„ keyä¸åŸæ¥çš„è¡¨è¿›è¡Œjoinï¼›
6. å°†å€¾æ–œ key join åçš„ç»“æœä¸æ™®é€š key join åçš„ç»“æœï¼Œunionèµ·æ¥ã€‚

ä»£ä»·ï¼šshuffle æ¬¡æ•°å¢å¤šäº†ï¼Œä½†æ˜¯æ¯æ¬¡ shuffle æ•°æ®æ›´å‡åŒ€äº†ã€‚

## Job ä¼˜åŒ–

### Map ç«¯ä¼˜åŒ–

#### Map ç«¯èšåˆ

parkSQL ä¼šåœ¨ map ç«¯ä¼šåšä¸€ä¸ª partial aggregateï¼ˆé¢„èšåˆæˆ–è€…åèšåˆï¼‰ï¼Œå³åœ¨ shuffle å‰å°†åŒä¸€åˆ†åŒºå†…æ‰€å±åŒ key çš„è®°å½•å…ˆè¿›è¡Œä¸€ä¸ªé¢„ç»“ç®—ï¼Œå†å°†ç»“æœè¿›è¡Œ shuffleï¼Œå‘é€åˆ° reduce ç«¯åšä¸€ä¸ªæ±‡æ€»ï¼Œç±»ä¼¼ MR çš„æå‰ Combinerï¼Œæ‰€ä»¥æ‰§è¡Œè®¡åˆ’ä¸­ Hashaggregate é€šå¸¸æˆå¯¹å‡ºç°ã€‚

SparkSQL æœ¬èº«çš„ Hashaggregate å°±ä¼šå®ç°æœ¬åœ°é¢„èšåˆ + å…¨å±€èšåˆã€‚

#### è¯»å–å°æ–‡ä»¶çš„ä¼˜åŒ–

HIVE çš„ `CombineHiveInputformat` è¾“å…¥æ ¼å¼ä¼šå°†å°æ–‡ä»¶è¯»åˆ°åŒä¸€ä¸ª Map ç«¯é‡Œé¢å»ã€‚

SparkSQL ä¸­ä¹Ÿä¼šè‡ªåŠ¨åˆå¹¶ï¼Œå‚æ•°å¦‚ä¸‹ï¼š

```shell
spark.sql.files.maxPartitionBytes=128MB #é»˜è®¤ 128m
spark.files.openCostInBytes=4194304 #é»˜è®¤ 4m
```

+ åˆ‡ç‰‡å¤§å° = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))

+ bytesPerCore = totalBytes / defaultParallelism

+ è®¡ç®— totalBytes æ—¶ï¼Œæ¯ä¸ªæ–‡ä»¶éƒ½è¦åŠ ä¸Šä¸€ä¸ª open å¼€é”€ï¼š

  å½“ (æ–‡ä»¶1å¤§å° + openCostInBytes) + (æ–‡ä»¶2å¤§å° + openCostInBytes)+...  <= maxPartitionBytes æ—¶ï¼Œnä¸ªæ–‡ä»¶å¯ä»¥è¯»å…¥åŒä¸€åˆ†åŒºã€‚

#### å¢å¤§ map æº¢å†™æ—¶è¾“å‡ºæµ buffer

![image-20220824110005487](image-20220824110005487.png "SortShuffle")

æºç ç†è§£ï¼š

```scala
/**
 * Spills the current in-memory collection to disk if needed. Attempts to acquire more
 * memory before spilling.
 *
 * @param collection collection to spill to disk
 * @param currentMemory estimated size of the collection in bytes
 * @return true if `collection` was spilled to disk; false otherwise
 */
protected def maybeSpill(collection: C, currentMemory: Long): Boolean = {
  var shouldSpill = false
  if (elementsRead % 32 == 0 && currentMemory >= myMemoryThreshold) {
    // Claim up to double our current memory from the shuffle memory pool
    val amountToRequest = 2 * currentMemory - myMemoryThreshold
    val granted = acquireMemory(amountToRequest)
    myMemoryThreshold += granted
    // If we were granted too little memory to grow further (either tryToAcquire returned 0,
    // or we already had more memory than myMemoryThreshold), spill the current collection
    shouldSpill = currentMemory >= myMemoryThreshold
  }
  shouldSpill = shouldSpill || _elementsRead > numElementsForceSpillThreshold
  // Actually spill
  if (shouldSpill) {
    _spillCount += 1
    logSpillage(currentMemory)
    spill(collection)
    _elementsRead = 0
    _memoryBytesSpilled += currentMemory
    releaseMemory()
  }
  shouldSpill
}
```

1. map ç«¯ Shuffle Write æœ‰ä¸€ä¸ªç¼“å†²åŒºï¼Œåˆå§‹é˜ˆå€¼ 5mï¼Œè¶…è¿‡é˜ˆå€¼å°è¯•å¢åŠ åˆ° 2*å½“å‰ä½¿ç”¨å†…å­˜ï¼Œè‡ªåŠ¨æ‰©å®¹ã€‚å¦‚æœç”³è¯·ä¸åˆ°å†…å­˜ï¼Œåˆ™è¿›è¡Œæº¢å†™ã€‚è¿™ä¸ªå‚æ•°æ˜¯ internalï¼ŒæŒ‡å®šæ— æ•ˆã€‚

2. æº¢å†™æ—¶ä½¿ç”¨è¾“å‡ºæµç¼“å†²åŒºé»˜è®¤ 32kï¼Œè¿™äº›ç¼“å†²åŒºå‡å°‘äº†ç£ç›˜æœç´¢å’Œç³»ç»Ÿè°ƒç”¨æ¬¡æ•°ï¼Œ**é€‚å½“æé«˜å¯ä»¥æå‡æº¢å†™æ•ˆç‡ã€‚**

   | Property Name               | Default | Meaning                                                      |
   | --------------------------- | ------- | ------------------------------------------------------------ |
   | `spark.shuffle.file.buffer` | 32k     | Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files. |

   

3. Shuffle æ–‡ä»¶æ¶‰åŠåˆ°åºåˆ—åŒ–ï¼Œæ˜¯é‡‡ç”¨æ‰¹çš„æ–¹å¼è¯»å†™ï¼Œé»˜è®¤æ²¡æ‰¹æ¬¡ 1 ä¸‡æ¡å»è¯»å†™ï¼Œè®¾ç½®å¾—å¤ªä½ä¼šå¯¼è‡´åœ¨åºåˆ—åŒ–æ—¶è¿‡åº¦å¤åˆ¶ã€‚

### Reduce ç«¯ä¼˜åŒ–

#### åˆç†è®¾ç½® Reduce æ•°é‡ 

`Reduce çš„æ•°é‡ = shuffle åçš„åˆ†åŒºæ•° = ä¹Ÿå°±æ˜¯ Task æ•°é‡ = ä¹Ÿå°±æ˜¯å¹¶è¡Œåº¦ = spark.sql.shuffle.partitions é»˜è®¤çš„200ã€‚`

`å¹¶å‘åº¦æ˜¯æ•´ä¸ªé›†ç¾¤ spark çš„æ ¸æ•°ï¼Œå¦‚å¹¶å‘åº¦æ˜¯12ï¼Œå¹¶è¡Œåº¦ä¸ºå¹¶å‘åº¦çš„ 3~6 å€ï¼Œè®¾ç½®ä¸º 36`

è¿‡å¤šçš„ CPU èµ„æºå‡ºç°ç©ºè½¬æµªè´¹ï¼Œè¿‡å°‘å½±å“ä»»åŠ¡æ€§èƒ½ã€‚

#### è¾“å‡ºäº§ç”Ÿå°æ–‡ä»¶ä¼˜åŒ–

##### Joinåçš„ç»“æœæ’å…¥æ–°è¡¨

join ç»“æœæ’å…¥æ–°è¡¨ï¼Œç”Ÿæˆçš„æ–‡ä»¶æ•°ç­‰äº shuffle å¹¶è¡Œåº¦ï¼Œé»˜è®¤å°±æ˜¯ 200 ä»½æ’å…¥åˆ° hdfs ä¸Šã€‚

è§£å†³æ–¹å¼ä¸€ï¼šåœ¨æ’å…¥è¡¨æ•°æ®å‰è¿›è¡Œç¼©å°åˆ†åŒºæ“ä½œæ¥è§£å†³å°æ–‡ä»¶è¿‡å¤šé—®é¢˜ï¼Œå¦‚ coalesceã€repartition ç®—å­ã€‚

è§£å†³æ–¹å¼äºŒï¼šè°ƒæ•´å¹¶è¡Œåº¦ã€‚

##### åŠ¨æ€åˆ†åŒºæ’å…¥æ•°æ®

1. æ²¡æœ‰ shuffle çš„æƒ…å†µä¸‹ã€‚æœ€å·®çš„æƒ…å†µã€‚æ¯ä¸ª Task ä¸­éƒ½æœ‰å„ä¸ªåˆ†åŒºçš„è®°å½•ï¼Œé‚£æœ€ç»ˆæ–‡ä»¶æ•°è¾¾åˆ° Task æ•°é‡ * è¡¨åˆ†åŒºæ•°ã€‚è¿™ç§æƒ…å†µææ˜“äº§ç”Ÿå°æ–‡ä»¶ã€‚

   ```hive
   INSERT overwrite table A partition (aa)
   SELECT * FROM B;
   ```

![image-20220824122130265](image-20220824122130265.png "åŠ¨æ€åˆ†åŒºæ’å…¥-æ²¡æœ‰shuffle")

2. æœ‰ shuffle çš„æƒ…å†µä¸‹ã€‚ä¸Šé¢çš„ Task æ•°é‡å°±å˜æˆäº† 200ã€‚é‚£ä¹ˆæœ€å·®æƒ…å†µå°±ä¼šæœ‰ 200 * è¡¨åˆ†åŒºæ•°ã€‚

   å½“ `shuffle.partitions` è®¾ç½®å¤§äº†å°æ–‡ä»¶é—®é¢˜å°±äº§ç”Ÿäº†ï¼Œè®¾ç½®å°äº†ï¼Œä»»åŠ¡çš„å¹¶è¡Œåº¦å°±ä¸‹é™äº†ï¼Œæ€§èƒ½éšä¹‹å—åˆ°å½±å“ã€‚

   æœ€ç†æƒ³çš„æƒ…å†µæ˜¯æ ¹æ®åˆ†åŒºå­—æ®µè¿›è¡Œ shuffleï¼Œåœ¨ä¸Šé¢çš„ SQL ä¸­åŠ å…¥ distribute by aaã€‚æŠŠåŒä¸€åˆ†åŒºçš„è®°å½•éƒ½å“ˆå¸Œåˆ°åŒä¸€åˆ†åŒºä¸­å»ï¼Œç”±ä¸€ä¸ª Spark çš„ Task è¿›è¡Œå†™å…¥ï¼Œè¿™æ ·åªä¼šäº§ç”Ÿ N ä¸ªæ–‡ä»¶ï¼Œä½†è¿™ç§æƒ…å†µä¹Ÿå®¹æ˜“å‡ºç°æ•°æ®å€¾æ–œçš„é—®é¢˜ã€‚

![image-20220824122300415](image-20220824122300415.png "åŠ¨æ€åˆ†åŒºæ’å…¥-shuffle")

æ•°æ®å€¾æ–œè§£å†³æ€è·¯ï¼š

ç»“åˆä¹‹å‰è§£å†³æ•°æ®å€¾æ–œçš„æ€è·¯ï¼Œåœ¨ç¡®å®šå“ªä¸ªåˆ†åŒºé”®å€¾æ–œçš„æƒ…å†µä¸‹ï¼Œå°†å€¾æ–œçš„åˆ†åŒºé”®å•ç‹¬æ‹¿å‡ºï¼š

å°†å…¥åº“çš„ SQL æ‹†æˆï¼ˆwhere åˆ†åŒº != å€¾æ–œåˆ†åŒºé”®ï¼‰å’Œï¼ˆwhere åˆ†åŒº = å€¾æ–œåˆ†åŒºé”®ï¼‰ä¸¤ä¸ªéƒ¨åˆ†ï¼Œéå€¾æ–œåˆ†åŒºé”®çš„éƒ¨åˆ†æ­£å¸¸ distribute by åˆ†åŒºå­—æ®µï¼Œå€¾æ–œåˆ†åŒºé”®çš„éƒ¨åˆ† distribute by éšæœºæ•°ï¼š

```hive
#1.éå€¾æ–œéƒ¨åˆ†
INSERT overwrite table A partition (aa)
SELECT *
FROM B where aa != å¤§key
distribute by aa; #ä¸»åŠ¨äº§ç”Ÿ shuffleï¼Œå°†åŒä¸€ä¸ªåˆ†åŒºçš„æ•°æ®æ”¾åˆ°ä¸€ä¸ª task ä¸­ï¼Œå†æ‰§è¡Œå†™å…¥

#2.å€¾æ–œé”®éƒ¨åˆ†
INSERT overwrite table A partition (aa)
SELECT *
FROM B where aa = å¤§key
distribute by cast(rand() * 5 as int); #æ‰“æ•£æˆ5ä»½ï¼Œ5ä¸ªtaskï¼Œå†™å…¥5ä¸ªæ–‡ä»¶
```

#### å¢å¤§ reduce ç¼“å†²åŒºï¼Œå‡å°‘æ‹‰å»æ¬¡æ•°

ä¸€èˆ¬ä¸ä¼šè°ƒæ•´ï¼ŒShuffleReader.scalaï¼Œé»˜è®¤å€¼ reduce ä¸€æ¬¡è¯»å– 48Mã€‚

#### è°ƒèŠ‚ reduce ç«¯æ‹‰å–æ•°æ®é‡è¯•æ¬¡æ•°

ä¸€èˆ¬ä¸ä¼šè°ƒæ•´ï¼Œé»˜è®¤ä¸º 3 æ¬¡ã€‚

#### è°ƒèŠ‚ reduce ç«¯æ‹‰å–æ•°æ®ç­‰å¾…é—´éš”

ä¸€èˆ¬ä¸ä¼šè°ƒæ•´ï¼Œé»˜è®¤ä¸º 5 ç§’ã€‚

#### åˆç†åˆ©ç”¨ bypass

| Property Name                             | Default | Meaning                                                      |
| ----------------------------------------- | ------- | ------------------------------------------------------------ |
| `spark.shuffle.sort.bypassMergeThreshold` | 200     | (Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions. |

å½“ shuffleManager ä¸º SortShuffleManager æ—¶ï¼Œå¦‚æœ shuffle read task çš„æ•°é‡å°äºè¿™ä¸ªé˜ˆå€¼ï¼ˆé»˜è®¤200ï¼‰ä¸”ä¸éœ€è¦ map ç«¯è¿›è¡Œåˆå¹¶æ“ä½œï¼ˆä½¿ç”¨ groupbyã€sum èšåˆç®—å­ä¼šé¢„èšåˆï¼Œä»æ‰§è¡Œè®¡åˆ’å¯ä»¥å¾—çŸ¥æœ‰ä¸€ä¸ª hashaggregate -> exchange -> hashaggregateï¼‰ï¼Œåˆ™shuffle write è¿‡ç¨‹ä¸ä¼šè¿›è¡Œæ’åºï¼Œä½¿ç”¨ `BypassMergeSortShuffleWriter` å»å†™æ•°æ®ï¼Œä½†æœ€åä¼šå°†æ¯ä¸ª task äº§ç”Ÿçš„æ‰€æœ‰ä¸´æ—¶ç£ç›˜æ–‡ä»¶éƒ½åˆå¹¶æˆä¸€ä¸ªæ–‡ä»¶ï¼Œå¹¶ä¼šåˆ›å»ºå•ç‹¬çš„ç´¢å¼•æ–‡ä»¶ã€‚

å½“ä½¿ç”¨ shuffleManager æ—¶ï¼Œå¦‚æœç¡®å®ä¸éœ€è¦æ’åºæ“ä½œï¼Œé‚£ä¹ˆå»ºè®®å°†è¿™ä¸ªå‚æ•°è°ƒå¤§ä¸€äº›ï¼Œå¤§äº shuffle read task çš„æ•°é‡ã€‚é‚£ä¹ˆæ­¤æ—¶å°±ä¼šè‡ªåŠ¨å¯åŠ¨ bypass æœºåˆ¶ï¼Œmap-side å°±ä¸ä¼šè¿›è¡Œæ’åºäº†ï¼Œå‡å°‘äº†æ’åºçš„æ€§èƒ½å¼€é”€ã€‚ä½†è¿™ç§æ–¹å¼ä¸‹ï¼Œä¾ç„¶ä¼šäº§ç”Ÿå¤§é‡çš„ç£ç›˜æ–‡ä»¶ï¼Œå› æ­¤ shuffle write æ€§èƒ½æœ‰å¾…æé«˜ã€‚

```scala
  /**
   * Register a shuffle with the manager and obtain a handle for it to pass to tasks.
   */
  override def registerShuffle[K, V, C](
      shuffleId: Int,
      numMaps: Int,
      dependency: ShuffleDependency[K, V, C]): ShuffleHandle = {
    if (SortShuffleWriter.shouldBypassMergeSort(SparkEnv.get.conf, dependency)) {
      // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don't
      // need map-side aggregation, then write numPartitions files directly and just concatenate
      // them at the end. This avoids doing serialization and deserialization twice to merge
      // together the spilled files, which would happen with the normal code path. The downside is
      // having multiple files open at a time and thus more memory allocated to buffers.
      new BypassMergeSortShuffleHandle[K, V](
        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
    } else if (SortShuffleManager.canUseSerializedShuffle(dependency)) {
      // Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:
      new SerializedShuffleHandle[K, V](
        shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]])
    } else {
      // Otherwise, buffer map outputs in a deserialized form:
      new BaseShuffleHandle(shuffleId, numMaps, dependency)
    }
  }
```

### æ•´ä½“ä¼˜åŒ–

#### è°ƒèŠ‚æ•°æ®æœ¬åœ°åŒ–ç­‰å¾…æ—¶é•¿

åœ¨ Spark é¡¹ç›®å¼€å‘é˜¶æ®µï¼Œå¯ä»¥ä½¿ç”¨ client æ¨¡å¼å¯¹ç¨‹åºè¿›è¡Œæµ‹è¯•ï¼Œæ­¤æ—¶å¯ä»¥çœ‹åˆ°æ¯”è¾ƒå…¨çš„æ—¥å¿—ä¿¡æ¯ï¼Œæ—¥å¿—ä¿¡æ¯ä¸­æœ‰æ˜ç¡®çš„ task æ•°æ®æœ¬åœ°åŒ–çš„çº§åˆ«ï¼Œå¦‚æœå¤§éƒ¨åˆ†éƒ½æ˜¯ Process_LOCAL(è¿›ç¨‹æœ¬åœ°åŒ–: æ•°æ®å’Œè®¡ç®—æ˜¯åœ¨åŒä¸€ä¸ªJVMè¿›ç¨‹é‡Œé¢)ã€NODE_LOCAL(èŠ‚ç‚¹æœ¬åœ°åŒ–: æ•°æ®å’Œè®¡ç®—æ˜¯åœ¨åŒä¸€ä¸ªæœåŠ¡å™¨ä¸Š)ï¼Œé‚£ä¹ˆå°±æ— éœ€è¿›è¡Œè°ƒèŠ‚ï¼Œä½†å¦‚æœå¾ˆå¤šæ˜¯ RACK_LOCAL(æœºæ¶æœ¬åœ°åŒ–: æ•°æ®å’Œè®¡ç®—æ˜¯åœ¨åŒä¸€ä¸ªæœºæ¶ä¸Š)ã€ANYï¼Œé‚£ä¹ˆéœ€è¦å¯¹æœ¬åœ°åŒ–çš„ç­‰å¾…æ—¶é•¿è¿›è¡Œè°ƒèŠ‚ï¼Œæ…¢æ…¢è°ƒæ•´å¤§ä¸€äº›ï¼Œåº”è¯¥æ˜¯åå¤è°ƒèŠ‚ï¼Œæ¯æ¬¡è°ƒèŠ‚åè§‚å¯Ÿè¿è¡Œæ—¥å¿—ï¼Œçœ‹çœ‹å¤§éƒ¨åˆ†çš„ task çš„æœ¬åœ°åŒ–çº§åˆ«æœ‰æ²¡æœ‰æå‡ï¼Œè§‚å¯Ÿæ•´ä¸ª spark ä½œä¸šçš„è¿è¡Œæ—¶é—´æœ‰æ²¡æœ‰ç¼©çŸ­ã€‚

#### ä½¿ç”¨å †å¤–å†…å­˜

å †å¤–å†…å­˜å¯ä»¥å‡è½»åƒåœ¾å›æ”¶çš„å·¥ä½œï¼Œä¹ŸåŠ å¿«äº†å¤åˆ¶çš„é€Ÿåº¦ã€‚

å½“éœ€è¦ç¼“å­˜éå¸¸å¤§çš„æ•°æ®é‡æ—¶ï¼Œè™šæ‹Ÿæœºå°†æ‰¿å—éå¸¸å¤§çš„ GC å‹åŠ›ï¼Œå› ä¸ºè™šæ‹Ÿæ¥å¿…é¡»æ£€æŸ¥æ¯ä¸ªå¯¹è±¡æ˜¯å¦å¯ä»¥æ‰‹æœºå¹¶å¿…é¡»è®¿é—®æ‰€æœ‰å†…å­˜ä¹Ÿï¼Œæœ¬åœ°ç¼“å­˜æ˜¯æœ€å¿«çš„ï¼Œä½†ä¼šç»™è™šæ‹Ÿæœºå¸¦æ¥ GC å‹åŠ›ï¼Œæ‰€ä»¥å½“éœ€è¦å¤„ç†éå¸¸å¤šçš„æ•°æ®é‡æ—¶å¯ä»¥è€ƒè™‘ä½¿ç”¨å †å¤–å†…å­˜æ¥è¿›è¡Œä¼˜åŒ–ï¼Œå› ä¸ºè¿™ä¸ä¼šç»™ Java GC å¸¦æ¥ä»»ä½•å‹åŠ›ï¼Œè®© Java GC ä¸ºå¼•ç”¨ç¨‹åºå®Œæˆå·¥ä½œï¼Œç¼“å­˜æ“ä½œäº¤ç»™å †å¤–ã€‚

`result.persist(StorageLevel.OFF_HEAP)`

#### è°ƒèŠ‚è¿æ¥ç­‰å¾…æ—¶é•¿

## æ•…éšœæ’é™¤

### æ§åˆ¶ reduce ç«¯ç¼“å†²å¤§å°ä»¥é¿å… OOM

reduce ç¼“å†²åŒºé»˜è®¤ 48 mï¼Œè°ƒå¤§æ˜¯ä»¥æ€§èƒ½æ¢æ‰§è¡Œã€‚

### JVM GC å¯¼è‡´çš„ shuffle æ–‡ä»¶æ‹‰å–å¤±è´¥

GC å¯¼è‡´è¿æ¥åœæ»ï¼Œè¿æ¥åœæ»å¯¼è‡´ timeoutã€‚æé«˜é‡è¯•æ¬¡æ•°å’Œç­‰å¾…æ—¶é•¿ã€‚

### å„ç§åºåˆ—åŒ–å¯¼è‡´çš„æŠ¥é”™

ä¸å¯ä»¥åœ¨ RDD çš„å…ƒç´ ç±»å‹ã€ç®—å­å‡½æ•°é‡Œä½¿ç”¨ç¬¬ä¸‰æ–¹ä¸æ”¯æŒåºåˆ—åŒ–çš„ç±»å‹ï¼Œä¾‹å¦‚ connectionã€‚

