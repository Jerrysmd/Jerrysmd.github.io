<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=robots content="noodp"><title>Spark Performance Tuning - JerrysBlog</title><meta name=Description content><meta property="og:title" content="Spark Performance Tuning"><meta property="og:description" content="Spark SQL is the top active component in spark 3.0 release. Most of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in latest release."><meta property="og:type" content="article"><meta property="og:url" content="https://jerrysmd.github.io/20220719_spark-optimization/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-07-19T14:54:16+08:00"><meta property="article:modified_time" content="2022-07-19T14:54:16+08:00"><meta property="og:site_name" content="JerrysBlog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Spark Performance Tuning"><meta name=twitter:description content="Spark SQL is the top active component in spark 3.0 release. Most of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in latest release."><meta name=application-name content="JerrysBlog"><meta name=apple-mobile-web-app-title content="JerrysBlog"><meta name=theme-color content="#ffffff"><meta name=msapplication-TileColor content="#da532c"><link rel=icon href=https://cdn-icons-png.flaticon.com/512/321/321211.png><link rel=apple-touch-icon sizes=180x180 href=https://cdn-icons-png.flaticon.com/256/738/738671.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link rel=manifest href=/site.webmanifest><link rel=canonical href=https://jerrysmd.github.io/20220719_spark-optimization/><link rel=prev href=https://jerrysmd.github.io/20220706_nonviolent-communication/><link rel=next href=https://jerrysmd.github.io/20220805_spring-intro/><link rel=stylesheet href=/css/style.min.css><link rel=preload href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.1.1/css/all.min.css></noscript><link rel=preload href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css as=style onload='this.onload=null,this.rel="stylesheet"'><noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/animate.css@4.1.1/animate.min.css></noscript><script type=application/ld+json>{"@context":"http://schema.org","@type":"BlogPosting","headline":"Spark Performance Tuning","inLanguage":"en","mainEntityOfPage":{"@type":"WebPage","@id":"https:\/\/jerrysmd.github.io\/20220719_spark-optimization\/"},"genre":"posts","keywords":"Tuning, Spark, SQL, Hive","wordcount":1685,"url":"https:\/\/jerrysmd.github.io\/20220719_spark-optimization\/","datePublished":"2022-07-19T14:54:16+08:00","dateModified":"2022-07-19T14:54:16+08:00","publisher":{"@type":"Organization","name":"xxxx","logo":"https:\/\/jerrysmd.github.io\/images\/avatar.png"},"author":{"@type":"Person","name":" "},"description":""}</script></head><body data-header-desktop=fixed data-header-mobile=auto><script type=text/javascript>(window.localStorage&&localStorage.getItem("theme")?localStorage.getItem("theme")==="dark":"auto"==="auto"?window.matchMedia("(prefers-color-scheme: dark)").matches:"auto"==="dark")&&document.body.setAttribute("theme","dark")</script><div id=mask></div><div class=wrapper><header class=desktop id=header-desktop><div class=header-wrapper><div class=header-title><a href=/ title=JerrysBlog><span class=header-title-pre><i class='far fa-edit fa-fw' aria-hidden=true></i></span>JERRYSBLOG</a></div><div class=menu><div class=menu-inner><a class=menu-item href=/ title=Home>Home </a><a class=menu-item href=/categories/technology/ title="Technology article list">Technology </a><a class=menu-item href=/categories/life/ title="Life article list">Life </a><a class=menu-item href=/tags/ title="Tags cloud">Tags </a><a class=menu-item href=/about/ title="About blog">About </a><a class=menu-item href=https://github.com/Jerrysmd title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i> </a><span class="menu-item delimiter"></span><span class="menu-item search" id=search-desktop>
<input type=text placeholder="Search titles or contents..." id=search-input-desktop>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-desktop title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-desktop title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-desktop><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span>
</span><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme"><i class="fas fa-moon fa-fw" aria-hidden=true></i></a></div></div></div></header><header class=mobile id=header-mobile><div class=header-container><div class=header-wrapper><div class=header-title><a href=/ title=JerrysBlog><span class=header-title-pre><i class='far fa-edit fa-fw' aria-hidden=true></i></span>JERRYSBLOG</a></div><div class=menu-toggle id=menu-toggle-mobile><span></span><span></span><span></span></div></div><div class=menu id=menu-mobile><div class=search-wrapper><div class="search mobile" id=search-mobile><input type=text placeholder="Search titles or contents..." id=search-input-mobile>
<a href=javascript:void(0); class="search-button search-toggle" id=search-toggle-mobile title=Search><i class="fas fa-search fa-fw" aria-hidden=true></i></a>
<a href=javascript:void(0); class="search-button search-clear" id=search-clear-mobile title=Clear><i class="fas fa-times-circle fa-fw" aria-hidden=true></i></a>
<span class="search-button search-loading" id=search-loading-mobile><i class="fas fa-spinner fa-fw fa-spin" aria-hidden=true></i></span></div><a href=javascript:void(0); class=search-cancel id=search-cancel-mobile>Cancel</a></div><a class=menu-item href=/ title=Home>Home</a><a class=menu-item href=/categories/technology/ title="Technology article list">Technology</a><a class=menu-item href=/categories/life/ title="Life article list">Life</a><a class=menu-item href=/tags/ title="Tags cloud">Tags</a><a class=menu-item href=/about/ title="About blog">About</a><a class=menu-item href=https://github.com/Jerrysmd title=GitHub rel="noopener noreffer" target=_blank><i class='fab fa-github fa-fw' aria-hidden=true></i></a><a href=javascript:void(0); class="menu-item theme-switch" title="Switch Theme">
<i class="fas fa-moon fa-fw" aria-hidden=true></i></a></div></div></header><div class="search-dropdown desktop"><div id=search-dropdown-desktop></div></div><div class="search-dropdown mobile"><div id=search-dropdown-mobile></div></div><main class=main><div class=container><div class=toc id=toc-auto><h2 class=toc-title>Contents</h2><div class=toc-content id=toc-content-auto></div></div><article class="page single"><h1 class="single-title animate__animated animate__flipInX">Spark Performance Tuning</h1><div class=post-meta><div class=post-meta-line><span class=post-category>Category in
<a href=/categories/technology/><i class="far fa-folder fa-fw" aria-hidden=true></i>Technology</a></span></div><div class=post-meta-line><i class="far fa-calendar-alt fa-fw" aria-hidden=true></i>&nbsp;<time datetime=2022-07-19>2022-07-19</time>&nbsp;<i class="fas fa-pencil-alt fa-fw" aria-hidden=true></i>&nbsp;1685 words&nbsp;
<i class="far fa-clock fa-fw" aria-hidden=true></i>&nbsp;8 minutes&nbsp;<span id=/20220719_spark-optimization/ class=leancloud_visitors data-flag-title="Spark Performance Tuning">
<i class="far fa-eye fa-fw" aria-hidden=true></i>&nbsp;<span class=leancloud-visitors-count></span>&nbsp;views
</span>&nbsp;</div></div><div class="details toc" id=toc-static data-kept><div class="details-summary toc-title"><span>Contents</span>
<span><i class="details-icon fas fa-angle-right" aria-hidden=true></i></span></div><div class="details-content toc-content" id=toc-content-static><nav id=TableOfContents><ul><li><a href=#explain-查看执行计划>Explain 查看执行计划</a><ul><li><a href=#语法>语法</a></li><li><a href=#执行计划处理流程>执行计划处理流程</a></li></ul></li><li><a href=#资源调优>资源调优</a><ul><li><a href=#资源规划>资源规划</a><ul><li><a href=#资源设定考虑>资源设定考虑</a><ul><li><a href=#总体原则>总体原则</a></li><li><a href=#具体提交参数>具体提交参数</a></li></ul></li><li><a href=#内存设置>内存设置</a></li></ul></li><li><a href=#持久化和序列化>持久化和序列化</a><ul><li><a href=#rdd>RDD</a><ul><li><a href=#kryo-序列化缓存>Kryo 序列化缓存</a></li></ul></li><li><a href=#dfds>DF、DS</a></li></ul></li><li><a href=#cpu优化>CPU优化</a><ul><li><a href=#cpu-低效原因>CPU 低效原因</a><ul><li><a href=#并行度>并行度</a></li><li><a href=#并发度>并发度</a></li><li><a href=#cpu-低效原因-1>CPU 低效原因</a></li></ul></li><li><a href=#cpu-资源调整>CPU 资源调整</a></li></ul></li></ul></li><li><a href=#sparksql-语法优化>SparkSQL 语法优化</a><ul><li><a href=#基于-rbo-优化>基于 RBO 优化</a><ul><li><a href=#谓词下推>谓词下推</a></li><li><a href=#列裁剪>列裁剪</a></li><li><a href=#常量替换>常量替换</a></li></ul></li><li><a href=#基于-cbo-优化>基于 CBO 优化</a><ul><li><a href=#statistics-收集>Statistics 收集</a></li><li><a href=#使用-cbo>使用 CBO</a></li></ul></li><li><a href=#广播-join>广播 Join</a><ul><li><a href=#通过参数指定自动广播>通过参数指定自动广播</a></li><li><a href=#指定广播>指定广播</a></li></ul></li><li><a href=#smb-join>SMB Join</a></li></ul></li><li><a href=#数据倾斜>数据倾斜</a><ul><li><a href=#现象>现象</a></li><li><a href=#原因>原因</a></li><li><a href=#数据倾斜大-key-定位>数据倾斜大 key 定位</a></li><li><a href=#单表数据倾斜优化>单表数据倾斜优化</a><ul><li><a href=#单表优化>单表优化</a></li><li><a href=#二次聚合>二次聚合</a></li></ul></li><li><a href=#join数据倾斜优化>Join数据倾斜优化</a><ul><li><a href=#广播join>广播Join</a><ul><li><a href=#通过参数指定自动广播-1>通过参数指定自动广播</a></li><li><a href=#指定广播-1>指定广播</a></li></ul></li><li><a href=#拆分大-key-打散大表-扩容小表>拆分大 key 打散大表 扩容小表</a></li></ul></li></ul></li><li><a href=#job-优化>Job 优化</a><ul><li><a href=#map-端优化>Map 端优化</a><ul><li><a href=#map-端聚合>Map 端聚合</a></li><li><a href=#读取小文件的优化>读取小文件的优化</a></li><li><a href=#增大-map-溢写时输出流-buffer>增大 map 溢写时输出流 buffer</a></li></ul></li><li><a href=#reduce-端优化>Reduce 端优化</a><ul><li><a href=#合理设置-reduce-数量>合理设置 Reduce 数量</a></li><li><a href=#输出产生小文件优化>输出产生小文件优化</a><ul><li><a href=#join后的结果插入新表>Join后的结果插入新表</a></li><li><a href=#动态分区插入数据>动态分区插入数据</a></li></ul></li><li><a href=#增大-reduce-缓冲区减少拉去次数>增大 reduce 缓冲区，减少拉去次数</a></li><li><a href=#调节-reduce-端拉取数据重试次数>调节 reduce 端拉取数据重试次数</a></li><li><a href=#调节-reduce-端拉取数据等待间隔>调节 reduce 端拉取数据等待间隔</a></li><li><a href=#合理利用-bypass>合理利用 bypass</a></li></ul></li><li><a href=#整体优化>整体优化</a><ul><li><a href=#调节数据本地化等待时长>调节数据本地化等待时长</a></li><li><a href=#使用堆外内存>使用堆外内存</a></li><li><a href=#调节连接等待时长>调节连接等待时长</a></li></ul></li></ul></li><li><a href=#故障排除>故障排除</a><ul><li><a href=#控制-reduce-端缓冲大小以避免-oom>控制 reduce 端缓冲大小以避免 OOM</a></li><li><a href=#jvm-gc-导致的-shuffle-文件拉取失败>JVM GC 导致的 shuffle 文件拉取失败</a></li><li><a href=#各种序列化导致的报错>各种序列化导致的报错</a></li></ul></li></ul></nav></div></div><div class=content id=content><p>Spark SQL is the top active component in spark 3.0 release. Most of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in latest release.</p><h2 id=explain-查看执行计划>Explain 查看执行计划</h2><h3 id=语法>语法</h3><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=o>.</span><span class=n>explain</span><span class=o>(</span><span class=n>mode</span><span class=o>=</span><span class=s>&#34;xxx&#34;</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><code>explain(mode="simple")</code>: 只展示物理执行计划</li><li><code>explain(mode="extended")</code>: 展示物理计划和逻辑执行计划</li><li><code>"codegen"</code>: 展示 codegen 生成的可执行 Java 代码</li><li><code>"cost"</code>: 展示优化后的逻辑执行计划以及相关的统计</li><li><code>"formatted"</code>: 分隔输出，输出更易读的物理执行计划并展示每个节点的详细信息</li></ul><h3 id=执行计划处理流程>执行计划处理流程</h3><p><figure><a class=lightgallery href=/20220719_spark-optimization/image-20220819153115834.png title=image-20220819153115834 data-thumbnail=/20220719_spark-optimization/image-20220819153115834.png data-sub-html="<h2>处理流程</h2><p>image-20220819153115834</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20220719_spark-optimization/image-20220819153115834.png data-srcset="/20220719_spark-optimization/image-20220819153115834.png, /20220719_spark-optimization/image-20220819153115834.png 1.5x, /20220719_spark-optimization/image-20220819153115834.png 2x" data-sizes=auto alt=/20220719_spark-optimization/image-20220819153115834.png width=930 height=258></a><figcaption class=image-caption>处理流程</figcaption></figure></p><p><figure><a class=lightgallery href=/20220719_spark-optimization/image-20220819153308501.png title=image-20220819153308501 data-thumbnail=/20220719_spark-optimization/image-20220819153308501.png data-sub-html="<h2>核心过程</h2><p>image-20220819153308501</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20220719_spark-optimization/image-20220819153308501.png data-srcset="/20220719_spark-optimization/image-20220819153308501.png, /20220719_spark-optimization/image-20220819153308501.png 1.5x, /20220719_spark-optimization/image-20220819153308501.png 2x" data-sizes=auto alt=/20220719_spark-optimization/image-20220819153308501.png width=923 height=149></a><figcaption class=image-caption>核心过程</figcaption></figure></p><p><strong>🔴Unresolved 逻辑执行计划：== Parsed Logical Plan ==</strong></p><ul><li>Parser 组件检查 SQL 语法是否有问题，然后生成 Unresolved 的逻辑计划，不检查表名、不检查列明</li></ul><p><strong>🟠Resolved 逻辑执行计划：==Analyzed Logical Plan ==</strong></p><ul><li>Spark 中的 Catalog 存储库来解析验证语义、列名、类型、表名等</li></ul><p>🟡<strong>优化后的逻辑执行计划：== Optimized Logical Plan ==</strong></p><ul><li>Catalyst 优化器根据各种规则进行优化</li></ul><p><strong>🟢物理执行计划：== Physical Plan ==</strong></p><ol><li><code>HashAggregate</code> 运算符表示数据聚合，一般 HashAggregate 是成对出现，第一个 HashAggregate 是将执行节点本地的数据进行局部聚合，另一个 HashAggregate 是将各个分区的数据进行聚合计算</li><li><code>Exchange</code> 运算符其实就是 shuffle，表示需要在集群上移动数据。很多时候 HashAggregate 会以 Exchange 分隔开</li><li><code>Project</code>运算符是 SQL 中的选择列，select name, age</li><li><code>BroadcastHashJoin</code>表示通过基于广播方式进行 HashJoin</li><li><code>LocalTableScan</code> 表示全表扫描本地的表</li></ol><h2 id=资源调优>资源调优</h2><h3 id=资源规划>资源规划</h3><h4 id=资源设定考虑>资源设定考虑</h4><h5 id=总体原则>总体原则</h5><p>单台服务器 128G 内存，32线程。</p><p>先设定单个 Executor 核数，根据 Yarn 配置得出每个节点最多的 Executor 数量，(Yarn 总核数 / 每个executor核数(通常为4)) = 单个节点的executor数量；</p><p>28 / 4 = 7 单个节点的executor数量；</p><p>总的 executor 数 = 单节点executor数量 * nm节点数。</p><h5 id=具体提交参数>具体提交参数</h5><ol><li><p>executor-cores</p><p>每个 executor 的最大核数。3 ~ 6 之间比较合理，通常为4</p></li><li><p>num-executors</p><p>num-executors = 每个节点的 executor 数 * work 节点数；</p><p>每个 node 的 executor 数 = 单节点 yarn 总核数 / 每个 executor 的最大 cpu 核数；</p><p>32线程有28线程用在 Yarn 上；</p><p>那么每个 node 的 executor 数 = 28 / 4 = 7；</p><p>假设集群节点为10；</p><p>那么 num-executors = 7 * 10 = 70。</p></li><li><p>executor-memory⭐</p><p>executor-memory = Yarn 内存 / 单个节点的executor数量；</p><p>100G(总128G, 100G 给 Yarn) / 7 = 14G;</p></li></ol><h4 id=内存设置>内存设置</h4><p><strong>一个 executor 内部</strong></p><p><figure><a class=lightgallery href=/20220719_spark-optimization/image-20220819173801820.png title=image-20220819173801820 data-thumbnail=/20220719_spark-optimization/image-20220819173801820.png data-sub-html="<h2>堆内存模型</h2><p>image-20220819173801820</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20220719_spark-optimization/image-20220819173801820.png data-srcset="/20220719_spark-optimization/image-20220819173801820.png, /20220719_spark-optimization/image-20220819173801820.png 1.5x, /20220719_spark-optimization/image-20220819173801820.png 2x" data-sizes=auto alt=/20220719_spark-optimization/image-20220819173801820.png width=911 height=505></a><figcaption class=image-caption>堆内存模型</figcaption></figure></p><p>🟢估算 Other 内存 = 自定义数据结构 * 每个 Executor 核数</p><p>🔵估算 Storage 内存 = 广播变量 + cache/Executor 数量</p><p>🟣估算 Executor 内存 = 每个 Executor 核数 * (数据集大小/并行度)</p><ul><li>Sparksql 并行度默认为 200，并行度即为 Task 数量</li></ul><h3 id=持久化和序列化>持久化和序列化</h3><h4 id=rdd>RDD</h4><h5 id=kryo-序列化缓存>Kryo 序列化缓存</h5><p>使用：</p><ol><li>sparkConf 指定 kryo 序列化器</li><li>sparkConf 注册样例类</li></ol><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>new</span> <span class=nc>SparkConf</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl><span class=o>.</span><span class=n>set</span><span class=o>(</span><span class=s>&#34;spark.serializer&#34;</span><span class=o>,</span><span class=s>&#34;...kryoSerializer&#34;</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>.</span><span class=n>registerKryoClasses</span><span class=o>(</span><span class=nc>Array</span><span class=o>(</span><span class=n>classOf</span><span class=o>[</span><span class=kt>...</span><span class=o>]))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>result</span><span class=o>.</span><span class=n>persist</span><span class=o>(</span><span class=nc>StorageLevel</span><span class=o>.</span><span class=nc>MEMORY_ONLY_SER</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p>测试：</p><blockquote><p>2G 的 HIVE 元数据，使用 RDD 缓存，完成 100% Fraction Cached 需要 7 G左右内存，使 partition 很容易挂掉。使用 Kryo 序列器完成 Cached 需要 1 G内存。</p></blockquote><h4 id=dfds>DF、DS</h4><p>cache 默认使用 <code>MEMORY_AND_DISK</code>缓存</p><div class="details admonition note open"><div class="details-summary admonition-title"><i class="icon fas fa-pencil-alt fa-fw" aria-hidden=true></i>Note<i class="details-icon fas fa-angle-right fa-fw" aria-hidden=true></i></div><div class=details-content><div class=admonition-content><ol><li>序列化器(Java, Kryo)是针对 RDD 而言的；而 DF、DS 是由 encoder 选择的。</li><li>encoder 由 SparkSql 自己实现的，也有可能使用 kryo 的方式。</li><li>对 DF、DS使用序列化差别不大。</li></ol></div></div></div><h3 id=cpu优化>CPU优化</h3><h4 id=cpu-低效原因>CPU 低效原因</h4><h5 id=并行度>并行度</h5><p>并行度就是 Task 数量。</p><p>🟠RDD 并行度参数：</p><ul><li><p><code>spark.default.parallelism</code></p></li><li><p>不设置时，默认由 join、reduceByKey 和 parallelize 等转换决定。</p></li></ul><p>🟡SparkSQL 并行度参数：与 RDD 并行度互不影响</p><ul><li><p><code>spark.sql.shuffle.partitions</code></p></li><li><p>默认是 200，只能控制 SparkSQL、DataFrame、Dataset 分区个数。</p></li></ul><h5 id=并发度>并发度</h5><p>并发度：同时执行的 Task 数量。</p><h5 id=cpu-低效原因-1>CPU 低效原因</h5><ol><li>并行度较低、数据分片较大容易导致 CPU 线程挂起；</li><li>并行度过高、数据过于分散会让调度开销更多；</li></ol><h4 id=cpu-资源调整>CPU 资源调整</h4><p><code>spark-submit --master yarn --deploy-mode client --driver-memory 1g --num-executors 3 --executor-cores 4 --executor-memory 6g --class com.jar</code></p><p>🟣官方推荐并行度（Task 数）设置成并发度（vcore 数）的 2 倍到 3 倍。</p><p>例：如果以目前的资源（3 个 executor）去提交，每个 executor 有两个核，总共 6 个核，则并行度设置为 12 ~ 18。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=nc>SparkConf</span><span class=o>()</span>
</span></span><span class=line><span class=cl><span class=o>...</span>
</span></span><span class=line><span class=cl><span class=o>.</span><span class=n>set</span><span class=o>(</span><span class=s>&#34;spark.sql.shuffle.partitions&#34;</span><span class=o>,</span> <span class=s>&#34;18&#34;</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><h2 id=sparksql-语法优化>SparkSQL 语法优化</h2><h3 id=基于-rbo-优化>基于 RBO 优化</h3><h4 id=谓词下推>谓词下推</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=c1>//=============Inner on 左表=============
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>spark</span><span class=o>.</span><span class=n>sqlContext</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>    |select
</span></span></span><span class=line><span class=cl><span class=s>    | l.id,
</span></span></span><span class=line><span class=cl><span class=s>    | l.name,
</span></span></span><span class=line><span class=cl><span class=s>    | r.id,
</span></span></span><span class=line><span class=cl><span class=s>    | r.name
</span></span></span><span class=line><span class=cl><span class=s>    |from course l join student r
</span></span></span><span class=line><span class=cl><span class=s>    | on l.id=r.id and l.dt=r.dt and l.dn=r.dn
</span></span></span><span class=line><span class=cl><span class=s>    |on l.id&lt;2
</span></span></span><span class=line><span class=cl><span class=s>    |&#34;&#34;&#34;</span><span class=o>.</span><span class=n>stripMargin</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=c1>//=============Inner where 左表=============
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=n>spark</span><span class=o>.</span><span class=n>sqlContext</span><span class=o>.</span><span class=n>sql</span><span class=o>(</span>
</span></span><span class=line><span class=cl>  <span class=s>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s>    |select
</span></span></span><span class=line><span class=cl><span class=s>    | l.id,
</span></span></span><span class=line><span class=cl><span class=s>    | l.name,
</span></span></span><span class=line><span class=cl><span class=s>    | r.id,
</span></span></span><span class=line><span class=cl><span class=s>    | r.name
</span></span></span><span class=line><span class=cl><span class=s>    |from course l join student r
</span></span></span><span class=line><span class=cl><span class=s>    | on l.id=r.id and l.dt=r.dt and l.dn=r.dn
</span></span></span><span class=line><span class=cl><span class=s>    |where l.id&lt;2
</span></span></span><span class=line><span class=cl><span class=s>    |&#34;&#34;&#34;</span><span class=o>.</span><span class=n>stripMargin</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><p><strong>inner join</strong></p><ul><li>无论是 ON 还是 WHERE，无论条件是右表还是左表。从 logic plan -> Analyzed logical plan 到 <strong>optimized logical plan</strong>，sparkSQL 都会优化<strong>先过滤数据再进行 join</strong> 连接，而且其中一表过滤，<strong>另一表也优化提前过滤</strong>（最终要过滤数据，另一表也没有存在的必要）</li></ul><p><strong>left join</strong></p><ul><li><table><thead><tr><th></th><th>条件在 左表</th><th>条件在 右表</th></tr></thead><tbody><tr><td>条件在 on 后</td><td>只下推右表</td><td>只下推右表</td></tr><tr><td>条件在 where 后</td><td>两表都下推</td><td>两表都下推</td></tr></tbody></table></li><li><p>注意：外关联时，过滤条件在 on 与 where，语义是不同的，结果也是不同的。</p></li></ul><h4 id=列裁剪>列裁剪</h4><p>扫描数据源的时候，只读取那些与查询相关的字段。</p><h4 id=常量替换>常量替换</h4><p>Catalyst 会使用 constantFolding 规则，自动用表达式的结果进行替换。</p><h3 id=基于-cbo-优化>基于 CBO 优化</h3><h4 id=statistics-收集>Statistics 收集</h4><p>需要先执行特定的 SQL 语句来收集所需的表和列的统计信息。</p><p>🔵 生成表级别统计信息（扫表）：</p><p><code>ANALYZE TABLE 表明 COMPUTE STATISTICS</code></p><h4 id=使用-cbo>使用 CBO</h4><p>通过 <code>spark.sql.cbo.enabled</code> 来开启，默认是 false。CBO 优化器可以基于表和列的统计信息，选择出最优的查询计划。比如：Build 侧选择、优化 Join 类型、优化多表 Join 顺序。</p><h3 id=广播-join>广播 Join</h3><h4 id=通过参数指定自动广播>通过参数指定自动广播</h4><p>广播 join 默认值为 10MB，由 <code>spark.sql.autoBroadcastJoinThreshold</code>参数控制。</p><h4 id=指定广播>指定广播</h4><ol><li>sparkSQL 加 HINT 方式</li><li>使用 function._ broadcast API</li></ol><h3 id=smb-join>SMB Join</h3><p>大表 JOIN 大表，进行 SMB（sort merge bucket）操作：</p><p>需要进行分桶，首先会进行排序，然后根据 key 值合并，把相同 key 的数据放到同一个 bucket 中（按照 key 进行 hash）。分桶的目的就是把大表化成小表。相同的 key 的数据都在同一个桶中，再进行 join 操作，那么在联合的时候就会大幅度的减小无关项的扫描。</p><h2 id=数据倾斜>数据倾斜</h2><h3 id=现象>现象</h3><p>绝大多数 Task 任务运行速度很快，但几个 Task 任务运行速度极其缓慢，慢慢的可能接着报内存溢出的问题。</p><p><figure><a class=lightgallery href=/20220719_spark-optimization/image-20220822170107186.png title=image-20220822170107186 data-thumbnail=/20220719_spark-optimization/image-20220822170107186.png data-sub-html="<h2>Task数据倾斜</h2><p>image-20220822170107186</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20220719_spark-optimization/image-20220822170107186.png data-srcset="/20220719_spark-optimization/image-20220822170107186.png, /20220719_spark-optimization/image-20220822170107186.png 1.5x, /20220719_spark-optimization/image-20220822170107186.png 2x" data-sizes=auto alt=/20220719_spark-optimization/image-20220822170107186.png width=703 height=343></a><figcaption class=image-caption>Task数据倾斜</figcaption></figure></p><h3 id=原因>原因</h3><p>数据倾斜发生在 shuffle 类的算子，比如 distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup 等，涉及到数据重分区，如果其中某一个 key 数量特别大，就发生了数据倾斜。需要先对大 Key 进行定位。</p><h3 id=数据倾斜大-key-定位>数据倾斜大 key 定位</h3><p>使用抽取采样方法</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>val</span> <span class=n>top10key</span> <span class=k>=</span> <span class=n>df</span>
</span></span><span class=line><span class=cl><span class=o>.</span><span class=n>select</span><span class=o>(</span><span class=n>keyColumn</span><span class=o>).</span><span class=n>sample</span><span class=o>(</span><span class=kc>false</span><span class=o>,</span> <span class=mf>0.1</span><span class=o>).</span><span class=n>rdd</span> <span class=c1>//抽取 10%
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>k</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>k</span><span class=o>,</span> <span class=mi>1</span><span class=o>)).</span><span class=n>reduceByKey</span><span class=o>(</span><span class=k>_</span><span class=o>+</span><span class=k>_</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>.</span><span class=n>map</span><span class=o>(</span><span class=n>k</span> <span class=k>=&gt;</span> <span class=o>(</span><span class=n>k</span><span class=o>.</span><span class=n>_2</span><span class=o>,</span> <span class=n>k</span><span class=o>.</span><span class=n>_1</span><span class=o>)).</span><span class=n>sortByKey</span><span class=o>(</span><span class=kc>false</span><span class=o>)</span> <span class=c1>//按统计的key进行排序
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=o>.</span><span class=n>take</span><span class=o>(</span><span class=mi>10</span><span class=o>)</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=单表数据倾斜优化>单表数据倾斜优化</h3><h4 id=单表优化>单表优化</h4><p>为了减少 shuffle 以及 reduce 端的压力，SparkSQL 会在 map 端会做一个 partial aggregate（预聚合或者偏聚合），即在 shuffle 前将同一分区内所属同 key 的记录先进行一个预结算，再将结果进行 shuffle，发送到 reduce 端做一个汇总，类似 MR 的提前 Combiner，所以执行计划中 Hashaggregate 通常成对出现。</p><h4 id=二次聚合>二次聚合</h4><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>select 
</span></span><span class=line><span class=cl>	id,
</span></span><span class=line><span class=cl>	sum(course) total
</span></span><span class=line><span class=cl>from
</span></span><span class=line><span class=cl>(
</span></span><span class=line><span class=cl>	select 
</span></span><span class=line><span class=cl>		remove_random_prefix(random_courseid) courseid,
</span></span><span class=line><span class=cl>		course
</span></span><span class=line><span class=cl>	from
</span></span><span class=line><span class=cl>	(
</span></span><span class=line><span class=cl>    	select 
</span></span><span class=line><span class=cl>        	random_id,
</span></span><span class=line><span class=cl>        	sum(sellmoney) course
</span></span><span class=line><span class=cl>        from
</span></span><span class=line><span class=cl>        (
</span></span><span class=line><span class=cl>        	select
</span></span><span class=line><span class=cl>            	random_prefix(id, 6) random_id,
</span></span><span class=line><span class=cl>            	sellmoney
</span></span><span class=line><span class=cl>            from
</span></span><span class=line><span class=cl>            	doubleAggre
</span></span><span class=line><span class=cl>        )t1
</span></span><span class=line><span class=cl>        group by random_id
</span></span><span class=line><span class=cl>    )t2
</span></span><span class=line><span class=cl>)t3
</span></span><span class=line><span class=cl>group by id
</span></span></code></pre></td></tr></table></div></div><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=k>def</span> <span class=n>randomPrefixUDF</span><span class=o>(</span><span class=n>value</span><span class=o>,</span> <span class=n>num</span><span class=o>)</span><span class=k>:</span><span class=kt>String</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>new</span> <span class=nc>Random</span><span class=o>().</span><span class=n>nextInt</span><span class=o>(</span><span class=n>num</span><span class=o>).</span><span class=n>toString</span> <span class=o>+</span> <span class=s>&#34;_&#34;</span> <span class=o>+</span> <span class=n>value</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=n>removeRandomPrefixUDF</span><span class=o>(</span><span class=n>value</span><span class=o>)</span><span class=k>:</span><span class=kt>String</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=n>value</span><span class=o>.</span><span class=n>toString</span><span class=o>.</span><span class=n>split</span><span class=o>(</span><span class=s>&#34;_&#34;</span><span class=o>)(</span><span class=mi>1</span><span class=o>)</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=join数据倾斜优化>Join数据倾斜优化</h3><h4 id=广播join>广播Join</h4><h5 id=通过参数指定自动广播-1>通过参数指定自动广播</h5><p>广播 join 默认值为 10MB，由 <code>spark.sql.autoBroadcastJoinThreshold</code>参数控制。</p><h5 id=指定广播-1>指定广播</h5><ol><li>sparkSQL 加 HINT 方式</li><li>使用 function._ broadcast API</li></ol><h4 id=拆分大-key-打散大表-扩容小表>拆分大 key 打散大表 扩容小表</h4><blockquote><p>与单表数据倾斜优化的二次聚合不同，join 数据倾斜调优要对两表都进行调整。</p><p>因为大表为了分区加入了前缀，为了和小表匹配上，小表也应建立对应的前缀与之匹配。如：（假设有3个 task，把大key重新打散到所有task上）</p></blockquote><table><thead><tr><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th><th style=text-align:center></th></tr></thead><tbody><tr><td style=text-align:center>1<br>1<br>1<br>1<br>1<br>1<br>1<br>1<br>2<br>3</td><td style=text-align:center>Join</td><td style=text-align:center>1<br>2<br>3</td><td style=text-align:center>拆分大 key<br>打散大表<br>扩容小表<br>&mdash;></td><td style=text-align:center>0_1<br>1_1<br>2_1<br>0_1<br>1_1<br>2_1<br>0_1<br>1_1</td><td style=text-align:center>Join</td><td style=text-align:center>0_1<br>1_1<br>2_1<br>2<br>3</td></tr><tr><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>Union</td><td style=text-align:center></td></tr><tr><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center></td><td style=text-align:center>2<br>3</td><td style=text-align:center>Join</td><td style=text-align:center>1<br>2<br>3</td></tr></tbody></table><ol><li>拆分倾斜的 key：根据 key 过滤出倾斜的数据和除倾斜外的其他数据；</li><li>将切斜的 key 打散：打散成 task 数量的份数(比如有36个task)，key 值前加(0 ~ 36)随机数；</li><li>小表进行扩容：扩大成 task 数量的份数，key 值用 flatmap 生成 36 份，<code>i + "_" + key</code></li><li>倾斜的大 key 与扩容后的表进行join；</li><li>没有倾斜的 key与原来的表进行join；</li><li>将倾斜 key join 后的结果与普通 key join 后的结果，union起来。</li></ol><p>代价：shuffle 次数增多了，但是每次 shuffle 数据更均匀了。</p><h2 id=job-优化>Job 优化</h2><h3 id=map-端优化>Map 端优化</h3><h4 id=map-端聚合>Map 端聚合</h4><p>parkSQL 会在 map 端会做一个 partial aggregate（预聚合或者偏聚合），即在 shuffle 前将同一分区内所属同 key 的记录先进行一个预结算，再将结果进行 shuffle，发送到 reduce 端做一个汇总，类似 MR 的提前 Combiner，所以执行计划中 Hashaggregate 通常成对出现。</p><p>SparkSQL 本身的 Hashaggregate 就会实现本地预聚合 + 全局聚合。</p><h4 id=读取小文件的优化>读取小文件的优化</h4><p>HIVE 的 <code>CombineHiveInputformat</code> 输入格式会将小文件读到同一个 Map 端里面去。</p><p>SparkSQL 中也会自动合并，参数如下：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl>spark.sql.files.maxPartitionBytes<span class=o>=</span>128MB <span class=c1>#默认 128m</span>
</span></span><span class=line><span class=cl>spark.files.openCostInBytes<span class=o>=</span><span class=m>4194304</span> <span class=c1>#默认 4m</span>
</span></span></code></pre></td></tr></table></div></div><ul><li><p>切片大小 = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))</p></li><li><p>bytesPerCore = totalBytes / defaultParallelism</p></li><li><p>计算 totalBytes 时，每个文件都要加上一个 open 开销：</p><p>当 (文件1大小 + openCostInBytes) + (文件2大小 + openCostInBytes)+&mldr; &lt;= maxPartitionBytes 时，n个文件可以读入同一分区。</p></li></ul><h4 id=增大-map-溢写时输出流-buffer>增大 map 溢写时输出流 buffer</h4><p><figure><a class=lightgallery href=/20220719_spark-optimization/image-20220824110005487.png title=image-20220824110005487 data-thumbnail=/20220719_spark-optimization/image-20220824110005487.png data-sub-html="<h2>SortShuffle</h2><p>image-20220824110005487</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20220719_spark-optimization/image-20220824110005487.png data-srcset="/20220719_spark-optimization/image-20220824110005487.png, /20220719_spark-optimization/image-20220824110005487.png 1.5x, /20220719_spark-optimization/image-20220824110005487.png 2x" data-sizes=auto alt=/20220719_spark-optimization/image-20220824110005487.png width=752 height=390></a><figcaption class=image-caption>SortShuffle</figcaption></figure></p><p>源码理解：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl><span class=cm>/**
</span></span></span><span class=line><span class=cl><span class=cm> * Spills the current in-memory collection to disk if needed. Attempts to acquire more
</span></span></span><span class=line><span class=cl><span class=cm> * memory before spilling.
</span></span></span><span class=line><span class=cl><span class=cm> *
</span></span></span><span class=line><span class=cl><span class=cm> * @param collection collection to spill to disk
</span></span></span><span class=line><span class=cl><span class=cm> * @param currentMemory estimated size of the collection in bytes
</span></span></span><span class=line><span class=cl><span class=cm> * @return true if `collection` was spilled to disk; false otherwise
</span></span></span><span class=line><span class=cl><span class=cm> */</span>
</span></span><span class=line><span class=cl><span class=k>protected</span> <span class=k>def</span> <span class=n>maybeSpill</span><span class=o>(</span><span class=n>collection</span><span class=k>:</span> <span class=kt>C</span><span class=o>,</span> <span class=n>currentMemory</span><span class=k>:</span> <span class=kt>Long</span><span class=o>)</span><span class=k>:</span> <span class=kt>Boolean</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>  <span class=k>var</span> <span class=n>shouldSpill</span> <span class=k>=</span> <span class=kc>false</span>
</span></span><span class=line><span class=cl>  <span class=k>if</span> <span class=o>(</span><span class=n>elementsRead</span> <span class=o>%</span> <span class=mi>32</span> <span class=o>==</span> <span class=mi>0</span> <span class=o>&amp;&amp;</span> <span class=n>currentMemory</span> <span class=o>&gt;=</span> <span class=n>myMemoryThreshold</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=c1>// Claim up to double our current memory from the shuffle memory pool
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=k>val</span> <span class=n>amountToRequest</span> <span class=k>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=n>currentMemory</span> <span class=o>-</span> <span class=n>myMemoryThreshold</span>
</span></span><span class=line><span class=cl>    <span class=k>val</span> <span class=n>granted</span> <span class=k>=</span> <span class=n>acquireMemory</span><span class=o>(</span><span class=n>amountToRequest</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=n>myMemoryThreshold</span> <span class=o>+=</span> <span class=n>granted</span>
</span></span><span class=line><span class=cl>    <span class=c1>// If we were granted too little memory to grow further (either tryToAcquire returned 0,
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=c1>// or we already had more memory than myMemoryThreshold), spill the current collection
</span></span></span><span class=line><span class=cl><span class=c1></span>    <span class=n>shouldSpill</span> <span class=k>=</span> <span class=n>currentMemory</span> <span class=o>&gt;=</span> <span class=n>myMemoryThreshold</span>
</span></span><span class=line><span class=cl>  <span class=o>}</span>
</span></span><span class=line><span class=cl>  <span class=n>shouldSpill</span> <span class=k>=</span> <span class=n>shouldSpill</span> <span class=o>||</span> <span class=nc>_elementsRead</span> <span class=o>&gt;</span> <span class=n>numElementsForceSpillThreshold</span>
</span></span><span class=line><span class=cl>  <span class=c1>// Actually spill
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=k>if</span> <span class=o>(</span><span class=n>shouldSpill</span><span class=o>)</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=nc>_spillCount</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=n>logSpillage</span><span class=o>(</span><span class=n>currentMemory</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=n>spill</span><span class=o>(</span><span class=n>collection</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=nc>_elementsRead</span> <span class=k>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=nc>_memoryBytesSpilled</span> <span class=o>+=</span> <span class=n>currentMemory</span>
</span></span><span class=line><span class=cl>    <span class=n>releaseMemory</span><span class=o>()</span>
</span></span><span class=line><span class=cl>  <span class=o>}</span>
</span></span><span class=line><span class=cl>  <span class=n>shouldSpill</span>
</span></span><span class=line><span class=cl><span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><ol><li><p>map 端 Shuffle Write 有一个缓冲区，初始阈值 5m，超过阈值尝试增加到 2*当前使用内存，自动扩容。如果申请不到内存，则进行溢写。这个参数是 internal，指定无效。</p></li><li><p>溢写时使用输出流缓冲区默认 32k，这些缓冲区减少了磁盘搜索和系统调用次数，<strong>适当提高可以提升溢写效率。</strong></p><table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody><tr><td><code>spark.shuffle.file.buffer</code></td><td>32k</td><td>Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files.</td></tr></tbody></table></li><li><p>Shuffle 文件涉及到序列化，是采用批的方式读写，默认没批次 1 万条去读写，设置得太低会导致在序列化时过度复制。</p></li></ol><h3 id=reduce-端优化>Reduce 端优化</h3><h4 id=合理设置-reduce-数量>合理设置 Reduce 数量</h4><p><code>Reduce 的数量 = shuffle 后的分区数 = 也就是 Task 数量 = 也就是并行度 = spark.sql.shuffle.partitions 默认的200。</code></p><p><code>并发度是整个集群 spark 的核数，如并发度是12，并行度为并发度的 3~6 倍，设置为 36</code></p><p>过多的 CPU 资源出现空转浪费，过少影响任务性能。</p><h4 id=输出产生小文件优化>输出产生小文件优化</h4><h5 id=join后的结果插入新表>Join后的结果插入新表</h5><p>join 结果插入新表，生成的文件数等于 shuffle 并行度，默认就是 200 份插入到 hdfs 上。</p><p>解决方式一：在插入表数据前进行缩小分区操作来解决小文件过多问题，如 coalesce、repartition 算子。</p><p>解决方式二：调整并行度。</p><h5 id=动态分区插入数据>动态分区插入数据</h5><ol><li><p>没有 shuffle 的情况下。最差的情况。每个 Task 中都有各个分区的记录，那最终文件数达到 Task 数量 * 表分区数。这种情况极易产生小文件。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>INSERT overwrite table A partition (aa)
</span></span><span class=line><span class=cl>SELECT * FROM B;
</span></span></code></pre></td></tr></table></div></div></li></ol><p><figure><a class=lightgallery href=/20220719_spark-optimization/image-20220824122130265.png title=image-20220824122130265 data-thumbnail=/20220719_spark-optimization/image-20220824122130265.png data-sub-html="<h2>动态分区插入-没有shuffle</h2><p>image-20220824122130265</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20220719_spark-optimization/image-20220824122130265.png data-srcset="/20220719_spark-optimization/image-20220824122130265.png, /20220719_spark-optimization/image-20220824122130265.png 1.5x, /20220719_spark-optimization/image-20220824122130265.png 2x" data-sizes=auto alt=/20220719_spark-optimization/image-20220824122130265.png width=662 height=470></a><figcaption class=image-caption>动态分区插入-没有shuffle</figcaption></figure></p><ol start=2><li><p>有 shuffle 的情况下。上面的 Task 数量就变成了 200。那么最差情况就会有 200 * 表分区数。</p><p>当 <code>shuffle.partitions</code> 设置大了小文件问题就产生了，设置小了，任务的并行度就下降了，性能随之受到影响。</p><p>最理想的情况是根据分区字段进行 shuffle，在上面的 SQL 中加入 distribute by aa。把同一分区的记录都哈希到同一分区中去，由一个 Spark 的 Task 进行写入，这样只会产生 N 个文件，但这种情况也容易出现数据倾斜的问题。</p></li></ol><p><figure><a class=lightgallery href=/20220719_spark-optimization/image-20220824122300415.png title=image-20220824122300415 data-thumbnail=/20220719_spark-optimization/image-20220824122300415.png data-sub-html="<h2>动态分区插入-shuffle</h2><p>image-20220824122300415</p>"><img class=lazyload src=/svg/loading.min.svg data-src=/20220719_spark-optimization/image-20220824122300415.png data-srcset="/20220719_spark-optimization/image-20220824122300415.png, /20220719_spark-optimization/image-20220824122300415.png 1.5x, /20220719_spark-optimization/image-20220824122300415.png 2x" data-sizes=auto alt=/20220719_spark-optimization/image-20220824122300415.png width=927 height=492></a><figcaption class=image-caption>动态分区插入-shuffle</figcaption></figure></p><p>数据倾斜解决思路：</p><p>结合之前解决数据倾斜的思路，在确定哪个分区键倾斜的情况下，将倾斜的分区键单独拿出：</p><p>将入库的 SQL 拆成（where 分区 != 倾斜分区键）和（where 分区 = 倾斜分区键）两个部分，非倾斜分区键的部分正常 distribute by 分区字段，倾斜分区键的部分 distribute by 随机数：</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-fallback data-lang=fallback><span class=line><span class=cl>#1.非倾斜部分
</span></span><span class=line><span class=cl>INSERT overwrite table A partition (aa)
</span></span><span class=line><span class=cl>SELECT *
</span></span><span class=line><span class=cl>FROM B where aa != 大key
</span></span><span class=line><span class=cl>distribute by aa; #主动产生 shuffle，将同一个分区的数据放到一个 task 中，再执行写入
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>#2.倾斜键部分
</span></span><span class=line><span class=cl>INSERT overwrite table A partition (aa)
</span></span><span class=line><span class=cl>SELECT *
</span></span><span class=line><span class=cl>FROM B where aa = 大key
</span></span><span class=line><span class=cl>distribute by cast(rand() * 5 as int); #打散成5份，5个task，写入5个文件
</span></span></code></pre></td></tr></table></div></div><h4 id=增大-reduce-缓冲区减少拉去次数>增大 reduce 缓冲区，减少拉去次数</h4><p>一般不会调整，ShuffleReader.scala，默认值 reduce 一次读取 48M。</p><h4 id=调节-reduce-端拉取数据重试次数>调节 reduce 端拉取数据重试次数</h4><p>一般不会调整，默认为 3 次。</p><h4 id=调节-reduce-端拉取数据等待间隔>调节 reduce 端拉取数据等待间隔</h4><p>一般不会调整，默认为 5 秒。</p><h4 id=合理利用-bypass>合理利用 bypass</h4><table><thead><tr><th>Property Name</th><th>Default</th><th>Meaning</th></tr></thead><tbody><tr><td><code>spark.shuffle.sort.bypassMergeThreshold</code></td><td>200</td><td>(Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions.</td></tr></tbody></table><p>当 shuffleManager 为 SortShuffleManager 时，如果 shuffle read task 的数量小于这个阈值（默认200）且不需要 map 端进行合并操作（使用 groupby、sum 聚合算子会预聚合，从执行计划可以得知有一个 hashaggregate -> exchange -> hashaggregate），则shuffle write 过程不会进行排序，使用 <code>BypassMergeSortShuffleWriter</code> 去写数据，但最后会将每个 task 产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。</p><p>当使用 shuffleManager 时，如果确实不需要排序操作，那么建议将这个参数调大一些，大于 shuffle read task 的数量。那么此时就会自动启动 bypass 机制，map-side 就不会进行排序了，减少了排序的性能开销。但这种方式下，依然会产生大量的磁盘文件，因此 shuffle write 性能有待提高。</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-scala data-lang=scala><span class=line><span class=cl>  <span class=cm>/**
</span></span></span><span class=line><span class=cl><span class=cm>   * Register a shuffle with the manager and obtain a handle for it to pass to tasks.
</span></span></span><span class=line><span class=cl><span class=cm>   */</span>
</span></span><span class=line><span class=cl>  <span class=k>override</span> <span class=k>def</span> <span class=n>registerShuffle</span><span class=o>[</span><span class=kt>K</span>, <span class=kt>V</span>, <span class=kt>C</span><span class=o>](</span>
</span></span><span class=line><span class=cl>      <span class=n>shuffleId</span><span class=k>:</span> <span class=kt>Int</span><span class=o>,</span>
</span></span><span class=line><span class=cl>      <span class=n>numMaps</span><span class=k>:</span> <span class=kt>Int</span><span class=o>,</span>
</span></span><span class=line><span class=cl>      <span class=n>dependency</span><span class=k>:</span> <span class=kt>ShuffleDependency</span><span class=o>[</span><span class=kt>K</span>, <span class=kt>V</span>, <span class=kt>C</span><span class=o>])</span><span class=k>:</span> <span class=kt>ShuffleHandle</span> <span class=o>=</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=o>(</span><span class=nc>SortShuffleWriter</span><span class=o>.</span><span class=n>shouldBypassMergeSort</span><span class=o>(</span><span class=nc>SparkEnv</span><span class=o>.</span><span class=n>get</span><span class=o>.</span><span class=n>conf</span><span class=o>,</span> <span class=n>dependency</span><span class=o>))</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>      <span class=c1>// If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don&#39;t
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=c1>// need map-side aggregation, then write numPartitions files directly and just concatenate
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=c1>// them at the end. This avoids doing serialization and deserialization twice to merge
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=c1>// together the spilled files, which would happen with the normal code path. The downside is
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=c1>// having multiple files open at a time and thus more memory allocated to buffers.
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=k>new</span> <span class=nc>BypassMergeSortShuffleHandle</span><span class=o>[</span><span class=kt>K</span>, <span class=kt>V</span><span class=o>](</span>
</span></span><span class=line><span class=cl>        <span class=n>shuffleId</span><span class=o>,</span> <span class=n>numMaps</span><span class=o>,</span> <span class=n>dependency</span><span class=o>.</span><span class=n>asInstanceOf</span><span class=o>[</span><span class=kt>ShuffleDependency</span><span class=o>[</span><span class=kt>K</span>, <span class=kt>V</span>, <span class=kt>V</span><span class=o>]])</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span> <span class=k>else</span> <span class=k>if</span> <span class=o>(</span><span class=nc>SortShuffleManager</span><span class=o>.</span><span class=n>canUseSerializedShuffle</span><span class=o>(</span><span class=n>dependency</span><span class=o>))</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>      <span class=c1>// Otherwise, try to buffer map outputs in a serialized form, since this is more efficient:
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=k>new</span> <span class=nc>SerializedShuffleHandle</span><span class=o>[</span><span class=kt>K</span>, <span class=kt>V</span><span class=o>](</span>
</span></span><span class=line><span class=cl>        <span class=n>shuffleId</span><span class=o>,</span> <span class=n>numMaps</span><span class=o>,</span> <span class=n>dependency</span><span class=o>.</span><span class=n>asInstanceOf</span><span class=o>[</span><span class=kt>ShuffleDependency</span><span class=o>[</span><span class=kt>K</span>, <span class=kt>V</span>, <span class=kt>V</span><span class=o>]])</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span> <span class=k>else</span> <span class=o>{</span>
</span></span><span class=line><span class=cl>      <span class=c1>// Otherwise, buffer map outputs in a deserialized form:
</span></span></span><span class=line><span class=cl><span class=c1></span>      <span class=k>new</span> <span class=nc>BaseShuffleHandle</span><span class=o>(</span><span class=n>shuffleId</span><span class=o>,</span> <span class=n>numMaps</span><span class=o>,</span> <span class=n>dependency</span><span class=o>)</span>
</span></span><span class=line><span class=cl>    <span class=o>}</span>
</span></span><span class=line><span class=cl>  <span class=o>}</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=整体优化>整体优化</h3><h4 id=调节数据本地化等待时长>调节数据本地化等待时长</h4><p>在 Spark 项目开发阶段，可以使用 client 模式对程序进行测试，此时可以看到比较全的日志信息，日志信息中有明确的 task 数据本地化的级别，如果大部分都是 Process_LOCAL(进程本地化: 数据和计算是在同一个JVM进程里面)、NODE_LOCAL(节点本地化: 数据和计算是在同一个服务器上)，那么就无需进行调节，但如果很多是 RACK_LOCAL(机架本地化: 数据和计算是在同一个机架上)、ANY，那么需要对本地化的等待时长进行调节，慢慢调整大一些，应该是反复调节，每次调节后观察运行日志，看看大部分的 task 的本地化级别有没有提升，观察整个 spark 作业的运行时间有没有缩短。</p><h4 id=使用堆外内存>使用堆外内存</h4><p>堆外内存可以减轻垃圾回收的工作，也加快了复制的速度。</p><p>当需要缓存非常大的数据量时，虚拟机将承受非常大的 GC 压力，因为虚拟接必须检查每个对象是否可以手机并必须访问所有内存也，本地缓存是最快的，但会给虚拟机带来 GC 压力，所以当需要处理非常多的数据量时可以考虑使用堆外内存来进行优化，因为这不会给 Java GC 带来任何压力，让 Java GC 为引用程序完成工作，缓存操作交给堆外。</p><p><code>result.persist(StorageLevel.OFF_HEAP)</code></p><h4 id=调节连接等待时长>调节连接等待时长</h4><h2 id=故障排除>故障排除</h2><h3 id=控制-reduce-端缓冲大小以避免-oom>控制 reduce 端缓冲大小以避免 OOM</h3><p>reduce 缓冲区默认 48 m，调大是以性能换执行。</p><h3 id=jvm-gc-导致的-shuffle-文件拉取失败>JVM GC 导致的 shuffle 文件拉取失败</h3><p>GC 导致连接停滞，连接停滞导致 timeout。提高重试次数和等待时长。</p><h3 id=各种序列化导致的报错>各种序列化导致的报错</h3><p>不可以在 RDD 的元素类型、算子函数里使用第三方不支持序列化的类型，例如 connection。</p></div><div class=post-footer id=post-footer><div class=post-info><div class=post-info-line><div class=post-info-mod><span></span></div></div><div class=post-info-line><div class=post-info-md></div><div class=post-info-share><span></span></div></div></div><div class=post-info-more><section class=post-tags><i class="fas fa-tags fa-fw" aria-hidden=true></i>&nbsp;<a href=/tags/tuning/>Tuning</a>,&nbsp;<a href=/tags/spark/>Spark</a>,&nbsp;<a href=/tags/sql/>SQL</a>,&nbsp;<a href=/tags/hive/>Hive</a></section><section><span><a href=javascript:void(0); onclick=window.history.back()>Back</a></span>&nbsp;|&nbsp;<span><a href=/>Home</a></span></section></div><div class=post-nav><a href=/20220706_nonviolent-communication/ class=prev rel=prev title="Book Summary: Nonviolent Communication"><i class="fas fa-angle-left fa-fw" aria-hidden=true></i>Book Summary: Nonviolent Communication</a>
<a href=/20220805_spring-intro/ class=next rel=next title="Spring Framework Introduction">Spring Framework Introduction<i class="fas fa-angle-right fa-fw" aria-hidden=true></i></a></div></div><div id=comments><div id=valine class=comment></div><noscript>Please enable JavaScript to view the comments powered by <a href=https://valine.js.org/>Valine</a>.</noscript></div></article></div></main><footer class=footer><div class=footer-container><div class=footer-line itemscope itemtype=http://schema.org/CreativeWork><i class="far fa-copyright fa-fw" aria-hidden=true></i><span itemprop=copyrightYear>2018 - 2023</span>&nbsp;|&nbsp;<span class=license><a rel="license external nofollow noopener noreffer" href=https://creativecommons.org/licenses/by-nc/4.0/ target=_blank>CC BY-NC 4.0</a></span></div></div></footer></div><div id=fixed-buttons><a href=# id=back-to-top class=fixed-button title="Back to Top"><i class="fas fa-arrow-up fa-fw" aria-hidden=true></i>
</a><a href=# id=view-comments class=fixed-button title="View Comments"><i class="fas fa-comment fa-fw" aria-hidden=true></i></a></div><link rel=stylesheet href=/lib/valine/valine.min.css><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/css/lightgallery-bundle.min.css><script type=text/javascript src=https://cdn.jsdelivr.net/npm/valine@1.5.0/dist/Valine.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/autocomplete.js@0.38.1/dist/autocomplete.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/algoliasearch@4.13.1/dist/algoliasearch-lite.umd.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/lightgallery.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/thumbnail/lg-thumbnail.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/lightgallery@2.5.0/plugins/zoom/lg-zoom.min.js></script><script type=text/javascript src=https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js></script><script type=text/javascript>window.config={code:{copyTitle:"Copy to clipboard",maxShownLines:40},comment:{valine:{appId:"2jSYKh8PFUje3v7Ie3Nn5v1g-gzGzoHsz",appKey:"MyWn4P9G5N32q3oM5JDnlpdL",avatar:"retro",el:"#valine",emojiCDN:"https://img.t.sinajs.cn/t4/appstyle/expression/ext/normal/",enableQQ:!0,highlight:!0,lang:"en",meta:["nick","mail"],pageSize:10,placeholder:"Your comment ...",recordIP:!0,visitor:!0}},lightgallery:!0,search:{algoliaAppID:"BADKNNRXHD",algoliaIndex:"index",algoliaSearchKey:"7a8c2923330a44bdd9985698e3f28e0c",highlightTag:"em",maxResultLength:6,noResultsFound:"No results found",snippetLength:30,type:"algolia"}}</script><script type=text/javascript src=/js/theme.min.js></script></body></html>