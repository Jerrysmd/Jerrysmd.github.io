[{"categories":["Life"],"content":"Nonviolent Communication is about connecting with ourselves and others from the heart. It’s about seeing the humanity in all of us. It’s about recognizing our commonalities and differences and finding ways to make life wonderful for all of us. ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:0:0","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"非暴力沟通的过程 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:0","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"非暴力沟通的四要素 要素一：客观描述事物，不夹杂任何评判 我们的挑战在于不夹杂任何评判，无论喜欢与否，我们只是说出人们做了什么。 要素二：表达出我们看到这些行为时的感受 是感到伤心、害怕、喜悦、有趣还是心烦呢? 要素三：表达出我们感受的原因 要素四：一个具体的请求 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:1","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"四要素的使用举例 如： “菲利克斯，看到咖啡桌下的两团脏袜子和电视机旁边的三团脏袜子， 我很生气， 因为我希望在我们共用的空间里能多些整洁。 你愿意把你的袜子放在你房间或放进洗衣机里吗？” ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:2","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"四要素的重要性 对这四个要素的清晰表达，表达的形式也不拘泥于语言。而另一部分则是从对方的表达中了解这四个要素。我们通过感知对方此刻的观察、感受和需要，与他们建立连结，进而聆听他们的请求，来找到通过什么方式让他们的生命变得更丰富。 当我们将注意力持续聚焦在以上几个方面，并协助对方也这样做，我们便在彼此的沟通中创造了一种流动，如此你来我往，最终双方都能自然而然地展现善意： 我此刻的观察、感受和需要是什么； 为了让我的生命更美好，我的请求是什么； 你此刻的观察、感受和需要是什么； 为了让你的生命更美好，你的请求是什么。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:3","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"疏离生命的言语 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:2:0","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"道德评判 当他人的行为与我们的价值观不符，我们便认为这个人是错的或是恶的。例如： “你的问题就是太自私了。” “他们有偏见。” “这样做不恰当。” 当我们在分析和评判时，其实都是在表达自身的价值观和需要，但这样的表达方式却是悲剧性的，引发的是对方的防卫与抗拒。就算他人遵从了我们，很有可能是出于恐惧、内疚或羞愧，而非发自内心。而同时，人们这样做其实意味着他们接受了我们的评判，真是两败俱伤。迟早有一天，我们会发现对方不再那么友好，因为由于内部或外部压力而屈服的人们一定会心怀怨恨，他们由此失去尊严，在情绪上付出代价，更不可能怀着善意回应我们的需要和价值观。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:2:1","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"推卸责任 每个人都对自己的思想、情感与行为负责，若无法意识到这点，沟通也会疏离与生命的连结。 一位老师表示：“我厌恶打分。我认为这对学生没什么好处，反而为他们带来焦虑。但我不得不打分，这是学区的规定。”于是，我建议那位老师练习将“我不得不打分，因为这是学区的规定”转换为“我选择打分，因为我想要……”她脱口而出回答：“我选择打分，因为我想保住这份工作。” ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:2:2","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"不带评论的观察 非暴力沟通的第一个要素是区分观察与评论。 当我们想要清晰且诚恳地向他人表达我们的状态时，“观察”是一个重要的要素。如果我们在观察中夹杂着评论，人们便不那么容易真正听见我们想要表达的内容，反而会听到批评，甚至产生抗拒心理。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:3:0","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"区分观察和评论 表达方式 混淆观察和评论 区分观察和评论 1.未对自己的评论负责 “你太大方了” “当我看到你将午餐钱都给了别人，我认为你这样做太大方了” 2.使用的动词暗含评论 “你爱拖延” “你只在考试前一碗学习” 3.推断他人的想法、感受或愿望时，暗示这是唯一的可能 “她无法完成工作” “我不认为她能完成工作” 4.把预测当做确定的事实 “如果你的饮食不均衡，健康就会出问题” “如果你的饮食不均衡，我担心你的健康会出问题” 5.指代不具体 “这户移民家庭照顾不好自己的后院” “我没有看到住在罗斯路1679号的那户移民家庭铲路边的雪” 6.在描述他人能力时，并未表明那只是自己所做的评论 “史密斯是个糟糕的足球运动员” “史密斯在20场球赛中未能进一个球” 7.使用含有评论意味的形容词和副词 “库克长得难看” “库克的长相对我没有吸引力” 注意：总是、永远、从来、每次 之类的词语。这些词在负面表达中经常会引发他人的逆反心理，而非慈悲之情。“经常”、“很少”这样的词也可能混淆观察和评论。 评论 观察 你很少配合我 上一次当我提议一项活动时，你说你不想做 他时常旷课 他上个月每周至少三次旷课 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:3:1","tags":["Life","Book"],"title":"Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"Article description.","date":"2021-12-18","objectID":"/20211218_2021-summary-video/","tags":["Life","Video","Shang hai","Bei jing"],"title":"Memories of 2021","uri":"/20211218_2021-summary-video/"},{"categories":["Life"],"content":"Photos and videos recorded in 2021. ","date":"2021-12-18","objectID":"/20211218_2021-summary-video/:0:0","tags":["Life","Video","Shang hai","Bei jing"],"title":"Memories of 2021","uri":"/20211218_2021-summary-video/"},{"categories":["Technology"],"content":"Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information. ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:0:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"DWM 数据中间层 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"业务数据写入DWM 数据流和程序流程 数据流：：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) -\u003e [FlinkApp -\u003e Kafka(dwm)] 程序：mocklog -\u003e Mysql -\u003e FlinkCDC -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka/Phoneix(zk/hdfs/hbase) -\u003e [UniqueVisitApp -\u003e Kafka] 代码实现 - 用户第一次访问 //UniqueVisitApp // 1.获取执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.读取 kafka dwd_page_log 主题的数据 env.addSource(MyKafkaUtil.getKafkaConsumer(sourceTopic, groupId)); // 3.将每行数据转换为 JSON 对象 jsonObjDS = KafkaDS.map(JSON::parseObject); // 4.过滤数据、状态编程、只保留每天第一个数据 keyedStream = jsonObjDS.keyBy(jsonObj -\u003e jsonObj.getJSONObject(\"common\")).getString(\"mid\"); uvDS = keyedStream.filter(new RichFilterFunction\u003cJSONObject\u003e(){ private ValueState\u003cString\u003e dataState; private SimpleDateFormat simpleDateFormat; @Override public void open(Configuration parameters) throws Exception { valueStateDescriptor = new ValueStateDescriptor\u003c\u003e(\"data-state\", String.class); dataState = getRuntimeContext().getState(valueStateDescriptor); simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd\"); } @Override public boolean filter(JSONObject value) throws Exception{ return {...}; } }); // 5.将数据写入 Kafka uvDS.map(JSONAware::toJSONString).addSink(MyKafkaUtil.getKafkaProducer(sinkTopic)); // 6.启动 env.execute(\"UniqueVisitApp\"); 代码实现 - 用户跳出页面 //UserJumpDetailApp // 1.获取执行环境 // 2.读取 kafka 主题数据创建流 // 3.将每行数据转换为 JSON 对象，提取时间戳 // 4.定义模式序列 // 5.将模式序列作用到流上 // 6.提取匹配上的超时事件, UNION 两种事件 // 7.将数据写入 Kafka // 6.启动 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"Flink Stream Join Window Join stream.join(otherStream) .where(\u003cKeySelector\u003e) .equalTo(\u003cKeySelector\u003e) .window(\u003cWindowAssigner\u003e) .apply(\u003cJoinFunction\u003e) Tumbing Window Join 滚动窗口：和 SparkStreaming 直接 Join 一样 Sliding Window Join 滑动窗口：有可能数据重复 Join Session Window join 会话窗口：两个流的数据一段时间都没有数据，两个流开始 Join Interval Join 对被Join的流有 lower bound 和 upper bound 范围选择（范围选择则要用到状态编程、当前仅支持状态时间，需提取出状态时间） 代码实现 - 订单明细表双流 Join //OrderWideApp //1.获取执行环境 //2.读取 Kafka 主题的数据，转化为 JavaBean 对象并且提取时间戳生成 WaterMark //3.双流 Join //4.关联维度信息 //5.写入Kafka //6.启动任务 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:2","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"维度表关联 优化1：加入旁路缓存 维度关联时查询维度表，先查 Redis，再查 HBASE。 Redis 作为旁路缓存，订单宽表查询维度数据时先查旁路缓存。没有再查询维度数据，同时同步到旁路缓存。 注意问题 1.缓存要设过期时间，不然冷数据会常驻缓存浪费资源。 2.要考虑维度数据是否会发生变化，如果发生变化要主动清楚缓存。 缓存选型 两种：堆缓存 或者 独立缓存服务(Redis, memcache) 堆缓存，性能好，管理性差。 独立缓存服务，有创建连接、网络 IO 等消耗。管理性更强，更容易扩展。 联合使用(LRU Cache，最近最少使用) 代码实现 public class RedisUtil{...} public class DimUtil{//维度表关联 //查询 HBASE 表之前先查询 Redis Jedis jedis = RedisUtil.getJedis(); String dimInfoJsonStr = jedis.get(redisKey); // RedisKey 不用 hash 的原因： // 1.用户数据量大，使用大量数据可能到一条 hash 上，造成热点问题。 // 2.需要设置过期时间。 // RedisKey 不用 Set 的原因： // 1.查询不方便 // 2.需要设置过期时间。 if(dimInfoJsonStr != null){ //归还连接 jedis.close(); //重置过期时间 jedis.expire(redisKey, 24 * 60 * 60); //返回结果 return JsonObject.parseObject(dimInfoJsonStr); } //从 HBASE 拿数据 .... //将数据写入 Redis jedis.set(); jedis.expire(); jedis.close(); return dimInfoJson; } 优化2：异步查询 异步查询把查询操作托管给单独的线程池完成，这样不会因为某个查询造成阻塞，单个并行可以连续发送多个请求，提高并发效率。 代价：消耗更多的 Tasks，threads、Flink-internal network connections 数据流和程序流程 数据流：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) -\u003e [FlinkApp(redis)] -\u003e Kafka(dwm) 程序：mocklog -\u003e Mysql -\u003e FlinkCDC -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka/Phoneix(zk/hdfs/hbase) -\u003e [OrderWideApp(Redis) -\u003e Kafka] 代码实现 public class DimAsyncFunction\u003cT\u003e extends RichAsyncFunciton\u003cT, T\u003e{ @Override public void open(){} @Override public void asyncInvoke(){} @Override public void timeout(){} } public class ThreadPoolUtil{ private static ThreadPoolExecutor threadPoolExecutor = null; private ThreadPoolUtil(){ } public static ThreadPoolExecutor getThreadPool{ //懒汉式-单例模式 if(threadPoolExecutor == null){ synchronized(ThreadPoolUtil.class){ if(threadPoolExecutor == null){ threadPoolExecutor = new ThreadPoolExecutor(corePoolSize = 8,maximumPoolSize = 16,keepAliveTime = 1L,TimUnit.MiNUTES,new LinkedBlockingDeque\u003c\u003e()); } } } return threadPoolExecutor; } } ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:3","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"主流数据丢失 使用 intervalJoin 来管理流的状态时间，保证当支流数据到达时主流数据还保存在状态中。 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:4","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"业务数据写入DWM 代码实现 - 支付宽表 PaymentWideApp; //1.获取执行环境 //2.读取 kafka 主题数据创建流 //3.双流 Join //4.写入Kafka //5.启动任务 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:5","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"DWS 数仓汇总层 轻度聚合，因为 DWS 层要应对很多实时查询，如果完全的明细那么查询的压力是非常大的。 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:2:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"主题宽表写入DWS 数据流和程序流程 数据流：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) -\u003e FlinkApp(redis) -\u003e Kafka(dwm) -\u003e [FlinkApp -\u003e ClickHouse] 程序：mocklog -\u003e Mysql -\u003e FlinkCDC -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka/Phoneix(zk/hdfs/hbase) -\u003e OrderWideApp(Redis) -\u003e Kafka -\u003e [uv/uj -\u003e kafka -\u003e VisitorStatsApp -\u003e ClickHouse] 代码实现 - 访客主题宽表 VisitorStatsApp; //1.获取执行环境 //2.读取 kafka 数据创建流 //3.将每个流处理成相同的数据类型 //4.Union 所有流 //5.提取时间戳生成 WaterMark //6.按照维度信息分组 //7.开窗聚合 10s 的滚动窗口 WindowedStream\u003cVisitorStats, Tuple4\u003cString, String, String, String\u003e, TimeWindow\u003e windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(10))); windowedStream.reduce(new ReduceFunction\u003cVisitorStats\u003e()){} //8.数据写入 ClickHouse //9.启动任务 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:2:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information. real time process ","date":"2021-03-08","objectID":"/20210308_dw-flink/:0:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"ODS 数据采集层 原始数据，日志和业务数据 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"日志数据采集 springboot springboot 好处： 不需要那些繁琐重复的 xml 文件 内嵌 Tomcat，不需要外部 Tomcat 更方便的和各个第三方工具整合，只要维护一个配置文件即可（mysql、redis、es、dubbo、kafka） springboot 和 ssm 的关系 springboot 整合了 springmvc，spring 等核心功能，也就是说本质还是原有的 spring，springmvc 的包，但是 springboot 单独包装了一层，用户不必直接对 springmvc，spring 等在 xml 中配置。 springboot分层 掌握写数据接口 controller层：拦截用户请求，调用Service，响应请求 service层：调用 DAO，处理数据 DAO（使用 MyBatis 时称为 Mapper 层）：获取数据 持久化层：存储数据 SpringBoot 整合Kafka import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; //@Controller //注解标明 Controller @RestController //@Controller + @ResponseBody @Slf4j //lombok中的包 public class LoggerController{ @RequestMapping(\"test2\") public String test2(@RequestParam(\"name\") String nn, @RequestParam(\"age\", defaultValue = \"18\") int age){ return \"success\" + name + age; } @Autowired //自动注入 private KafkaTemplate\u003cString, String\u003e kafkaTemplate; @RequestMapping(\"applog\") public String test2(@RequestParam(\"param\") String jsonStr){ //sparkmall-mock：模拟生成数据模块 //mock服务器不停的访问 http://pc_IP:8080/applog?jsonStr=* 产生原始数据 /*1.修改SpringBoot核心配置： server.port=8081 指定kafka代理地址，可以多个：spring.kafka.bootstrap-servers=hadoop102.9092 指定消息key和消息体的编解码方式： spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerilizer spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerilizer*/ /*2.Resources中添加 logback.xml 配置文件：决定如何打印、落盘、打印哪些日志 ，类似log4j 在注释@Slf4j后，使用log.info(jsonStr);log.warn(jsonStr);log.debug();log.error();log.trace()*/ /*3.自动注入kafkaTemplate，producer send 数据，打开zk、kk服务 运行kafka消费者：bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic ods_base_log*/ kafkaTemplate.send(\"ods_base_log\", jsonStr); return \"success\"; } } 打包集群部署，Nginx 反向代理 简介： 高性能的 HTTP 和 反向代理服务器，特点是占有内存少，并发能力强，在同类型的网页服务器中表现较好。 反向代理和正向代理： 正向代理类似一个跳板机，代理访问外部资源，如 VPN。 反向代理是指以代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器。 Nginx 主要应用 静态网站部署：Nginx 是一个 HTTP 的 web 服务器，可以将服务器上的静态文件（如 HTML、图片等）通过 HTTP 协议返回给浏览器客户端。 负载均衡：如数据请求发到集群中不同的机器上，常用策略：轮询、权重、备机 静态代理：把所有静态资源的访问改为访问 Nginx 而不是 Tomcat，Nginx 更擅长静态资源的处理。 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"业务数据采集 Flink-CDC CDC Change Data Capture(变更数据获取)的简称，捕获增删改。 CDC的种类 CDC主要分为基于查询和基于Binlog两种方式 基于查询 基于Binlog 开源产品 Sqoop、Kafka JDBC Source Canal、Maxwell、Debezium 执行模式 Batch Streaming 是否可以捕获所有数据变化 否 是 延迟性 高延迟 低延迟 是否增加数据库压力 是 否 Flink-cdc-connectors 可以直接从 MySQL、PostgreSQL等数据库直接 读取全量数据和增量变更数据的 source 组件 组件 业务流程 Flink： MySQL -\u003e Canal -\u003e kafka -\u003e Flink -\u003e 业务处理 Flink-CDC: MySQL -\u003e Fink-CDC -\u003e 业务处理 Flink-cdc code public class FlinkCDC{ public static void main(String[] args){ //1.获取执行环境 //1.1开启 CD 并指定后端为FS memory fs rocksdb //1.2开启 checkpoint 实现断点续传 //2.通过 flinkcsc 构建 sourcefunction 并读取数据 //3.打印数据 //4.启动任务 } } FlinkSQL FlinkSQL只能做单表的监控 Debezium可以做多表的监控，需要自定义反序列化器 public class FlihnkCDCWithSQL{ public staic void main(String[] args){ //1.获取执行环境 //2.DDL方式建表 //3.查询数据 //4.将动态表转化为流 //5.启动任务 } } Flink-cdc DataStream CustomerDeserialization @Override public void deserialize(SourceRecord sourceRecord, Collector...){ //1.创建Json对象用于存储最终数据 //2.获取库名、表明 //3.获取\"before\"数据 //4.获取\"after\"数据 //5.获取操作类型 //6.写入JSON对象 //7.输出数据 } CDC Maxwell Canal 对比 FlinkCDC Maxwell Canal SQL和数据关系 无 无 一对一 初始化功能 有(多库多表) 有(单表) 无 断点续传 ck MySQL 本地磁盘 封装格式 自定义 JSON JSON(c/s自定义) 高可用 运行集群高可用 无 集群(ZK) 环境搭建 IDEA目录 功能 app 产生各层数据的 flink 任务 bean 数据对象 common 公共常量 utils 工具类 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:2","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"ODS 总结 保持数据原貌，不做任何修改 FlinkCDC: DataStream / FlinkSQL 区别 FlinkCDC / Maxwell / Canal 区别 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:3","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"DWD 明细数据层 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"日志数据写入DWD层 数据流和程序流程 数据流：：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) 程序：mocklog -\u003e Nginx -\u003e Logger.sh -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka 代码实现 public class BaseLogApp{ public static void main(String[] args){ //1.获取执行环境 //2.消费 ods_base_log 主题数据创建流 //3.将每行数据转换为JSON对象 //3.1发生异常，如不满足JSON格式，捕获写入侧输出流 //4.新老用户校验，状态编辑 //4.1 转态编程 jsonObjDS.keyBy(...).map(new RichMapFunction\u003cJSONObject, JSONObject\u003e()){ private ValueState\u003cString\u003e valueState; @Override public void open(Configuration parameters) throws Exception { valueState = getRuntimeContext().getState(new ValueStateDescriptor\u003cString\u003e(\"value-state\", String.class)); } @Override public JSONObject map(JSONObject value) throws Excepiton { //获取JSON中\"is_new\"标记 String isNew = value.getJSONObject(\"common\").getString(\"is_new\"); //isnew做处理，旧数据不处理 if(\"1\".equals(isNew)){ String state = valueState.value(); if(state != null){ //修改isnew标记 value.getJSONObject(\"common\").put(\"is_new\", \"0\"); }else{ valueState.update(\"1\"); } } return value; } }; //5.分流、侧输出流 页面：主流 启动：侧输出流 曝光：侧输出流 //6.提取侧输出流 //7.将三个流进行打印并输出到对应的kafka主题中 } } ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"业务数据写入DWD层 选型 在实时计算中一般把维度数据写入存储容器，如 HBASE、Redis、MySQL 等。 一般把实时数据写入流中，进入dwd层，最终形成宽表。 HBASE：HBase是建立在HDFS之上,提供高可靠性的列存储，实时读写的数据库系统。它介于Nosql和关系型数据库之间，仅通过主键和主键的range来检索数据，仅支持单行事务。主要用来存储非结构化和半结构化的松散数据。 Redis：分布式缓存，基于内存，强调缓存，支持数据持久化，支持事务操作，NoSQL 类型的Key/vale数据库，同时支持List、Set等更丰富的类型。适合放一些频繁使用，比较热的数据，因为是放在内存中，读写速度都非常快。 MySQL：并发压力大，读取速度慢。 Phoenix \u0026 HBASE 功能： Phoenix是一种专门针对于Hbase 所设计的SQL on Hbase 的一个工具 使用SQL对Hbase进行操作 使用phoenix自动构建二级索引来进行快速查询和维护 原理： 上层提供了SQL接口 底层全部通过Hbase Java API来实现，通过构建一系列的Scan和Put来实现数据的读写 功能非常丰富 底层封装了大量的内置的协处理器，可以实现各种复杂的处理需求，例如二级索引等 优点 支持SQL接口 支持自动维护二级索引 缺点 SQL支持的语法不全面 Bug比较多 实现动态分流方案 ZK存储，通过 watch 感知数据变化 MySQL存储，周期性的同步 *MySQL存储，使用广播流 数据流和程序流程 数据流：：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) 程序： mockDB -\u003e Mysql -\u003e FlinkCDC -\u003e Kafka(ZK) -\u003e BaseDBApp -\u003e Kafka/Hbase(Phoenix, zk, hdfs) 代码实现 BaseDBApp; //1.获取执行环境 //2.消费 kafka ods_base_db 主题数据创建流 //3.将每行数据转换成JSON对象并过滤 主流 //4.使用FlinkCDC消费配置表并处理成 广播流 //5.连接主流和广播流 //6.分流 处理数据 广播流数据和主流数据(根据广播流数据进行处理) //7.提取kafka流数据和HBASE流数据 //8.将kafka数据写入Kafka主题，将HBASE数据写入Phoenix表 //9.启动 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:2","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"维表层DIM层 层 业务 ODS / DWD / DIM 一般与业务需求无关 (离线数仓是 DWS 和 DWT) / (实时数仓是 DWM / DWS) 与业务相关性非常高 ADS 等于业务需求 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:3","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"DWD-DIM 总结 行为数据：DWD（Kafka） 过滤脏数据 –\u003e 侧输出流 脏数据率 新老用户校验 –\u003e 前台校验不准 分流 –\u003e 侧输出流 页面、启动、曝光、动作、错误 写入 kafka 业务数据：DWD（Kafka）- DIM（HBASE） 过滤数据 –\u003e 删除数据 读取配置表创建广播流 连接主流和广播流并处理 广播流数据： 解析数据，Phoenix建表 写入状态广播 主流数据 读取状态 过滤字段 分流（添加SinkTable字段） 提取Kafka和HBASE流分别对应的位置 HBASE流：自定义Sink Kafka流：自定义序列化方式 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:4","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"Article description.","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"CAPTCHA stands for ‘Completely Automated Public Turing test to tell Computers and Humans Apart’. It’s already possible to solve it with the rise of deep learning and computer vision. ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:0:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"大体流程 抓取验证码 给验证码打标签 图片预处理 保存数据集 构建模型训练 提取模型使用 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:1:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"抓取验证码 这个简单，随便什么方式，循环下载一大堆，这里不再赘述。我这里下载了 750 张验证码，用 500 张做训练，剩下 250 张验证模型效果。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:2:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"给验证码打标签 这里的验证码有750张之巨，要是手工给每个验证码打标签，那一定累尿了。这时候就可以使用人工打码服务，用廉价劳动力帮我们做这件事。人工打码后把识别结果保存下来。这里的代码就不提供了，看你用哪家的验证码服务。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:3:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"图片预处理 图片信息： 此验证码是 68x23，JPG格式 二值化： 我确信这个验证码足够简单，在丢失图片的颜色信息后仍然能被很好的识别。并且可以降低模型复杂度，因此我们可以将图片二值化。即只有两个颜色，全黑或者全白。 切割验证码： 观察验证码，没有特别扭曲或者粘连，所以我们可以把验证码平均切割成4块，分别识别，这样图片识别模型就只需要处理10个分类（如果有字母那将是36个分类而已）由于验证码外面有一圈边框，所以顺带把边框也去掉了。 处理结果： 16x21，黑白2位 相关 Python 代码如下： img = Image.open(file).convert('L') # 读取图片并灰度化 img = img.crop((2, 1, 66, 22)) # 裁掉边变成 64x21 # 分离数字 img1 = img.crop((0, 0, 16, 21)) img2 = img.crop((16, 0, 32, 21)) img3 = img.crop((32, 0, 48, 21)) img4 = img.crop((48, 0, 64, 21)) img1 = np.array(img1).flatten() # 扁平化，把二维弄成一维 img1 = list(map(lambda x: 1 if x \u003c= 180 else 0, img1)) # 二值化 img2 = np.array(img2).flatten() img2 = list(map(lambda x: 1 if x \u003c= 180 else 0, img2)) img3 = np.array(img3).flatten() img3 = list(map(lambda x: 1 if x \u003c= 180 else 0, img3)) img4 = np.array(img4).flatten() img4 = list(map(lambda x: 1 if x \u003c= 180 else 0, img4)) ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:4:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"保存数据集 数据集有输入输入数据和标签数据，训练数据和测试数据。 因为数据量不大，简便起见，直接把数据存成python文件，供模型调用。就不保存为其他文件，然后用 pandas 什么的来读取了。 最终我们的输入模型的数据形状为 [[0,1,0,1,0,1,0,1…],[0,1,0,1,0,1,0,1…],…] 标签数据很特殊，本质上我们是对输入的数据进行分类，所以虽然标签应该是0到9的数字，但是这里我们使标签数据格式是 one-hot vectors [[1,0,0,0,0,0,0,0,0,0,0],…] 一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0**，比如[1,0,0,0,0,0,0,0,0,0] 代表1，[0,1,0,0,0,0,0,0,0,0]代表2. 更进一步，这里的 one-hot 向量其实代表着对应的数据分成这十类的概率。概率为1就是正确的分类。 相关 Python 代码如下： # 保存输入数据 def px(prefix, img1, img2, img3, img4): with open('./data/' + prefix + '_images.py', 'a+') as f: print(img1, file=f, end=\",\\n\") print(img2, file=f, end=\",\\n\") print(img3, file=f, end=\",\\n\") print(img4, file=f, end=\",\\n\") # 保存标签数据 def py(prefix, code): with open('./data/' + prefix + '_labels.py', 'a+') as f: for x in range(4): tmp = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] tmp[int(code[x])] = 1 print(tmp, file=f, end=\",\\n\") 经过上面两步，我们在就获得了训练和测试用的数据和标签数据 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:5:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"构建模型训练 数据准备好啦，到了要搭建“管道”的时候了。 也就是你需要告诉 TensorFlow： ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"1. 输入数据的形状是怎样的？ x = tf.placeholder(tf.float32, [None, DLEN]) None 表示不定义我们有多少训练数据，DLEN是 16*21，即一维化的图片的大小。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:1","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"2. 输出数据的形状是怎样的？ y_ = tf.placeholder(\"float\", [None, 10]) 同样None 表示不定义我们有多少训练数据，10 就是标签数据的维度，即图片有 10 个分类。每个分类对应着一个概率，所以是浮点类型。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:2","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"3. 输入数据，模型，标签数据怎样拟合？ W = tf.Variable(tf.zeros([DLEN, 10])) # 权重 b = tf.Variable(tf.zeros([10])) # 偏置 y = tf.nn.softmax(tf.matmul(x, W) + b) 是不是一个很简单的模型？大体就是 y = softmax(Wx+b) 其中 W 和 b 是 TensorFlow 中的变量，他们保存着模型在训练过程中的数据，需要定义出来。而我们模型训练的目的，也就是把 W 和 b 的值确定，使得这个式子可以更好的拟合数据。 softmax 是所谓的激活函数，把线性的结果转换成我们需要的样式，也就是分类概率的分布。 关于 softmax 之类更多解释请查看参考链接。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:3","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"4. 怎样评估模型的好坏？ 模型训练就是为了使模型输出结果和实际情况相差尽可能小。所以要定义评估方式。 这里用所谓的交叉熵来评估。 cross_entropy = -tf.reduce_sum(y_*tf.log(y)) ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:4","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"5. 怎样最小化误差？ 现在 TensorFlow 已经知道了足够的信息，它要做的工作就是让模型的误差足够小，它会使出各种方法使上面定义的交叉熵 cross_entropy 变得尽可能小。 TensorFlow 内置了不少方式可以达到这个目的，不同方式有不同的特点和适用条件。在这里使用梯度下降法来实现这个目的。 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:5","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"训练准备 大家知道 Python 作为解释型语言，运行效率不能算是太好，而这种机器学习基本是需要大量计算力的场合。TensorFlow 在底层是用 C++ 写就，在 Python 端只是一个操作端口，所有的计算都要交给底层处理。这自然就引出了会话的概念，底层和调用层需要通信。也正是这个特点，TensorFlow 支持很多其他语言接入，如 Java, C，而不仅仅是 Python。 和底层通信是通过会话完成的。我们可以通过一行代码来启动会话： sess = tf.Session() # 代码... sess.close() 别忘了在使用完后关闭会话。当然你也可以使用 Python 的 with 语句来自动管理。 在 TensorFlow 中，变量都是需要在会话启动之后初始化才能使用。 sess.run(tf.global_variables_initializer()) ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:6","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"开始训练 for i in range(DNUM): batch_xs = [train_images.data[i]] batch_ys = [train_labels.data[i]] sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) 我们把模型和训练数据交给会话，底层就自动帮我们处理啦。 我们可以一次传入任意数量数据给模型（上面设置None的作用），为了训练效果，可以适当调节每一批次训练的数据。甚至于有时候还要随机选择数据以获得更好的训练效果。在这里我们就一条一条训练了，反正最后效果还可以。要了解更多可以查看参考链接。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:7","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"检验训练结果 这里我们的测试数据就要派上用场了 correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print(sess.run(accuracy, feed_dict={x: test_images.data, y_: test_labels.data})) 我们模型输出是一个数组，里面存着每个分类的概率，所以我们要拿出概率最大的分类和测试标签比较。看在这 250 条测试数据里面，正确率是多少。当然这些也是定义完操作步骤，交给会话来运行处理的。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:6:8","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"提取模型使用 在上面我们已经把模型训练好了，而且效果还不错哦，近 99% 的正确率，或许比人工打码还高一些呢（获取测试数据时候常常返回有错误的值）。但是问题来了，我现在要把这个模型用于生产怎么办，总不可能每次都训练一次吧。在这里，我们就要使用到 TensorFlow 的模型保存和载入功能了。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:7:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"保存模型 先在模型训练的时候保存模型，定义一个 saver，然后直接把会话保存到一个目录就好了。 saver = tf.train.Saver() # 训练代码 # ... saver.save(sess, 'model/model') sess.close() 当然这里的 saver 也有不少配置，比如保存最近多少批次的训练结果之类，可以自行查资料。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:7:1","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"恢复模型 同样恢复模型也很简单 saver.restore(sess, \"model/model\") 当然你还是需要定义好模型，才能恢复。我的理解是这里模型保存的是训练过程中各个变量的值，权重偏置什么的，所以结构架子还是要事先搭好才行。 ","date":"2021-02-04","objectID":"/20210204_tensorflow-captcha/:8:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20210204_tensorflow-captcha/"},{"categories":["Technology"],"content":"Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information. ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:0:0","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据仓库 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:0","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"概念 业务数据： 处理事务过程中产生的数据。登录、下单、支付。 用户行为数据： 与客户端产品交互过程产生的数据。浏览、点击、停留。 offline DW process 数仓是对数据进行 备份、清洗、聚合、统计 等操作。 ODS原始数据层；DWD明细数据层；DWS服务数据层；DWT数据主题层；ADS数据应用层 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:1","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"技术组件 数据采集传输：Flume，Kafka，DataX，Sqoop，Logstash 数据存储：MySQL，HDFS，HBASE，Redis，MongoDB 数据计算：Hive，Spark，Flink，Tez，Storm 数据查询：Presto，Kylin，Impala，Druid 数据可视化：Echarts、Superset、QuickBI、DataV 任务调度：Azkaban、Oozie 集群监控：Zabbix 元数据管理：Atlas 权限管理：Ranger ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:2","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"系统流程图 system_design 业务数据 / 前端 js 埋点行为数据：持久化或不持久化写入数据库。 Nginx：负载均衡，使每个节点数据量保持合理。 Flume：采集日志。可以直接采集到 Hadoop，但 hadoop 可能处理很慢，如双11。可以先写到 Kafka 里。 Flume 组成，Put 事务，Take 事务 Flume 三个器：拦截器，选择器，监控器 Flume 优化：内存，台数 Kafka：23 件事 1.kafka基本信息 2.挂了 3.丢失 4.重复 5.积压 6.优化 7.高效读写的原因 Zookeeper：分布式协调 1.部署多少台 2.选举机制，Paxos算法 Flume： 消费传到 Hadoop Hive： 1.Hive内部表、外部表区别 2.4个by 3.系统函数 4.UDF、UDTF函数 5.窗口函数 6.Hive优化 7.数据倾斜 8.Hive引擎 9.元数据备份 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:3","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"集群配置 配置原则: 消耗内存的组件要分开：HDFS 的 NameNode、Yarn 的 ResourceManager 分开配置 kafka、zk、flume 传输数据比较紧密的放在一起 客户端放在一到两台服务器上，方便外部访问 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:4","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据采集模块 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:0","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Hadoop 优化： 数据均衡：节点与节点间，磁盘与磁盘间，都可以使用命令使数据均衡，threshold 设置差值。 LZO 压缩：hadoop 额外支持 gzip、Snappy、Lzo、Lzop(分片) 压缩方式。LZO创建索引后可以实现分片。 需要压缩的三个地方： Map Reduce 输入端 快 输出端 先考虑数据量 1.小 =\u003e 快 snappy 看需求 1.永久保存：压缩越小越好 2.数据量大 =\u003e 切片 bzip2 lzo =\u003e 需要创建索引 lzo 2.下一个mapreduce输入端 数据量小 =\u003e 快 数据量大 =\u003e 切片 压缩 HDFS参数调优： 对于大集群或者大量客户端的集群来说，通常需要调大 dfs.namenode.handler.count 的个数，默认值为10个。建议调整为 20 * logecluster size Yarn参数调优 yarn-site.xml： 情景描述：总共7台机器，每天几亿条数据，数据源 -\u003e Flume -\u003e Kafka -\u003e HDFS -\u003e Hive 问题：数据统计主要用 HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启了 JVM 重用，而且IO 没有阻塞，内存用了不到 50%，但还是跑的非常慢，而且数据量洪峰时，整个集群都会宕掉，基于这种情况的优化方案是什么。 解决办法：典型的内存利用率不够，一般是 Yarn 的2个配置造成的，单个任务可以申请的最大内存大小，和 Hadoop 单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。 （NodeManager 默认内存大小 8g，单个任务默认内存大小 8g，maptask默认内存大小 1g，reducetask默认内存大小 1g。）NodeManager内存改为节点 80% 左右的内存大小。单个任务内存大小根据每 128M 数据分配1g内存。maptask 和 reducetask 内存若不支持压缩也需要调大。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:1","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Zookeeper ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:2","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Kafka kafka 机器数量计算： 机器数量（经验公式）= 2 *（峰值生产速度 * 副本数 / 100) + 1 先通过压测拿到峰值生产速度，比如峰值生产速度是 50M/s。副本数默认是1个，生产环境可以设置为2。Kafka 机器数量 = 2 * （50 * 2 / 100）+ 1 = 3 台。 kafka producer 生产者压力测试： bin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092 参数说明： –record-size：是一条信息有多大，单位是字节。 –num-records：是总共发送多少条信息。 –throughput：是每秒多少条信息，设成-1表示不限流，可测出生产者最大吞吐量。 kafka consumer 消费者压力测试： 如果 IO、CPU、内存、网络 这四个指标都不能改变，考虑增加分区数来提升性能。 参数说明： –broker-list：指定 kafka 集群地址 –topic：指定 topic 的名称 –fetch-size：指定每次 fetch 的数据大小 –messages：总共要消费的消息个数 kafka 分区数计算： 创建一个只有一个分区的topic 测试这个 topic 的 producer 吞吐量和 consumer 吞吐量。 假设他们的值是 Tp 和 Tc，单位是 MB/s。 然后假设总的目标吞吐量是 Tt，那么分区数 = Tt/min( Tp，Tc) 分区数一般设置为：3 - 10个 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:3","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Flume Flume 组件选型 Source： Taildir Source：支持断点续传、多目录。但可能有数据重复（重复数据在下一级解决：在 hive 的 dwd 层、spark streaming 使用 group by、开窗取窗口第一条数据） Channel： kafka channel：数据存放在 kafka 里面。kafka 数据存在磁盘里。 基于磁盘，可靠性高 kafka channel \u003e memory channel + kafka sink 如果下一级是 kafka，优先选择 kafka channel 如果下一级不是 kafka，对可靠性要求比较高，金融领域，优先选择 file channel 如果下一级不是 kafka，对可靠性要求不高，选择 memory channel，追求效率 kafka channel 1.6版本产生，但有 bug 产生的数据是 header + 内容，1.7 版本解决 Flume 拦截器 Flume 消费 Kafka 里面的数据时，可能已经是第二天，数据有可能发往第二天的 HDFS 路径。 解决思路：拦截 json 日志，通过 fastjson 解析 json，获取实际时间 ts。将获取的 ts 时间写入拦截器 header 头，header 的 key 必须是 timestamp，因为 Flume 框架会根据这个 key 的值识别为时间，写入到 HDFS。 创建 Maven 工程 flume-interceptor 创建包名：com.whatever.flume.interceptor 在 pom.xml 文件中添加配置 在 com.whatever.flume.interceptor 包下创建 JSONUtils 类 在 com.whatever.flume.interceptor 包下创建 ETLInterceptor 类，重写方法，判断是否符合 JSON 格式 打包，分发到 Flume 节点 flume_scs Flume优化： 内存参数设置优化： JVM heap 一般设置为 4G 或更高 -Xmx（启动内存） 与 -Xms（运行内存） 最好设置一致，减少内存抖动带来的性能影响，如果不一致容易倒置 fullgc ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:4","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"DataX 同步策略： 同步策略 优点 缺点 全量同步 逻辑简单 效率低 增量同步 效率高，无需同步和存储重复数据 逻辑复杂 DadaX 相比 Sqoop 功能更强大： 功能 DataX Sqoop 运行模式 单进程多线程 MR 分布式 不支持 可以通过调度系统规避 支持 流控 有 需定制 统计信息 有 分布式数据收集不方便 数据校验 有 需定制 监控 需定制 需定制 DataX 调度决策思路： 举例，用户提交了一个 DataX 作业，并且配置了总的并发数为 20，目的是对一个有 100 张分表的 MySQL 进行同步。调度决策思路是： dataX job 根据分库分表切分策略，将同步工作分成 100 个 Task。 根据配置的总的并发度 20，以及每个 TaskGroup 的并发度 5，dataX共需要 4 个 TaskGroup。 4 个 TaskGroup 分 100 个 Task，每个 TaskGroup 负责 25个 Task。 同步 HDFS 数据到 MySQL 案例： 选用 HDFSReader 和 MySQLWriter，分别设置 json 配置文件。 DataX优化： 提升 DataX job 内 channel 并发数，内存的占用显著增加，每个 channel 会有一个 Buffer，作为临时数据交换的缓冲区，而在 reader 和 writer 中，也会有 buffer，为了防止 OOM，需要调大 JVM 的堆内存。 调整 JVM xms xmx 参数的两种方式：一种是直接更改 datax.py 脚本；另一种是在启动的时候，加上参数： python datax/bin/datax.py --jvm=\"-Xms8G -Xmx8G\" /path/to/your/job.json ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:5","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Maxwell MySQL 变更数据抓取软件，实时监控 MySQL 的 insert、update、delete。并将变更数据发送给 Kafka 等流数据处理平台。 原理： 读取 MySQL 的二进制日志（Binlog），从中获取变更数据。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:6","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据采集模块总结 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:0","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Linux # 命令 top ps -ef df -h #磁盘空间 netstat #查看端口号 # 4个工具 awk sed sort cut # 写过什么脚本 1. 同步分发 2. 启动停止 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:1","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Sqoop 导入数据的方式： 全量：where 1=1 增量：where 创建时间 = 当天 新增及变化：where 创建时间 = 当天 or 操作时间 = 当天 特殊：只导入一次 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:2","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Hadoop 常用端口号： 2.x：50070/8020：HDFS 8088：任务情况 19888：Job history 3.x：9870/8020：HDFS 8088：任务情况 19888：Job history 配置文件： /etc/profile 2.x：core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml slaves 3.x：core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml workers ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:3","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"HDFS HDFS 的读写流程，笔试： 写详细步骤： 客户端向NameNode发出写文件请求。 检查是否已存在文件、检查权限。若通过检查，直接先将操作写入EditLog，并返回输出流对象。 （注：WAL，write ahead log，先写Log，再写内存，因为EditLog记录的是最新的HDFS客户端执行所有的写操作。如果后续真实写操作失败了，由于在真实写操作之前，操作就被写入EditLog中了，故EditLog中仍会有记录，我们不用担心后续client读不到相应的数据块，因为在第5步中DataNode收到块后会有一返回确认信息，若没写成功，发送端没收到确认信息，会一直重试，直到成功） client端按128MB的块切分文件。 client将NameNode返回的分配的可写的DataNode列表和Data数据一同发送给最近的第一个DataNode节点，此后client端和NameNode分配的多个DataNode构成pipeline管道，client端向输出流对象中写数据。client每向第一个DataNode写入一个packet，这个packet便会直接在pipeline里传给第二个、第三个…DataNode。 （注：并不是写好一个块或一整个文件后才向后分发） 每个DataNode写完一个块后，会返回确认信息。 （注：并不是每写完一个packet后就返回确认信息，因为packet中的每个chunk都携带校验信息，没必要每写一个就汇报一下，这样效率太慢。正确的做法是写完一个block块后，对校验信息进行汇总分析，就能得出是否有块写错的情况发生） 写完数据，关闭输输出流。 发送完成信号给NameNode。 （注：发送完成信号的时机取决于集群是强一致性还是最终一致性，强一致性则需要所有DataNode写完后才向NameNode汇报。最终一致性则其中任意一个DataNode写完后就能单独向NameNode汇报，HDFS一般情况下都是强调强一致性） 读详细步骤： client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。 就近挑选一台datanode服务器，请求建立输入流 。 DataNode向输入流中中写数据，以packet为单位来校验。 关闭输入流 小文件危害： 存储时间：小文件会占用 namenode 一整个区块，一个文件块150字节。128g内存的 namenode 能存储 9亿个文件块。 计算时间：1个文件块开启1个maptask，一个maptask就是1g内存。 如何解决： 合并：CombineTextinputformat =\u003e 减少 maptask har归档 JVM重用 HDFS 有几个副本： 3 HDFS 块大小： 2.x 3.x：128M ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:4","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Mapreduce shuffle 及其优化： shuffle 定义：map 方法之后，reduce方法之前混洗的过程。 map 阶段之后： 先调用 getpartition 方法，标记数据是哪一个分区。 进入环形缓冲区（缓冲区大小100M ），到达 80% 进行溢写（排序手段：对 key 的索引按照 字典序 进行 快排） 对溢写文件进行归并，写到指定磁盘，持久化到磁盘。 reduce 阶段之前： 拉取磁盘上指定的数据，放到内存中（内存不够在磁盘中） reduce 操作 优化： getpartition 方法时：自定义分区：解决数据倾斜问题。 调整缓冲区大小和溢写阈值，缓冲区大小由100M调整到200M，溢写阈值由80%调整到90%。产生大量的溢写文件，使用combiner聚类文件（前提是不影响最终业务逻辑）。这样减少了写文件的个数，合并会快一些。 溢写文件归并到磁盘：减少磁盘IO压缩，使用快的压缩算法：snappy/lzo Map Reduce 输入端 快 输出端 先考虑数据量 1.小 =\u003e 快 snappy 看需求 1.永久保存：压缩越小越好 2.数据量大 =\u003e 切片 bzip2 lzo =\u003e 需要创建索引 lzo 2.下一个mapreduce输入端 数据量小 =\u003e 快 数据量大 =\u003e 切片 压缩 reduce 拉取磁盘指定的数据，默认拉取5个，可以改成10 - 20个 yarn 和 hadoop内存大小调整：单个任务可以申请的最大内存大小，和 Hadoop 单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。 （NodeManager 默认内存大小 8g，单个任务默认内存大小 8g，maptask默认内存大小 1g，reducetask默认内存大小 1g。）NodeManager内存改为节点 80% 左右的内存大小。单个任务内存大小根据每 128M 数据分配1g内存。maptask 和 reducetask 内存若不支持压缩也需要调大。 进行了几次排序： map后进行一次快排，一次归并。 reduce时进行一次归并，一次分组。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:5","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Yarn yarn 的工作机制： 用户使用客户端向 RM 提交一个任务，同时指定提交到哪个队列和需要多少资源。用户可以通过每个计算引擎的对应参数设置，如果没有特别指定，则使用默认设置。 RM 在收到任务提交的请求后，先根据资源和队列是否满足要求选择一个 NM，通知它启动一个特殊的 container，称为 ApplicationMaster（AM），后续流程由它发起。 AM 向 RM 注册后根据自己任务的需要，向 RM 申请 container，包括数量、所需资源量、所在位置等因素。 如果队列有足够资源，RM 会将 container 分配给有足够剩余资源的 NM，由 AM 通知 NM 启动 container。 container 启动后执行具体的任务，处理分给自己的数据。NM 除了负责启动 container，还负责监控它的资源使用状况以及是否失败退出等工作，如果 container 实际使用的内存超过申请时指定的内存，会将其杀死，保证其他 container 能正常运行。 各个 container 向 AM 汇报自己的进度，都完成后，AM 向 RM 注销任务并退出，RM 通知 NM 杀死对应的 container，任务结束。 hadoop 调度器： 三种调度器：FIFO、容量调度器、公平调度器 Apache默认调度器：容量调度器（apache默认资源很小） CDH默认调度器：公平调度器 特点： FIFO调度器特点：支持单队列，任务先进先出，企业中几乎不用。 容量调度器：底层是FIFO调度器。支持多队列，可以借用其他队列资源。先进入的任务优先执行。 公平调度器：支持多队列(并发度比容量调度高)，可以借用其他队列资源。队列中所有任务公平享有队列资源。 容器调度器配置多队列： 按照分析引擎创建队列：mr、spark、flink、hive 按照业务：登录、注册、购物车、下单、支付 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:6","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Zookeeper 选举机制： 半数机制：3台服务器，2台投谁谁就是老大 常用命令： delete create 安装台数： 根据集群机器数量，3-11台。zk台数多了，增加可靠性，但降低选举效率。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:7","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Flume 组成：（source、channel、sink、put事务、take事务） source：（Taildirsource） 优点：断点续传、多目录，实时监控。缺点：文件名更名(如log4j就是更名的日志框架)之后重新读取该文件造成重复。注意：让Taildirsource判断文件时只看iNode值 Apache 1.7、CDH1.6版本产生的 没有断点续传怎么办：自定义 source，读取的位置始终持久到磁盘上 挂了：不会丢数据，但有可能重复数据 如何处理重复数据：不处理或下一级处理，采用事务方式效率太低 不支持递归遍历文件夹：自定义递归 channel：（KafkaChannel） 优点：将数据写入Kafka，省了一层sink file channel 磁盘|可靠性高|性能低 memory channel 内存|可靠性低|性能高 kafka channel 存kafka里，kafka存在磁盘|可靠性高|性能优于 memory channel + kafka sink sink：（HDFS sink） 用来解决小文件问题 设置大小128M、时间（1-2小时）、event个数禁止0 KafkaChannel：在kafka中既可以生产者也可以作为消费者 用法： source - kafkachannel - sink source - kafkachannel（作为消费者） kafkachannel - sink（作为生产者） 三个器：（拦截器、选择器、监控器） 拦截器：时间戳拦截器 或者 自定义拦截器 选择器：默认是replicating，把数据发往下一级所有通道 监控器：g, 监控 put、take 事务尝试提交的次数远远大于最终提交成功的次数，说明 flume 异常。自身：增加内存，其他：增加 flume 台数 优化： file channel 能配置多目录就配置多目录（不同磁盘） 使用 HDFS sink 用来解决小文件问题 通过监控器找寻原因，增加内存和台数 挂了恢复： channel 挂了：memory channel 100个event 数据丢失。（file channel 100万个event，没有丢失风险） taildir source 挂了：不会丢数，可能重复数据 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:8","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Kafka 基本信息： 组成： producer： ACK：0 1 -1 拦截器，序列化器，分区器 发送流程 幂等性，事务 分区规则： 有指定分区则发往指定分区 ；没有指定分区，则根据key值Hash；没有指定分区也没有key的时候，轮询（粘性） broker： Topic：副本（高可靠） ISR：LEO、HW 分区（高并发，负载均衡，防止热点） consumer 分区分配规则 offset保存： 默认：__consumer_offsets主题。 其他：手动维护Offset（MySQL） 保存数据\u0026保存Offset写到一个事物 精准一次消费 先保存数据后保存Offset 重复数据+幂等性（精准一次消费） 先保存Offset后保存数据 丢失数据 zk：存放 broker（id, topic），consumer（相应的 offset），没有生产者信息 安装台数：2 * (生产者峰值生产速率(压测) * 副本 / 100) + 1, 先使用3台机器压测 副本数：通常2个副本，默认1个副本。多：可靠性高，传输性能差 数据量：100万日活 一人100条 一条1k。 1M/s 数据保存多久：3天 磁盘预留：100g(一天数据) * 2个副本 * 3天 / 0.7 监控：kafka egale：开源框架 分区：提高并发度：先设置1个，通过压测生产者峰值生产速率tp、消费者峰值消费速率tc 期望吞吐量t 100m/s 分区数 = t / min (tp, tc) 分区分配策略： range 默认 roundrobin 采用 roundrobin 所有数据采用 hash 方式大散，再采用轮询的方式执行 isr：主要解决leader挂了谁当老大，在 isr 队列的都有机会当老大。 多少个 topic：满足下一级消费者即可。 挂了: 短期：flume channel 缓冲数据 长期：日志服务器保留30天日志 丢：ack 配置参数 0：发送数据，不需要应答。可靠性最差，传输性能最好。 1：发送数据，leader 应答。可靠性一般，传输性能一般。 -1：发送数据，leader 和 follower 共同应答。可靠性最高，传输性能最差。 企业中：0几乎不用，不太重要1，重要-1 重复： 采用 事务、幂等性、ack = -1 幂等性：单分区单会话内数据不重复 事务：每条数据比较，可以比较多个分区 事务： 在下一级处理： 积压：kafka 保存数据的时间是有限的，没有及时消费完 增加分区，同时增加消费者对应的CPU核数 增加消费者 batchsize 优化： 日志保存3天 增加网络通信延迟时间 高效读写原因： 集群，分区 顺序读写600m/s(vip)，随机读写100m/s 零拷贝 传输一条2m日志文件，现象：卡住。调整两个参数大小 数据过期：删除 或者 压缩，通常删除，副本备份30天数据 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:9","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Hive 组成： hive_structure 内部表和外部表区别： 内部表删除数据：元数据、原始数据 删掉 外表删数据：元数据删掉，原始数据保留 只有自己临时使用表，创建内部表。绝大部分场景都是外部表 4个by： order by：全局排序 （只有一个reduce task，数据倾斜，资源分配不足）不使用 sort by：局部排序，只保证每个 reducer task 输出有序 distribute by：分桶，取余分组，没有排序 cluster by： distribute by + sort by 自定义函数： 自定义udf函数步骤： 1进1出，一行，定义类 继承udf，重写 evaluate 方法 自定义udtf函数步骤：多进多出，继承 udtf，重写三个方法（初始化（定义名称、校验返回值类型）、close、process）。打包+上传hdfs+在hive客户端创建 窗口函数：rank、over、topn 优化： 行列过滤：join -\u003e where =\u003e where -\u003e join 分区 小文件：combineHiveInputformat 或者 JVM重用 或者 merge 压缩 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:10","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据仓库 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:0","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据分层 ODS（Operation Data Store）原始数据层：存放原始数据，起到备份的作用 DWD（Data Warehouse Detail）明细数据层：清洗，去空，去脏数据，维度退化 DWS（DW service）按天进行轻度汇总 DWT（DW Topic）按主题进行汇总 ADS（Application Data Store）为各种统计报表提供数据 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:1","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数仓建模 ODS层： 创建支持LZO压缩的表：减少存储空间，100g -\u003e 10g - 5g(取决于文件格式，比如JSON的嵌套方式，CSV的列数量) 创建分区表：防止全表扫描（导数据：全量、增量、新增和变化） 保持数据原貌不做任何修改，起到备份数据的作用 DWD层：🔶 选择业务过程 -\u003e 声明粒度 -\u003e 确认维度 -\u003e 确认事实 业务过程：哪些协议需要做处理 声明粒度：粒度可以是天、一人、区域 确认维度：时间维度、地区维度、用户维度 确认事实：业务中的度量值（次数、个数、件数、金额、可以进行累加） DWS层： 对各个主题对象进行统计：天、月、年、地区、用户 DWT层： 统计累计行为：从开始到结束，总结表 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:2","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据处理 数据处理大致分为两大类： OLTP（on-line transaction processing）联机事务处理 OLAP（on-line Analytical Processing）联机分析处理 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:3","tags":["DataWarehouse","Distribution"],"title":"Data Warehouse: Offline","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Article description.","date":"2020-12-15","objectID":"/20201215_parquet-format/","tags":["Spark","Distribution","Hdfs"],"title":"Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types. This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO. HDFS 默认存的是文本格式，所以 hive, presto，都是在文本格式上做计算，hadoop本身是全表扫，只是分布式而以，所以我们之前用的就是分布式的全表扫而以，没有发挥出数据仓库该有的功能,列式存储，天然擅长分析，千万级别的表，count，sum，group by ，秒出结果。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:0:0","tags":["Spark","Distribution","Hdfs"],"title":"Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"1、场景描述： 对客户日志做了数据仓库，但实际业务使用中有一些个共同点， A 需要关联维度表 B 最终仅取某个产品一段时间内的数据 C 只关注其中极少的字段 基于以上业务，我们决定每天定时统一关联维度表，对关联后的数据进行另外存储。各个业务直接使用关联后的数据进行离线计算。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:1:0","tags":["Spark","Distribution","Hdfs"],"title":"Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"2、选择parquet的外部因素 在各种列存储中，我们最终选择 parquet 的原因有许多。除了 parquet 自身的优点，还有以下因素： A、公司当时已经上线spark 集群，而spark天然支持parquet，并为其推荐的存储格式(默认存储为parquet)。 B、hive 支持 parquet 格式存储，如果以后使用hivesql 进行查询，也完全兼容。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:2:0","tags":["Spark","Distribution","Hdfs"],"title":"Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"3、选择 parquet 的内在原因 下面通过对比 parquet 和 csv，说说parquet自身都有哪些优势 csv在hdfs上存储的大小与实际文件大小一样。若考虑副本，则为实际文件大小*副本数目。（若没有压缩） 3.1 parquet采用不同压缩方式的压缩比 说明：原始日志大小为214G左右，120多的字段 采用csv（非压缩模式）几乎没有压缩。 采用 parquet 非压缩模式、gzip、snappy格式压缩后分别为17.4G、8.0G、11G，达到的压缩比分别是：12、27、19。 csv parquet 非压缩 parquet gzip parquet snappy 存储大小 214G 17.4G 8.0G 11G 压缩比例 12 27 19 若我们在 hdfs 上存储3份，压缩比仍达到4、9、6倍 3.2 分区过滤与列修剪 3.2.1分区过滤 parquet结合spark，可以完美的实现支持分区过滤。如，需要某个产品某段时间的数据，则hdfs只取这个文件夹。 spark sql、rdd 等的filter、where关键字均能达到分区过滤的效果。 使用spark的partitionBy 可以实现分区，若传入多个参数，则创建多级分区。第一个字段作为一级分区，第二个字段作为2级分区。。。。。 3.2.2 列修剪 列修剪：其实说简单点就是我们要取回的那些列的数据。 当取得列越少，速度越快。当取所有列的数据时，比如我们的120列数据，这时效率将极低。同时，也就失去了使用parquet的意义。 3.2.3 分区过滤与列修剪测试如下： 说明： A、task数、input值、耗时均为spark web ui上的真实数据。 B、之所以没有验证csv进行对比，是因为当200多G，每条记录为120字段时，csv读取一个字段算个count就直接lost excuter了。 C、注意：为避免自动优化，我们直接打印了每条记录每个字段的值。（以上耗时估计有多部分是耗在这里了） D、通过上图对比可以发现： 当我们取出所有记录时，三种压缩方式耗时差别不大。耗时大概7分钟。 当我们仅取出某一天时，parquet的分区过滤优势便显示出来。仅为6分之一左右。貌似当时全量为七八天左右吧。 当我们仅取某一天的一个字段时，时间将再次缩短。这时，硬盘将只扫描该列所在rowgroup的柱面。大大节省IO。如有兴趣，可以参考 深入分析Parquet列式存储格式 E、测试时请开启filterpushdown功能 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:3:0","tags":["Spark","Distribution","Hdfs"],"title":"Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"4、结论 parquet的gzip的压缩比率最高，若不考虑备份可以达到27倍。可能这也是spar parquet默认采用gzip压缩的原因吧。 分区过滤和列修剪可以帮助我们大幅节省磁盘IO。以减轻对服务器的压力。 如果你的数据字段非常多，但实际应用中，每个业务仅读取其中少量字段，parquet将是一个非常好的选择。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:4:0","tags":["Spark","Distribution","Hdfs"],"title":"Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"Article description.","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"A high performance columnar OLAP database management system for real-time analytics using SQL. ClickHouse can be customized with a new set of efficient columnar storage engines, and has realized rich functions such as data ordered storage, primary key indexing, sparse indexing, data sharding, data partitioning, TTL, and primary and backup replication. ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:0:0","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Intro Yandex，2016年，列式存储数据库，在线分析处理查询（OLAP），SQL查询实时生成分析数据报告。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:1:0","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Data Type MySQL Hive ClickHouse(区分大小写) byte tinyint Int8 short smallint Int16 int bigint Int64 timestamp timestamp DataTime … … … 枚举 数据类型中没有布尔值，可以通过枚举代替。 创建一个带有一个枚举 Enum8(’true’ = 1, ‘false’ = 2) 类型的列： CREATE TABLE t_enum ( x Enum8('true' = 1, 'false' = 2) ) ENGINE = TineLog 数组 Array(T)，不推荐使用多维数组，对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。 可以使用 array(T) 或 [] 创建数组 SELECT array(1, 2) AS arr, toTypeName(arr) 元组 Tuple(T1, T2, …)，每个元素都有单独的类型 SELECT array(1, 'a') AS arr, toTypeName(arr) ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:2:0","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Table Engine MySQL 默认的引擎：InnoDB 是事务型数据库的首选引擎，支持事务安全表（ACID）。 使用二十种表引擎决定了： 数据存储方式和位置，写到内存还是磁盘 支持哪些查询以及如何支持 并发访问数据 索引的使用 是否多线程请求 数据复制参数 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:0","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"1. TinyLog 最简单的表引擎，用于将数据存储在磁盘上。每列都存储在单独的压缩文件中，写入时，数据将附加到文件末尾。 磁盘 不支持索引 不支持并发写，不支持一边读一边写 create tabele t (a UInt16, b String) engine = TinyLog ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:1","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"2. Memory 内存引擎，重启数据就会消失，读写不互相阻塞，不支持索引。简单查询性能表现超过 10 G/s。测试场景或数据量又不太大的场景（上限大约 1 亿行）。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:2","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"3. Merge （不要和 MergeTree 引擎混淆）本身不存储数据，但可用于同时从任意多个其他的表中读取数据，读是自动并行的，不支持写入。读数据时，那些被真正读取到数据的表的索引会被使用。 create table t(id UInt16, name String) engine = Merge(currentDatabase(), '^t'); ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:3","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"4. MergeTree (重点) clickhouse 中最强大的表引擎，当巨量数据要插入到表中，需要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比插入时不断修改（重写）数据进行存储，这种策略会高效很多。 数据按照主键排序 可以使用分区（如果指定了主键） 支持数据副本 支持数据采样 参数： ENGINE = MergeTree() PARTITION BY: 分区键。要按月分区，可以使用表达式toYYYYMM(data_column) ORDER BY: 表的排序键，可以是一组列的元组或任意的表达式 PRIMARY KEY: 主键，需要与排序键字段不同，默认情况下主键跟排序键相同 SAMPLE BY: 用于抽样的表达式，如果要用抽样表达式，主键中必须包含这个表达式 SETTINGS: 影响 MergeTree 性能的额外参数： index_granularity: 索引粒度，即索引中相邻【标记】间的数据行树，默认 8192 use_minimalistic_part_header_in_zookeeper: 数据片段头在 Zookeeper 中的存储方式 min_merge_bytes_to_use_direct_io: 使用直接 I/O （不经过缓存 I/O）来操作磁盘的合并操作时要求的最小数据量。当数据量特别大时，没必要经过缓存 I/O，默认数据小于 10G 会开启缓存 I/O ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:4","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"5. ReplacingMergeTree 在 MergeTree 的基础上，添加了 “处理重复数据” 的功能，该引擎和 MergeTree 的不同之处在于它会删除具有相同主键的重复项。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:5","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"6. SummingMergeTree 在 MergeTree 的基础上，添加了 “合并重复数据” 的功能，会把具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。 create table smt_table (date Date, name String, sum Uint16, not_sum UInt16) engine = SummingMergeTree(sum) partition by date order by (date, name) ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:6","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"7. Distributed (重点) 分布式引擎，本身不存储数据，但可以在多个服务器上进行分布式查询。读是自动并行的。读取时，远程服务器表的索引会被使用。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:7","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Im/Export HDFS Clickhouse 从 18.16.0 版本开始支持从 HDFS 读取文件，在 19.1.6 版本支持读和写，在 19.4 版本开始支持 Parquet 格式。 案例一：client 通过 clickhouse 查询引擎访问 HDFS 上的文件 # 上传 csv 到 hdfs 根目录 hadoop fs -put module.csv / # 进入 clickhouse 命令 clickhouse-client -h hadoop2 -m # 建表 create table hdfs_module_csv ( id Int8, name String ) Engine = HDFS('hdfs://hadoop2:9000/module.csv','CSV'); 验证： # 删除 HDFS 上的 CSV，验证是否在 clickhouse 上占用空间 hadoop fs -rm -r /module.csv # sql SELECT * from hdfs_module_csv; # error 案例二：HDFS 插入数据到本地存储引擎，client 通过 clickhouse 查询引擎查询 clickhouse 本地数据 # 通过 sql 插入到本地 insert into student_local select * from hdfs_module_csv ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:4:0","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Optimize max_memory_usage 此参数在 /etc/clickhouse-server/user.xml 中，表示单词 Query 占用内存最大值，超过 Query 失败，尽量调大。 删除多个节点上的同一张表 使用 on cluster 关键字。 drop table * on cluster table_name 自动数据备份 只用 MergeTree 引擎支持副本。 设置分片和分片副本节点。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:5:0","tags":["Distribution","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Article description.","date":"2020-10-26","objectID":"/20201026_hbase-optimize/","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"HBase is a high reliability, high performance, column-oriented, and scalable distributed database. However, the READ/write performance deteriorates when a large amount of concurrent data or existing data is generated. You can use the following methods to improve the HBase search speed. ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:0:0","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"HBase 数据表优化 HBase 是一个高可靠性、高性能、面向列、可伸缩的分布式数据库，但是当并发量过高或者已有数据量很大时，读写性能会下降。我们可以采用如下方式逐步提升 HBase 的检索速度。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:0","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"预先分区 默认情况下，在创建 HBase 表的时候会自动创建一个 Region 分区，当导入数据的时候，所有的 HBase 客户端都向这一个 Region 写数据，直到这个 Region 足够大了才进行切分。一种可以加快批量写入速度的方法是通过预先创建一些空的 Regions，这样当数据写入 HBase 时，会按照 Region 分区情况，在集群内做数据的负载均衡。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:1","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"Rowkey 优化 HBase 中 Rowkey 是按照字典序存储，因此，设计 Rowkey 时，要充分利用排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。 此外，Rowkey 若是递增的生成，建议不要使用正序直接写入 Rowkey，而是采用 reverse 的方式反转 Rowkey，使得 Rowkey 大致均衡分布，这样设计有个好处是能将 RegionServer 的负载均衡，否则容易产生所有新数据都在一个 RegionServer 上堆积的现象，这一点还可以结合 table 的预切分一起设计。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:2","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"减少 ColumnFamily 数量 不要在一张表里定义太多的 ColumnFamily。目前 Hbase 并不能很好的处理超过 2~3 个 ColumnFamily 的表。因为某个 ColumnFamily 在 flush 的时候，它邻近的 ColumnFamily 也会因关联效应被触发 flush，最终导致系统产生更多的 I/O。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:3","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"缓存策略 (setCaching） 创建表的时候，可以通过 HColumnDescriptor.setInMemory(true) 将表放到 RegionServer 的缓存中，保证在读取的时候被 cache 命中。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:4","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"设置存储生命期 创建表的时候，可以通过 HColumnDescriptor.setTimeToLive(int timeToLive) 设置表中数据的存储生命期，过期数据将自动被删除。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:5","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"硬盘配置 每台 RegionServer 管理 10~1000 个 Regions，每个 Region 在 1~2G，则每台 Server 最少要 10G，最大要 1000*2G=2TB，考虑 3 备份，则要 6TB。方案一是用 3 块 2TB 硬盘，二是用 12 块 500G 硬盘，带宽足够时，后者能提供更大的吞吐率，更细粒度的冗余备份，更快速的单盘故障恢复。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:6","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"分配合适的内存给 RegionServer 服务 在不影响其他服务的情况下，越大越好。例如在 HBase 的 conf 目录下的 hbase-env.sh 的最后添加 export HBASE_REGIONSERVER_OPTS=”-Xmx16000m $HBASE_REGIONSERVER_OPTS” 其中 16000m 为分配给 RegionServer 的内存大小。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:7","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"写数据的备份数 备份数与读性能成正比，与写性能成反比，且备份数影响高可用性。有两种配置方式，一种是将 hdfs-site.xml 拷贝到 hbase 的 conf 目录下，然后在其中添加或修改配置项 dfs.replication 的值为要设置的备份数，这种修改对所有的 HBase 用户表都生效，另外一种方式，是改写 HBase 代码，让 HBase 支持针对列族设置备份数，在创建表时，设置列族备份数，默认为 3，此种备份数只对设置的列族生效。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:8","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"WAL（预写日志） 可设置开关，表示 HBase 在写数据前用不用先写日志，默认是打开，关掉会提高性能，但是如果系统出现故障 (负责插入的 RegionServer 挂掉)，数据可能会丢失。配置 WAL 在调用 Java API 写入时，设置 Put 实例的 WAL，调用 Put.setWriteToWAL(boolean)。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:9","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"批量写 HBase 的 Put 支持单条插入，也支持批量插入，一般来说批量写更快，节省来回的网络开销。在客户端调用 Java API 时，先将批量的 Put 放入一个 Put 列表，然后调用 HTable 的 Put(Put 列表) 函数来批量写。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:10","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"客户端一次从服务器拉取的数量 通过配置一次拉去的较大的数据量可以减少客户端获取数据的时间，但是它会占用客户端内存。有三个地方可进行配置： 1）在 HBase 的 conf 配置文件中进行配置 hbase.client.scanner.caching； 2）通过调用 HTable.setScannerCaching(int scannerCaching) 进行配置； 3）通过调用 Scan.setCaching(int caching) 进行配置。三者的优先级越来越高。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:11","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"RegionServer 的请求处理 IO 线程数 较少的 IO 线程适用于处理单次请求内存消耗较高的 Big Put 场景 (大容量单次 Put 或设置了较大 cache 的 Scan，均属于 Big Put) 或 ReigonServer 的内存比较紧张的场景。 较多的 IO 线程，适用于单次请求内存消耗低，TPS 要求 (每秒事务处理量 (TransactionPerSecond)) 非常高的场景。设置该值的时候，以监控内存为主要参考。 在 hbase-site.xml 配置文件中配置项为 hbase.regionserver.handler.count。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:12","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"Region 大小设置 配置项为 hbase.hregion.max.filesize，所属配置文件为 hbase-site.xml.，默认大小 256M。 在当前 ReigonServer 上单个 Reigon 的最大存储空间，单个 Region 超过该值时，这个 Region 会被自动 split 成更小的 Region。小 Region 对 split 和 compaction 友好，因为拆分 Region 或 compact 小 Region 里的 StoreFile 速度很快，内存占用低。缺点是 split 和 compaction 会很频繁，特别是数量较多的小 Region 不停地 split, compaction，会导致集群响应时间波动很大，Region 数量太多不仅给管理上带来麻烦，甚至会引发一些 Hbase 的 bug。一般 512M 以下的都算小 Region。大 Region 则不太适合经常 split 和 compaction，因为做一次 compact 和 split 会产生较长时间的停顿，对应用的读写性能冲击非常大。 此外，大 Region 意味着较大的 StoreFile，compaction 时对内存也是一个挑战。如果你的应用场景中，某个时间点的访问量较低，那么在此时做 compact 和 split，既能顺利完成 split 和 compaction，又能保证绝大多数时间平稳的读写性能。compaction 是无法避免的，split 可以从自动调整为手动。只要通过将这个参数值调大到某个很难达到的值，比如 100G，就可以间接禁用自动 split(RegionServer 不会对未到达 100G 的 Region 做 split)。再配合 RegionSplitter 这个工具，在需要 split 时，手动 split。手动 split 在灵活性和稳定性上比起自动 split 要高很多，而且管理成本增加不多，比较推荐 online 实时系统使用。内存方面，小 Region 在设置 memstore 的大小值上比较灵活，大 Region 则过大过小都不行，过大会导致 flush 时 app 的 IO wait 增高，过小则因 StoreFile 过多影响读性能。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:13","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"HBase 配置 建议 HBase 的服务器内存至少 32G，表 1 是通过实践检验得到的分配给各角色的内存建议值。 表 1. HBase 相关服务配置信息 模块 服务种类 内存需求 HDFS HDFS NameNode 16GB HDFS DataNode 2GB HBase HMaster 2GB HRegionServer 16GB ZooKeeper ZooKeeper 4GB HBase 的单个 Region 大小建议设置大一些，推荐 2G，RegionServer 处理少量的大 Region 比大量的小 Region 更快。对于不重要的数据，在创建表时将其放在单独的列族内，并且设置其列族备份数为 2（默认是这样既保证了双备份，又可以节约空间，提高写性能，代价是高可用性比备份数为 3 的稍差，且读性能不如默认备份数的时候。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:14","tags":["Hbase","Hdfs","Distribution"],"title":"Hbase Optimize","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"Article description.","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"Rows in HBase are sorted lexicographically by row key. This design optimizes for scans, allowing you to store related rows, or rows that will be read together, near each other. However, poorly designed row keys are a common source of hotspotting. Hotspotting occurs when a large amount of client traffic is directed at one node, or only a few nodes, of a cluster. This traffic may represent reads, writes, or other operations. The traffic overwhelms the single machine responsible for hosting that region, causing performance degradation and potentially leading to region unavailability. This can also have adverse effects on other regions hosted by the same region server as that host is unable to service the requested load. It is important to design data access patterns such that the cluster is fully and evenly utilized. ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:0:0","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey的作用 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:1:0","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey在查询中的作用 HBase中RowKey可以唯一标识一行记录，在HBase中检索数据有以下三种方式： 通过 get 方式，指定 RowKey 获取唯一一条记录 通过 scan 方式，设置 startRow 和 stopRow 参数进行范围匹配 全表扫描，即直接扫描整张表中所有行记录 当大量请求访问HBase集群的一个或少数几个节点，造成少数RegionServer的读写请求过多、负载过大，而其他RegionServer负载却很小，这样就造成热点现象。大量访问会使热点Region所在的主机负载过大，引起性能下降，甚至导致Region不可用。所以我们在向HBase中插入数据的时候，应尽量均衡地把记录分散到不同的Region里去，平衡每个Region的压力。 下面根据一个例子分别介绍下根据RowKey进行查询的时候支持的情况。 如果我们RowKey设计为uid+phone+name，那么这种设计可以很好的支持一下的场景: uid=873969725 AND phone=18900000000 AND name=zhangsanuid= 873969725 AND phone=18900000000uid= 873969725 AND phone=189?uid= 873969725 难以支持的场景： phone=18900000000 AND name = zhangsanphone=18900000000 name=zhangsan 从上面的例子中可以看出，在进行查询的时候，根据RowKey从前向后匹配，所以我们在设计RowKey的时候选择好字段之后，还应该结合我们的实际的高频的查询场景来组合选择的字段，越高频的查询字段排列越靠左。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:1:1","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey在Region中的作用 在 HBase 中，Region 相当于一个数据的分片，每个 Region 都有StartRowKey和StopRowKey，这是表示 Region 存储的 RowKey 的范围，HBase 表的数据时按照 RowKey 来分散到不同的 Region，要想将数据记录均衡的分散到不同的Region中去，因此需要 RowKey 满足这种散列的特点。此外，在数据读写过程中也是与RowKey 密切相关，RowKey在读写过程中的作用： 读写数据时通过 RowKey 找到对应的 Region； MemStore 中的数据是按照 RowKey 的字典序排序； HFile 中的数据是按照 RowKey 的字典序排序。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:1:2","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey的设计 在HBase中RowKey在数据检索和数据存储方面都有重要的作用，一个好的RowKey设计会影响到数据在HBase中的分布，还会影响我们查询效率，所以一个好的RowKey的设计方案是多么重要。首先我们先来了解下RowKey的设计原则。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:0","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey设计原则 长度原则 RowKey是一个二进制码流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]形式保存，一般设计成定长。建议越短越好，不要超过16个字节，原因如下： 数据的持久化文件HFile中时按照Key-Value存储的，如果RowKey过长，例如超过100byte，那么1000w行的记录，仅RowKey就需占用近1GB的空间。这样会极大影响HFile的存储效率。 MemStore会缓存部分数据到内存中，若RowKey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。 目前操作系统都是64位系统，内存8字节对齐，控制在16字节，8字节的整数倍利用了操作系统的最佳特性。 唯一原则 必须在设计上保证RowKey的唯一性。由于在HBase中数据存储是Key-Value形式，若向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据覆盖。 排序原则 HBase的RowKey是按照ASCII有序排序的，因此我们在设计RowKey的时候要充分利用这点。 散列原则 设计的RowKey应均匀的分布在各个HBase节点上。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:1","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey字段选择 RowKey字段的选择，遵循的最基本原则是唯一性，RowKey必须能够唯一的识别一行数据。无论应用的负载特点是什么样，RowKey字段都应该参考最高频的查询场景。数据库通常都是以如何高效的读取和消费数据为目的，而不是数据存储本身。然后，结合具体的负载特点，再对选取的RowKey字段值进行改造，组合字段场景下需要重点考虑字段的顺序。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:2","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"避免数据热点的方法 在对HBase的读写过程中，如何避免热点现象呢？主要有以下几种方法： Reversing 如果经初步设计出的RowKey在数据分布上不均匀，但RowKey尾部的数据却呈现出了良好的随机性，此时，可以考虑将RowKey的信息翻转，或者直接将尾部的bytes提前到RowKey的开头。Reversing可以有效的使RowKey随机分布，但是牺牲了RowKey的有序性。 缺点： 利于Get操作，但不利于Scan操作，因为数据在原RowKey上的自然顺序已经被打乱。 Salting Salting（加盐）的原理是在原RowKey的前面添加固定长度的随机数，也就是给RowKey分配一个随机前缀使它和之间的RowKey的开头不同。随机数能保障数据在所有Regions间的负载均衡。 缺点： 因为添加的是随机数，基于原RowKey查询时无法知道随机数是什么，那样在查询的时候就需要去各个可能的Regions中查找，Salting对于读取是利空的。并且加盐这种方式增加了读写时的吞吐量。 Hashing 基于 RowKey 的完整或部分数据进行 Hash，而后将Hashing后的值完整替换或部分替换原RowKey的前缀部分。这里说的 hash 包含 MD5、sha1、sha256 或 sha512 等算法。 缺点： 与 Reversing 类似，Hashing 也不利于 Scan，因为打乱了原RowKey的自然顺序。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:3","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey设计案例剖析 1. 查询某用户在某应用中的操作记录 reverse(userid) + appid + timestamp 2. 查询某用户在某应用中的操作记录（优先展现最近的数据） reverse(userid) + appid + (Long.Max_Value - timestamp) 3. 查询某用户在某段时间内所有应用的操作记录 reverse(userid) + timestamp + appid 4. 查询某用户的基本信息 reverse(userid) 5. 查询某eventid记录信息 salt + eventid + timestamp 如果 userid是按数字递增的，并且长度不一，可以先预估 userid 最大长度，然后将userid进行翻转，再在翻转之后的字符串后面补0（至最大长度）；如果长度固定，直接进行翻转即可（如手机号码）。 在第5个例子中，加盐的目的是为了增加查询的并发性，加入Slat的范围是0~n，可以将数据分为n个split同时做scan操作，有利于提高查询效率。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:3:0","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey总结 在HBase的使用过程，设计RowKey是一个很重要的一个环节。我们在进行RowKey设计的时候可参照如下步骤： 结合业务场景特点，选择合适的字段来做为RowKey，并且按照查询频次来放置字段顺序 通过设计的RowKey能尽可能的将数据打散到整个集群中，均衡负载，避免热点问题 设计的RowKey应尽量简短 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:4:0","tags":["Hbase","Distribution","Hdfs"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":[],"content":"JIARUI LIU ☎️Phone：+86 15735184098 📬E-mail：0x004c2@gmail.com 📖BLOG：jerrysmd.github.io ","date":"2020-10-01","objectID":"/about/:0:0","tags":[],"title":"About","uri":"/about/"},{"categories":["Technology"],"content":"Article description.","date":"2020-09-11","objectID":"/20200911_es-wildcard-search/","tags":["Distribution","Spark"],"title":"Elasticsearch Wildcard Search","uri":"/20200911_es-wildcard-search/"},{"categories":["Technology"],"content":"Elasticsearch is the distributed, RESTful search and analytics engine at the heart of the Elastic Stack. You can use Elasticsearch to store, search, and manage data for Logs，Metrics，A search backend，Application monitoring，Endpoint security. 问题描述：ES 使用 wildcard 进行模糊查询，有些情况模糊查询失败，如：\"*日本*\"，但测试别的数据，如 “*192.168*” 可以模糊匹配。这是因为 ES 对查询文本分词造成的结果。 ","date":"2020-09-11","objectID":"/20200911_es-wildcard-search/:0:0","tags":["Distribution","Spark"],"title":"Elasticsearch Wildcard Search","uri":"/20200911_es-wildcard-search/"},{"categories":["Technology"],"content":"match：分词模糊查询 比如“Everything will be OK, All is well”，会被分词一个一个单词（不是单个字母） { \"from\": 0, \"size\": 20, \"query\": { \"bool\": { \"should\": [{ \"term\": { \"form_name\": \"will\" } } ] } } } ","date":"2020-09-11","objectID":"/20200911_es-wildcard-search/:1:0","tags":["Distribution","Spark"],"title":"Elasticsearch Wildcard Search","uri":"/20200911_es-wildcard-search/"},{"categories":["Technology"],"content":"match_phrase ：短语模糊查询 match_phrase是短语搜索，即它会将给定的短语（phrase）当成一个完整的查询条件。 比如查询 “Everything will”，会当成一个完整的短语进行查询， 会查出含有该查询条件的内容。 GET /basic_index*/_search { \"from\": 0, \"size\": 20, \"query\": { \"bool\": { \"should\": [{ \"match\": { \"form_name\": \"Everything will\" } } ] } } } 如果是查询单个字母，match就不管用了。 ","date":"2020-09-11","objectID":"/20200911_es-wildcard-search/:2:0","tags":["Distribution","Spark"],"title":"Elasticsearch Wildcard Search","uri":"/20200911_es-wildcard-search/"},{"categories":["Technology"],"content":"wildcard：通配符模糊查询 ? 匹配任意字符 * 匹配0个或多个字符 GET /basic_index*/_search { \"size\": 20, \"from\": 0, \"query\": { \"bool\": { \"should\": [{ \"wildcard\": { \"form_name\": \"*very* } }] } } } 记录是存在的，但是没有查出来？ 因为分词的影响，添加keyword 进行处理 { \"wildcard\": { \"form_name.keyword\": \"*very*\" } } Wildcard 性能会比较慢。如果非必要，尽量避免在开头加通配符 ? 或者 *，这样会明显降低查询性能 如果查询的内容非空，怎么处理？ 直接用** { \"wildcard\": { \"form_name\": \"*\" } } ","date":"2020-09-11","objectID":"/20200911_es-wildcard-search/:3:0","tags":["Distribution","Spark"],"title":"Elasticsearch Wildcard Search","uri":"/20200911_es-wildcard-search/"},{"categories":["Technology"],"content":"总结： Es 模糊查询， 分词的用match； 短语的用match_phrase；查询任意的，用wildcard通配符，注意查询的内容是否分词，分词的添加keyword，查询非空的情况，用\"**\"。 ","date":"2020-09-11","objectID":"/20200911_es-wildcard-search/:4:0","tags":["Distribution","Spark"],"title":"Elasticsearch Wildcard Search","uri":"/20200911_es-wildcard-search/"},{"categories":["Technology"],"content":"Article description.","date":"2020-08-11","objectID":"/20200811_yarn-clusteryarn-client/","tags":["Distribution","Spark"],"title":"Spark On Yarn: yarn-cluster, yarn-client","uri":"/20200811_yarn-clusteryarn-client/"},{"categories":["Technology"],"content":"YARN is a generic resource-management framework for distributed workloads; in other words, a cluster-level operating system. Although part of the Hadoop ecosystem, YARN can support a lot of varied compute-frameworks (such as Tez, and Spark) in addition to MapReduce. Spark支持可插拔的集群管理模式(Standalone、Mesos以及YARN )，集群管理负责启动executor进程，编写Spark application 的人根本不需要知道Spark用的是什么集群管理。Spark支持的三种集群模式，这三种集群模式都由两个组件组成:master和slave。Master服务(YARN ResourceManager,Mesos master和Spark standalone master)决定哪些application可以运行，什么时候运行以及哪里去运行。而slave服务( YARN NodeManager, Mesos slave和Spark standalone slave)实际上运行executor进程。 当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器(container)运行。Spark可以使得多个Tasks在同一个容器(container)里面运行。这是个很大的优点。 注意这里和Hadoop的MapReduce作业不一样，MapReduce作业为每个Task开启不同的JVM来运行。虽然说MapReduce可以通过参数来配置。详见mapreduce.job.jvm.numtasks 从广义上讲，yarn-cluster适用于生产环境；而yarn-client适用于交互和调试，也就是希望快速地看到application的输出。 Application Master。在YARN中，每个Application实例都有一个Application Master进程，它是Application启动的第一个容器。它负责和ResourceManager打交道，并请求资源。获取资源之后告诉NodeManager为其启动container。 从深层次的含义讲，yarn-cluster和yarn-client模式的区别其实就是Application Master进程的区别，yarn-cluster模式下，driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。然而yarn-cluster模式不适合运行交互类型的作业。而yarn-client模式下，Application Master仅仅向YARN请求executor，client会和请求的container通信来调度他们工作，也就是说Client不能离开。（上图是yarn-cluster模式，下图是yarn-client模式）： spark-yarn-f31 spark-yarn-f22 ","date":"2020-08-11","objectID":"/20200811_yarn-clusteryarn-client/:0:0","tags":["Distribution","Spark"],"title":"Spark On Yarn: yarn-cluster, yarn-client","uri":"/20200811_yarn-clusteryarn-client/"},{"categories":["Technology"],"content":"Article description.","date":"2020-08-03","objectID":"/20200803_spark-guide3/","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:0:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Project 业务场景 统计出租车利用率(有乘客乘坐的时间和无乘客空跑的时间比例) 技术要点 数据清洗 Json解析 地理位置信息处理 探索性数据分析 会话分析 数据读取 class TaxiAnalysisRunner{ def main(args: Array[String]): Unit = { //1创建SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"taxi\") .getOrCreate() //2导入隐式转换和函数 import spark.implicits._ import org.apache.spark.sql.functions._ //3数据读取 val taxiRaw: Dataset[Row] = spark.read .option(\"header\", value = true) .csv(\"dataset/half_trip.csv\") } } 抽象数据类 //dataframe是ROW类型的dataset //读取的dataframe是row类型的，如果是dataset[trip]把类型抽象，对数据方便处理 case class Trip( license: String, pickUpTime: Long, dropOffTime: Long, pickUpX: Double, pickUpy: Double, dropOffX: Double, dropOffY: Double ) 转换DF类型、清洗异常数据 //dataframe[Row] =\u003e dataset[] val taxiParsed:RDD[Either[Trip,(Row,Exception)]] = taxiRaw.rdd.map(safe(parse)) //异常数据 val exceptionResult = taxiParsed.filter(e =\u003e e.isRight) .map(e =\u003e e.right.get._1) val taxi Good: Dataset[Trip] = taxiParsed.map(either =\u003e either.left.get).toDS() def parse(row: Row): Trip = { val richRow = new richRow(row) val license = richRow.getAs[String](\"hack_license\").orNull val pickUpTime = parseTime(richRow, \"...\") val dropOffTime = parseTime(richRow, \"...\") val pickUpX = parseLocation(richRow, \"...\") val pickUpy = parseLocation(richRow, \"...\") val dropOffX = parseLocation(richRow, \"...\") val dropOffY = parseLocation(richRow, \"...\") Trip(license, pickUpTime, dropOffTime, pickUpX, pickUpy, dropOffX, dropOffY) } class RichRow(row: Row){ def getAs[T](field: String): Option[T] = { if(row.isNullAt(row.fieldIndex(field))){ None }else{ Some(row.getAs[T](field)) } } } def parseTime(row: RichRow, field: String): Long = { //规定格式 val pattern = \"yyyy-MM-dd HH:mm:ss\" val formatter = new SimpleDateFormat(pattern, locale.ENGLISH) //执行转换 val time = row.getAs[String](field) val timeOption = time.map(time =\u003e formatter.parse(time).getTime) //Option代表某个方法，结果可能为空，使得方法调用出必须处理为null的情况 //Option对象本身提供了一些对于null的支持 timeOption.getOrElse(0L) } def parseLocation(row: RickRow, field: String): Double = { val location = row.getAs[String](fiecld) val locationOption = location.map(loc =\u003e loc.toDouble) locationOption.getOrElse(0D) } //parse异常处理 //出现异常-\u003e返回异常信息，和当前调用 def safe[P, R](f: P =\u003e R): P =\u003e Either[R,(P,Exception)]={ new Function[P, Either[R,(P,Exception)]] with Serializable { override def apply(param: P): Either[R, (P, Exception)] = { try{ Left(param) }catch{ case e: Exception =\u003e Right((param, e)) } } } } 统计分布 //编写udf，将毫秒转为小时单位 val hours = (pickUpTime: Long, dropOffTime: Long) =\u003e { val duration = dropOffTime - pickUpTime val hours = TimeUnit.HOURS.convert(duration, TimeUnit.MILLISECONDS) hours } val hoursUDF = udf(hours) //统计 taxiGood.groupBy(hoursUDF($\"pickUpTime\",$\"dropOffTime\") as \"duration\") .count() .sort(\"duration\") .show() //直方图 spark.udf.register(\"hours\", hours) val taxiClean = taxiGood.where(\"hours(pickUpTime, dropOffTime) BETWEEN 0 AND 3\") JSON地理信息 case class FeatureCollection(features: List[Feature]) case class Feature(Properties: Map[String, String], geometry: JObject) { def getGeometry(): Geometry = { import org.json4s._ import org.json4s.jackson.JsonMethods._ val mapGeo = GeometryEngine.geoJsonToGeometry(compact(render(geometry)), 0, Geometry.Type.Unknown) mapGeo.getGeometry } } object FeatureExtraction{ //JSON解析 def parseJson(json: String): FeatureCollection = { //1导入一个formats隐式转换 implicit val formats = Serialization.formats(NoTypeHints) //2JSON -\u003e Obj import org.json4s.jackson.Serialization.read val featureCollection = read[FeatureCollection](json) featureCollection } } //链接行政区信息 //1读取数据 val geoJson = Source.fromFile(\"dataset/districts.geojson\").mkString val featureColleciton = FeatureExtraction.parseJson(geoJson) //2排序 //理论上大的区域数量多，把大的区域放在前面，减少搜索次数 val sortedFeatures = featureCollection.features.sortBy(feature =\u003e { (feature.properties(\"boroughCode\"), - feature.getGeometry().calculateArea2D()) }) //3广播 val featuresBC = spark.sparkContext.broadcast(sortedFeatures) //4UDF val boroughLookUp = (x: Double, y: Double) =\u003e { //1搜索经纬度所在的区域 val feature","date":"2020-08-03","objectID":"/20200803_spark-guide3/:1:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Spark Streaming Spark Streaming 的特点 Spark Streaming 并不是实时流，而是按时间切分小批量，一个一个的小批处理 Spark Streaming 对数据是按照时间切分为一个又一个的RDD，然后针对RDD进行处理 处理架构 批处理：HDFS 流处理：Kafka 混合处理：流式计算和批处理结合 Netcat Netcat以在两台设备上面相互交互，即侦听模式/传输模式 功能：Telnet功能、获取banner信息、传输文本信息、传输文件/目录、加密传输文件，默认不加密、远程控制、加密所有流量、流媒体服务器、远程克隆硬盘 object StreamingWordCount { def main(args: Array[String]): Unit = { val sparkConf = new SparkConff().setAppName(\"stream word count\").setMaster(\"local[6]\") val ssc = new StreamingContext(sparkConf, Seconds(1))//批次时间，每1秒收集一次数据 //在创建Streaming Context的时候也要用到conf，说明Spark Streaming是基于Spark Core的 //在执行master的时候，不能指定一个线程：因为在Streaming运行的时候，需要开一个新的线程去一直监听数据的获取 //socketTextStream方法会创建一个DStream，监听Socket输入，当做文本处理 //DStream可以理解是一个流式的RDD val lines: ReceiverInputDStream[String] = ssc.socketTextStream( hostnmae = \"192.168.169.101\", port = 9999, storageLevel = StoreageLevel.MEMORY_AND_DISK_SER ) //2数据处理 // 1拆分单词 val words = lines.flatMap(_.split(\" \")) // 2转换单词 val tuples = words.map((_, 1)) // 3词频reduce val counts = tuples.reduceByKey(_ + _) ssc.start() // main方法执行完毕后整个程序就会退出，所以需要阻塞主线程 ssc.awaitTermination() } } 容错 热备 当Receiver获取数据，交给BlockManager存储 如果设置了StorageLevel.MEMORY_AND_DISK_SER，则意味着BlockManager 不仅会在本机存储，也会发往其它的主机存储，本质就是冗余备份 如果某一个计算失败了，通过冗余的备份，再次进行计算即可 冷备 WAL 预写日志 当数据出错时，根据Redo log去重新处理数据 重放 有一些上游的外部系统是支持重放的，如 Kafka Kafka 可以根据Offset来获取数据 出错时，只需通过Kafka再次读取即可 ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:2:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Structured Streaming 编程模型演进 RDD： 针对自定义的数据对象进行处理，可以处理任意类型的对象，比较符合面向对象 RDD处理数据速较慢 RDD无法感知数据的结构，无法针对数据结构进行编程 DataFrame： 保留元信息，针对数据结构进行处理，例如根据某一列进行排序或者分组 DF在执行的时候会经过catalyst进行优化，并且序列化更加高效，性能会更好 DF无法处理非结构化数据，因为DF内部使用Row对象保存数据 DF的读写框架更加强大，支持多种数据源 DataSet： DS结合了RDD和DF的特点，可以处理结构化数据，也可以处理非结构化数据 序列化 将对象的内容变成二进制或存入文件中保存 数据场景： 持久化对象数据 网络中不能传输Java对象，只能将其序列化后传输二进制数据 序列化应用场景 Task分发：Master的driver往Worker的Executor任务分发 RDD缓存：序列化后分布式存储 广播变量：序列化后分布式存储 Shuffle过程 Spark Streaming 的 Receiver：kafka传入的数据是序列化的数据 RDD的序列化 Kryo是Spark引入的一个外部的序列化工具，可以增快RDD的运行速度 因为Kryo序列化后的对象更小，序列化和反序列化速度非常快 val conf = new SparkConf() .setMaster(\"local[2]\") .setAppName(\"KyroTest\") conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") conf.registerKryoClasses(Array(classOf[Person])) val sc = new SparkContext(conf) rdd.map(arr =\u003e Person(arr(0), arr(1), arr(2))) StructuredStreaming区别 StructuredStreaming相比于SparkStreaming的进步类似于RDD到Dataset的进步 StructuredStreaming支持连续流模型，类似于Flink那样的实时流 StructuredStreaming Project 需求 对流式数据进行累加词频统计 整体结构 Socket Server 发送数据， Structured Streaming 程序接受数据 Socket Server 使用 Netcat nc 来实现 //1.创建sparkSession //2.数据读取 val source: DataFrame = spark.readStream .format(\"socket\") .option(\"host\", \"192.168.168.101\") .option(\"port\", 9999) .load() val sourceDS: Dataset[String] = source.as[String] //3.数据处理 val words = sourceDS.flatMap(_.split(\" \")) .map((_, 1)) .groupByKey(_._1) .count() //4.结果生成 words.writeStream .outputMode(OutputMode.Complete()) .format(\"console\") .start() .awaitTermination() # 开启Netcat nc -lk 9999 StreamExecution 分为三个重要的部分 Source 从外部数据源读取数据，例如kafka LogicalPlan 逻辑计划，在流上查询计划，根据源头DF处理生成逻辑计划 Sink 写入结果 StateStore Structured Streaming 虽然从API角度上模拟出来的是一个无线扩展的表，但其内部还是增量处理。 每一批次处理完成，会将结果写入状态。每一批次处理之前，拉出来最新的状态，合并到处理过程中 ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:3:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Structured Streaming HDFS 场景 Sqoop MySQL -\u003e Sqoop -\u003e HDFS[增量数据1，增量数据2，… ] -\u003e Structured Streaming -\u003e Hbase Ngix Ngix[log1, log2，… ] -\u003e Flume -\u003e HDFS[增量数据1，增量数据2，… ] -\u003e Structured Streaming -\u003e Hbase 特点 会产生大量小文件在HDFS上 Project Python程序生成数据到HDFS Structured Streaming 从HDFS中获取数据 Structured Streaming 处理数据 # Python程序生成数据到HDFS import os for index in range(100): #1.文件内容 content = \"\"\" {\"name\": \"Michael\"} {\"name\": \"Andy\", \"age\": 30} {\"name\": \"Justin\", \"age\": 19} \"\"\" #2.文件路径 file_name = \"/export/dataset/text{0}.json\".format(index) #3.打开文件，写入内容 with open(file_name, \"w\") as file: file.write(content) #4.执行HDFS命令，创建HDFS目录，上传文件到HDFS中 os.system(\"/export/servers/haddop/bin/hdfs dfs -mkdir -p /dataset/dataset/\") os.system(\"/export/servers/haddop/bin/hdfs dfs -put {0} /dataset/dataset\".format(file_name)) //Structured Streaming 从HDFS中获取数据 object HDFSSource{ def main(args: Array[String]): Unit = { System.setProperty(\"hadoop.home.dir\", \"C:\\\\winutil\") //1.创建SparkSession val spark = SparkSession.builder() .appName(\"hdfs_souce\") .master(\"local[6]\") .getOrCreate() //2.数据读取 val schema = new StructType() .add(\"name\", \"string\") .add(\"age\", \"integer\") val souce = spark.readStream .scheme(schema) .json(\"hdfs://node01:8020/dataset/dataset\") //3.输出结果 source.writeStream .outputMode(OutputMode.Append()) .format(\"console\") .start() .awaitTermination() } } ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:4:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Structured Streaming Kafka Kafka是一个 Pub/Sub 系统 Publisher / Subscriber 发布订阅系统 发布者 --\u003e kafka --\u003e 订阅者 发布订阅系统可以有多个Publisher对应一个Subscriber，例如多个系统都会产生日志，一个日志处理器可以简单的获取所有系统产生的日志 用户系统 --\u003e 订单系统 --\u003e kafka --\u003e 日志处理器 内容系统 --\u003e 发布订阅系统也可以一个Publisher对应多个Subscriber， 这样就类似于广播了，例如通过这样的方式可以非常轻易的将一个订单的请求分发给所有感兴趣的系统，减少耦合性 --\u003e 日志处理器 用户系统 --\u003e kafka --\u003e 日志处理器 --\u003e 日志处理器 大数据系统中，消息系统往往可以作为整个数据平台的入口，左边对接业务系统各个模块，右边对接数据系统各个计算工具 业务系统 数据系统 [用户系统] --\u003e --\u003e [HDFS] [订单系统] --\u003e kafka --\u003e [Structured Streaming] [服务系统] --\u003e --\u003e [MapReduce] Kafka 的特点 Kafka 非常重要的应用场景就是对接业务系统和数据系统，作为一个数据管道，其需要流通的数据量惊人，所以 Kafka 一定有： 高吞吐量 高可靠性 Topic 和 Partitions 消息和事件经常是不同类型的，例如用户注册是一种消息，订单创建也是一种消息 创建订单事件 --\u003e kafka --\u003e structured Streaming 用户注册事件 --\u003e Kafka 中使用 Topic 来组织不同类型的消息 创建订单事件 --\u003e Topic Order --\u003e structured Streaming 用户注册事件 --\u003e Topic Order Kafka 中的 Topic 要承受非常大的吞吐量，所以 Topic 应该是可以分片的，应该是分布式的 Anatomy of a Topic Partition 0 [0][1][2][3] Partition 1 [0][1] Partition 3 [0][1][2] Old --\u003e New Kafka 和 Structured Streaming 整合的结构 Structured Streaming 中使用 Source 对接外部系统，对接 Kafka 的 Source 叫做 KafkaSource KafkaSource 中会使用 KafkaSourceRDD 来映射外部 Kafka 的 Topic，两者的 Partition 一一对应 Structured Streaming 会并行的从 Kafka 中获取数据 Structured Streaming 读取 Kafka 消息的三种方式 Earlist 从每个 Kafka 分区最开始处开始获取 Assign 手动指定每个 Kafka 分区中的 Offset Latest 不再处理之前的消息，只获取流计算启动后新产生的数据 PROJECT 需求 模拟物联网系统的数据统计 使用生产者在 Kafka 的 Topic：Streaming-test 中输入 JSON 数据 使用 Structured Streaming 过滤出来家里有人的数据 创建 Topic 并输入数据到 Topic 使用命令创建 Topic bin/kafka-topics.sh --create streaming-test --replication-factor 1 --partitions 3 --zookeeper node01:2181 开启 Producer bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 -topic streaming-test 把 Json 转为单行输入 Spark 读取 kafka 的 Topic object KafkaSource{ def main(args: Array[String]): Unit = { //1.创建 SparkSession //2.读取 Kafka 数据 val source: Dadaset[String] = spark.readSteam .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"node01:9092,node02:9092,node03:9092\") .option(\"subscribe\", \"streaming_test_1\") .option(\"startingOffsets\", \"earliest\") .load() .selectExpr(\"CAST(value AS STRING) as value\") .as[String] //3.处理数据，Dataset(String) -\u003e Dataset(id, name, category) //1::Toy Story (1995)::Animation|Children's|Comedy source.map(item =\u003e { val arr = item.split(\"::\") (arr(0).toInt, arr(1).toString, arr(2).toString) }).as[(Int, String, String)].toDF(\"id\", \"name\", \"category\") //4.Sink to HDFS result.writeStream .format(\"parquet\") .option(\"path\", \"/dataset/streaming/movies/\") .option(\"checkpointLocation\", \"checkpoint\") .start() .awaitTermination() //4.Sink to Kafka result.writeStream .format(\"kafka\") .outputMode(OutputMode.Append()) .option(\"checkpointLocation\", \"checkpoint\") .option(\"kafka.bootstrap.servers\", \"node01:9092, node2:9092\") .option(\"topic\", \"streaming_test_3\") .start() .awaitTermination() //4.Sink to Mysql //使用foreachWriter } } Sink Trigger 微批次 默认一秒间隔 连续流 Trigger.Continuous(“1 second”)，只支持Map类的类型操作，不支持聚合，Source和Sink只支持Kafka ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:5:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Article description.","date":"2020-07-07","objectID":"/20200707_spark-guide2/","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:0:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Advanced Operation closure def test(): Unit = { val f = closure() f(5) } def closure(): Int =\u003e Double = { val factor = 3.14 val areaFunction = (r: int) =\u003e { math.pow(r,2) * factor } areaFunction } f就是闭包，闭包的本质就是一个函数 在scala中函数是一个特殊的类型，FunctionX 闭包也是一个FunctionX类型的对象 闭包是一个对象 class MyClass{ val field = \"Hello\" def doStuff(rdd: RDD[String]): RDD [String] = { rdd.map(x =\u003e field + x) //引用Myclass对象中的一个成员变量，说明其可以访问MyClass这个类的总用域，也是一个闭包。封闭的是MyClass这个作用域。 //在将其分发的不同的Executor中执行的时候，其依赖MyClass这个类当前的对象，因为其封闭了这个作用域。MyClass和函数都要一起被序列化。发到不同的结点中执行。 //1. 如果MyClass不能被序列化，将会报错 //2. 如果在这个闭包中，依赖了一个外部很大的集合，那么这个集合会随着每一个Task分发 } } Global accumulator 在任意地方创建long accumulator 累加 结果 val counter = sc.longAccumulator(\"counter\") val result = sc.parallelize(Seq(1,2,3,4,5)).foreach(counter.add(_)) counter.value Broadcast 广播变量允许将一个Read-Only的变量缓存到集群中的每个节点上，而不是传递给每一个Task一个副本 集群中的每个节点指的是一个机器 每一个Task，一个Task是一个Stage中的最小处理单元，一个Executor中可以有多个Stage，每个Stage有多个Task 所以在需要多个Stage的多个Task中使用相同数据的情况下，广播特别有用 val v = Map(\"Spark\" -\u003e \"http[123]\", \"scala\" -\u003e \"http[456]\") val config = new SparkConf().setMaster(\"local[6]\").setAppName(\"bc\") val sc = new SparkContext(config) //创建广播 val bc = sc.broadcast(v) val r = sc.parallelize(Seq(\"Spark\", \"Scala\")) //使用广播变量代替直接引用集合，只会复制和executor一样的数量 //在使用广播之前，复制map了task数量份 //在使用广播之后，复制次数和executor数量一致 val result = r.map(item =\u003e bc.value(item)).collect() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:1:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"SparkSQL Spark的RDD主要用于处理非结构化数据和半结构化数据 SparkSQL主要用于处理结构化数据 SparkSQL支持：命令式、SQL 优势： 虽然SparkSQL是基于RDD的，但是SparkSQL的速度比RDD要快很多 SparkSQL提供了更好的外部数据源读写支持 SparkSQL提供了直接访问列的能力 case class Person(name: String, age: Int) val spark: SparkSession = new sql.SparkSession.Builder() .appName(\"hello\") .master(\"local[6]\") .getOrCreate() impart spark.implicits._ val personRDD: RDD[people] = spark.sparkContext.parallelize(Seq(Person(\"zs\", 10),Person(\"ls\", 15))) val personDS: Dataset[Person] = PersonRDD.toDS() val teenagers: Dataset[String] = PersonDS.where('age \u003e 10) .where('age \u003c 20) .select('name) .as[String] RDD和SparkSQL运行时的区别 RDD的运行流程： RDD-\u003eDAGScheduler-\u003eTaskScheduleri-\u003eWorker 先将RDD解析为由Stage组成的DAG，后将Stage转为Task直接运行 SparkSQL的运行流程： 解析SQL，并且生成AST（抽象语法树） 在AST中加入元数据信息，做这一步主要是为了一些优化，例如 col = col 这样的条件 对已经加入元数据的AST，输入优化器，进行优化（例如：谓词下推，列值裁剪） 生成的AST其实最终还没办法直接运行，这个AST是逻辑计划，结束后，需要生成物理计划，从而生成RDD来运行。 Dataset \u0026 DataFrame RDD 优点： JVM对象组成的分布式数据集合 不可变并且有容错能力 可处理机构化和非结构化的数据 支持函数式转换 RDD缺点： 没有Schema 用户自己优化程序 从不同的数据源读取数据非常困难 合并多个数据源中的数据也非常困难 DataFrame: DataFrame类似一张关系型数据的表 在DataFrame上的操作，非常类似SQL语句 DataFrame中有行和列，Schema DataFrame的优点： Row对象组成的分布式数据集 不可变并且有容错能力 处理结构化数据 自带优化器Catalyset,可自动优化程序 Data source API DataFrame让Spark对结构化数据有了处理能力 DataFrame的缺点： 编译时不能类型转化安全检查，运行时才能确定是否有问题 对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象 Dataset的优点： DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据 和RDD一样，支持自定义对象存储 和DataFrame一样，支持结构化数据的sql查询 采用堆外内存存储，gc友好 类型转化安全，代码友好 def dataset1(): Unit = { //1.创建SparkSession val spark = new sql.SparkSession.Builder() .master(\"local[6]\") .appName(\"dateset1\") .getOrCreate() //2.导入隐式转化 import spark.implicits._ //3.demo val sourceRDD = spark.sparkContext.parallelize(Seq(Person(\"zs\", 10),Person(\"ls\", 15))) val dataset = sourceRDD.toDS() //Dataset支持强类型API dataset.filter(item =\u003e item.age \u003e 10).show() //Dataset支持弱类型API dataset.filter( 'age \u003e 10 ).show() dataset.filter( $\"age\" \u003e 10 ).show() //Dataset可以直接编写SQL表达式 dataset.filter( \"age \u003e 10\").show() DataFrame Practice: def dataframe1(): Unit = { //1. 创建SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"pm analysis\") .getOrCreate() //2.读取数据集 val souceDF = spark.read .option(\"header\", value = true) .csv(\"dataset/beijingPM.csv\") //3.处理数据集 sourceDF.select('year, 'month, 'PM_Dongsi) .where('PM_Dongsi =!= \"NA\") .groupBy('year, 'month) .count() .show() spark.stop() } DataFrame \u0026 Dataset 区别： DataFrame是Dataset的一种特殊情况，DataFrame是Dataset[Row]的别名 DataFrame表达的含义是一个支持函数式操作的表，而Dataset表达是一个类似RDD的东西，Dataset可以处理任何对象 DataFrame中存放的是Row对象，而Dataset中可以存放任何类型的对象 DataFrame是弱类型，Dataset是强类型。DataFrame的操作方式和Dataset是一样的，但是对于强类型的操作而言，他们处理的类型是不同的 DataFrame在进行强类型操作的时候，例如map算子，所处理的数据类型永远是Row 而Dataset，其中是什么类型，他就处理什么类型。 val df: DataFrame = personList.toDF() df.map( (row: Row) =\u003e Row(row.get(0), row,getAs[Int](1) * 2))(RowEncoder.apply(df.schema)) val ds: Dataset[person] = personList.toDS() ds.map((person: Person =\u003e Person(person.name, person.age * 2))) DataFrame只能做到运行时类型检查，Dataset能做到编译和运行都有类型检查 DataFrame弱类型是编译时不安全(df.groupBy(“name, school”)) Dataset所代表的操作，是类型安全的，编译时安全的(ds.filter(person =\u003e person.name)) Row DataFrame就是Row集合加上Schema信息 case class Person(name: String, age: Int) def row(): Unit = { //1.Row如何创建，是什么 //row对象必须配合Schema对象才会有列名 val person = Person(\"zs\", 15) val row = Row(\"zs\", 15) //2.如何从Row中获取数据 row.getString(0) row.getInt(1) //3.Row也是样例类 row match{ case Row(name, age) =\u003e println(name, age) } } Reader def reader1(): Unit = { //1.create SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"reader1\") .getOrCreate() //2.firstWay spark.read .format(\"csv\") .option(\"header\", value = true) .option(\"inferSchema\", value = true) .load(\"dataset/bjPM.csv\") .show(10) //3.sencendWay spark.read .option(\"header\", value = true) .option(\"inferSchema\", value = true) .csv(\"dataset/bjPM.csv\") .show(10) } Writer def writer1(): Unit = { System.setProperty(\"hodoop.home.dir\",\"c:\\\\winutils\") //1.create SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"reader1\") .getOrCreate() //2.read data","date":"2020-07-07","objectID":"/20200707_spark-guide2/:2:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Data Type Transformation flatMap,map,mapPartitions,transform,as: class TypedTransformation{ //1.创建sparksession val spark = SparkSession.builder().master(\"local[6]\").appName(\"typed\").getOrCreate() import spark.implicits._ @Test def trans():Unit = { //flatmap val ds = Seq(\"hello spark\", \"hello hadoop\").toDS ds.flatMap(item =\u003e item.split(\" \")).show() //map val ds2 = Seq(Persion(\"zs\",15),Persion(\"lisi\",20)).toDS() ds2.map(person =\u003e Person(person.name, person.age*2)).show() //mappartitions ds2.mapPartitions{ //iter 不能大到每个Executor的内存放不下，不然就会OOM //对每个元素进行转换，后生成一个新的集合 iter =\u003e{ val result = iter.map(person =\u003e Person(person.name, person.age * 2)) result } } } } def trans1(): Unit = { val ds = spark.rage(10) //0-10 ds.transform(dataset =\u003e dataset.withColumn(\"doubled\", 'id * 2')) .show() } DF转成DS rdd.toDF -\u003e DataFrame //toDF把rdd转成DF dataFrame -\u003e Dataset //DataFrame就是Dataset[Row] case class Student(name:String, age:Int, gpa:Float) //读取 val schema = StructType( Seq( StructField(\"name\",StringType), StructField(\"age\",IntegerType), StructField(\"gpa\",FloatType) ) ) val df = spark.read .schema(schema) .option(\"delimiter\",\"\\t\") .csv(\"dataset/studenttab10k\") //转换 //本质上dataset[Row].as[Student] =\u003e Dataset[Student] val ds: Dataset[Student] = df.as[Student] //输出 ds.show() Filter def filter(): Unit = { val ds = Seq(Person(\"zs\",15),Person(\"ls\",20)).toDS() ds.filter(person =\u003e person.age \u003e 15).show() } Group groupByKey: val ds = Seq(Person(\"zs\",15),Person(\"ls\",20)).toDS() val grouped: KeyValueGroupedDataset[String, Person] = ds.groupByKey(person =\u003e person.name) val result: Dataset[(String, Long)] = grouped.count() result.show() Split val ds = spark.range(15) //randomSplit, the number of part, weight val datasets: Array[Dataset[lang.Long]] =ds.randomSplit(Array(5,2,3)) datasets.foeach(_.show()) //split ds.sample(withReplacement = false, fraction = 0.4).show() Sort val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() ds.orderBy('name.desc).show() ds.sort('name.asc).show() Distinct distinct,dropDuplicates: def dropDuplicates(): Unit = { val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() //重复列完全匹配 ds.distinct().show() //指定列去重 ds.dropDuplicates(\"age\").show() } Collection 差集、交集、并集、limit def collection(): Unit ={ val ds1 = spark.range(1,10) val ds2 = spark.range(5,15) ds1.except(ds2) ds1.intersect(ds2) ds1.union(ds2) ds1.limit(3) } ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:3:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Data Typeless Transformation select val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() ds.sort() .... .secect('name).show() ds.selectExpr(\"sum(age)\").show() import org.apache.spark.sql.funcitons._ ds.select(exper(\"sum(age)\")).show() Column val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() import org.apache.spark.sql.funcitons._ //如果想使用函数的功能 //1.使用functions.xx //2.使用表达式，可以使用expr(\"...\")随时编写表达式 ds.withColumn(\"random\",expr(\"rand()\")).show() ds.withColumn(\"name_new\",'name + ...).show() ds.withColumn(\"name_jok\",'name === \"\").show() ds.withColumnRenamed(\"name\",\"new_name\").show() Drop val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() ds.drop('age).show() GroupBy val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() //为什么groupByKey是有类型的，最主要的原因是因为groupByKey所生成的对象中的算子是有类型的 ds.groupByKey(item =\u003e item.name).mapValues() //为什么groupBy是无类型的，因为groupBy所生成的对象中的算子是无类型的，针对列进行处理 ds.groupBy('name).agg(mean(\"age\")).show() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:4:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Column Creation class Column{ val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"column\") .getOrCreate() def creation():Unit = { val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() val df = Seq((\"zs\",15),(\"ls\",20),(\"zs\",8)).toDF(\"name\",\"age\") //1. ' 必须导入spark的隐式转化才能使用str.intern() val column: Symbol = 'name //2. $ 必须导入spark的隐式转化才能使用 val column1: ColumnName = $\"name\" //3. col 必须导入functions import org.apache.spark.sql.functions._ val column2:sql.Column = col(\"name\") //4. column 必须导入functions val column3:sql.Column = column(\"name\") //Dataset可以，DataFrame可以使用column对象 ds.select(column).show() df.select(column).show() //column有四种创建方式 //column对象可以用作于Dataset和DataFrame中 //column可以和命令式的弱类型的API配合使用:select where //5. dataset.col //使用dataset来获取column对象，会和某个dataset进行绑定，在逻辑计划中，就会有不同的表现 val column4 = ds.col(\"name\") val column5 = ds1.col(\"name\") ds.select(column5).show() //为什么要和dataset来绑定呢？ ds.join(ds1, ds.col(\"name\") === ds1.col(\"name\")) //6. dataset.apply val column6 = ds.apply(\"name\") val column7 = ds(\"name\") } } Type ds.select('name as \"new_name\").show() ds.select('age.as[Long]).show() API //添加新列 df.withColun(\"age\", 'age * 2).show() //模糊查询 ds.where('name like \"zhang%\").show() //排序 ds.sort('age asc).show() //枚举判断 ds.where('name isin (\"zs\",\"wu\",\"ls\")).show() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:5:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"N/A 缺失值的处理： 丢弃缺失值的行 替换初始值 DataFrameNaFunctions 创建 val naf: DataFrameNaFunctions = df.na 功能 naf.drop… naf.fill … df.na.drop.show() df.na.fill.show() class NullProcessor { @Test def nullAndNaN(): Unit = { //ss val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"null processor\") .getOrCreate() //导入 //读取 // 1.通过spark-csv自动的推断类型来读取，推断数字的时候会将NaN推断为字符串 spark.read .option(\"header\", true) .option(\"inferSchema\",true) .csv(dataset/ds) // 2.直接读取字符串，在后续的操作中使用map算子转换类型 spark.read.csv().map(row =\u003e row...) // 3.指定Schema,不要自动推断 val schema = structType( list( StructField(\"id\",LongType), StructField(\"year\",IntegerType), StructField(\"day\",IntegerType), StructField(\"season\",IntegerType), StructField(\"pm\",DoubleType) ) ) val sourceDF = spark.read .option(\"header\", value = true) .schema(schema) .csv(\"dataset/data.csv\") .show() //丢弃 // 规则： // 1.any：只要有一个NaN就丢弃 sourceDF.na.drop(\"any\").show() sourceDF.na.drop().show() // 2.all: 所有数据NaN才丢弃 sourceDF.na.drop(\"all\").show() // 3.某些列 sourceDF.na.drop(\"any\",List(\"year\",\"month\",\"day\")).show() //填充 // 规则： // 1.针对所有列默认值填充 sourceDF.na.fill(0).show() // 2.针对特定列填充 sourceDF.na.fill(0,List(\"year\", \"month\")).show() } } SparkSQL处理异常字符串: def strProcessor(): Unit = { //1.丢弃 import spark.implicits._ sourceDF.where('PM_dongsi =!= \"NA\").show() //2.替换 import org.apache.spark.sql.functions._ sourceDF.select( 'No as \"id\", 'year, 'month, 'day, when('PM_Dongsi === \"NA\", Double.NaN) .otherwise('PM_Dongsi cast DoubleType) .as(\"pm\") ).show() sourceDF.na.replace(\"PM_Dongsi\", Map(\"NA\" -\u003e \"NaN\", \"NULL\" -\u003e \"null\")).show() } ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:6:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"groupBy groupBy //分组 val groupedDF = cleanDF.groupBy($\"year\",$\"month\") //使用functions函数来完成聚合 import org.apache.spark.sql.functions._ groupedDF.agg(avg($\"pm\") as \"pm_avg\") .orderBy($\"pm_avg\".desc) //分组第二种方式 groupedDF.avg(\"pm\") .select($\"avg(pm)\" as \"pm_avg\") .orderBy(\"pm_avg\") 多维聚合 //requirement 1:不同年，不同来源PM值的平均数 val postAndYearDF = pmFinal.groupBy('source,'year) .agg(avg($pm) as \"pm\") //requirement 2:按照不同的来源统计PM值的平均数 val postDF = pmFinal.groupBy($source) .agg(avg($pm) as \"pm\") .select($source, lit(null) as \"year\", $pm) //合并在同一个结果集中 postAndYearDF.union(postDF) .sort($source, $year asc_nulls_last, $pm) rollup 滚动分组：rollup(A, B)，生成三列：AB分组，A null分组，null(全局)的分组 //requirement 1: 每个城市，每年的销售额 //requirement 2: 每个城市，一共的销售额 //requirement 3: 总体销售额 val sales = Seq( (\"Bj\", 2016, 100), (\"Bj\", 2017, 200), (\"shanghai\", 2015, 50), (\"shanghai\", 2016, 150), (\"Guangzhou\", 2017, 50), ).toDF(\"city\", \"year\", \"amount\") sales.rollup($city, $year) .agg(sum($amount) as \"amount\") .sort($city asc asc_nulls_last, $year.asc_nulls_last) cube rollup对参数顺序有要求，cube是对rollup的弥补 rollup(A, B)，生成四列：AB分组，A null分组，null B分组，null(全局)的分组 import org.apache.spark.sql.functions._ pmFinal.cube($source, $year) .agg(avg($pm) as \"pm\") .sort($source.asc_nulls_last, $year.asc_nulls_last) RelationalGroupedDataset groupBy, rollup, cube后的数据类型都是RelationalGroupedDataset RelationalGroupedDataset并不是DataFrame，所以其中并没有DataFrame的方法，只有如下一些聚合相关的方法，下列方法调用后会生成DataFrame对象，然后就可以再次使用DataFrame的算子进行操作 操作符 解释 avg average count count max max min min mean average sum sum agg 聚合，可以使用sql.funcitons中的函数来配合进行操作 pmDf.groupBy($year).agg(avg($pm) as \"pm_avg\") ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:7:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Table Join Join class JoinProcessor{ //create Spark //import implicits._ @Test def introJoin(): Unit = { val person = Seq((0, \"Lu\", 0), (1, \"Li\", 0), (2,\"Tim\", 0)) .toDF(\"id\", \"name\", \"cityID\") val cities = Seq((0, \"BJ\"), (1, \"SH\"), (2,\"GZ\")) .toDF(\"id\", \"name\") val df = person.join(cities, person.col(\"cityID\") === cities.col(\"id\")) .select(person.col(\"id\"),person.col(\"name\"),cities.col(\"name\")) df.createOrReplaceTempView(\"user_city\") } } cross 交叉连接，笛卡尔积 def crossJoin(): Unit = { person.crossJoin(cities) .where(person.col(\"cityId\") === cities.col(\"id\")) spark.sql(\"select u.id, u.name, from person u cross join cities c\" + \"where u.cityId = c.id\") } inner 交集 select * from person inner join cities on person.cityId = cities.id person.join(right = cities, joinExprs = person(\"cityId\") === citeis(\"id\"), joinType = \"inner\") outer 全外连接 内连接的结果只有连接上的数据，而全外连接可以包含没有连接上的数据。 leftouter 左外连接 全外连接含没有连接上的数据，左外连接只包含左边没有连接上的数据。 semi\u0026anti Semi-join 通常出现在使用了exists或in的sql中，所谓semi-join即在两表关联时，当第二个表中存在一个或多个匹配记录时，返回第一个表的记录； 与普通join的区别在于semi-join时，第一个表里的记录最多只返回一次； Anti-join 而anti-join则与semi-join相反，即当在第二张表没有发现匹配记录时，才会返回第一张表里的记录； 当使用not exists/not in的时候会用到，两者在处理null值的时候会有所区别 使用not in且相应列有not null约束 not exists，不保证每次都用到anti-join代 ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:8:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"UDF 自定义列操作函数 ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:9:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Over Rank //1定义窗口 val window = Window.partitionBy($category) .orderBy($revenue.desc) //2处理数据 import org.apache.spark.sql.functions._ source.select($production, $category, dense_rank() over window as \"rank\") .where($rank \u003c= 2) .show() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:10:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Article description.","date":"2020-06-15","objectID":"/20200615_hive-key-point/","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive is a Hadoop-based data warehouse tool that maps structured data files into a database table and provides complete SQL query functionality that converts SQL statements into MapReduce tasks for execution. It is very suitable for statistical analysis of data warehouse. ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:0:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive的两张表关联，使用MapReduce怎么实现？ 如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合。 如果两张都是大表，那么采用联合key，联合key的第一个组成部分是joinon中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息；在Mapper中同时处理两张表的信息，将joinon公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:1:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive的特点，Hive和RDBMS有什么异同？ hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析，但是Hive不支持实时查询。 Hive与关系型数据库的区别： hqlDifferents ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:2:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"hive中SortBy，OrderBy，ClusterBy，DistrbuteBy各代表什么意思？ Orderby：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 Sortby：不是全局排序，其在数据进入reducer前完成排序。 Distributeby：按照指定的字段对数据进行划分输出到不同的reduce中。 Clusterby：除了具有distributeby的功能外还兼具sortby的功能。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:3:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive中split、coalesce及collect_list函数的用法（举例）？ split将字符串转化为数组，即：split(‘a,b,c,d’,’,’)==\u003e[“a”,“b”,“c”,“d”]。 coalesce(Tv1,Tv2,…)返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL。 collect_list列出该字段所有的值，不去重=\u003eselectcollect_list(id)fromtable。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:4:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive有哪些方式保存元数据，各有哪些特点？ Hive支持三种不同的元存储服务器，分别为：内嵌式元存储服务器、本地元存储服务器、远程元存储服务器，每种存储方式使用不同的配置参数。 内嵌式元存储主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。 在本地模式下，每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。 在远程模式下，所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据，元数据服务器和客户端之间使用Thrift协议通信。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:5:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive内部表和外部表的区别？ 创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。 删除表时：在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:6:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive的函数：UDF、UDAF、UDTF的区别？ UDF：单行进入，单行输出 UDAF：多行进入，单行输出 UDTF：单行输入，多行输出 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:7:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"所有的Hive任务都会有MapReduce的执行吗？ 不是，从Hive0.10.0版本开始，对于简单的不需要聚合的类似SELECTfrom LIMITn语句，不需要起MapReducejob，直接通过Fetchtask获取数据。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:8:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive桶表的理解？ 桶表是对数据某个字段进行哈希取值，然后放到不同文件中存储。 数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。 桶表专门用于抽样查询，是很专业性的，不是日常用来存储数据的表，需要抽样查询时，才创建和使用桶表。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:9:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive底层与数据库交互原理？ Hive的查询功能是由HDFS和MapReduce结合起来实现的，对于大规模数据查询还是不建议在hive中，因为过大数据量会造成查询十分缓慢。Hive与MySQL的关系：只是借用MySQL来存储hive中的表的元数据信息，称为metastore（元数据信息）。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:10:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive本地模式 大多数的HadoopJob是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。 用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:11:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive中的压缩格式TextFile、SequenceFile、RCfile、ORCfile各有什么区别？ 1、TextFile 默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，压缩后的文件不支持split，Hive不会对数据进行切分，从而无法对数据进行并行操作。并且在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。 2、SequenceFile SequenceFile是HadoopAPI提供的一种二进制文件支持，存储方式为行存储，其具有使用方便、可分割、可压缩的特点。 SequenceFile支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，一般建议使用BLOCK压缩。 优势是文件和hadoopapi中的MapFile是相互兼容的 3、RCFile 存储方式：数据按行分块，每块按列存储。结合了行存储和列存储的优点： 首先，RCFile保证同一行的数据位于同一节点，因此元组重构的开销很低； 其次，像列存储一样，RCFile能够利用列维度的数据压缩，并且能跳过不必要的列读取； 4、ORCFile 存储方式：数据按行分块每块按照列存储。 压缩快、快速列存取。 效率比rcfile高，是rcfile的改良版本。 小结： 相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，但是具有较好的压缩比和查询响应。 数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明显的优势。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:12:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive表关联查询，如何解决数据倾斜的问题？ 1）倾斜原因：map输出数据按keyHash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce上的数据量差异过大。 （1）key分布不均匀; （2）业务数据本身的特性; （3）建表时考虑不周; （4）某些SQL语句本身就有数据倾斜; 如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。 2）解决方案 （1）参数调节： hive.map.aggr=true hive.groupby.skewindata=true 有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MRJob。第一个MRJob中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupByKey有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupByKey分布到Reduce中（这个过程可以保证相同的GroupByKey被分布到同一个Reduce中），最后完成最终的聚合操作。 （2）SQL语句调节： ①选用joinkey分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。 ②大小表Join： 使用mapjoin让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。 ③大表Join大表： 把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。 ④countdistinct大量相同特殊值: countdistinct时，将值为空的情况单独处理，如果是计算countdistinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行groupby，可以先将值为空的记录单独处理，再和其他计算结果进行union。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:13:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Fetch抓取 Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT*FROMemployees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。 在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:14:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"小表、大表Join 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。 实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:15:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"大表Join大表 1）空KEY过滤 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空。2）空key转换 有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:16:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"GroupBy 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。 1）开启Map端聚合参数设置 （1）是否在Map端进行聚合，默认为True hive.map.aggr=true （2）在Map端进行聚合操作的条目数目 hive.groupby.mapaggr.checkinterval=100000 （3）有数据倾斜的时候进行负载均衡（默认是false） hive.groupby.skewindata=true当选项设定为true，生成的查询计划会有两个MRJob。第一个MRJob中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupByKey有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupByKey分布到Reduce中（这个过程可以保证相同的GroupByKey被分布到同一个Reduce中），最后完成最终的聚合操作。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:17:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Count(Distinct)去重统计 数据量小的时候无所谓，数据量大的情况下，由于COUNTDISTINCT操作需要用一个ReduceTask来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNTDISTINCT使用先GROUPBY再COUNT的方式替换 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:18:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"笛卡尔积 尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:19:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"行列过滤 列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT*。 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:20:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"并行执行 Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。 通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:21:0","tags":["Distribution","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-27","objectID":"/20200527_spark-guide1/","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:0:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Spark Introduction 1 Spark Component Spark提供了批处理（RDDs），结构化查询（DataFrame），流计算（SparkStreaming），机器学习（MLib），图计算（GraphX）等组件 这些组件均是依托于通用的计算引擎RDDs而构建出，所以spark-core的RDDs是整个Spark的基础 sparkStructure 2 Spark \u0026 Hadoop Hadoop Spark 类型 基础平台，包含计算，存储，调度 分布式计算工具（主要代替Hadoop的计算功能） 场景 大规模数据集上的批处理 迭代计算，交互式计算，流计算 延迟 大 小 易用性 API较为底层，算法适应性差 API较为顶层，方便使用 价格 性能要求低，便宜 对内存要求高，相对较贵 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:1:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Spark Cluster 1 Cluster relation clusterManager Driver：该进程调用Spark程序的main方法，并且启动SparkContext Cluster Manager：该进程负责和外部集群工具打交道，申请或释放集群资源 Worker：该进程是一个守护进程，负责启动和管理Executor Executor：该进程是一个JVM虚拟机，负责运行Spark Task 运行一个Spark程序大致经历如下几个步骤： 启动Driver，创建SparkContext Client提交程序给Drive，Drive向Cluster Manager申请集群资源 资源申请完毕，在Worker中启动Executor Driver将程序转化为Tasks，分发给Executor执行 2 Build Cluster Download Spark Upload Config HistoryServer Distribute: scp -r spark node02: $PWD Start 3 High Availability 对于 Spark Standalone 集群来说，当Worker调度出现问题时，会自动的弹性容错，将出错的Task调度到其他Worker执行。 但对于Master来说，是会出现单点失败的，为了避免可能出现的单点失败问题，Spark提供了两种方式满足高可用 使用Zookeeper实现Master的主备切换(Zookeeper是一个分布式强一致性的协调服务，Zookeeper最基本的一个保证是：如果多个节点同时创建一个ZNode)只有一个能够成功创建，这个做法的本质使用的是Zookeeper的ZAB协议，能够在分布式环境下达成一致。 使用文件系统做主备切换 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:2:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Running Process 1 Spark-Shell Run val rdd1 = sc.textFile(\"/data/wordcount.txt\") //Hadoop默认读取hdfs路径：hdfs:///data/wordcount.txt val rdd2 = rddflatMap(item =\u003e item.split(\" \")) val rdd3 = rdd2.map(item =\u003e (item,1)) val rdd4 = rdd3.reduceByKey((curr,agg) =\u003e curr + agg) rdd4.collect() sparkRunProcess 2 Local IDEA Run def main(args:Arrary[String]): Unit = { // 创建SparkContext val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"word_count\") val sc = new SparkContext(conf) //2. 加载文件 // 准备文件 // 2.读取文件 val rdd1 = sc.testFile(path = \"dataset/wordcount.txt\") //3. 处理 // 拆分为多个单词 val rdd2 = rddflatMap(item =\u003e item.split(\" \")) // 2.把每个单词指定一个词频 val rdd3 = rdd2.map(item =\u003e (item,1)) // 3.聚合 val rdd4 = rdd3.reduceByKey((curr,agg) =\u003e curr + agg) //4.得到结果 val result = rdd4.collect() result.foreach(item =\u003e println(item)) } 3 Submit Run 修改代码 去掉master设置，并修改文件路径 Maven打包上传 在集群中运行 bin/spark -submit --class cn.demo.spark.rdd.WordCount --master spark://node01:7077 ~/original -spark-0.0.jar ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:3:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"RDD 1 Cause of creation 在RDD出现之前，MapReduce是比较主流的 但多个MapReduce任务之间没有基于内存的数据共享方式，只能通过磁盘来进行共享，这种方式明显比较低效。 RDD如何解决迭代计算非常低效的问题 在Spark中，最终Job3从逻辑上的计算过程是：Job3 = (Job1.map).filter，整个过程是共享内存的，而不需要中间结果存放在可靠的分布式文件系统中。 2 Resilient Distributed Datasets 分布式 RDD支持分区，可以运行在集群中 弹性 RDD支持高效的容错 RDD中的数据即可以缓存在内存中，也可以缓存在磁盘中，也可以缓存在外部存储中 数据集 RDD可以不保存具体数据，只保留创建自己的必备信息，例如依赖和计算函数 RDD也可以缓存起来，相当于存储具体数据 3 Feature RDD是数据集 RDD不仅是数据集，也是编程模型 RDD的算子大致分为两类： Transformation转化操作，例如：map、flatMap、filter等 Action动作操作，例如：reduce、collect、show等 RDD是编程模型 RDD相互之间有依赖关系 RDD是可以分区的 RDD是只读的 RDD需要容错，可以惰性求值，可以移动计算，所以很难支持修改，显著降低问题的复杂度。 sparkRdd 4 sparkContext SparkContext是spark功能的主要入口。其代表与spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。每个JVM里只能存在一个处于激活状态的SparkContext，在创建新的SparkContext之前必须调用stop()来关闭之前的SparkContext。 每一个Spark应用都是一个SparkContext实例，可以理解为一个SparkContext就是一个spark application的生命周期，一旦SparkContext创建之后，就可以用这个SparkContext来创建RDD、累加器、广播变量，并且可以通过SparkContext访问Spark的服务，运行任务。spark context设置内部服务，并建立与spark执行环境的连接。 @Test def sparkContext(): Unit = { // Spark Context 编写 // 创建SparkConf val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"spark_context\") // 2.创建SparkContext val sc = new SparkContext(conf) //SparkContext身为大入口API，应该能够创建RDD，并且设置参数，设置Jar包 //sc... //2. 关闭SparkContext，释放集群资源 } 5 Creation Way 三种RDD的创建方式 通过本地集合创建RDD @Test def rddCreationLocal(): Unit = { val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"spark_context\") val sc = new SparkContext(conf) val rdd1 = sc.parallelize(Seq(\"Hello1\", \"Hello2\", \"Hello3\"), 2) val rdd2 = sc.makeRDD(seq, 2) // parallelize和makeRDD区别：parallelize可以不指定分区数 } 2. 通过外部数据创建RDD @Test def rddCreationFiles(): Unit = { sc.textFile(\"/.../...\") //testFile: 传入* hdfs:// file:// /.../...(这种方式分为在集群还是本地执行，在集群中读的是hdfs，本地读本地文件) //2.是否支持分区：支持，在hdfs中由hdfs文件的block决定 //3.支持什么平台：支持aws和阿里云... } 3. 通过RDD衍生新的RDD @Test def rddCreationFromRDD(): Unit = { val rdd1 = sc.parallelize(Seq(1,2,3)) //通过在rdd上执行算子操作，会生成新的rdd //非原地计算：str.substr 返回新的字符串，非原地计算。字符串不可变，RDD也不可变 val rdd2: RDD[Int] = rddmap(item =\u003e item) } ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:4:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Transformation Operator map() @Test def mapTest(): Unit = { //创建RDD val rdd1 = sc.parallelize(Seq(1,2,3)) //2.执行map操作 val rdd2 = rdd1.map(item =\u003e item * 10) //3.得到结果 val result = rdd2.collect() result.foreach(item =\u003e println(item)) } flatmap() 把rdd中的数据转化成数组或集合形式 把集合展开 生成了多条数据 flatmap是一对多 @Test def flatMapTest(): Unit = { val rdd1 = sc.parallelize(Seq(\"Hello a\",\"Hello b\",\"Hello c\")) val rdd2 = rddf1.latMap( item =\u003e item.split(\" \")) val result = rdd2.collect() result.foreach(item =\u003e println(item)) sc.stop() } reducebykey() reduceByKey第一步先按照key分组，然后对每一组进行聚合，得到结果。 @Test def reduceBykeyTest(): Unit = { //创建RDD val rdd1 = sc.parallelize(Seq(\"Hello a\",\"Hello b\",\"Hello c\")) //2.处理数据 val rdd2 = rdd1.flatMap( item =\u003e item.split(\" \")) .map( item =\u003e (item,1) ) .reduceByKey( (curr, agg) =\u003e curr + agg)//curr是当前的总值，agg是单个item的值 //3.得到结果 val result = rdd2.collect() result.foreach(item =\u003e println(item)) //4.关闭sc sc.stop() } Q\u0026A 数据量过大，如何处理？ 集群中处理，利用集群多台计算机来并行处理 如何放在集群中运行? sparkPutFile2Cluster 并行计算就是同时使用多个计算资源解决一个问题，有四个要点 解决的问题可以分解为多个可以并发计算的部分 每个部分可以在不同处理器上被同时执行 需要一个共享内存的机制 需要一个总体上的协作机制来进行调度 如果放在集群中，如何对整个计算任务进行分解？ sparkFile2Cluster2 概述 对于HDFS中的文件，是分为不同的Block 在进行计算的时候，就可以按照Block来划分，每一个Block对应一个不同的计算单元 扩展 RDD并没有真实的存放数据，数据是从HDFS中读取的，在计算的过程中读取即可 RDD至少是需要可以分片的，因为HDFS中的文件就是分片的，RDD可以分片也意味着可以并行计算 移动数据不如移动计算是一个基础的优化，如何做到？ 每一个计算单元需要记录其存储单元的位置，尽量调度过去 集群中运行，需要多节点配合，出错的概率也更高，出错了怎么办？ RDD1-\u003eRDD2-\u003eRDD3这个过程中，RDD2出错了，有两种解决办法 缓存RDD2的数据，直接恢复RDD2，类似HDFS的备份机制 记录RDD2的依赖关系，通过其父级的RDD来恢复RDD2，这种方式会少很多数据的交互和保存 如何通过父级RDD恢复？ 记录RDD2的父亲是RDD1 记录RDD2的计算函数，例如RDD2 = RDD1.map(…)等计算函数 通过父级RDD和计算函数来恢复RDD2 任务特别复杂，流程特别长，有很多RDD之间有依赖关系，如何优化？ 上面提到了可以使用依赖关系来进行容错，但是如果依赖关系特别长的时候，这种方式其实也比较低效，这个时候就应该使用另外一种方式，也就是记录数据集的状态 在Spark中有两个手段可以做到 缓存 Checkpoint map() \u0026 mapPartitions() mapPartitions 和 map 算子是一样的，只不过map是针对每一条数据进行转换，mapPartitions针对一整个分区的数据进行转换 所以 map 的 func 参数是单条数据，mapPartitions 的 func 参数是一个集合(一个分区整个所有的数据) map 的 func 返回值也是单条数据，mapPartition 的 func 返回值是一个集合 mapPartitionWithIndex 和 mapPartition 的区别是 func 中多分区数量参数 filter() 保留满足条件的元素 sample() filter按照规律过滤，sample则是随机采样 def sample( withReplacement: Boolean, //是否重复取样 fraction: Double, //取样比例 seed: Long = Utils.random.nextLong): RDD[T] = {...} mapValues() mapValue也是map，map作用于全部数据，mapValue作用于value collection operation 交集：rdd1.intersection(rdd2) 并集：rdd1.union(rdd2) 差集：rdd1.subract(rdd2) groupByKey() 聚合操作： reduceByKey -\u003e按照key分组，然后把每一组数据reduce。reduceByKey在map端combiner能减少IO，一个分区放多个数据。 groupByKey 运算结果的格式：（k，（value1，value2）），没有减少IO sc.parallelize(Seq((\"a\",1),(\"a\",1),(\"b\",1))) .groupByKey() .collect() .foreach(println(_)) combineByKey() 接收三个参数： 转化数据的函数（初始函数，作用于第一条数据，用于开启整个计算） 在分区上进行聚合 把所有的分区的聚合结果聚合为最终结果 val result = rdd.combineBykey( createCombiner = curr =\u003e (curr,1), mergeValue = (curr: (Double, Int), nextValue: Double) =\u003e (curr._1 + nextValue, curr._2 + 1)), mergeCombiners = (curr: (Double,Int), agg: (Double, Int)) =\u003e (curr._1 + agg._1, curr._2 + agg._2) ) result.map(item =\u003e (item._1, item._2._1 / item._2._2)) foldByKey() 功能等同于reduceByKey()，增加了初始值。reduceByKey底层是combineByKey()，foldByKey()底层是aggregateByKey()。 aggregateByKey() join() 按照相同的Key进行连接 sortBy() 排序：sortBy()，sortByKey() coalesce() 一般涉及到分区操作的算子常见的有两个，repartition和coalesce，都可以调大或者调小分区数量 summary 所有的转化操作的算子都是惰性的，在执行时候不会调度运行求得结果，而只是生成了对应的RDD 只有在Action操作的时候，才会真的运行 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:5:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Action Operator reduce((T, T) - U) 对整个结果集规约，最终生成一条数据，是整个数据集的总汇 reduceByKey和reduce有什么区别： reduce是action算子，reduceByKey是一个转换算子 RDD里有一万条数据，大部分key是相同的，有10个不同的key生成10条数据 reduce生成1条数据 reduceByKey是按Key分组，然后把每组聚合 reduce是针对一整个数据集进行聚合 reduceByKey是对KV数据进行计算 reduce可针对所有类型数据 reduce算子是一个shuffle操作吗？ shuffle操作分为mapper和reducer，mapper将数据放入paritioner的函数计算，求得往哪个reducer里放 reduce操作没有mapper和reducer，因为reduce算子会作用于RDD中的每个分区，然后分区求得局部结果，最终汇总到Driver中求得最终结果 RDD有五大属性，partitioner在shuffle过程中使用 paritioner只有kv型的RDD才有 collect() 以数组的形式返回数据集中所有元素 countByKey() count和countByKey countByKey结果：Map(Key -\u003e Key的count) 调用Action会生成一个job，job会运行获取结果，所以在两个job中有大量的log 数据倾斜：解决数据倾斜的问题，需要先通过countByKey查看Key对应的数量 first() 返回第一个元素 take(N) 返回前N个元素 takeSample(withReplacement, num) 类似于sample，区别这是action，直接返回结果 withReplacement：取数据有无放回 first() first()速度相比其他方法最快 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:6:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Data Type in RDD RDD中存放的数据类型 基本类型，String，对象 KV类型 数字类型 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:7:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Practice @Test def process(): Unit = { //1. 创建sc对象 val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"practice\") val sc = new SparkContext(conf) //2. 读取文件 //1,2010,1,1,0,4,NA,NA,NA,NA,-21,43,1021,-11,NW,1.79,0,0 val source = sc.textFile(\"dataset/parctive.csv\") //3. 处理数据 val resultRDD = source.map(item =\u003e ((item.split(\",\")(1), item.split(\",\")(2)),item.split(\",\")(6))) .filter(item =\u003e StringUtils.isNotEmpty(item._2) \u0026\u0026 ! item._2.equalsIgnoreCase(\"NA\")) .map(item =\u003e (item._1, item._2.toInt)) .reduceByKey((curr,agg) =\u003e curr + agg) .sortBy(item =\u003e item._2, ascending = false) //4. 获取结果 resultRDD.take(10).foreach(item =\u003e println(item)) //5. 关闭sc sc.stop() } ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:8:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"RDD Feature RDD’s shuffle and partition RDD经常需要通过读取外部系统的数据来创建，外部存储系统往往是支持分片的。RDD需要支持分区，来和外部系统的分片一一对应 RDD的分区是一个并行计算的实现手段 partition function RDD使用分区来分布式处理，当使用RDD读取数据时，会尽量在屋里上靠近数据源。比如读HDFS或Cassandra时，会尽量的保持RDD的分区和数据源的分区数，分区模式一一对应 shuffle 从mapper端到reducer端 Spark支持宽依赖的转换，例如groupByKey和reduceByKey。在这些依赖项中，计算单个分区中的记录所需的数据可以来自于父数据集的许多分区中。要执行这些转换，具有相同key的所有元组必须最终位于同一分区中，由同一任务处理。为了满足这一要求，Spark产生一个shuffle，它在集群内部传输数据，并产生一个带有一组新分区的新stage。 Hash base shuffle Reduce 找到每个Mapper中对应自己哈希桶拉取数据 缺点：过多占用资源占用 Sort base shuffle 先按照partition ID 排序， 后按照Key的HashCode排序 partition and shuffle relation 分区主要用来实现并行计算，和shuffle没什么关系，但数据处理时，例如reduceByKey，groupByKey等聚合操作，需要把Key相同的Value拉取到一起进行计算，这个时候因为这些Key的相同的Value可能会在不同的分区，所以理解分区才能理解shuffle的根本原理 shuffle feature 只有KV型的RDD才会有Shuffle操作 早期版本spark的shuffle算法是 hash base shuffle，后来改为 sort base shuffle，更适合大吞吐量的场景 check partition 指定分区数 通过本地集合创建的时候指定分区数 val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"practice\")//创建App并开启6个分区 val sc = new SparkContext(conf) 通过读取文件创建的时候指定分区数 val rdd1 = sc.parallelize(Seq(1, 2, 3, 4, 5, 6, 7), 3) //指定分区数3 val rdd2 = sc.testFile(\"hdfs://node01:8020/data/test.txt\", 6) //这里指定的是最小分区数6 查看方法 通过WebUI查看。端口：4040 通过partitions来查看。rdd1.partitions.size 重分区 coalesce(num, true) repartitions(num) RDD Cache //1. 取出IP val countRDD = source.map(item =\u003e (item.split(\" \")(0), 1)) //2. 数据清洗 val cleanRDD = countRDD.filter(item =\u003e StingUtils.isNotEmpty(item._1)) //3. 统计ip的出现次数 val aggRDD = cleanRDD.reduceBykey((curr,agg) =\u003e curr + agg) //4. 统计出现最少的ip val leastIP = aggRDD.sortBy(item =\u003e item._2, ascending = true).first() //5. 统计出现最多的ip val mostIP = aggRDD.sortBy(item =\u003e item._2, ascending = false).first() println(leastIP, mostIP) sc.stop() 第一次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey) 第二次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey) 转换算子的作用：生成RDD，以及RDD之间的依赖关系 Action算子的作用：生成job，执行job 全局执行了四个shuffle 使用缓存的意义： 减少shuffle操作 容错，减少开销：rdd1-\u003erdd2-\u003erdd3，若rdd3算错会再次计算rdd1和rdd2整个流程。 缓存API: cache()或persist(null/level) //1. 处理 val countRDD = source.map(item =\u003e (item.split(\" \")(0), 1)) val cleanRDD = countRDD.filter(item =\u003e StingUtils.isNotEmpty(item._1)) val aggRDD = cleanRDD.reduceBykey((curr,agg) =\u003e curr + agg) //2. cache aggRDD = aggRDD.cache() //3. 两个RDD的action操作 val leastIP = aggRDD.sortBy(item =\u003e item._2, ascending = true).first() val mostIP = aggRDD.sortBy(item =\u003e item._2, ascending = false).first() println(leastIP, mostIP) sc.stop() //1. 处理 val countRDD = source.map(item =\u003e (item.split(\" \")(0), 1)) val cleanRDD = countRDD.filter(item =\u003e StingUtils.isNotEmpty(item._1)) val aggRDD = cleanRDD.reduceBykey((curr,agg) =\u003e curr + agg) //2. cache aggRDD = aggRDD.persist(storageLevel.MEMORY_ONLY) //3. 两个RDD的action操作 val leastIP = aggRDD.sortBy(item =\u003e item._2, ascending = true).first() val mostIP = aggRDD.sortBy(item =\u003e item._2, ascending = false).first() println(leastIP, mostIP) sc.stop() 缓存级别： MEMORY_ONLY: CPU效率最高 MEMORY_ONLY_SER: 更加节省空间 Checkpoint 斩断RDD的依赖链，并且将数据存储在可靠的存储引擎中，例如HDFS HDFS的NameNode中主要职责就是维护两个文件，一个是edits，另一个是fsimage。 edits中主要存放Editlog，FsImage保存了当前系统中所有目录和文件的信息，这个FsImage其实就是一个Checkpoint。 每一次修改文件的时候，都会在Edits中添加一条记录。 在一定条件满足的情况下，把edits删掉添加一个新的FSimage，包含了系统当前最新的状态。好处：增加速度，提高稳定性 Checkpoint和Cache的区别： Cache可以吧RDD计算出来放到内存中，但RDD的依赖链(相当于NameNode中的Edits日志)是不能丢的，若出现错误，只能重计算出来。 Checkpoint把结果存放在HDFS这类存储中，就变成了可靠的数据，如果出错了，则通过复制HDFS中的文件来实现容错。 如何使用： 两步： val conf = new.SparkConf().setMaster(\"local[6]\").setAppName(\"debug_string\") //1. setCheckPointDir：设置保存目录，也可以设置为HDFS上的目录 sc.setCheckpointDir(\"checkpoint\") val interimRDD = sc.textFile(\"dataset/test.txt\") .map(item =\u003e (item.split(\" \")(0), 1)) .filter(item =\u003e StringUtils.isNotBlank(item._1)) .reduceByKey((curr, agg) =\u003e curr + agg) //2. setCheckPoint：是一个action操作，也就是说如果调用checkpoint，则会重新计算一下RDD，然后把结果存在HDFS或者本地目录中 interimRDD.checkpoint() interimRDD.collect().foreach(println(_)) sc.stop() ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:9:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Spark Running Process //1. 创建sc对象 //2. 创建数据集 val textRDD = sc.parallelize(Seq(\"hadoop spark\", \"hadoop flume\", \"spark soo\")) //3. 数据处理 // 1.拆词2.赋予初始词频3.聚合4.将结果转为字符串 val splitRDD = textRDD.flatMap(_.split(\" \")) val tupleRDD = splitRDD.map((_, 1)) val reduceRDD = tupleRDD.reduceByKey(_ + _) val strRDD = reduceRDD.map(item =\u003e s\"${item._1}, ${item._2}\") //4. 结果获取 strRDD.collect().foreach(item =\u003e println(_)) //5. 关闭sc sc.stop() 集群组成 Node1主节点: Master Daemon： 负责管理Master节点， 协调资源的获取，以及连接Worker节点来运行Executor，是spark集群中的协调节点 Node2: Worker Daemon： 也称之为Slaves，是spark集群中的计算节点，用于和Master交互和并管理Driver， 当一个spark job 提交后，会创建sparkContext，worker会启动对应的Executor Driver: ction算子操作获取的结果，会把结果存放在Driver中 Executor Backend： Worker用于控制Executor的启停，其实worker是通过 Executor Backend来进行控制的。 Executor Backend是一个进程（是一个JVM实例），持有一个Executor对象。 Executor Task1 Task2 Task3 逻辑执行图 val textRDD = sc.parallelize(Seq(\"hadoop spark\", \"hadoop flume\", \"spark soo\")) val splitRDD = textRDD.flatMap(_.split(\" \")) val tupleRDD = splitRDD.map((_, 1)) val reduceRDD = tupleRDD.reduceByKey(_ + _) val strRDD = reduceRDD.map(item =\u003e s\"${item._1}, ${item._2}\") println(strRDD.toDebugString) (8) MapPartitionsRDD[4] at map at test.scala:12 [] | ShuffledRDD[3] at reduceByKey at test.scala:11 [] +-(8) MapPartitionsRDD[2] at map at test.scala:10 [] | MapPartitionsRDD[1] at flatMap at test.scala:9 [] | ParallelCollectionRDD[0] at parallelize at test.scala:6 [] RDDlogic 物理执行图 当触发Action执行的时候，这一组互相依赖的RDD要被处理，所以要转化为可运行的物理执行图，调度到集群中执行。 因为大部分RDD是不真正存放数据的，只是数据从中流转，所以不能直接在集群中运行RDD，要有一种pipeline的思想，需要将这组RDD转为Stage和Task，从而运行Task，优化整体执行速度。 RDDphysic 小结： ① -\u003e ① -\u003e ① 在第一个stage中，每一个这样的执行流程是一个Task，也就是在同一个Stage中的所有RDD的对应分区，在同一个Task中执行 Stage的划分是由Shuffle操作来确定的，有Shuffle的地方，Stage断开 数据流动 val sc = ... val textRDD = sc.parallelize(Seq(\"Hadoop Spark\", \"Hadoop Flume\", \"Spark Squad\")) val splitRDD = textRDD.flatMap(_.split(\" \")) val tupleRDD = splitRDD.map((_,1)) val reduceRDD = tupleRDD.reduceByKey(_ + _) val strRDD = reduceRDD.map(item =\u003e s\"${item._1, ${item._2}}\") strRDD.collect.foreach(item =\u003e println(item)) Job和Stage的关系 Job是一个最大的调度单位，DAGScheduler会首先创建一个Job的相关信息，然后去调度Job，但是没办法直接调度Job。 ​ 为什么Job需要切分 因为job的含义是对整个RDD求值，但RDD之间可能有一些宽依赖 如果遇到宽依赖的话，两个RDD之间需要进行数据拉取和复制 那么一个RDD就必须等待它所依赖的RDD所有分区先计算完成，然后再进行拉取 所以，一个Job是无法计算完整的RDD血统的 ​ Stage和Task的关系 Stage中的RDD之间是窄依赖： 窄依赖RDD理论上可以放在同一个Pipeline中执行的 RDD还有分区： 一个RDD只是一个概念，而真正存放和处理数据时，都是以分区作为单位的 Stage对应的是多个整体上的RDD，而真正的运行是需要针对RDD的分区来进行的 一个Task对应一个RDD的分区： 一个比Stage粒度更细的单元叫做Task，Stage是由Task组成的，之所以有Task这个概念，是因为Stage针对整个RDD，而计算的时候，要针对RDD的分区。 总结： Job\u003eStage\u003eTask 一个Job由多个Stage组成(这个取决有多少个宽依赖)，一个Stage由多个Task组成（这个取决有多少个分区数量 而Stage中经常会有一组Task需要同时执行，所以针对每一个Task来进行调度太过频繁没有意义，所以每个Stage中的Task们会被收集起来，放入一个TaskSet集合中。 一个Stage有一个TaskSet TaskSet中Task的个数由Stage中的最大分区数决定 sparkFlow ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:10:0","tags":["Distribution","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-26","objectID":"/20200526_app-architecture-development/","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"The features of large applications: high availability, high concurrency and big data. High availability: system need to provide service without interruption. High concurrency: still stable under the big access. Big data: store and manage big data well. ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:0:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"简单的架构 开始数据量少，所以只需一台服务器 smapleArchitecture 应用程序、文件、数据库往往都部署在一台服务器上，应用程序部署在Tomcat服务器上，数据库可以使用MySQL ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:1:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"应用于数据服务分隔 随着业务越来越复杂，访问量越来越大，导致性能越来越差，存储空间严重不足，这时我们考虑把服务器增加到三台。分离出应用服务器、数据库服务器、文件服务器。 应用服务器需要处理大量的访问，所以需要性能更好的CPU 数据库服务器需要存储大量的数据以及快速的检索，所以需磁盘的检索速度较快以及存储空间大 文件服务器需要存储上传的文件，需要更大的磁盘；现在通常情况下会选择第三方的存储服务 appSeparateData ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:2:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"应用服务器集群 在高并发、大流量的情况下，一台服务器是肯定处理不过来的，这个时候增加服务器，部署集群提供服务，来分担每台服务器的压力。部署集群的另一个好处是可伸缩性。比如双十一增加服务器分摊流量，双十一过后再减少服务器。 Cluster 如果应用服务器是Tomcat，那么可以部署一个Tomcat的集群，外部在部署一个负载均衡器，可以采用随机、轮询或者一致性哈希算法达将用户的请求分发到不同应用服务集群；通常选择的免费的负载均衡是nginx。在这种架构下，应用服务器的负载将不会是整个应用的瓶颈点； 虽然应用程序的处理速度在这种架构下提升了许多，但是又会暴露一个问题，数据库的压力大大增大，导致访问响应延迟，影响整个应用的性能。这种架构还有个问题，通常应用是有状态的，需要记录用户的登录信息，如果每次用户的请求都是随机路由到后端的应用服务器，那么用户的会话将会丢失；解决这个问题两个方案： 采用一致性hash把用户的请求路由到同一个Tomcat，如果有一台服务器跪了，那么这台服务器上面的用户信息将会丢失 Tomcat集群之间通过配置session复制，达到共享，此方案效率较低 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:3:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"缓存 根据二八原则，80%的的业务都是集中访问20%的数据，这20%的数据通常称为热点数据，但是这20%的数据占用的内存也不会小，如果每个应用服务器都存放一份，有些浪费存储空间，所以这时候需要考虑加入分布式缓存服务器（常用的是Redis）；当引入了分布式缓存服务器，再来看上面那个方案的问题，就可以解决了，把用户的会话存放到缓存服务器，不仅可以防止用户数据丢失，效率也不低；架构图如下： cache 由于分布式缓存服务器毕竟存放在远程，需要经过网络，所以取数据还是要花一点时间；本地缓存访问速度更快，但是内存空间有限，并且还会出现和应用程序争抢资源；所以这种架构搭配了分布式缓存和本地缓存，本地缓存存放少量常用热点数据，当本地缓存中没有命中时在去集中式缓存取 在引进缓存之后，数据库的访问压力可以的一定的缓解 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:4:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"数据库读写分离 虽然在加入了缓存之后，部分数据可以直接走缓存，不需要访问数据库，但是任然会有一些请求，会访问数据库，比如：缓存失效，缓存未命中；当流量大的时候，数据库的访问量也不小。这时候我们需要考虑搭建数据库集群，读写分离 rwSeparate 当应用服务器有写操作时，访问主库，当应用程序有读操作时，访问从库；大多数的应用都是读的操作远远大于写的操作，所以可以配置数据库一主多从来分担数据库的压力；为了让应用程序对应主库和从库无感知，通常需要引入一些读写分离的框架做一个统一的数据访问模块。 这种架构通常需要警惕的一个问题是主从延迟，当在高并发的场景下，主库刚写成功，数据库还未成功同步完从库，这时候另一个请求进入读取数据发现不存在；解放方案是在应用程序中高并发的场景下设置强制走主库查询 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:5:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"反向代理和CDN 假如随着业务的不断扩大，全国各地都会使用到我们的应用，由于各地区的网络情况不同，所以有的人请求响应速度快，有的人请求响应速度慢，这会严重的影响到用户的体验。为了提高响应速度需要引入反向代理和CDN；CDN和反向代理都是采用的缓存，目的： 尽可能快的把数据呈现给用户 减轻后端服务器的压力 架构图如下： cdn CDN: 部署在网络提供商的机房，当用户来访问的时候，从距离用户最近的服务器返回数据，尽快呈现给用户；通常情况下在CDN中缓存的是静态资源（html,js,css），达到动静分离；但是有时候遇到了某些数据访问量特别大的时候，后端会生成静态资源放入到CDN，比如：商城的首页，每个用户进入都需要访问的页面，如果每次请求都进入到后端，那么服务器的压力肯定不小，这种情况下会把首页生成静态的文件缓存到cdn和反向代理服务器 反向代理：部署在应用的中心机房，通常也是缓存的静态资源，当用户通过CDN未请求到需要的数据时，先进入反向代理服务器，如果有缓存用户访问的数据，那么直接返回给用户；这里也有特殊情况，对于有些场景下的热点数据，在这里根据用户的请求去分布式缓存服务器中获取，能拿到就直接返回。 这种架构已经把缓存做到了4级 第一级：CDN 缓存静态资源 第二级：反向代理缓存静态资源以及部分热点数据 第三级：应用服务器的本地缓存 第四级：分布式缓存服务器 通常情况下经过了这4级缓存，能够进入到数据库的请求也不多了，很好的释放了数据库的压力 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:6:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"搜索引擎和NoSQL 随着业务的不断扩大，对于数据的存储和查询的需求也越来越复杂，通常情况我们需要引入非关系型数据库，比如搜索引擎和NoSQL数据库 NoSQL 有时候我们的查询场景很复杂，需要查询很多数据表，经过一系列的计算才能完成，这时候可以考虑通过数据同步工具（比如canal）拉去数据到大数据平台，使用批处理框架离线计算，把输出的结果存放到搜索引擎或者NoSQL数据库中，应用程序直接查询计算的结果返回给用户。也有可能我们需要汇总多个表的数据做一张宽表，方便应用程序查询 由于引入的数据存储方式增多，为了减轻应用程序的管理多个数据源的麻烦，需要封装统一数据访问模块，如果使用的时Java，可以考虑spring-data ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:7:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"业务纵向拆分 互联网公司通常的宗旨是小步迭代试错快跑，当业务发展到足够大，对于单体应用想要达到这个宗旨是有难度的，随着业务的发展，应用程序越来越大，研发、维护、发布的成本也越来越大，这时候就需要考虑根据业务把单体应用拆分为多个服务，服务之间可以通过RPC远程调用和消息队列来一起完成用户的请求。 由于业务的拆分，通常情况下也会相应的对数据库进行拆分，达到一个服务对应一个数据库的理想状态 vertical 引入MQ的好处： 提高系统的可用性：当消费服务器发送故障时，消息还在消息队列中，数据不会丢失 加快请求的响应：当用户请求到达服务器后，把请求中可以异步处理的数据放入到MQ，让系统逐一消费，不需要用户等待，加快了响应速度 削峰填谷：当大量请求都同时进入到系统之后，会全部放入到消息队列，系统逐一消费，不会对系统造成很大的冲击 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:8:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"总结 还有一个情况未谈及到，就是数据库的水平拆分，这也是数据库拆分的最后手段，只有当单表数据特别大，不能满足业务的需要才使用。使用最多的还是进行数据库的业务纵向拆分，把数据库中不同业务的数据放到不同的物理服务器上。 应用当前到底选择什么架构，一定要根据实际业务的需求进行灵活的选择，驱动技术架构发展的主要动力还是在于业务的发展，不要为了技术而技术。 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:9:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-26","objectID":"/20200526_final-key-word-java/","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"Final key word no doubt be mentioned most time in Java language. There are some points about final key word. First of all, final key word can modify three objects: first is modify variables, second is modify way, third is modify class. ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:0:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"final 关键字修饰变量 在使用final修饰变量时，又可以分为两种情况：一种是基本数据类型的变量，另一种是引用类型的变量。final关键字在修饰基本数据类型时必须对变量赋予初始值，因此final也常常和static关键字一起用来声明常量值。final正好限制了必须赋值，static声明了静态变量。 final static String str = \"Hollow world\"; 修饰引用变量时，该引用变量如果已经被赋值则不可以再被赋值，否则也会出现不能编译的情况 //定义一个main对象并实例化 final Main main = new Main(); //被final关键字修饰后，再次对Main对象进行赋值就会报错 main = new Main(); ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:1:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"final关键字修饰方法 通过final修饰的方法是不能被子类的方法重写的。一般情况下，一个方法确定好不再修改可以使用final，因为final修饰的方法是在程序编译的时候就被动态绑定了不用等到程序运行的时候被动态绑定，这样就大大提高了执行效率。 public final void method(){ System.out.println(\"It is final method\"); } ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:2:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"final关键字修饰类 被final修饰的类叫final类，final类是不能被继承的，这也就以为这final类的功能是比较完整的。因此，jdk中有很多类使用final修饰的，它们不需要被继承，其中，最常见的就是String类。 public final class String implements java.io.Serializable, Comparable\u003cString\u003e public final class Integer extends Number implements ... public final class Long extends Number implements ... 包括其他的装饰类都是被final关键字修饰的，它自身提供的方法和功能也是非常完备的。 ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:3:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"static关键字的不同 final static 成对出现的频率也是比较高的，使用static修饰的变量只会在类加载的时候被初始化，不会因为对象的再次创建而改变。 static double num = Math.random(); ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:4:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-19","objectID":"/20200519_zookeeper/","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"ZooKeeper is an open source distributed coordination framework. It is positioned to provide consistent services for distributed applications and is the administrator of the entire big data system. ZooKeeper will encapsulate key services that are complex and error-prone, and provide users with efficient, stable, and easy-to-use services. ","date":"2020-05-19","objectID":"/20200519_zookeeper/:0:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1. Introduce ZooKeeper 是一个开源的分布式协调框架，它的定位是为分布式应用提供一致性服务，是整个大数据体系的管理员。ZooKeeper 会封装好复杂易出错的关键服务，将高效、稳定、易用的服务提供给用户使用。 如果上面的官方言语你不太理解，你可以认为 ZooKeeper = 文件系统 + 监听通知机制。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1.1 File System filesys Zookeeper维护一个类似文件系统的树状数据结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为1M。每个子目录项如 NameService 都被称作为 znode(目录节点)。和文件系统一样，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的。默认有四种类型的znode： 持久化目录节点 PERSISTENT：客户端与zookeeper断开连接后，该节点依旧存在。 持久化顺序编号目录节点 PERSISTENT_SEQUENTIAL：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。 临时目录节点 EPHEMERAL：客户端与zookeeper断开连接后，该节点被删除。 临时顺序编号目录节点 EPHEMERAL_SEQUENTIAL：客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1.2 Watcher Watcher 监听机制是 Zookeeper 中非常重要的特性，我们基于 Zookeeper 上创建的节点，可以对这些节点绑定监听事件，比如可以监听节点数据变更、节点删除、子节点状态变更等事件，通过这个事件机制，可以基于 Zookeeper 实现分布式锁、集群管理等功能。 Watcher 特性： 当数据发生变化的时候， Zookeeper 会产生一个 Watcher 事件，并且会发送到客户端。但是客户端只会收到一次通知。如果后续这个节点再次发生变化，那么之前设置 Watcher 的客户端不会再次收到消息。（Watcher 是一次性的操作）。可以通过循环监听去达到永久监听效果。 ZooKeeper 的 Watcher 机制，总的来说可以分为三个过程： 客户端注册 Watcher，注册 watcher 有 3 种方式，getData、exists、getChildren。 服务器处理 Watcher 。 客户端回调 Watcher 客户端。 监听流程： 首先要有一个main()线程 在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 通过connect线程将注册的监听事件发送给Zookeeper。 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。 listener线程内部调用了process()方法。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1.3 Feature feature 集群：Zookeeper是一个领导者（Leader），多个跟随者（Follower）组成的集群。 高可用性：集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。 全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 更新请求顺序进行：来自同一个Client的更新请求按其发送顺序依次执行。 数据更新原子性：一次数据更新要么成功，要么失败。 实时性：在一定时间范围内，Client能读到最新数据。 从设计模式角度来看，zk是一个基于观察者设计模式的框架，它负责管理跟存储大家都关心的数据，然后接受观察者的注册，数据反生变化zk会通知在zk上注册的观察者做出反应。 Zookeeper是一个分布式协调系统，满足CP性，跟SpringCloud中的Eureka满足AP不一样。 分布式协调系统：Leader会同步数据到follower，用户请求可通过follower得到数据，这样不会出现单点故障，并且只要同步时间无限短，那这就是个好的 分布式协调系统。 CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"2. Function 通过对 Zookeeper 中丰富的数据节点进行交叉使用，配合 Watcher 事件通知机制，可以非常方便的构建一系列分布式应用中涉及的核心功能，比如 数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列 等功能。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1. 数据发布/订阅 当某些数据由几个机器共享，且这些信息经常变化数据量还小的时候，这些数据就适合存储到ZK中。 数据存储：将数据存储到 Zookeeper 上的一个数据节点。 数据获取：应用在启动初始化节点从 Zookeeper 数据节点读取数据，并在该节点上注册一个数据变更 Watcher 数据变更：当变更数据时会更新 Zookeeper 对应节点数据，Zookeeper会将数据变更通知发到各客户端，客户端接到通知后重新读取变更后的数据即可。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"2. 分布式锁 关于分布式锁其实在 Redis 中已经讲过了，并且Redis提供的分布式锁是比ZK性能强的。基于ZooKeeper的分布式锁一般有如下两种。 保持独占 核心思想：在zk中有一个唯一的临时节点，只有拿到节点的才可以操作数据，没拿到的线程就需要等待。缺点：可能引发羊群效应，第一个用完后瞬间有999个同时并发的线程向zk请求获得锁。 控制时序 主要是避免了羊群效应，临时节点已经预先存在，所有想要获得锁的线程在它下面创建临时顺序编号目录节点，编号最小的获得锁，用完删除，后面的依次排队获取。 distributedLock ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3. 负载均衡 多个相同的jar包在不同的服务器上开启相同的服务，可以通过nginx在服务端进行负载均衡的配置。也可以通过ZooKeeper在客户端进行负载均衡配置。 多个服务注册 客户端获取中间件地址集合 从集合中随机选一个服务执行任务 ZooKeeper负载均衡和Nginx负载均衡区别： ZooKeeper不存在单点问题，zab机制保证单点故障可重新选举一个leader只负责服务的注册与发现，不负责转发，减少一次数据交换（消费方与服务方直接通信），需要自己实现相应的负载均衡算法。 Nginx存在单点问题，单点负载高数据量大,需要通过 KeepAlived + LVS 备机实现高可用。每次负载，都充当一次中间人转发角色，增加网络负载量（消费方与服务方间接通信），自带负载均衡算法。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4. 命名服务 命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5. 分布式协调/通知 对于系统调度来说，用户更改zk某个节点的value， ZooKeeper会将这些变化发送给注册了这个节点的 watcher 的所有客户端，进行通知。 对于执行情况汇报来说，每个工作进程都在目录下创建一个携带工作进度的临时节点，那么汇总的进程可以监控目录子节点的变化获得工作进度的实时的全局情况。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:5","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"6. 集群管理 大数据体系下的大部分集群服务好像都通过ZooKeeper管理的，其实管理的时候主要关注的就是机器的动态上下线跟Leader选举。 动态上下线： 比如在zookeeper服务器端有一个znode叫 /Configuration，那么集群中每一个机器启动的时候都去这个节点下创建一个EPHEMERAL类型的节点，比如server1 创建 /Configuration/Server1，server2创建**/Configuration /Server1**，然后Server1和Server2都watch /Configuration 这个父节点，那么也就是这个父节点下数据或者子节点变化都会通知到该节点进行watch的客户端。 Leader选举： 利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /Master 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选举了。 就是动态Master选举。这就要用到 EPHEMERAL_SEQUENTIAL类型节点的特性了，这样每个节点会自动被编号。允许所有请求都能够创建成功，但是得有个创建顺序，每次选取序列号最小的那个机器作为Master 。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:6","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3. Choose Leader ZooKeeper集群节点个数一定是奇数个，一般3个或者5个就OK。为避免集群群龙无首，一定要选个大哥出来当Leader。这是个高频考点。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.1 预备知识 3.1.1. 节点四种状态。 LOOKING：寻 找 Leader 状态。当服务器处于该状态时会认为当前集群中没有 Leader，因此需要进入 Leader 选举状态。 FOLLOWING：跟随者状态。处理客户端的非事务请求，转发事务请求给 Leader 服务器，参与事务请求 Proposal(提议) 的投票，参与 Leader 选举投票。 LEADING：领导者状态。事务请求的唯一调度和处理者，保证集群事务处理的顺序性，集群内部个服务器的调度者(管理follower,数据同步)。 OBSERVING：观察者状态。3.0 版本以后引入的一个服务器角色，在不影响集群事务处理能力的基础上提升集群的非事务处理能力，处理客户端的非事务请求，转发事务请求给 Leader 服务器，不参与任何形式的投票。 3.1.2 服务器ID 既Server id，一般在搭建ZK集群时会在myid文件中给每个节点搞个唯一编号，编号越大在Leader选择算法中的权重越大，比如初始化启动时就是根据服务器ID进行比较。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.1.3 ZXID ZooKeeper 采用全局递增的事务 Id 来标识，所有 proposal(提议)在被提出的时候加上了ZooKeeper Transaction Id ，zxid是64位的Long类型，这是保证事务的顺序一致性的关键。zxid中高32位表示纪元epoch，低32位表示事务标识xid。你可以认为zxid越大说明存储数据越新。 每个leader都会具有不同的epoch值，表示一个纪元/朝代，用来标识 leader 周期。每个新的选举开启时都会生成一个新的epoch，新的leader产生的话epoch会自增，会将该值更新到所有的zkServer的zxid和epoch， xid是一个依次递增的事务编号。数值越大说明数据越新，所有 proposal（提议）在被提出的时候加上了zxid，然后会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.2 Leader选举 Leader的选举一般分为启动时选举跟Leader挂掉后的运行时选举。 3.2.1 启动时Leader选举 我们以5台机器为例，只有超过半数以上，即最少启动3台服务器，集群才能正常工作。 服务器1启动，发起一次选举。 服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING。 服务器2启动，再发起一次选举。 服务器1和2分别投自己一票，此时服务器1发现服务器2的id比自己大，更改选票投给服务器2。此时服务器1票数0票，服务器2票数2票，不够半数以上（3票），选举无法完成。服务器1，2状态保持LOOKING。 服务器3启动，发起一次选举。 与上面过程一样，服务器1和2先投自己一票，然后因为服务器3id最大，两者更改选票投给为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数（3票），服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING； 服务器4启动，发起一次选举。 此时服务器1、2、3已经不是LOOKING状态，不会更改选票信息，交换选票信息结果。服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，服务器4并更改状态为FOLLOWING。 服务器5启动，发起一次选举 同4一样投票给3，此时服务器3一共5票，服务器5为0票。服务器5并更改状态为FOLLOWING； 最终 Leader是服务器3，状态为LEADING。其余服务器是Follower，状态为FOLLOWING。 3.2.2 运行时Leader选举 运行时候如果Master节点崩溃了会走恢复模式，新Leader选出前会暂停对外服务，大致可以分为四个阶段 选举、发现、同步、广播。 chooseLeader 每个Server会发出一个投票，第一次都是投自己，其中投票信息 = (myid，ZXID) 收集来自各个服务器的投票 处理投票并重新投票，处理逻辑：优先比较ZXID，然后比较myid。 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定leader，注意epoch的增加跟同步。 改变服务器状态Looking变为Following或Leading。 当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的 ZXID 进行比对，比对结果要么回滚，要么和 Leader 同步，保证集群中各个节点的事务一致。 集群恢复到广播模式，开始接受客户端的写请求。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.3 脑裂 脑裂问题是集群部署必须考虑的一点，比如在Hadoop跟Spark集群中。而ZAB为解决脑裂问题，要求集群内的节点数量为2N+1。当网络分裂后，始终有一个集群的节点数量过半数，而另一个节点数量小于N+1, 因为选举Leader需要过半数的节点同意，所以我们可以得出如下结论： 有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4. ZAB of Consistence 建议先看下 浅谈大数据中的2PC、3PC、Paxos、Raft、ZAB ，不然可能看的吃力。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.1 ZAB 协议介绍 ZAB (Zookeeper Atomic Broadcast 原子广播协议) 协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的一致性协议。基于该协议，ZooKeeper 实现了一种主从模式的系统架构来保持集群中各个副本之间的数据一致性。 分布式系统中leader负责外部客户端的写请求。follower服务器负责读跟同步。这时需要解决俩问题。 Leader 服务器是如何把数据更新到所有的Follower的。 Leader 服务器突然间失效了，集群咋办？ 因此ZAB协议为了解决上面两个问题而设计了两种工作模式，整个 Zookeeper 就是在这两个模式之间切换： 原子广播模式：把数据更新到所有的follower。 崩溃恢复模式：Leader发生崩溃时，如何恢复。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.2 原子广播模式 你可以认为消息广播机制是简化版的 2PC协议，就是通过如下的机制保证事务的顺序一致性的。 leader从客户端收到一个写请求后生成一个新的事务并为这个事务生成一个唯一的ZXID， leader将将带有 zxid 的消息作为一个提案(proposal)分发给所有 FIFO队列。 FIFO队列取出队头proposal给follower节点。 当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK。 FIFO队列把ACK返回给Leader。 当leader收到超过一半以上的follower的ack消息，leader会进行commit请求，然后再给FIFO发送commit请求。 当follower收到commit请求时，会判断该事务的ZXID是不是比历史队列中的任何事务的ZXID都小，如果是则提交，如果不是则等待比它更小的事务的commit(保证顺序性) ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.3 崩溃恢复 消息广播过程中，Leader 崩溃了还能保证数据一致吗？当 Leader 崩溃会进入崩溃恢复模式。其实主要是对如下两种情况的处理。 Leader 在复制数据给所有 Follwer 之后崩溃，咋搞？ Leader 在收到 Ack 并提交了自己，同时发送了部分 commit 出去之后崩溃咋办？ 针对此问题，ZAB 定义了 2 个原则： ZAB 协议确保执行那些已经在 Leader 提交的事务最终会被所有服务器提交。 ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。 至于如何实现确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务呢？关键点就是依赖上面说到过的 ZXID了。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.4 ZAB 特性 一致性保证 可靠提交(Reliable delivery) ：如果一个事务 A 被一个server提交(committed)了，那么它最终一定会被所有的server提交 全局有序(Total order) 假设有A、B两个事务，有一台server先执行A再执行B，那么可以保证所有server上A始终都被在B之前执行 因果有序(Causal order) 如果发送者在事务A提交之后再发送B,那么B必将在A之后执行 高可用性 只要大多数（法定数量）节点启动，系统就行正常运行 可恢复性 当节点下线后重启，它必须保证能恢复到当前正在执行的事务 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.5 ZAB 和 Paxos 对比 相同点： 两者都存在一个类似于 Leader 进程的角色，由其负责协调多个 Follower 进程的运行. Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交. ZAB 协议中，每个 Proposal 中都包含一个 epoch 值来代表当前的 Leader周期，Paxos 中名字为 Ballot 不同点： ZAB 用来构建高可用的分布式数据主备系统（Zookeeper），Paxos 是用来构建分布式一致性状态机系统。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:5","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5. ZooKeeper 零散知识 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.1 常见指令 Zookeeper 有三种部署模式： 单机部署：一台机器上运行。 集群部署：多台机器运行。 伪集群部署：一台机器启动多个 Zookeeper 实例运行。 部署完毕后常见指令如下： 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 显示所有操作命令 ls path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建， -s 含有序列， -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.2 Zookeeper客户端 5.2.1. Zookeeper原生客户端 Zookeeper客户端是异步的哦！需要引入CountDownLatch 来确保连接好了再做下面操作。Zookeeper原生api是不支持迭代式的创建跟删除路径的，具有如下弊端。 会话的连接是异步的；必须用到回调函数 。 Watch需要重复注册：看一次watch注册一次 。 Session重连机制：有时session断开还需要重连接。 开发复杂性较高：开发相对来说比较琐碎。 5.2.2. ZkClient 开源的zk客户端，在原生API基础上封装，是一个更易于使用的zookeeper客户端，做了如下优化。 优化一 、在session loss和session expire时自动创建新的ZooKeeper实例进行重连。优化二、 将一次性watcher包装为持久watcher。 5.2.3. Curator 开源的zk客户端，在原生API基础上封装，apache顶级项目。是Netflix公司开源的一套Zookeeper客户端框架。了解过Zookeeper原生API都会清楚其复杂度。Curator帮助我们在其基础上进行封装、实现一些开发细节，包括接连重连、反复注册Watcher和NodeExistsException等。目前已经作为Apache的顶级项目出现，是最流行的Zookeeper客户端之一。 5.2.4. Zookeeper图形化客户端工具 ZooInspector工具 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.3 ACL 权限控制机制 ACL全称为Access Control List 即访问控制列表，用于控制资源的访问权限。zookeeper利用ACL策略控制节点的访问权限，如节点数据读写、节点创建、节点删除、读取子节点列表、设置节点权限等。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.4 Zookeeper使用注意事项 集群中机器的数量并不是越多越好，一个写操作需要半数以上的节点ack，所以集群节点数越多，整个集群可以抗挂点的节点数越多(越可靠)，但是吞吐量越差。集群的数量必须为奇数。 zk是基于内存进行读写操作的，有时候会进行消息广播，因此不建议在节点存取容量比较大的数据。 dataDir目录、dataLogDir两个目录会随着时间推移变得庞大，容易造成硬盘满了。建议自己编写或使用自带的脚本保留最新的n个文件。 默认最大连接数 默认为60，配置maxClientCnxns参数，配置单个客户端机器创建的最大连接数。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"Article description.","date":"2020-04-27","objectID":"/20200427_kafka/","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Apache Kafka aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a “message set” abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This “leads to larger network packets, larger sequential disk operations, contiguous memory blocks […] which allows Kafka to turn a bursty stream of random message writes into linear writes.” ","date":"2020-04-27","objectID":"/20200427_kafka/:0:0","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Message queue ","date":"2020-04-27","objectID":"/20200427_kafka/:1:0","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"1. Why 为什么使用消息队列？ 从系统之间有通信需求开始，就自然产生了消息队列。 在计算机科学中，消息队列（英语：Message queue）是一种进程间通信或同一进程的不同线程间的通信方式，软件的贮列用来处理一系列的输入，通常是来自用户。消息队列提供了异步的通信协议，每一个贮列中的纪录包含详细说明的资料，包含发生的时间，输入设备的种类，以及特定的输入参数，也就是说：消息的发送者和接收者不需要同时与消息队列交互。消息会保存在队列中，直到接收者取回它。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:1","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"2. Feature 解耦： 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余： 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性： 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。 灵活性 \u0026 峰值处理能力： 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性： 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证： 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性） 缓冲： 有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 异步通信： 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:2","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"3. Usage 服务解耦： 下游系统可能只需要当前系统的一个子集，应对不断增加变化的下游系统，当前系统不停地修改调试与这些下游系统的接口，系统间耦合过于紧密。引入消息队列后，当前系统变化时发送一条消息到消息队列的一个主题中，所有下游系统都订阅主题，这样每个下游系统都可以获得一份实时完整的订单数据。 异步处理： 以秒杀为例：风险控制-\u003e库存锁定-\u003e生成订单-\u003e短信通知-\u003e更新统计数据 限流削峰/流量控制 一个设计健壮的程序有自我保护的能力，也就是说，它应该可以在海量的请求下，还能在自身能力范围内尽可能多地处理请求，拒绝处理不了的请求并且保证自身运行正常。使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:3","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"4. Realize 点对点： 系统 A 发送的消息只能被系统 B 接收，其他任何系统都不能读取 A 发送的消息。 日常生活的例子比如电话客服就属于这种模型： 同一个客户呼入电话只能被一位客服人员处理，第二个客服人员不能为该客户服务。 发布/订阅模型 这个模型可能存在多个发布者向相同的主题发送消息，而订阅者也可能存在多个，它们都能接收到相同主题的消息。 生活中的报纸订阅就是一种典型的发布 / 订阅模型。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:4","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Kafka ","date":"2020-04-27","objectID":"/20200427_kafka/:2:0","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"1. Intro kafka是一个分布式流处理平台。 类似一个消息系统，读写流式的数据 编写可扩展的流处理应用程序，用于实时事件响应的场景 安全的将流式的数据存储在一个分布式，有副本备份，容错的集群 ","date":"2020-04-27","objectID":"/20200427_kafka/:2:1","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"2. History Kafka从何而来?我们为什么要开发Kafka? Kafka到底是什么? Kafka 最初是 LinkedIn 的一个内部基础设施系统。我们发现虽然有很多数据库和系统可以用来存储数据，但在我们的架构里，刚好缺一个可以帮助处理持续数据流的组件。在开发Kafka之前，我们实验了各种现成的解决方案，从消息系统到日志聚合系统，再到ETL工具，它们都无法满足我们的需求。 最后，我们决定从头开发一个系统。我们不想只是开发一个能够存储数据的系统，比如传统的关系型数据库、键值存储引擎、搜索引擎或缓存系统，我们希望能够把数据看成是持续变化和不断增长的流，并基于这样的想法构建出一个数据系统。事实上，是一个数据架构。 这个想法实现后比我们最初预想的适用性更广。Kafka 一开始被用在社交网络的实时应用和数据流当中，而现在已经成为下一代数据架构的基础。大型零售商正在基于持续数据流改造他们的基础业务流程，汽车公司正在从互联网汽车那里收集和处理实时数据流，银行也在重新思考基于 Kafka 改造他们的基础。 它可以用于两大类别的应用: 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。(相当于message queue) 构建实时流式应用程序，对这些流数据进行转换或者影响。(就是流处理，通过kafka stream topic和topic之间内部进行变化) 版本号 备注 0.7 上古版本，提供了最基础的消息队列功能 0.8 引入了副本机制，成为了一个真正意义上完备的分布式高可靠消息队列解决方案 0.8.2 新版本 Producer API，即需要指定 Broker 地址的 Producer 0.9 增加了基础的安全认证 / 权限，Java 重写了新版本消费者 API 0.10.0.0 引入了 Kafka Streams 0.11.0.0 提供幂等性 Producer API 以及事务（Transaction） API，对 Kafka 消息格式做了重构。 1.0 Kafka Streams 的各种改进 2.0 Kafka Streams 的各种改进 ","date":"2020-04-27","objectID":"/20200427_kafka/:2:2","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"3. Item 消息：Record。这里的消息就是指 Kafka 处理的主要对象。 服务：Broker。一个 Kafka 集群由多个 Broker 组成，Broker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化。 主题：Topic。主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。 分区：Partition。一个有序不变的消息序列。每个主题下可以有多个分区。 消息位移：Offset。表示分区中每条消息的位置信息，是一个单调递增且不变的值。 副本：Replica。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为领导者副本和追随者副本，各自有不同的角色划分。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。 生产者：Producer。向主题发布新消息的应用程序。 消费者：Consumer。从主题订阅新消息的应用程序。 消费者位移：Consumer Offset。表征消费者消费进度，每个消费者都有自己的消费者位移。 消费者组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。 重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。 kafkaItem ","date":"2020-04-27","objectID":"/20200427_kafka/:2:3","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"4. Topic 日志 日志可能是一种最简单的不能再简单的存储抽象，只能追加、按照时间完全有序（totally-ordered）的记录序列。日志看起来的样子： kafkaLog 在日志的末尾添加记录，读取日志记录则从左到右。每一条记录都指定了一个唯一的顺序的日志记录编号。 日志记录的次序（ordering）定义了『时间』概念，因为位于左边的日志记录表示比右边的要早。日志记录编号可以看作是这条日志记录的『时间戳』。把次序直接看成是时间概念，刚开始你会觉得有点怪异，但是这样的做法有个便利的性质：解耦了 时间 和 任一特定的物理时钟（physical clock）。引入分布式系统后，这会成为一个必不可少的性质。 日志 和 文件或数据表（table）并没有什么大的不同。文件是一系列字节，表是由一系列记录组成，而日志实际上只是一种按照时间顺序存储记录的数据表或文件。 对于每一个topic， Kafka集群都会维持一个分区日志，如下所示： kafkaPartitionLog 实操 启动zk cd /usr/local/kara/kafka_2.13-2.6.0/bin zookeeper-server-start.sh ../config/zookeeper.properties 启动kafka服务器 kafka-server-start.sh ../config/server.properties 创建topic，4个分区，一个副本 kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 4 --topic partition_test 发送一些消息 kafka-console-producer.sh --broker-list localhost:9092 --topic partition_test 启动一个consumer kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic partition_test --from-beginning 分区 partition存储分布 一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1 partition文件存储 1.每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。 2.每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。 segment文件存储 segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，分别表示为segment索引文件、数据文件. segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。 segment中的消息message物理结构字段说明 关键字 解释说明 8 byte offset 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message 4 byte message size message大小 4 byte CRC32 用crc32校验message 1 byte “magic” 表示本次发布Kafka服务程序协议版本号 1 byte “attributes” 表示为独立版本、或标识压缩类型、或编码类型。 4 byte key length 表示key的长度,当key为-1时，K byte key字段不填 K byte key 可选 value bytes payload 表示实际消息数据。 文件系统 Kafka 对消息的存储和缓存严重依赖于文件系统。人们对于“磁盘速度慢”具有普遍印象，事实上，磁盘的速度比人们预期的要慢的多，也快得多，这取决于人们使用磁盘的方式。 使用6个7200rpm、SATA接口、RAID-5的磁盘阵列在JBOD配置下的顺序写入的性能约为600MB/秒，但随机写入的性能仅约为100k/秒，相差6000倍以上。 线性的读取和写入是磁盘使用模式中最有规律的，并且由操作系统进行了大量的优化。 read-ahead 是以大的 data block 为单位预先读取数据 write-behind 是将多个小型的逻辑写合并成一次大型的物理磁盘写入 关于该问题的进一步讨论可以参考 ACM Queue article，他们发现实际上顺序磁盘访问在某些情况下比随机内存访问还要快！ 为了弥补这种性能差异，现代操作系统主动将所有空闲内存用作 disk caching（磁盘高速缓存），所有对磁盘的读写操作都会通过这个统一的 cache（ in-process cache）。 即使进程维护了 in-process cache，该数据也可能会被复制到操作系统的 pagecache 中，事实上所有内容都被存储了两份。 此外，Kafka 建立在 JVM 之上，任何了解 Java 内存使用的人都知道两点： 对象的内存开销非常高，通常是所存储的数据的两倍(甚至更多)。 随着堆中数据的增加，Java 的垃圾回收变得越来越复杂和缓慢。 kafka选择了一个非常简单的设计：相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将数据 flush 到文件系统，我们把这个过程倒过来。所有数据一开始就被写入到文件系统的持久化日志中，而不用在 cache 空间不足的时候 flush 到磁盘。实际上，这表明数据被转移到了内核的 pagecache 中。 Pagecache页面缓存 Page cache（页面缓存） Page cache 也叫页缓冲或文件缓冲，是由好几个磁盘块构成，大小通常为4k，在64位系统上为8k，构成的几个磁盘块在物理磁盘上不一定连续，文件的组织单位为一页， 也就是一个page cache大小，文件读取是由外存上不连续的几个磁盘块，到buffer cache，然后组成page cache，然后供给应用程序。 Buffer cache（块缓存） Buffer cache 也叫块缓冲，是对物理磁盘上的一个磁盘块进行的缓冲，其大小为通常为1k，磁盘块也是磁盘的组织单位。设立buffer cache的目的是为在程序多次访问同一磁盘块时，减少访问时间。 Page cache（页面缓存）与Buffer cache（块缓存）的区别 磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。 我们通过文件系统操作文件，那么文件将被缓存到Page Cache，如果需要刷新文件的时候，Page Cache将交给Buffer Cache去完成，因为Buffer Cache就是缓存磁盘块的。 简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。 Buffer(Buffer Cache)以块形式缓冲了块设备的操作，定时或手动的同步到硬盘，它是为了缓冲写操作然后一次性将很多改动写入硬盘，避免频繁写硬盘，提高写入效率。 Cache(Page Cache)以页面形式缓存了文件系统的文件，给需要使用的程序读取，它是为了给读操作提供缓冲，避免频繁读硬盘，提高读取效率。 降低时间复杂度 消息系统使用的持久化数据结构通常是和 BTree 相关联的消费者队列或者其他用于存储消息源数据的通用随机访问数据结构。BTree 的操作复杂度是 O(log N)，通常我们认为 O(log N) 基本等同于常数时间，但这条在磁盘操作中不成立。 存储系统将非常快的cache操作和非常慢的物理磁盘操作混合在一起，当数据随着 fixed cache 增加时，可以看到树的性能通常是非线性的——比如数据翻倍时性能下降不只两倍。 kafka选择把持久化队列建立在简单的读取和向文件后追加两种操作之上，这和日志解决方案相同。这种架构的优点在于所有的操作复杂度都是O(1)，而且读操作不会阻塞写操作，读操作之间也不会互相影响。 在不产生任何性能损失的情况下能够访问几乎无限的硬盘空间，Kafka 可以让消息保留相对较长的一段时间(比如一周)，而不是试图在被消费后立即删除。 降低大量小型IO操作的影响 小型的 I/O 操作发生在客户端和服务端之间以及服务端自身的持久化操作中。 为了避免这种情况，kafka的协议是建立在一个 “消息块” 的抽象基础上，合理将消息分","date":"2020-04-27","objectID":"/20200427_kafka/:2:4","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Article description.","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"The major difference between the two is Replication/Fault Tolerance. HDFS was designed to survive failures. NFS does not have any fault tolerance built in. Other than fault tolerance, HDFS does support multiple replicas of files. This eliminates (or eases) the common bottleneck of many clients accessing a single file. Since files have multiple replicas, on different physical disks, reading performance scales better than NFS. ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:0:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"NFS NFS (Network File system): A protocol developed that allows clients to access files over the network. NFS clients allow files to be accessed as if the files reside on the local machine, even though they reside on the disk of a networked machine. In NFS, the data is stored only on one main system. All the other systems in that network can access the data stored in that as if it was stored in their local system. But the problem with this is that, if the main system goes down, then the data is lost and also, the storage depends on the space available on that system. ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:1:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"HDFS HDFS (Hadoop Distributed File System): A file system that is distributed amongst many networked computers or nodes. HDFS is fault tolerant because it stores multiple replicas of files on the file system, the default replication level is 3. In HDFS, data is distributed among different systems called datanodes. Here, the storage capacity is comparatively high. HDFS is mainly used to store Big Data and enable fast data transaction. ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:2:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"Similarities 两者的文件系统数据均能够在相关系统内的多台机器上进行数据读取和写入，都是分布式文件系统 ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:3:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"Differences NFS是通过RPC通信协议进行数据共享的文件系统，所以NFS必须在运行的同时确保RPC能够正常工作。在不同的文件进行读取和写入时，实际上是对服务端的共享文件地址进行操作，一旦服务端出现问题，那么其他所有的机器无法进行文件读取和写入，并且数据无法找回。所以NFS系统的文件其实并没有备份，并且其服务端没有做高可用处理。 HDFS是通过数据备份进行的大数据存储文件系统。HDFS有系统备份，并且其namenode有secondnamenode进行备份处理，更加安全可靠。数据在经过多副本存储后，能够抵御各种灾难，只要有一个副本不丢失，数据就不会丢失。所以数据的安全性很高。 ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:4:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"Article description.","date":"2020-02-08","objectID":"/20200208_scalaintroduction/","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"Scala combines object-oriented and functional programming in one concise, high-level language. Scala’s static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries. ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:0:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"1. REPL \u0026 Scaladoc Scala解释器读到一个表达式，对它进行求值，将它打印出来，接着再继续读下一个表达式。这个过程被称做Read-Eval-Print-Loop，即：REPL。 从技术上讲，scala程序并不是一个解释器。实际发生的是，你输入的内容被快速地编译成字节码，然后这段字节码交由Java虚拟机执行。正因为如此，大多数scala程序员更倾向于将它称做“REPL” scala api文档，包含了scala所有的api以及使用说明，class、object、trait、function、method、implicit等 为什么要查阅Scaladoc：如果只是写一些普通的Scala程序基本够用了；但是如果（在现在，或者未来，实际的工作环境中）要编写复杂的scala程序，那么还是需要参考Scaladoc的。（纯粹用scala开发spark应用程序，应该不会特别复杂；用scala构建类似于spark的公司内的分布式的大型系统） 以下是一些Scaladoc使用的tips： 直接在左上角的搜索框中，搜索你需要的寻找的包、类即可 C和O，分别代表了类和伴生对象的概念 t和O，代表了特制(trait)(类似于Java的接口) 标记为implicit的方法，代表的是隐式转换 举例：搜索StringOps，可以看到String的增强类，StringOps的所有方法说明 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:1:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"2. Data Type ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:2:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"2.1 Data Type 数据类型 描述 Byte 8位有符号补码整数。数值区间为 -128 到 127 Short 16位有符号补码整数。数值区间为 -32768 到 32767 Int 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 Float 32 位, IEEE 754 标准的单精度浮点数 Double 64 位 IEEE 754 标准的双精度浮点数 Char 16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF String 字符序列 Boolean true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null 或空引用 Nothing Nothing类型在Scala的类层级的最底端；它是任何其他类型的子类型。 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:2:1","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"2.2 val、var \u0026 Lazy Value 内容是否可变：val修饰的是不可变的，var修饰是可变的 val修饰的变量在编译后类似于java中的中的变量被final修饰 lazy修饰符可以修饰变量，但是这个变量必须是val修饰的 ps. lazy相当于延迟加载（懒加载），当前变量使用lazy修饰的时候，只要变量不被调用，就不会进行初始化，什么时候调用，什么时候进行初始化 lazy val words = scala.io.Source.fromFile(\"/usr/share/dict/words\").mkString //当val被声明为lazy时，它的初始化将被推迟，直到我们首次对他取值 懒值对于开销大的初始化语句十分有用。它还可以用来应对其他初始化问题，比如循环依赖。更重要的是，它是开发懒数据结构的基础。 val words = ... //在words被定义时即被取值 lazy val words = ... //在words被首次使用时取值 def words = ... //在每一次words被使用时取值 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:2:2","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"3. Control and Function if表达式也有值 块也有值：是它最后一个表达式的值 Scala的for循环就像是“增强版”的Java for循环 分好在绝大数情况下不是必须的 void类型是Unit 避免在函数使用return 注意别再函数式定义中使用return 异常的工作方式和Java或C++基本一样，不同的是你在catch语句中使用“模式匹配” Scala没有受检异常 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:3:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"4. Array 若长度固定可用Array，若长度可能有变化则使用ArrayBuffer 提供初始值时不要使用new 用()来访问元素 用for(elem \u003c- arr)来遍历元素 用for(elem \u003c- arr if …) yield … 来将原数据转型为新数组 Scala数组和Java数组可以相互操作，用ArrayBuffer，使用scala.collection.JavaConversions中的转换函数 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:4:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"5. Map and Tuple Scala有十分易用的语法来创建、查询和遍历映射(Map) 你需要从可变的和不可变的映射中做出选择 默认情况下，你得到的是一个哈希映射(Hash Map)，不过你也可以指明要树形映射 你可以很容易地在Scala映射和Java映射之间来回切换 元组可以用来聚集值 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:5:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"6. Class 类中的字段自动带有getter方法和setter方法 可以用定制的getter/setter方法替换掉字段的定义，而不必修改使用类的客户端——这就是所谓的“统一访问原则” 用@BeanProperty注解来生成JavaBeans的get*/set*方法 每个类都有一个主要的构造器，这个构造器和类定义\"交织\"在一起。它的参数直接为类的字段。主构造器执行类体中所有的语句 辅助构造器是可选的。他们叫做this ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:6:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"7. Object 对象作为单例或存放工具方法 类可以拥有一个同名的伴生对象 对象可以扩展类或特质 对象的apply方法通常用来构造伴生类的新实例 如果不想显示定义main方法，可以用扩展App特质的对象 可以通过扩展Enumeration对象来实现枚举 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:7:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"8. Package 包也可以像内部类那样嵌套 包路径不是绝对路径 包声明链x.y.z并不自动将中间包x和x.y变成可见 位于文件顶部不带花括号的包声明在整个文件范围内有效 包对象可以持有函数和变量 引入语句可以引入包、类和对象 引入语句可以出现在任何位置 引入语句可以重命名和隐藏特定成员 java.lang、scala和predef总是被引入 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:8:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"9. Extends extends、final关键字和Java中相同 重写方法时必须用override 只有主构造器可以用超类的主构造器 可以重写字段 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:9:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"10. File\u0026Regex Source.fromFile(…).getLines.toArray将交出文件的所有行 Source.fromFile(…).mkString将以字符串形式交出文件内容 将字符串转化为数字，可以用toInt或toDouble方法 使用Java的PrintWriter来写入文本文件 “正则”.r是一个Regex对象 如果你的正则表达式包含反斜杠的话，用\"\"\"…\"\"\" 如果正则模式包含分组，你可以用如下语法来提取它们的内容for(regex(变量1,…,变量n) \u003c- 字符串) ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:10:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"11. Feature 类可以实现任意数量的特质 特质可以要求实现它们的类具备特定的字段、方法或超类 和Java接口不同，Scala特质可以提供方法和字段的实现 当将多个特质叠加在一起时，顺序很重要——其方法先被执行啊的特质排在更后面 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:11:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"12. Advanced function 函数可以直接赋值给变量，就和数字一样 可以创建匿名函数，通常还会把它们交给其他函数 函数参数可以给出需要稍后执行的行为 许多集合方法都接受函数参数，将函数应用到集合中的值 有很多语法上的简写让你以简短且易读的方式表达函数参数 可以创建操作代码块的函数，它们看上去就像是内建的控制语句 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:12:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"13. Collection 所有集合都扩展自Iterable特质 集合有三大类，分别为序列、集合映射 对于几乎所有集合类，Scala都同时提供了可变的和不可变的版本 Scala列表要么是空的，要么拥有一头一尾，其中尾部本身又是一个列表 集是无先后次序的集合 用LinkedHashSet保留插入顺序，或者用SortedSet按顺序进行迭代 +将元素添加到无先后次序的集合中；+:和:+向前或向后追加到序列；++将两个集合串接在一起；-和–移除元素 映射、折叠和拉链操作是很有用的技巧，用来将函数或操作应用到集合中的元素 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:13:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"14. Pattern match match表达式是一个更好的switch，不会有意外掉入下一个分支的问题 如果没有模式能够匹配，会抛出MatchError。可以用case _模式来避免 模式可以包含一个随意定义的条件，称作守卫（guard） 可以对表达式的类型进行匹配；优先选择模式匹配而不是isInstanceOf/asInstanceOf 可以匹配数组、元组和样例类的模式，然后将匹配到的不同部分绑定到变量 在for表达式中，不能匹配的情况会被安静地跳过 样例类是是编译器会为之自动产出模式匹配所需要的方法的类 样例类继承层级中的公共超类应该是sealed的 用Option来存放对于可能存在也可能不存在的值——比null更安全 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:14:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"15. Annotation 可以为类、方法、字段、局部变量、参数、表达式、类型参数以及各种类型定义添加注解 对于表达式和类型，注解跟在被注解的条目之后 注解的形式有 @Annotation、@Annotation(value)或@Annotation(name = value1, …) @volatile、@transient、@strictfp和@native分别生成等效的Java修饰符 用@throws来生成与Java兼容的throws规格说明 @tailrec注解让你校验某个递归函数使用了尾递归优化 assert函数利用了@elidable注解。你可以选择从Scala程序中移除所有断言 用@deprecated注解来标记已过时的特性 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:15:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"16. XML XML字面量this的类型为NodeSeq 可以在XML里字面量中嵌套Scala代码 Node的child属性交出的是子节点 Node的attributes属性交出的是包含节点属性的MetaData对象 \\和\\操作符执行类XPath匹配 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:16:0","tags":["Scala","Distribution"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"Article description.","date":"2020-01-22","objectID":"/20200122_java-static/","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"The static keyword can be used for variables, methods, code blocks, and inner classes to indicate that a particular member belongs only to a class itself, and not to an object of that class. ","date":"2020-01-22","objectID":"/20200122_java-static/:0:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"1. Static Variable 静态变量也叫类变量，它属于一个类，而不是这个类的对象。 public class Writer { private String name; private int age; public static int countOfWriters; public Writer(String name, int age) { this.name = name; this.age = age; countOfWriters++; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 其中，countOfWriters 被称为静态变量，它有别于 name 和 age 这两个成员变量，因为它前面多了一个修饰符 static。 这意味着无论这个类被初始化多少次，静态变量的值都会在所有类的对象中共享。 Writer w1 = new Writer(\"沉默王二\",18); Writer w2 = new Writer(\"沉默王三\",16); System.out.println(Writer.countOfWriters); 按照上面的逻辑，你应该能推理得出，countOfWriters 的值此时应该为 2 而不是 1。从内存的角度来看，静态变量将会存储在 Java 虚拟机中一个名叫“Metaspace”（元空间，Java 8 之后）的特定池中。 静态变量和成员变量有着很大的不同，成员变量的值属于某个对象，不同的对象之间，值是不共享的；但静态变量不是的，它可以用来统计对象的数量，因为它是共享的。就像上面例子中的 countOfWriters，创建一个对象的时候，它的值为 1，创建两个对象的时候，它的值就为 2。 Summary: 由于静态变量属于一个类，所以不能通过对象引用来访问，而应该直接通过类名来访问； w1.countOfWriters #不应该通过类实例访问静态成员 不需要初始化类就可以访问静态变量。 public static void main(String[] args) { System.out.println(Writer.countOfWriters); // 输出 0 } ","date":"2020-01-22","objectID":"/20200122_java-static/:1:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"2. Static Method 静态方法也叫类方法，它和静态变量类似，属于一个类，而不是这个类的对象。 public static void setCountOfWriters(int countOfWriters) { Writer.countOfWriters = countOfWriters; } setCountOfWriters() 就是一个静态方法，它由 static 关键字修饰。 如果你用过 java.lang.Math 类或者 Apache 的一些工具类（比如说 StringUtils）的话，对静态方法一定不会感动陌生。 Math 类的几乎所有方法都是静态的，可以直接通过类名来调用，不需要创建类的对象。 Math. random() abs(int a) sin(double a) cos(double a) ... Summary: Java 中的静态方法在编译时解析，因为静态方法不能被重写（方法重写发生在运行时阶段，为了多态）。 抽象方法不能是静态的。 static abstract void paly(); #修饰符abstract 和 static的组合非法 静态方法不能使用 this 和 super 关键字。 成员方法可以直接访问其他成员方法和成员变量。 成员方法也可以直接方法静态方法和静态变量。 静态方法可以访问所有其他静态方法和静态变量。 静态方法无法直接访问成员方法和成员变量。 ","date":"2020-01-22","objectID":"/20200122_java-static/:2:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"3. Static Code Block 静态代码块可以用来初始化静态变量，尽管静态方法也可以在声明的时候直接初始化，但有些时候，我们需要多行代码来完成初始化。 public class StaticBlockDemo { public static List\u003cString\u003e writes = new ArrayList\u003c\u003e(); static { writes.add(\"a\"); writes.add(\"b\"); writes.add(\"c\"); System.out.println(\"第一块\"); } static { writes.add(\"d\"); writes.add(\"e\"); System.out.println(\"第二块\"); } } writes 是一个静态的 ArrayList，所以不太可能在声明的时候完成初始化，因此需要在静态代码块中完成初始化。 Summary: 一个类可以有多个静态代码块。 静态代码块的解析和执行顺序和它在类中的位置保持一致。为了验证这个结论，可以在 StaticBlockDemo 类中加入空的 main 方法，执行完的结果如下所示： 第一块 第二块 ","date":"2020-01-22","objectID":"/20200122_java-static/:3:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"4. Static Inner Class Java 允许我们在一个类中声明一个内部类，它提供了一种令人信服的方式，允许我们只在一个地方使用一些变量，使代码更具有条理性和可读性。 常见的内部类有四种，成员内部类、局部内部类、匿名内部类和静态内部类。 静态内部类： public class Singleton { private Singleton() {} private static class SingletonHolder { public static final Singleton instance = new Singleton(); } public static Singleton getInstance() { return SingletonHolder.instance; } } 以上这段代码是不是特别熟悉，对，这就是创建单例的一种方式，第一次加载 Singleton 类时并不会初始化 instance，只有第一次调用 getInstance() 方法时 Java 虚拟机才开始加载 SingletonHolder 并初始化 instance，这样不仅能确保线程安全也能保证 Singleton 类的唯一性。不过，创建单例更优雅的一种方式是使用枚举。 Summary: 静态内部类可以访问外部类的所有成员变量，包括私有变量。 外部类不能声明为 static。 ","date":"2020-01-22","objectID":"/20200122_java-static/:4:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"Article description.","date":"2020-01-01","objectID":"/20200101_java-interview/","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"JVM, Garbage Collection, Multi - thread, Redis ","date":"2020-01-01","objectID":"/20200101_java-interview/:0:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"JVM Java从编码到执行 javac 将 x.java(任何语言) 文件编码成 x.class 文件 JVM 中的 ClassLoader 将 x.class 文件装载到内存里，通常也会把 java类库也装载到内存里 JVM 调用 字节码解释器 或 JIT即时编译器(常用的代码使用即时编译，第二次编译的时候直接调用) 来进行解释或编译 *jvm是解释执行也是编译执行的，也可以混合执行 JVM 执行引擎开始执行 JVM jvm 是一种规范 它是虚构出来的一台计算机 ClassFileFormat 二进制字节流 CLASS文件结构 前4个字节：magic 两个字节：minor version 两个字节：major version 两个字节：constant pool count 常量池个数 ，2个字节，最多65535个 常量池结构： constant_utf8_info: tag:1,占用空间一个字节。length: utf-8字符串占用的字节数。bytes: 长度位length的字符串 constant_integer_info: tag:3,占用空间一个字节。bytes: 4个字节，big-endian（高位在前）存储的int值 constant_methodref_info: tag:10 index:2字节，指向声明方法的类或者接口描述符constant_class_info的索引项 index: 2字节，指向字段描述符constant_nameAndType的索引项 class/this_class/super_class interfaces fields methods Class Loading Linking Initializing class文件load到内存的过程 loading: class文件放到内存 双亲委派，安全 lazyLoading 五中情况 classLoader：findInCache -\u003e parent.loadClass -\u003e findClass() 自定义类加载器 extends ClassLoader overwrite findClass() -\u003e defineClass(byte[] -\u003e Class clazz) 混合执行 linking verification：文件符合jvm规定：0xCAFEBABE… preparation：静态成员变量赋默认值，int = 0… resolution：类、方法、属性符号应用解析为直接引用，常量池中的各个符号引用解析为指针、偏移量等内存地址的直接引用 initializing：静态变量这时才赋为初始值 load 静态成员变量 -\u003e 默认值 -\u003e 初始值 new Object -\u003e 申请内存 -\u003e 默认值 -\u003e 初始值：单例模式中，volatile 确保指令执行顺序，先初始化后再把内存给 t ，否则有可能先指向内存再初始化 类加载器 JVM 是按需动态加载，采用双亲委派机制 （自顶向下，进行实际查找和加载child方向） Bootstrap -----\u003e 加载lib/rt.jar charset.jar 等核心类，C++实现 | Extension -----\u003e 加载扩展jar包(jre/lib/ext/*.jar)，或由 Djava.ext.dirs指定 | Application -----\u003e 加载classpath指定内容 | Custom ClassLoader -----\u003e 自定义 classLoader （自底向上检查该类是否已经加载parent方向） 双亲委派 父加载器 -父加载器不是\"类加载器的加载器\"，！！也不是\"类加载器的父类加载器\" 双亲委派是一个孩子向父亲方向，然后父亲向孩子方向的双亲委派过程 classLoadProcess 为什么要做双亲委派 安全 双亲委派机制能够保证类加载器加载某个类时，最终都是由一个加载器加载，确保最终加载结果相同 比如 java.long.object, 从 custom classLoader 从下向上开始，到Bootstrap 发现已经加载过了，就不再加载了 补充 class load到内存中有两块东西，一是class二进制文件，二是class对象。其他自己写的对象访问内存中的class对象，通过class对象访问内存中的二进制文件 method Area ： class对象是存储在 method Area 中，method Area 在内存是存储在 metaspace中，也就是 permanent generation。1.8之前叫 permanent generation， 1.8之后叫 metaspace 什么时候需要调用loadclass函数？ spring 里有动态代理，spring 就调用 loadclass 把 class 加载到缓存里 tomcat，load自定义的class 热部署，热启动 类编译 解释器 -bytecode intepreter JIT -Just In Time compiler 混合模式： -混合使用解释器 + 热点代码编译 -起始阶段采用解释执行 -热点代码检测 多次被调用的方法（方法计数器：监测方法执行频率） 多次被调用的方法（循环计数器：检测循环执行频率） 进行编译 Xmixed: 默认为混合模式，开始解释执行，启动速度较快，对热点代码实行检测和编译 Xint: 解释模式，启动很快执行稍慢 Xcomp: 纯编译模式，执行很快，启动很慢 懒加载 new/ get static / put static / invoke static 指令，访问 final 变量除外 java.lang.reflect 对类进行反射调用时 初始化子类的时候，父类首先初始化 jvm启动时，被执行的主类必须初始化 动态语言支持REF_putstatic/REF_getstatic/REF_invokestatic的方法句柄时，该类必须初始化 Initializing //例：求 T.count main(){ System.out.println(T.count); } class T { public static T t = new T(); public static int count = 2; private T(){ count ++; } } //T.class load，然后linking， //到preparation，静态成员变量赋值默认值。t为null，count为0 //然后到initializing，t先执行构造方法，count赋值为2, count为2 class T { public static int count = 2; public static T t = new T(); private T(){ count ++; } } //initializing，count赋值为2，t再执行构造方法 count = 2++ = 3 ","date":"2020-01-01","objectID":"/20200101_java-interview/:1:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"GC 熟悉GC常用算法，熟悉常见垃圾收集器，具有实际JVM调优实战经验 gc1 程序的栈和堆 栈： 每个线程一个栈，栈中照先进先出，存放方法 堆： 动态内存块，比如 new 对象 什么是垃圾 没有引用指向他了就是垃圾 回收垃圾的方法 引用计数法（reference count） 当引用指向为0，回收 缺点：当三个内存垃圾互相指向，无法回收 Python 根可达算法（root searching） GC roots: 线程栈变量，静态变量，常量池，JNI指针 GC 的演化 随着内存大小的不断增长而演进 堆内存逻辑分区 分代模型： 刚刚诞生的对象优先放在新生代内存区， 随着GC器的扫描新生代，新生代内存若多次没被回收(在Surviver两个区反复横跳多次)会变成老年代(gc正常不管这片区域) GC 算法 Mark - Sweep (标记清除) 标记分为：存货对象，未使用内存区，可回收内存区 缺点：碎片化严重，分大块内存时不便 Copying 基于标记，整齐拷贝到新区域，原内存整体性回收 缺点：浪费内存 Mark - Compact (标记压缩) 基于copying，回收时直接整理内存 缺点：效率最低 GC 器 Serial GC: 优点：单线程精简的GC实现，无需维护复杂的数据结构，初始化简单，是client模式下JVM默认选项。最古老的GC。 缺点：会进入\"Stop-The World\"状态。 ParNew GC： 新生代GC实现，是SerialGC的多线程版本，最常见的应用场景是配合老年代的CMS GC 工作 CMS（Concurrent Mark Sweep）GC : 初始标记 (STW) -\u003e 并发标记 -\u003e 重新标记 (STW) (三色标记)-\u003e 并发清理 三色标记算法： 黑色：自己已经标记，子节点都标记完成。下次扫描不扫描 灰色：自己已经标记，子节点还没标记。下次扫描只扫描子节点 白色：没有遍历到的节点。 Incremental update 优点： 基于标记-清除（Mark-Sweep）算法，尽量减少停顿时间。 缺点： Incremental update天然bug，会有漏标的问题，所以CMS的remark阶段，必须重头扫描一遍，STW是所有时间最长的。存在碎片化问题，在长时间运行的情况下会发生full GC，导致恶劣停顿。会占用更多的CPU资源，和用户争抢线程。在JDK 9中被标记为废弃。 Parrallel GC（parallel Scavenge + parallel old）： 在JDK8等版本中，是server模式JVM的默认GC选择，也被称为吞吐量优先的GC，算法和Serial GC相似，特点是老生代和新生代GC并行进行，更加高效。 G1 GC： 兼顾了吞吐量和停顿时间的GC实现，是Oracle JDK 9后默认的GC 可以直观的设值停顿时间，相对于CMS GC ，G1未必能做到CMS最好情况下的延时停顿，但比最差情况要好得多 G1 仍存在年代的概念，G1物理上不分代，逻辑分代，使用了Region棋盘算法，实际上是标记-整理（Mark-Compact）算法，可以避免内存碎片，尤其是堆非常大的时候，G1优势更明显。 G1 吞吐量和停顿表现都非常不错。 ZGC： colored Pointer + Load Barrier 不再分代 shenandoah： 和 ZGC 类似 GC 调优 java -Xms200M -Xmx200m -XX:+PrintGC com.jvm -Xms200M -Xmx200m : 防止内存抖动，消耗资源 cpu 占用率居高不下如何调试 jvm 项目中，产生内存泄露的问题， 频繁GC但是回收不到内存，通过定位发现泄露是因为一个类创建了海量的对象 top #查看哪个进程占CPU比较高 top -Hp PID #查看进程里哪个线程占CPU jstack/PrintGC #找到进程，看是 VM GC 进程还是业务进程 #若是 GC 则一定是频繁的 full GC，使用 PrintGC 查看GC每次回收是否正常 #java -printgc -heapDumpOnOutOfMemoryError, OOM会下载dump文件 jmap #查看堆中对象占用内存的情况；查看堆转储文件 MAT/jhat/jvisualbm #进行dump文件分析 jmap 为了把里面的对象全输出出来，会 STW，让整个JVM卡死 jmap 命令在压测环境上观察的 机器做了负载均衡，发现问题后把有问题的机器从负载环境摘出来，再把堆转储文件导出来 使用tcpcopy复制两份，一份到生产环境，一份到测试环境 arthas： java -jar arthas-boot.jar # 运行后会自动找机器中java的进程 # 性能上有所降低，但不会stw dashboard # 展示线程占用、年代堆栈内存 heapdump # 替代jmap命令 thread -b # 查找线程中死锁，代替 jstack jvm jad # 在线反编译，在线定位一些问题 redefine # 在线修改class，临时解决版本bug。多台服务器写个脚本批量修改 trace # 查看方法所用时间 ","date":"2020-01-01","objectID":"/20200101_java-interview/:2:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"Multi Thread 启动线程的三种方式: extends Thread implements Runnable Executors.newCachedThread 线程的状态： classLoadProcess JVM内存模型 存储器的层次结构 l0:寄存器 l1:高速缓存 l2:高速缓存--------------cpu内部 -------------------------------- l3:高速缓存 l4:主存 l5:硬盘 l6:远程文件存储-----------cpu共享 -------------------------------- cache line 的概念 / 缓存行对其 / 伪共享 缓存行： 缓存行越大，局部性空间效率越高，但读取时间慢 缓存行越小，局部性空间效率越低，但读取时间快 目前用：64字节 缓存对其，在cpu内部的L2高速缓存处理时多线程，属于硬件问题： [x y main memory] [x y L3 cache] [x y L2][x y L2] [x y L1][x y L1] [计算单元与寄存器][计寄存器] 高速缓存数据一致性解决方法 老的CPU：总线锁 大大降低了性能 新的CPU：MESI Cache 数据一致性协议等(intel 用 MESI) + 总线锁 Modified Exclusive Shared Invalid 有些无法被缓存的数据，或者跨越多个缓存行的数据，依然必须使用总线锁 缓存行对其 / 伪共享 位于同一缓存行的两个不同数据，被两个不同CPU锁定，产生互相影响的伪共享问题 解决方法： 缓存行对其：扩大字节数，使其不在统一缓存行 private static class Padding { public long p1, p2, p3, p4, p5, p6, p7; //cache line padding private volatile long cursor = INITIAL_CURSOR_VALUE; public long p8, p9, p10, p11, p12, p13, p14; //cache line padding } 乱序问题 cpu为了提高指令执行效率，去同时执行另一条指令（前提两条指令没有依赖关系:int a = 0; a++;）这样的cpu的执行就是乱序的。 而且 必须使用Memory Barrier来做好指令排序 volatile的底层就是这么实现的(windows 是 lock 指令) 如何保证特定情况下不乱序 volatile 有序： 使用 CPU 内存屏障， 原理： sfence指令前的写操作必须在sfence指令后的写操作前完成 Ifence指令前的读操作必须在Ifence指令后的读操作前完成 mfence指令前的读写操作必须在mfence指令后的读写操作前完成 实际使用的是 Intel lock 汇编指令 volatile 作用： 保持线程间的可见性 禁止指令重排序（通过内存屏障） 关于 Object o = new Object() 解释对象的创建过程（半初始化） T t = new T() jvm 编译成 class 汇编码5条指令: new #2 \u003cT\u003e //申请内存空间, 成员变量是默认值 dup invokespecial #3 \u003cT.\u003cinit\u003e\u003e //调用初始化方法，成员变量初始化 astore //t 和 new 出的对象进行关联 return 由于指令重排的存在 invokespecial #3 \u003cT.\u003cinit\u003e\u003e astore //有可能乱序执行,先指向内存再初始化变量 DCL与volatile问题（指令重排） 1.Double Check Lock public static volatile T INSTANCE; public static T getInstance(){ if (INSTANCE == null){ synchronized(T.class){ if(INSTANCE == null){ try{ Thread.sleep(1); }catch(InterruptedException e) { e.printStackTrace(); } INSTANCE = new T(); } } } } 2.DCL单例必须要加volatile 指令重排，对象初始化时先指向内存，再初始化赋值 若此时线程2进入，则直接拿到未初始化赋值的对象 对象在内存中的存储布局（对象与数组的存储不同） 对象头具体包括什么（markword klasspointer） synchronized锁信息 对象怎么定位（直接 间接） 对象怎么分配（栈上 线程本地 Eden Old） Object o = new Object()在内存中占用多少字节 ","date":"2020-01-01","objectID":"/20200101_java-interview/:3:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"Redis 项目中Redis的应用场景 五大value类型 基本上就是缓存 为的是服务无状态，（延伸：看项目中有哪些数据结构，如分布式锁，抽出来放到Redis） 无锁化 Redis是单线程还是多线程 无论哪个版本，工作线程就是一个 6.x 高版本出现了IO多线程 ","date":"2020-01-01","objectID":"/20200101_java-interview/:4:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"Article description.","date":"2020-01-01","objectID":"/20200101_leetcode/","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Algorithms are used in every part of computer science. They form the field’s backbone. In computer science, an algorithm gives the computer a specific set of instructions, which allows the computer to do everything, be it running a calculator or running a rocket. 异或运算 特点： 与自己异或等于0，与0异或等于自己 遵循交换律和结合律 例： //1.两数交换： arr[i] = arr[i] ^ arr[j]; // a = a ^ b arr[j] = arr[i] ^ arr[j]; // b = (a ^ b) ^ b = a arr[i] = arr[i] ^ arr[j]; // a = (a ^ b) ^ a = b //2.数组中有一个出现奇数次的数 int eor = 0; for(int cur : arr){ eor ^= cur; //[a,a,b,b,c,c,c] =\u003e a^a^b^b^c^c^c = c } System.out.println(eor); //3.数组中有两个出现奇数次的数 int eor = 0; for(int cur : arr){ eor ^= cur; //[a,a,b,b,b,c,c,c] =\u003e a^a^b^b^b^c^c^c = b^c } //eor = b ^ c //eor != 0 //eor必然有个一位置上是1, 说明这个位置上b和c不同 int rightOne = eor \u0026 (~eor + 1); //（取反 + 1） \u0026 自己 = 提取出只保留最右侧的1 int result1 = 0; for(int cur : arr){ if((cur \u0026 rightOne) == 0){ //已知在rightOne这个位置上b和c不同，通过(\u0026 rightOne)把b和c区分开,别的数是偶数个，异或起来是0不同管 result1 ^= cur; //这样result1是b或者c } } int result2 = eor ^ result1; ","date":"2020-01-01","objectID":"/20200101_leetcode/:0:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"排序 冒泡排序 bubble_sort 选择排序 select_sort ","date":"2020-01-01","objectID":"/20200101_leetcode/:1:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"插入排序 insert-sort 概念： 将后部分的数据(从第二开始)按照一定的顺序一个一个的插入到前部分有序的表中 时间复杂度：O(N^2) 额外空间复杂度：O(1) 数据状况不同，时间复杂度不同: 时间复杂度是按照最差情况考虑 public static void insertionSort(int[] arr) { if (arr == null || arr.length \u003c 2){ return; } for (int i = 1; i \u003c arr.length; i++){ for(int j = i - 1; j \u003e= 0 \u0026\u0026 arr[j] \u003e arr[j+1]; j--){ swap(arr, j, j+1); } } } public static void swap(int[] arr, int i, int j){ arr[i] = arr[i] ^ arr[j]; arr[j] = arr[i] ^ arr[j]; arr[i] = arr[i] ^ arr[j]; } 递归 public static int getMax(int[] arr){ return process(arr, 0, arr.length - 1); } public static int process(int[] arr, int L, int R){ if(L == R){ return arr[L]; } int mid = L + ((R - L) \u003e\u003e 1); int leftMax = process(arr, L, mid); int rightMax = process(arr, mid + 1, R); return Math.max(leftMax, rightMax); } 递归时间复杂度估算 Master公式： T(N) = a * T (N / b) + 0 (N ^ d) log(b, a) \u003e d, 则复杂度为O(N ^ log(b, a)) log(b, a) = d, 则复杂度为O(N ^ d * logN) log(b, a) \u003c d, 则复杂度为O(N ^ d) ","date":"2020-01-01","objectID":"/20200101_leetcode/:1:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"归并排序 merge-sort 概念：将n个元素分成个含n/2个元素的子序列，用合并排序法对两个子序列递归的排序，合并两个已排序的子序列已得到排序结果。 时间复杂度：使用Master公式： T(N) = NlogN 代码实现： public static void process(int[] arr, int L, int R){ if(L == R){ return; } int mid = L + ((R - L) \u003e\u003e 1); process(arr, L, mid); process(arr, mid + 1, R); merge(arr, L, mid, R); } public static void merge(int[] arr, int L, int M, int R){ int[] extra = new int[R - L + 1]; int i = 0; int p1 = L; int p2 = M + 1; while(p1 \u003c= M \u0026\u0026 p2 \u003c= R){ extra[i++] = arr[p1] \u003c= arr[p2] ? arr[p1++]: arr[p2++]; } while(p1 \u003c= M){ extra[i++] = arr[p1++]; } while(p2 \u003c= R){ extra[i++] = arr[p2++]; } for(i = 0; i \u003c extra.length; i++){ arr[L + i] = extra[i]; } } 小和问题、逆序对问题 [1, 3, 4, 2, 5] 的小和是 0 + 1 + 4 + 1 +10 = 16（ 在一个数组中,每一个数左边比当前数小的数累加起来,叫做这个数组的小和。） 归并排序思路，目的就是减少对比次数。增加一个外部空间，记录在一次merge中的小和。 public static int process(int[] arr, int l, int r){ if (l == r){ return 0; } int mid = 1 + ((r - l) \u003e\u003e 1); return process(arr, 1 , mid) + process(arr, mid + 1 , r) + merge(arr, 1 , mid, r); } public static void merge(int[] arr, int L, int mid, int R){ int[] extr = new int[R - L + 1]; int i = 0; int p1 = L;1 int p2 = M + 1; int res = 0; while(p1 \u003c= M \u0026\u0026 p2 \u003c= R){ res += arr[p1] \u003c arr[p2] ? (r - p2 + 1) * arr[p1] : 0 extr[i++] = arr[p1] \u003c arr[p2] ? arr[p1++]: arr[p2++]; } while(p1 \u003c= M){ extr[i++] = arr[p1++]; } while(p2 \u003c= R){ extr[i++] = arr[p2++]; } for(i = 0; i \u003c extra.length; i++){ arr[L + i] = extra[i]; } return res; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:1:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"快速排序 quick_sort 时间复杂度：O(NlogN)，因为有随机概率行为，所以时间复杂度为期望值。 空间复杂度：O(logN) 荷兰国旗问题 问题1：给一个 arr 和 num，要求把小于等num的数放左，大于num放右，时间复杂度为O(N)，空间为O(1) 问题2：给一个 arr 和 num，要求把小于等num的数放左，等于放中间，大于num放右，时间复杂度为O(N)，空间为O(1) public static void quickSort(int[] arr, int L, int R){ if(L \u003c R){ swap(arr, L + (int)(Math.random() * (R - L + 1)), R);//随机选一个数和最后位置交换 int[] p = partition(arr, L, R);//=区域的左右边界 quickSort(arr, L, p[0] - 1); quickSort(arr, p[1] + 1, R); } } public static int[] partition(int[] arr, int L, int R){ int less = L - 1;// \u003c区域的右边界 int more = R; //\u003e区的左边界 while(L \u003c more){ // L表示当前数的位置 if(arr[L] \u003c arr[R]){ swap(arr, ++less, L++); }else if(arr[L] \u003e arr[R]){ swap(arr, --more, L); }else{ L++; } } swap(arr, more, R); return new int[]{ less + 1, more }; } private static void swap(int[] arr, int i, int j) { if(i != j){ arr[i] = arr[i] ^ arr[j]; arr[j] = arr[i] ^ arr[j]; arr[i] = arr[i] ^ arr[j]; } } ","date":"2020-01-01","objectID":"/20200101_leetcode/:1:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"堆排序 heap_sort 时间复杂度：O(NlogN) 大根堆：在完全二叉树（最后一层的结点都连续集中在最左边，其余层满节点）的前提下，每一棵子树的最大值是头结点的数。 小根堆：在完全二叉树（最后一层的结点都连续集中在最左边，其余层满节点）的前提下，每一棵子树的最小值是头结点的数。 优先级队列结构，就是堆结构 //处在index位置上的数，往上继续移动 public static void heapInsert(int[] arr, int index){ while(arr[index] \u003e arr[(index - 1) / 2]) { swap(arr, index, (index - 1) / 2); index = (index - 1) / 2; } } 返回最大值（arr[0]上的数），并删除最大值： 返回根节点。 把 arr[heapSize] 的数放到根节点。 heapSize–，最后一个节点即与堆断连。 while 循环，与子节点比较，小则替换，直到满足大根堆。 // 处在index位置上的数，往下继续移动 public static void heapify(int[] arr, int index, int heapSize){ int left = index * 2 + 1; //节点左孩子下标 while(left \u003c heapSize){ //判断下方是否有孩子 //两个孩子中，谁的值大，把下标给最大的 int largest = left + 1 \u003c heapSize \u0026\u0026 arr[left + 1] \u003e arr[left] ? left + 1 : left; //节点与孩子比较，谁的值大，把下标给最大的 largest = arr[largest] \u003e arr[index] ? largest : index; if(largest == index){ break; } swap(arr, largest, index); index = largest; left = index * 2 + 1; } } HeapSort public static void heapSort(int[] arr){ if(arr == null || arr.length \u003c 2){ return; } for(int i = 0; i \u003c arr.length; i++){ heapInsert(arr, i);//数组整体范围变成大根堆 } int heapSize = arr.length; swap(arr, 0, --heapSize);//大根堆的root(最大值)放到arr最后 while(heapSize \u003e 0){ heapify(arr, 0, heapSize); swap(arr, 0, --heapSize); } } 一个数组如果把它排好序的话，每个元素移动的距离不超过k，k相对于数组长度比较小，选择一个合适的排序算法排序 方法： 最小堆排方法，先取出前k个数字组成最小堆，根位置为0位置上的数；添加一个数，堆化，根位置为第二个位置上的数。以此类推 PriorityQueue\u003cInteger\u003e heap = new PriorityQueue\u003c\u003e();//优先级队列结构，就是堆结构 扩容方式：双倍扩容 public void sortedArrDistanceLessK(int[] arr, int k){ PriorityQueue\u003cInteger\u003e heap = new PriorityQueue\u003c\u003e();//默认小根堆 int index = 0; for(; index \u003c= Math.min(arr.length, k); index++){//生成前k个数的最小堆 heap.add(arr[index]); } int i = 0; for(;index \u003c arr.length; i++, index++){//弹一个，放入一个 heap.add(arr[index]); arr[i] = heap.poll(); } while(!heap.isEmpty()){ arr[i++] = heap.poll(); } } ","date":"2020-01-01","objectID":"/20200101_leetcode/:1:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"桶排序 计数排序： 基数排序： radix_sort public static void radixSort(int[] arr, int L, int R, int digit){//digit: 最大数的位数 final int radix = 10; int i = 0, j = 0; int[] bucket = new int[R - L + 1];//有多少个数准备多少个辅助空间 for(int d = 1; d \u003c= digit; d++){//最大位数有多少位就进出桶多少次 int[] count = new int[radix]; //count[0..9] //count[0] 当前(d)位是0的数字有多少个 //count[1] 当前(d)位小于等于1的数字有多少个 //count[2] 当前(d)位小于等于2的数字有多少个 //count[i] 当前(d)位小于等于i的数字有多少个 for(i = L; i \u003c= R; i++){ count[getDigit(arr[i], d)]++;//先统计每个数字(num)出现了几次(times)，先记录在count[num] = times } for(i = 1; i \u003c radix; i++){ count[i] = count[i] + count[i - 1];//再统计小于等于出现的次数 } for(i = R; i \u003e= L; i--){//从右向左 j = getDigit(arr[i], d); bucket[count[j] - 1] = arr[i]; } for(i = L, j = 0; i \u003c= R; i++, j++){ arr[i] = bucket[j];//将桶中数据倒回数组 } } } 小结 时间复杂度 空间复杂度 稳定性 选择 O(N2) O(1) x 冒泡 O(N2) O(1) √ 插入 O(N2) O(1) √ 归并 O(NlogN) O(N) √ 快排 O(NlogN) O(logN) x 堆排 O(NlogN) O(1) x 一般优先选择 快速排序，快排的复杂度在常数项是最低的。 对空间有要求选择 堆排序。 对稳定性有要求选择 归并排序。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:1:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 排序 ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"快速选择 用于求解 Kth Element 问题，也就是第 K 个元素的问题。 可以使用快速排序的 partition() 进行实现。需要先打乱数组，否则最坏情况下时间复杂度为 O(N2)。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"堆 用于求解 TopK Elements 问题，也就是 K 个最小元素的问题。使用最小堆来实现 Top 问题，最小堆使用大顶堆来实现，大顶堆的堆顶元素为当前的最大元素。 实现过程：不断地往大顶堆中插入新元素，当堆中元素的数量大于 k 时，移除堆顶元素，也就是当前堆中最大的元素，剩下的元素都为当前添加过的元素中最小的K个元素。插入和移除堆顶元素的时间复杂度都为 log2N 快速选择也可以求解 TopK Elements 问题，因为找到了 Kth Element 之后，所有小于等于 Kth Element 的元素都是 TopK Elements。 快速选择和堆排序都可以求解 Kth Element 和 TopK Elements 问题。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目一：Kth Element Kth Largest Element in an Array (Medium) Input: [3,2,1,5,6,4] and k = 2 Output: 5 题目描述：找到倒数第 k 个的元素。 解题： 方法一：排序：时间复杂度 O(NlogN)，空间复杂度 O(1) sort() 方法：元素少于NSERTION_SORT_THRESHOLD⽤插⼊排序，大于NSERTION_SORT_THRESHOLD，使用快排。 public int findKthLargest(int[] nums, int k) { Arrays.sort(nums); return nums[nums.length - k]; } 方法二：堆：时间复杂度 O(NlogK)，空间复杂度 O(K)。 public int findKthLargest(int[] nums, int k) { PriorityQueue\u003cInteger\u003e pq = new PriorityQueue\u003c\u003e(); //小项堆 for(int val : nums){ pq.add(val); if(pq.size() \u003e k) pq.poll(); //维护堆的大小为K，弹出最小项，留下的 k 项为数组中最大的。 } return pq.peek(); } 方法三：快速选择 ：时间复杂度 O(N)，空间复杂度 O(1) public int findKthLargest(int[] nums, int k) { k = nums.length - k; int l = 0, h = nums.length - 1; while (l \u003c h) { int j = partition(nums, l, h); if (j == k) { break; } else if (j \u003c k) { l = j + 1; } else { h = j - 1; } } return nums[k]; } private int partition(int[] a, int l, int h) { int i = l, j = h + 1; while (true) { while (a[++i] \u003c a[l] \u0026\u0026 i \u003c h) ; while (a[--j] \u003e a[l] \u0026\u0026 j \u003e l) ; if (i \u003e= j) { break; } swap(a, i, j); } swap(a, l, j); return j; } private void swap(int[] a, int i, int j) { int t = a[i]; a[i] = a[j]; a[j] = t; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"桶排序 ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目二：出现频率最多的 k 个元素 Top K Frequent Elements (Medium) Given [1,1,1,2,2,3] and k = 2, return [1,2]. 解题： 设置若干个桶，每个桶存储出现频率相同的数。桶的下标表示数出现的频率，即第 i 个桶中存储的数出现的频率为 i。 把数都放到桶之后，从后向前遍历桶，最先得到的 k 个数就是出现频率最多的的 k 个数。 public int[] topKFrequent(int[] nums, int k) { Map\u003cInteger, Integer\u003e frequencyForNum = new HashMap\u003c\u003e(); for(int num : nums){ frequencyForNum.put(num, frequencyForNum.getOrDefault(num, 0) + 1); } List\u003cInteger\u003e[] buckets = new ArrayList[nums.length + 1]; for(int key : frequencyForNum.keySet()){ int frequency = frequencyForNum.get(key); if (buckets[frequency] == null){ buckets[frequency] = new ArrayList\u003c\u003e(); } buckets[frequency].add(key); } List\u003cInteger\u003e topK = new ArrayList\u003c\u003e();//res for(int i = buckets.length - 1; i \u003e= 0 \u0026\u0026 topK.size() \u003c k; i--){ if(buckets[i] == null){ continue; } if(buckets[i].size() \u003c= (k - topK.size())){ topK.addAll(buckets[i]); }else{ topK.addAll(buckets[i].subList(0, k - topK.size())); } } int[] res = new int[k]; for (int i = 0; i \u003c k; i++) { res[i] = topK.get(i); } return res; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目三：按照字符出现次数对字符串排序 Sort Characters By Frequency (Medium) Input: \"tree\" Output: \"eert\" Explanation: 'e' appears twice while 'r' and 't' both appear once. So 'e' must appear before both 'r' and 't'. Therefore \"eetr\" is also a valid answer. 解题： public String frequencySort(String s) { Map\u003cCharacter, Integer\u003e frequencyForNum = new HashMap\u003c\u003e(); for (char c : s.toCharArray()) frequencyForNum.put(c, frequencyForNum.getOrDefault(c, 0) + 1); List\u003cCharacter\u003e[] frequencyBucket = new ArrayList[s.length() + 1]; for (char c : frequencyForNum.keySet()) { int f = frequencyForNum.get(c); if (frequencyBucket[f] == null) { frequencyBucket[f] = new ArrayList\u003c\u003e(); } frequencyBucket[f].add(c); } StringBuilder str = new StringBuilder(); for (int i = frequencyBucket.length - 1; i \u003e= 0; i--) { if (frequencyBucket[i] == null) { continue; } for (char c : frequencyBucket[i]) { for (int j = 0; j \u003c i; j++) { str.append(c); } } } return str.toString(); } ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"荷兰国旗问题：快排 partition Input: [2,0,2,1,1,0] Output: [0,0,1,1,2,2] 解题： public void sortColors(int[] nums) { int zero = -1, one = 0, two = nums.length; while (one \u003c two) { if (nums[one] == 0) { swap(nums, ++zero, one++); } else if (nums[one] == 2) { swap(nums, --two, one); } else { ++one; } } } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:2:7","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 二分查找 时间复杂度：O(logN) (logN默认指log以2为底) ","date":"2020-01-01","objectID":"/20200101_leetcode/:3:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"求中间数 mid = (L + R) / 2 // L + R 可能会越界65535，这时mid算出负数 mid = L + (R - L) / 2 //不会越界 mid = L + (R - L) \u003e\u003e 1 //右移一位比除2快 ","date":"2020-01-01","objectID":"/20200101_leetcode/:3:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"binarySearch public int binarySearch(int[] nums, int key) { int l = 0, h = nums.length; while (l \u003c h) { //int m = l + (h - l) / 2; int m = l + (h - l) \u003e 2; if (nums[m] \u003e= key) { h = m; } else { l = m + 1; } } return l; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:3:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目一：求开方 Sqrt(x) (Easy) Input: 4 Output: 2 Input: 8 Output: 2 Explanation: The square root of 8 is 2.82842..., and since we want to return an integer, the decimal part will be truncated. 解题： 一个数 x 的开方 sqrt 一定在 0 ~ x 之间，并且满足 sqrt == x / sqrt。可以利用二分查找在 0 ~ x 之间查找 sqrt。 对于 x = 8，它的开方是 2.82842…，最后应该返回 2 而不是 3。在循环条件为 l \u003c= h 并且循环退出时，h 总是比 l 小 1，也就是说 h = 2，l = 3，因此最后的返回值应该为 h 而不是 l。 public int mySqrt(int x){ if(x \u003c= 1){ return x; } int l = 1, h = x; while(l \u003c= h){ int mid = l + (h - l) \u003e\u003e 2; int sqrt = x / mid; if (sqrt == mid){ return mid; } else if (mid \u003e sqrt){ h = mid - 1; } else { l = mid + 1; } } return h; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:3:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目二：大于给定元素的最小元素 Find Smallest Letter Greater Than Target (Easy) Input: letters = [\"c\", \"f\", \"j\"] target = \"d\" Output: \"f\" Input: letters = [\"c\", \"f\", \"j\"] target = \"k\" Output: \"c\" 解题： public char nextGreatestLetter(char[] letters, char target) { int n = letters.length; int l = 0, h = n - 1; while (l \u003c= h) { int m = l + (h - l) / 2; if (letters[m] \u003c= target) { l = m + 1; } else { h = m - 1; } } return l \u003c n ? letters[l] : letters[0]; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:3:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目三：有序数组的 Single Element in a Sorted Array (Medium) Input: [1, 1, 2, 3, 3, 4, 4, 8, 8] Output: 2 题目描述：一个有序数组只有一个数不出现两次，找出这个数。要求以 O(logN) 时间复杂度求解，因此不能遍历数组并进行异或操作来求解，这么做的时间复杂度为O(N) 解题： 令 index 为 Single Element 在数组中的位置。在 index 之后，数组中原来存在的成对状态被改变。如果 m 为偶数，并且 m + 1 \u003c index，那么 nums[m] == nums[m + 1]；m + 1 \u003e= index，那么 nums[m] != nums[m + 1]。 public int singleNonDuplicate(int[] nums) { int l = 0, h = nums.length - 1; while (l \u003c h) { int m = l + (h - l) / 2; if (m % 2 == 1) { m--; // 保证 l/h/m 都在偶数位，使得查找区间大小一直都是奇数 } if (nums[m] == nums[m + 1]) { l = m + 2; } else { h = m; } } return nums[l]; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:3:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目：找一个局部最小（i-1 \u003c i \u003c i+1）的数，复杂度小于 O(N) 例题： 在一个有序数组中，找某个数是否存在 在一个有序数组中，找 \u003e= 某个数最左侧的位置：二分查找直到左侧没有数 无序，相邻数一定不相等，找一个局部最小(i-1 \u003c i \u003c i+1)的数，复杂度能否好于O(N) 先判断首尾项是否满足要求，若首尾项不满足要求，一定是\\进/出，中间一定有低谷拐点。 然后判断中间点是否满足要求，若不满足，分为两种情况： 1. 斜坡，则一方可与起点或终点组成\\进/出。 2. 顶峰，则两方向都满足\\进/出条件，两个方向都可继续进行二分法 直到判断出某个二分法中点满足要求。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:3:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 哈希表 - 有序表 哈希表介绍 哈希表使用层面上可以理解为一种集合结构。 有无伴随数据，是HashMap和HashSet唯一的区别，底层的数据结构一样。 使用哈希表 put，remove，put，get 的操作，可以认为时间复杂度为O(1)，但常数时间比较大。 哈希表使用 O(N) 空间复杂度存储数据，并且以 O(1) 时间复杂度求解问题。 Java 中的 HashSet 用于存储一个集合，可以查找元素是否在集合中。如果元素有穷，并且范围不大，那么可以用一个布尔数组来存储一个元素是否存在。例如对于只有小写字符的元素，就可以用一个长度为 26 的布尔数组来存储一个字符集合，使得空间复杂度降低为 O(1)。 Java 中的 HashMap 主要用于映射关系，从而把两个元素联系起来。HashMap 也可以用来对元素进行计数统计。在对一个内容进行压缩或者其他转换时，利用 HashMap 可以把原始内容和转换后的内容联系起来。例如在一个简化 url 的系统中。 利用 HashMap 就可以存储精简后的 url 到原始 url 的映射，使得不仅可以显示简化的 url，也可以根据简化的 url 得到原始 url 从而定位到正确的资源。 有序表介绍 有序表在使用层面上可以理解为一种集合结构。 有无伴随数据，是TreeSet和TreeMap唯一的区别，底层的数据结构一样。 有序表和哈希表的区别是，有序表把key按照顺序组织起来，而哈希表完全不组织。 红黑树、AVL树、size-balance-tree和跳表都属于有序表结构，只是底层具体实现不同。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:4:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"1. 数组中两个数的和为给定值 Two Sum (Easy) 解题： 可以先对数组进行排序，然后使用双指针方法或者二分查找方法。这样做的时间复杂度为 O(NlogN)，空间复杂度为O(1)。 用 HashMap 存储数组元素和索引的映射，在访问到 nums[i] 时，判断 HashMap 中是否存在 target - nums[i]，如果存在说明 target - nums[i] 所在的索引和 i 就是要找的两个数。该方法的时间复杂度为 O(N)，空间复杂度为O(N)，使用空间来换取时间。 public int[] twoSum(int[] nums, int target){ HashMap\u003cIntger, Intger\u003e indexForNum = new HashMap\u003c\u003e(); for(int i = 0; i \u003c nums.length; i++){ if(indexForNum.containsKey(target - nums[i])){ return new int[]{indexForNum.get(target - nums[i]), i}; } else { indexForNum.put(nums[i], i); } } return null; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:4:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"2. 判断数组是否含有重复元素 Contains Duplicate (Easy) public boolean containsDuplicate(int[] nums){ Set\u003cInteger\u003e set = new HashSet\u003c\u003e(); for (int num : nums){ set.add(num); } return set.size() \u003c nums.length; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:4:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"3. 最长和谐序列 Longest Harmonious Subsequence (Easy) 和谐序列中最大数和最小数之差正好为 1，应该注意的是序列的元素不一定是数组的连续元素。 Input: [1,3,2,2,5,2,3,7] Output: 5 Explanation: The longest harmonious subsequence is [3,2,2,2,3]. public int findLHS(int[] nums){ Map\u003cInteger, Integer\u003e countForNum = new HashMap\u003c\u003e(); for(int num : nums){ countForNum.put(num, countForNum.getOrDefault(nums, 0) + 1); } int longest = 0; for(int num : countForNum.keySet()){ if(countForNum.containsKey(num + 1)){ longest = Math.max(longest, countForNum.get(num + 1) + countForNum.get(num)); } } return longest; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:4:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"4. 最长连续序列 Longest Consecutive Sequence (Hard) ","date":"2020-01-01","objectID":"/20200101_leetcode/:4:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"链表 笔试 面试区分，面试时需考虑空间复杂度。 题目一：判断一个链表是否回文 方法一：笔试用：进栈出栈 public static boolean isPalindrome1(Node head) { Stack\u003cNode\u003e stack = new Stack\u003cNode\u003e(); Node cur = head; while (cur != null) { stack.push(cur); cur = cur.next; } while (head != null) { if (head.value != stack.pop().value) { return false; } head = head.next; } return true; } 方法2：快慢指针：空间复杂度O(1)，使用了有限几个变量 public static boolean isPalindrome3(Node head) { if (head == null || head.next == null) { return true; } Node n1 = head; Node n2 = head; while (n2.next != null \u0026\u0026 n2.next.next != null) { n1 = n1.next; //n1 -\u003e mid n2 = n1.next.next; //n2 -\u003e end } n2 = n1.next; //n2 -\u003e right part first node Node n3 = null; while (n2 != null) { //right part convert n3 = n2.next; // save next node n2.next = n1; // right direct to left direct n1 = n2; n2 = n3; } n3 = n1; // last node n2 = head; // fist node boolean res = true; while (n1 != null \u0026\u0026 n2 != null) { if (n1.value != n2.value) { res = false; break; } n1 = n1.next; n2 = n2.next; } n1 = n3.next; // recover list n3.next = null; while (n1 != null) { n2 = n1.next; n1.next = n3; n3 = n1; n1 = n2; } return res; } 题目二：单链表按某值划分成左边小，中间相等，右边大 方法一：（笔试）放到数组里，在数组里partition 方法二：（面试）创建6个空节点，每两个一组，作为三个区域的首尾节点。遍历原链表，放到不同的区域并调整各组首尾节点，最后三个拼装。 题目三：复制含有随机指针节点的链表 class Node { int value; Node next; Node rand; Node(int val) { value = val; } } 方法一：hashmap public static Node copyListWithRand1(Node head) { HashMap\u003cNode, Node\u003e map = new HashMap\u003cNode, Node\u003e(); Node cur = head; while (cur != null) { map.put(cur, new Node(cur.value)); cur = cur.next; } cur = head; while (cur != null) { map.get(cur).next = map.get(cur.next); map.get(cur).rand = map.get(cur.rand); cur = cur.next; } return map.get(head); } 方法二：利用位置关系省去哈希表。 当前节点的下一个就放克隆节点。 curCopy = cur.next; curCopy.rand = cur.rand != null ? cur.rand.next:null; 跳过旧链表。 题目四：两个单链表相交的一系列问题 先判断有无环： 方法一：Hashset。get一个，看之前是否加入过，否则put进set。 方法二：快慢指针。不会走到空节点，快慢指针一定会相遇，而且快指针在环中不会超过两圈。相遇后快指针回到开头，然后两个指针都走一步，一定会在入环节点相遇。 情况一：两链表都无环 判断链表长度 判断end节点是否是一个，不同则不相交。 长链表先走差值步，然后两链表一起走，一定会一起走到第一个相交点。 情况二：两链表都有环 情况二之一：链表无交集 情况二之二：入环节点是同一个节点。 情况二之三：入环节点不同。 public static Node bothLoop(Node head1, Node head2, Node loop1, Node loop2) {//两链表头节点，入环节点 Node cur1 = null; Node cur2 = null; if (loop1 == loop2) { //情况二之二：入环节点是同一个节点。 cur1 = head1; cur2 = head2; int n = 0; while (cur1 != loop1) { //判断到达入环节点的长度 n++; cur1 = cur1.next; } while (cur2 != loop2) { //判断是1链表头结点到入环节点长度和2链表到入环节点长度的差值 n--; cur2 = cur2.next; } cur1 = n \u003e 0 ? head1 : head2; cur2 = cur1 == head1 ? head2 : head1; n = Math.abs(n); while (n != 0) {// 使两个链表到入环点长度一样 n--; cur1 = cur1.next; } while (cur1 != cur2) { cur1 = cur1.next; cur2 = cur2.next; } return cur1; } else { // 情况二之三：入环节点不同 或 情况二之一：链表无交集 cur1 = loop1.next; while (cur1 != loop1) { // 限定只转一圈，碰不到loop2就是无交集 if (cur1 == loop2) { return loop1; } cur1 = cur1.next; } return null; } } ","date":"2020-01-01","objectID":"/20200101_leetcode/:5:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 双指针 ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目一：有序数组的 Two Sum Two Sum II - Input array is sorted (Easy) Input: numbers={2, 7, 11, 15}, target=9 Output: index1=1, index2=2 题目描述：在有序数组中找出两个数，使它们的和为 target。 解题： 使用双指针，一个指针指向最小的元素，一个指针指向最大的元素，两指针向中间遍历 如果两指针的和 sum == garget，return 结果； 如果 sum \u003e target，移动较大的元素，使 sum 变小一些； 如果 sum \u003c target，移动较小的元素，使 sum 变小一些。 最多遍历一遍，时间复杂度为 O(N)。只是用两个额外的变量，空间按复杂度为 O(1) public int[] twoSum(int[] numbers, int target){ if (numbers == null) return null; int i = 0,j = numbers.length - 1; while(i \u003c j){ int sum = numbers[i] + numbers[j]; if (sum == target) { return new int[]{i + 1, j + 1}; } else if (sum \u003c target) { i++; } else { j--; } } return null; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目二：两数平方和 Sum of Square Numbers (Easy) 题目描述：判断一个非负整数是否为两个整数的平方和。 解题： 可以看成是在元素为 0 ~ target 的有序数组中查找两个数，使得这两个数的平方和为 target。 与 题一 逻辑一致，不同的是：左指针从 0位置上的0开始，右指针从 sqrt(target) 位置开始； 时间复杂度 O(sqrt(target))，时间复杂度 O(1) ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目三：反转字符串中的元音字符 Reverse Vowels of a String (Easy) Given s = \"leetcode\", return \"leotcede\". 解题： 使用双指针，一个指针从头遍历，一个指针从尾遍历，当两个指针都遇到元音时，交换这两个元音字符。 为了快速判断字符是不是元音字符，将元音字符添加到集合 HashSet 中，从而以 O(1) 的时间复杂度进行该操作 private final static HashSet\u003cCharacter\u003e vowels = new HashSet\u003c\u003e( Arrays.asList('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U')); 时间复杂度 O(N)，空间复杂度 O(1) ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目四：回文字符串 Valid Palindrome II (Easy) Input: \"abca\" Output: True Explanation: You could delete the character 'c'. 题目描述：可以删除一个字符，判断是否能构成回文字符串。 使用 双指针（stack进栈出栈、快慢指针（空间复杂度为O(1)））容易判断一个字符串是否是回文字符串。 本题的关键是处理删除一个字符。在使用双指针遍历字符串时，如果出现两个指针指向的字符不相等的情况，我们就试着删除一个字符，再判断删除完之后的字符串是否是回文字符串。 在判断是否为回文字符串时，我们不需要判断整个字符串，因为左指针左边和右指针右边的字符之前已经判断过具有对称性质，所以只需要判断中间的子字符串即可。 在试着删除字符时，我们既可以删除左指针指向的字符，也可以删除右指针指向的字符。 public boolean validPalindrome(String s){ for(int i = 0, j = s.length() - 1; i \u003c j; i++,j--){ if(s.charAt(i) != s.charAt(j)){ return isPalindrome(s, i, j - 1) || isPalindrome(s, i + 1, j); } } return true; } private bool isPalindrome(String s, int i, int j) { while (i \u003c j){ if(s.charAt(i++) != s.charAt(j--)){ return false; } } return ture; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目五：归并两个有序数组 Merge Sorted Array (Easy) 题目描述：把归并结果存到第一个数组上。 Input: nums1 = [1,2,3,0,0,0], m = 3 nums2 = [2,5,6], n = 3 Output: [1,2,2,3,5,6] 解题： 类似归并排序合并字符串。 public void merge(int[] nums1, int m, int[] nums2, int n) { int index1 = m - 1, index2 = n - 1; int indexMerge = m + n - 1; while (index2 \u003e= 0) { if (index1 \u003c 0) { nums1[indexMerge--] = nums2[index2--]; } else if (index2 \u003c 0) { nums1[indexMerge--] = nums1[index1--]; } else if (nums1[index1] \u003e nums2[index2]) { nums1[indexMerge--] = nums1[index1--]; } else { nums1[indexMerge--] = nums2[index2--]; } } } //归并排序merge方法 public static void merge(int[] arr, int L, int M, int R){ int[] extra = new int[R - L + 1]; int i = 0; int p1 = L; int p2 = M + 1; while(p1 \u003c= M \u0026\u0026 p2 \u003c= R){ extra[i++] = arr[p1] \u003c= arr[p2] ? arr[p1++]: arr[p2++]; } while(p1 \u003c= M){ extra[i++] = arr[p1++]; } while(p2 \u003c= R){ extra[i++] = arr[p2++]; } for(i = 0; i \u003c extra.length; i++){ arr[L + i] = extra[i]; } } ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目六：判断链表是否存在环 Linked List Cycle (Easy) 解题： 使用快慢指针。如果存在环，那么这两个指针一定会相遇。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目七：最长子序列 Longest Word in Dictionary through Deleting (Medium) Input: s = \"abpcplea\", d = [\"ale\",\"apple\",\"monkey\",\"plea\"] Output: \"apple\" 题目描述：删除 s 中的一些字符，使得它构成字符串列表 d 中的一个字符串，找出能构成的最长字符串。如果有多个相同长度的结果，返回字典序的最小字符串。 解题： 通过删除字符串 s 中的一个字符能得到字符串 t，可以认为 t 是 s 的子序列，我们可以使用双指针来判断一个字符串是否为另一个字符串的子序列。 public String findLongestWord(String s, List\u003cString\u003e d) { String longestWord = \"\"; for(String target: d){ int l1 = longestWord.length(), l2 = target.length(); if(l1 \u003e l2 || (l1 == l2 \u0026\u0026 longestWord.compareTo(target) \u003c 0)){ continue; } if(isSubstr(s,target)){ } } return longestWord; } private boolean isSubstr(String s,String target){ int i = 0,j = 0; while(i \u003c s.length() \u0026\u0026 j \u003c target.length()){ if(s.charAt(i) == target.charAt(j)){ j++; } i++; } return j == target.length(); } ","date":"2020-01-01","objectID":"/20200101_leetcode/:6:7","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 树 递归遍历： public static void orderRecur(Node head){ if(head == null){ return; } //operation(...) //先序遍历 orderRecur(head.left); //operation(...) //中序遍历 orderRecur(head.right); //operation(...) //后序遍历 } 深度优先遍历：中序遍历 宽度优先遍历并求最大宽度：队列 LinkedList public static void weight(Node head) { if (head == null) { return; } Queue\u003cNode\u003e queue = new LinkedList\u003c\u003e(); queue.add(head); HashMap\u003cNode, Integer\u003e levelMap = new HashMap\u003c\u003e();//记录行数 levelMap.put(head, 1); int curLevel = 1; int curLevelNodes = 0; int max = Integer.MIN_VALUE; while (!queue.isEmpty()) { Node cur = queue.poll(); int curNodeLevel = levelMap.get(cur); if (curNodeLevel == curLevel) { curLevelNodes++; } else { max = Math.max(max, curLevelNodes); curLevel++; curLevelNodes = 0; } System.out.println(cur.value); if (cur.left != null) { levelMap.put(cur.left, curNodeLevel + 1); queue.add(cur.left); } if (cur.right != null) { levelMap.put(cur.right, curNodeLevel + 1); queue.add(cur.right); } } } 判断是否是搜索二叉树：左孩子小于父节点，右孩子大于父节点。 中序遍历：中间打印节点值的地方改成和前节点值比较。 判断是否是完全二叉树：每一层是满的，或者最后一层节点都在左边。 非递归方法： 同时满足 任一节点有右无左 –\u003e false 遇到一个左右子树不全的节点，后续皆是叶节点。 public static boolean isCBT(Node head){ if(head == null){ return true; } LinkedList\u003cNode\u003e queue = new LinkedList\u003c\u003e(); //是否遇到过左右两个孩子不双全的节点 boolean leaf = false; Node l = null; node r = null; queue.add(head); while(!queue.isEmpty()){ head = queue.poll(); l = head.left; r = head.right; if( //节点不双全，又发现有孩子 (leaf \u0026\u0026 (l ！= null || r ！= null)) || (l == null \u0026\u0026 r != null) ){ return false; } if (l != null){ queue.add(l); } if (r != null){ queue.add(r); } if (l == null || r == null){ leaf = true; } return true; } } 判断满二叉树： 二叉树DP题目固定套路：递归 二叉树DP题目: 可以通过从左右树要信息解决问题可以使用这个固定方法。 public static class Info { public int height; public int nodes; public Info(int h, int n) { height = h; nodes = n; } } public static Info f(Node x) { if (x == null) { return new Info(0, 0); } Info leftData = f(x.left); Info rightData = f(x.right); int height = Math.max(leftData.height, rightData.height) + 1; int nodes = leftData.nodes + rightData.nodes + 1; return new Info(height, nodes); } 判断平衡二叉树：左树高度和右树高度的差都不超过1 同时满足： 左子树是平衡二叉树 右子树是平衡二叉树 左树高度和右树高度的差不超过1 public static class ReturnType{ public boolean isBalanced; public int height; public ReturnType(boolean isB, int hei){ isBalanced = isB; height = hei; } } public static ReturnType Process(Node x){ if(x = null){ return new ReturnType(true, 0); } ReturnType leftData = process(x.left); ReturnType rightData = process(x.right); int height = Math.max(leftData.height, rightData.height) + 1; boolean isBalanced = leftData.isBalanced \u0026\u0026 rightData.isBalanced \u0026\u0026 Math.abs(leftData.height - rightData.height) \u003c 2; return new ReturnType(isBalanced, height); } ","date":"2020-01-01","objectID":"/20200101_leetcode/:7:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"递归 1. 树的高度 Maximum Depth of Binary Tree (Easy) public int maxDepth(TreeNode root){ if(root == null) return 0; return Math.max(maxDepth(root.left), maxDepth(root.right)) + 1; } 2. 平衡树 Balanced Binary Tree (Easy) 平衡树左右子树高度差都小于等于 1 private boolean result = true; public boolean isBalanced(TreeNode root){ maxDepth(root); return result; } public int maxDepth(TreeNode root){ if(root == null) return 0; int l = maxDepth(root.left); int r = maxDepth(root.right); if(Math.abs(l - r) \u003e 1) result = false; return Math.max(l, r) + 1; } 3. 两节点的最长路径 Diameter of Binary Tree (Easy) Input: 1 / \\ 2 3 / \\ 4 5 Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3]. private int max = 0; public int diameterOfBinaryTree(TreeNode root){ depth(root); return max; } private int depth(TreeNode root){ if(root == null) return 0; int l = depth(root.left); int r = depth(root.right); max = Math.max(max, l + r); return Math.max(l, r) + 1; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:7:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"BFS层次遍历 使用 BFS 时，不需要使用两个队列来分别存储当前层的节点和下一层的节点，因为在开始遍历一层的节点时，当前队列的节点数就是当前层的节点数，只要控制遍历这么多节点数，就能保证这次遍历的都是当前层的节点。 1. 一棵树每层节点的平均数 Average of Levels in Binary Tree (Easy) public List\u003cDouble\u003e averageOfLevels (TreeNode root){ List\u003cDouble\u003e ret = new ArrayList\u003c\u003e(); if (root == null) return ret; Queue\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); queue.add(root); while (!queue.isEmpty()){ int cnt = queue.size(); double sum = 0; for(int i = 0; i \u003c cnt; i++){ TreeNode node = queue.poll(); sum += node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } ret.add(sum / cnt); } return ret; } 2. 得到左下角的节点 Find Bottom Left Tree Value (Easy) Input: 1 / \\ 2 3 / / \\ 4 5 6 / 7 Output: 7 解题： 宽度优先遍历 BFS，每一行从左至右改成从右至左，然后最后一个数 public int findBottomLeftValue(TreeNode root) { Queue\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); queue.add(root); while(!queue.isEmpty()){ root = queue.poll(); if (root.right != null) queue.add(root.right); if (root.left != null) queue.add(root.left); } return root.val; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:7:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"前中后序遍历 非递归实现前中后序遍历 前序遍历、后续遍历 public List\u003cInteger\u003e preOrderTraversal(TreeNode root){ List\u003cInteger\u003e ret = new ArrayList\u003c\u003e(); Stack\u003cTreeNode\u003e stack = new Stack\u003c\u003e(); stack.push(root); while(!stack.isEmpty()){ TreeNode node = stack.pop(); if (node == null) continue; ret.add(node.val); stack.push(node.right); //先右后左，保证左子树先遍历。先左后右即为后续遍历。 stack.push(node.left); } return ret; } 中序遍历 public List\u003cInteger\u003e inorderTraversal(TreeNode root) { List\u003cInteger\u003e ret = new ArrayList\u003c\u003e(); if (root == null) return ret; Stack\u003cTreeNode\u003e stack = new Stack\u003c\u003e(); TreeNode cur = root; while(cur != null || !stack.isEmpty()){ while(cur != null){ stack.push(cur); cur = cur.left; } TreeNode node = stack.pop(); ret.add(node.val); cur = node.right; } return ret; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:7:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"BST二叉查找树 二叉查找树（BST）：根节点大于等于左子树所有节点，小于等于右子树所有节点。 二叉查找树中序遍历有序。 1. 修剪二叉查找树 Trim a Binary Search Tree Input: 3 / \\ 0 4 \\ 2 / 1 L = 1 R = 3 Output: 3 / 2 / 1 题目描述：只保留值在 L ~ R 之间的节点 解题： public TreeNode trimBST(TreeNode root, int L, int R){ if(root == null) return null; if(root.val \u003e R) return trimBST(root.left, L, R); if(root.val \u003c L) return trimBST(root.right, L, R); root.left = trimBST(root.left, L, R); root.right = trimBST(root.right, L, R); return root; } 2.寻找二叉查找树第K个元素 Kth Smallest Element in a BST (Medium) 中序遍历解法： private int cnt = 0; private int val; public int kthSmallest(TreeNode root, int k){ if(node == null) return; inOrder(node.left, k); //中序遍历查找树有序，左子树走到底为最小数 cnt++; if(cnt == k){ val = node.val; return; } inOrder(node.right, k); //中序遍历向右子树 } 3. 把二叉查找树每个节点的值都加上比它大的节点的值 Convert BST to Greater Tree (Easy) Input: The root of a Binary Search Tree like this: 5 / \\ 2 13 Output: The root of a Greater Tree like this: 18 / \\ 20 13 解题： 先遍历右子树。反向中序遍历即从大到小. private int sum = 0; public TreeNode convertBST(TreeNode root){ traver(root); return root; } private void traver(TreeNode node){ if(node == null) return; traver(node.right); //到最大的节点 sum += node.val; node.val = sum; traver(node.left); } 4. 二叉查找树的最近公共祖先 Lowest Common Ancestor of a Binary Search Tree (Easy) _______6______ / \\ ___2__ ___8__ / \\ / \\ 0 4 7 9 / \\ 3 5 For example, the lowest common ancestor (LCA) of nodes 2 and 8 is 6. Another example is LCA of nodes 2 and 4 is 2, since a node can be a descendant of itself according to the LCA definition. 解题： 前序遍历，第一个满足区间中的数即为最近根。 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q){ if(root.val \u003e p.val \u0026\u0026 root.val \u003e q.val) return lowestCommonAncestor(root.left, p, q); if(root.val \u003c p.val \u0026\u0026 root.val \u003c q.val) return lowestCommonAncestor(root.right, p, q); } 6. 从有序数组中构造二叉查找树 Convert Sorted Array to Binary Search Tree (Easy) public TreeNode sortedArrayToBST(int[] nums){ return toBST(nums, 0, nums.length - 1); } private TreeNode toBST(int[] nums, int sIdx, int eIdx){ if(sIdx \u003e eIdx) return null; int mIdx = (sIdx + eIdx) / 2; TreeNode root = new TreeNode(nums[mIdex]); root.left = toBST(nums, sIdx, mIdx - 1); root.right = toBST(nums, mIdx + 1, eIdx); return root; } 7. 根据有序链表构造平衡的二叉查找树 Convert Sorted List to Binary Search Tree (Medium) 8. 在二叉查找树中寻找两个节点，使它们的和为一个给定值 Two Sum IV - Input is a BST (Easy) Input: 5 / \\ 3 6 / \\ \\ 2 4 7 Target = 9 Output: True 解题： 使用中序遍历得到有序数组之后，再利用双指针对数组进行查找。 应该注意到，这一题不能用分别在左右子树两部分来处理这种思想，因为两个待求的节点可能分别在左右子树中。 public boolean findTarget(TreeNode root, int k) { List\u003cInteger\u003e nums = new ArrayList\u003c\u003e(); inOrder(root, nums); int i = 0, j = nums.size() - 1; while (i \u003c j) { int sum = nums.get(i) + nums.get(j); if (sum == k) return true; if (sum \u003c k) i++; else j--; } return false; } private void inOrder(TreeNode root, List\u003cInteger\u003e nums) { if (root == null) return; inOrder(root.left, nums); nums.add(root.val); inOrder(root.right, nums); } 9. 在二叉查找树中查找两个节点之差的最小绝对值 Minimum Absolute Difference in BST (Easy) Input: 1 \\ 3 / 2 Output: 1 解题： 利用二叉查找树的中序遍历有序的性质，计算中序遍历中临近的两个节点之差的绝对值，取最小值。 private int minDiff = Integer.MAX_VALUE; private TreeNode preNode = null; public int getMinimumDifference(TreeNode root){ inOrder(root); return minDiff; } private void inOrder(TreeNode node){ if (node == null) return; inOrder(node.left); if (preNode != null) minDiff = Math.min(minDiff, node.val - preNode.val); preNode = node; inOrder(node.right); } 10. 寻找二叉查找树中出现次数最多的值 Find Mode in Binary Search Tree (Easy) private int curCnt = 1; private int maxCnt = 1; private TreeNode preNode = null; public int[] findMode(TreeNode root){ List\u003cInteger\u003e maxCntNums = new ArrayList\u003c\u003e(); inOrder(root, maxCntNums); int[] ret = new int[maxCntNums.size()]; int idx = 0; for(int num : maxCntNums){ ret[idx++] = num; } return ret; } private void inOrder(TreeNode node, List\u003cInteger\u003e nums){ if(node == null) return; inOrder(node.left, nums); if(preNode != null){ if (preNode.val == node.val) curCnt++; else curCnt = 1; } if(curCnt \u003e maxCnt){ maxCnt = curCnt; nu","date":"2020-01-01","objectID":"/20200101_leetcode/:7:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Trie 前缀树 Trie 概念：单词集合生成由字母组成的树。又称前缀树或字典树，用于判断字符串是否存在或者是否具有某种字符串前缀。 trie 1.实现一个 Trie Implement Trie (Prefix Tree) (Medium) class Trie{ private class Node{ Node[] childs = new Node[26]; boolean isLeaf; } private Node root = new Node(); public Trie(){ } public void insert(String word, Node node){ insert(word, root); } private void insert(String word, Node node){ if(node == null) return; if(word.length() == 0){ node.isLeaf = true; return; } int index = indexForChar(word.charAt(0)); if(node.childs[index] == null){ node.childs[index] = new Node(); } insert(word.substring(1), node.childs[index]); } public boolean search(String word){ return search(word, root); } private boolean search(String word, Node node){ if(node == null) return false; if(word.length() == 0) node.isLeaf; int index = indexForChar(word.charAt(0)); return search(word.substring(1), node.childs[index]); } public boolean startsWith(String prefix){ return startWith(prefix, root); } private boolean startWith(String prefix, Node node){ if(node == null) return false; if(prefix.length() == 0) return true; int index = indexForChar(prefix.charAt(0)); return startWith(prefix.substring(1), node.childs[index]); } private int indexForChar(char c) { return c - 'a'; } } ","date":"2020-01-01","objectID":"/20200101_leetcode/:7:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 栈和队列 ","date":"2020-01-01","objectID":"/20200101_leetcode/:8:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"1. 用栈实现队列 Implement Queue using Stacks (Easy) 解题： 栈的顺序为后进先出，而队列的顺序为先进先出。使用两个栈实现队列，一个元素需要经过两个栈才能出队列，在经过第一个栈时元素顺序被反转，经过第二个栈时再次被反转。 class MyQueue{ private Stack\u003cInteger\u003e in = new Stack\u003c\u003e(); private Stack\u003cInteger\u003e out = new Stack\u003c\u003e(); public void push(int x){ in.push(x); } public int pop(){ in2out(); return out.pop(); } public int peek(){ in2out(); return out.peek(); } private void in2out(){ if(out.isEmpty()){ while(!in.isEmpty()){ out.push(in.pop()); } } } public boolean empty(){ return in.isEmpty() \u0026\u0026 out.isEmpty(); } } ","date":"2020-01-01","objectID":"/20200101_leetcode/:8:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"2. 用队列实现栈 Stack using Queues (Easy) 解题： 在将一个元素x插入队列时，为了维护原来的后进先出顺序，需要让x插入队列首部。而队列默认插入顺序是队列尾部，因此在将x插入队列尾部之后，需要让除了x之外的所有元素出队列，再入队列。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:8:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"3. 最小值栈 Min Stack (Easy) class MinStack { private Stack\u003cInteger\u003e dataStack; private Stack\u003cInteger\u003e minStack; private int min; public MinStack() { dataStack = new Stack\u003c\u003e(); minStack = new Stack\u003c\u003e(); min = Integer.MAX_VALUE; } public void push(int x) { dataStack.add(x); min = Math.min(min, x); minStack.add(min); } public void pop() { dataStack.pop(); minStack.pop(); min = minStack.isEmpty() ? Integer.MAX_VALUE : minStack.peek(); } public int top() { return dataStack.peek(); } public int getMin() { return minStack.peek(); } } 对于实现最小值队列问题，可以先将队列使用栈来实现，然后就将问题转换为最小值栈。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:8:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"4. 用栈实现括号匹配 Valid Parentheses (Easy) \"()[]{}\" Output : true public boolean isValid(String s){ Stack\u003cCharacter\u003e stack = new Stack\u003c\u003e(); for(char c : s.toCharArray()){ if(c == '(' || c == '{' || c == '['){ stack.push(c); } else { if(stack.isEmpty()){ return false; } char cStack = stack.pop(); boolean b1 = c == ')' \u0026\u0026 cStack != '('; boolean b2 = c == ']' \u0026\u0026 cStack != '['; boolean b3 = c == '}' \u0026\u0026 cStack != '{'; if(b1 || b2 || b3){ return false; } } } return stack.isEmpty(); } ","date":"2020-01-01","objectID":"/20200101_leetcode/:8:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"5. 数组中元素与下一个比它大的元素之间的距离 Daily Temperatures (Medium) Input: [73, 74, 75, 71, 69, 72, 76, 73] Output: [1, 1, 4, 2, 1, 1, 0, 0] 解题： 在遍历数组时用栈把数组中的数存起来，如果当前遍历的数比栈顶元素来的大，说明栈顶元素的下一个比它大的数就是当前元素。 public int[] dailyTemperatures(int[] temperatures){ int n = temperatures.length; int[] dist = new int[n]; Stack\u003cInteger\u003e indexs = new Stack\u003c\u003e(); for(int curIndex = 0; curIndex \u003c n; curIndex ++){ while(!indexs.isEmpty() \u0026\u0026 temperatures[curIndex] \u003e temperatures[indexs.peek()]){ int preIndex = indexs.pop(); dist[preIndex] = curIndex - preIndex; } indexs.add(curIndex); } return dist; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:8:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"6. 循环数组中比当前元素大的下一个元素 Next Greater Element II (Medium) Input: [1,2,1] Output: [2,-1,2] Explanation: The first 1's next greater number is 2; The number 2 can't find next greater number; The second 1's next greater number needs to search circularly, which is also 2. 解题： 与 Daily Temperatures (Medium) 不同的是，数组是循环数组，并且最后要求的不是距离而是下一个元素。 public int[] nextGreaterElements(int[] nums){ int n = nums.length; int[] next = new int[n]; Stack\u003cInteger\u003e pre = new Stack\u003c\u003e(); for(int i = 0; i \u003c n * 2; i++){ int num = nums[i % n]; while(!pre.isEmpty() \u0026\u0026 nums[pre.peek()] \u003c num){ next[pre.pop()] = num; } if(i \u003c n){ pre.push(i); } } return next; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:8:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 分治 ","date":"2020-01-01","objectID":"/20200101_leetcode/:9:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目一：给表达式加括号 Different Ways to Add Parentheses (Medium) Input: \"2-1-1\". ((2-1)-1) = 0 (2-(1-1)) = 2 Output : [0, 2] 解题： 利用分治算法，在顺序扫描表达式的时候，当遇到一个运算符，该运算符将整个表达式分成了两部分，表达式左右均是完整的表达式，可以对两边的表达式分开处理，分别得到两个表达式的结果数组，然后根据该运算符对两边的结果数组做相应的处理，再对整体表达式的下一个运算符做上述处理，直到所有运算符。 public List\u003cInteger\u003e diffWaysToCompute(String input) { List\u003cInteger\u003e ways = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c input.length(); i++) { char c = input.charAt(i); if (c == '+' || c == '-' || c == '*') { List\u003cInteger\u003e left = diffWaysToCompute(input.substring(0, i)); List\u003cInteger\u003e right = diffWaysToCompute(input.substring(i + 1)); for (int l : left) { for (int r : right) { switch (c) { case '+': ways.add(l + r); break; case '-': ways.add(l - r); break; case '*': ways.add(l * r); break; } } } } } if (ways.size() == 0) { ways.add(Integer.valueOf(input)); } return ways; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:9:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"题目二：不同的二叉搜索树 Unique Binary Search Trees II (Medium) 给定一个数字 n，要求生成所有值为 1…n 的二叉搜索树。 二叉搜索树：对于树中每个节点： 若其左子树存在，则其左子树中每个节点的值都不大于该节点值； 若其右子树存在，则其右子树中每个节点的值都不小于该节点值。 Input: 3 Output: [ [1,null,3,2], [3,2,null,1], [3,1,null,null,2], [2,1,3], [1,null,2,null,3] ] Explanation: The above output corresponds to the 5 unique BST's shown below: 1 3 3 2 1 \\ / / / \\ \\ 3 2 1 1 3 2 / / \\ \\ 2 1 2 3 解题： public List\u003cTreeNode\u003e generateTrees(int n) { if (n \u003c 1) { return new LinkedList\u003cTreeNode\u003e(); } return generateSubtrees(1, n); } private List\u003cTreeNode\u003e generateSubtrees(int s, int e) { List\u003cTreeNode\u003e res = new LinkedList\u003cTreeNode\u003e(); if (s \u003e e) { res.add(null); return res; } for (int i = s; i \u003c= e; ++i) { List\u003cTreeNode\u003e leftSubtrees = generateSubtrees(s, i - 1); List\u003cTreeNode\u003e rightSubtrees = generateSubtrees(i + 1, e); for (TreeNode left : leftSubtrees) { for (TreeNode right : rightSubtrees) { TreeNode root = new TreeNode(i); root.left = left; root.right = right; res.add(root); } } } return res; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:9:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 动态规划 递归和动态规划的区别 都是将原问题拆成多个子问题然后求解，他们之间最本质的区别是，动态规划保存了子问题的解，避免重复计算。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:10:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"斐波那契数列 题目一：爬楼梯 Climbing Stairs (Easy) 题目描述：有 N 阶楼梯，每次可以上一阶或者两阶，求有多少种上楼梯的方法。 解题： 定义一个数组 dp 存储上楼梯的方法数（为了方便讨论，数组下标从 1 开始），dp[i] 表示走到第 i 个楼梯的方法数目。 第 i 个楼梯可以从第 i-1 和 i-2 个楼梯再走一步到达，走到第 i 个楼梯的方法数为走到第 i-1 和第 i-2 个楼梯的方法数之和。 dp[i] = dp[i - 1] + dp[i - 2] 考虑到 dp[i] 只与 dp[i - 1] 和 dp[i - 2] 有关，因此可以只用两个变量来存储 dp[i - 1] 和 dp[i - 2]，使得原来的 O(N) 空间复杂度优化为 O(1) 复杂度。 public int climbStairs(int n){ if(n \u003c= 2){ return n; } int pre2 = 1, pre1 = 2; for(int i = 2;i \u003c n;i++){ int cur = pre1 + pre2; pre2 = pre1; pre1 = cur; } return pre1; } 题目二：强盗抢劫 House Robber (Easy) 题目描述：抢劫一排住户，但是不能抢邻近的住户，求最大抢劫量。 解题： ","date":"2020-01-01","objectID":"/20200101_leetcode/:10:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"图 图的描述方式: public class Node { public int value; //点的编号 public int in; //入度的个数 public int out; //出度的个数 public ArrayList\u003cNode\u003e nexts; //邻居点 public ArrayList\u003cEdge\u003e edges; //邻居边 public Node(int =value) { this.value = value; in = 0; out = 0; nexts = new ArrayList\u003c\u003e(); edges = new ArrayList\u003c\u003e(); } } public class Edge { public int weight; public Node from; public Node to; public Edge(int weight, Node from, Node to) { this.weight = weight; this.from = from; this.to = to; } } public class Graph { public HashMap\u003cInteger, Node\u003e nodes; //点集 public HashSet\u003cEdge\u003e edges; //边集 public Graph() { nodes = new HashMap\u003c\u003e(); edges = new HashSet\u003c\u003e(); } } 图的宽度优先遍历： 利用队列实现 从源节点开始依次按照宽度进队列，然后弹出 每弹出一个点，把该节点所有没有进过队列的邻接点放入队列 直到队列变空 public static void bfs(Node node){ if (node == null){ return; } Queue\u003cNode\u003e queue = new LinkedList\u003c\u003e(); HashSet\u003cNode\u003e set = new HashSet\u003c\u003e(); //放入已处理的节点，检查点是否重复。 queue.add(node); set.add(node); while(!queue.isEmpty()){ Node cur = queue.poll(); System.out.println(cur.value); for(Node next : cur.nexts){ if(!set.contains(nexts)){ set.add(next); queue.add(next); } } } } 图的广度优先遍历： 利用栈实现 从源节点开始依次按照深度进队列，然后弹出 每弹出一个点，把该节点所有没有进过队列的邻接点放入队列 直到队列变空 public static void dfs(Node node){ if (node == null){ return; } Stack\u003cNode\u003e stack = new Stack\u003c\u003e(); HashSet\u003cNode\u003e set = new HashSet\u003c\u003e(); stack.add(node); set.add(node); System.out.println(node.value); while(!stack.isEmpty()){ Node cur = stack.pop(); for(Node next : cur.nexts){ if(!set.contains(next)){ stack.push(cur); stack.push(next); set.add(next); System.out.println(next.value); break; } } } } 拓扑排序问题： 实际问题：编译依赖是个有向无环图。如何决定编译顺序。 方法：找到入度为0的点输出，然后擦掉该节点和该节点边，再找到入度为0点输出。以此往复。 public static List\u003cNode\u003e sortedTopoloty(Graph graph){ HashMap\u003cNode, Integer\u003e inMap = new HashMap\u003c\u003e(); // key: 某一个Node； value：剩余的入度 Queue\u003cNode\u003e zeroInQueue = new LinkedList\u003c\u003e(); // 入度为0放入这个队列。 for(Node node: graph.nodes.values()){ inMap.put(node, node.in); if(node.in == 0){ zeroInQueue.add(node); } } // 拓扑排序结果，依次加入result List\u003cNode\u003e result = new ArrayList\u003c\u003e(); while(!zeroInQueue.isEmpty()){ Node cur = zeroInQueue.poll(); result.add(cur); for(Node next : cur.nexts){ inMap.put(next, inMap.get(next) - 1); if(inMap.get(next) == 0){ zeroInQueue.add(next); } } } return result; } kruskal算法：适用范围要求无向图 prim算法：适用范围要求无向图 Dijkstra算法：适用范围没有权值为负数的边 ","date":"2020-01-01","objectID":"/20200101_leetcode/:11:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"暴力递归 ","date":"2020-01-01","objectID":"/20200101_leetcode/:12:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"哈希函数 问题：2的32次方个由 0 - 2 的32次方组成的随机数，用1G内存，返回出现次数最多的数 如果直接使用哈希表 {key ：次数}，会占用8 * 2 32 个字节（32G）。 可以将每个数算出哈希值后%100 后存入哈希表，这样 2 32 个 key：value 变成了一百个key：value。 再将次数最多的 key 进行统计。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:13:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"布隆过滤器 解决问题： 黑名单系统（100亿 URL 组成的黑名单，每次访问要判断是否在黑名单中，用 HashSet 则占用 640G 内存） 爬虫去重问题等 （使用1000个线程爬虫，不希望爬已经爬过的网站，需要把已经爬过的网站做成集合） 功能： 只有添加和搜索功能，没有删除功能 使用很少的内存，允许有一定的失误 位图代码： int [] arr = new int[10]; //每个 int 4个字节32位，10个int数组可以表示 32bit * 10 -\u003e 320bit //例：想拿到第178位的状态 int numIndex = 178 / 32; int bitIndex = 178 % 32; //拿到178位的状态 int s = ((arr[numIndex] \u003e\u003e (bitIndex)) \u0026 1); //把178位的状态改成1 arr[numIndex] = arr[numIndex] | (1 \u003c\u003c (bitIndex)); //把178位的状态改成0 arr[numIndex] = arr[numIndex] \u0026 (~ (1 \u003c\u003c (bitIndex))); 布隆过滤器逻辑： 生成布隆过滤器： //1. 生成 m 长度的位图 //2. 加入黑名单：URL1 通过k个 hash 函数算出k个 hash 值 %m ，确立k个位数值位 1 。更新到位图上 //3. 此时位图已记录 URL1 。以此重复, 已经为1继续为1，直到写入 100亿 URL 查询： // URL 通过k个 hash 函数算出k个 hash 值 %m，在位图上验证，都为 1 则满足 m和k值如何确定： // m 越大越好，k 过大过小都不好。有计算公式 ","date":"2020-01-01","objectID":"/20200101_leetcode/:14:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"一致性哈希 解决问题： 数据服务器如何组织，解决分布式数据库负载均衡，高低频查询。 传统分布式存储： 海量数据分布式存储：数据通过key值算出哈希值 % 机器数量 决定数据存放服务器节点。 当服务器硬盘大小不够，需要增加服务器。这时需要数据全量迁移，而且重新将hash值 % 新的服务器数量 通过一致性哈希存储保证不 % ，并减少迁移代价： 如有 3 台机器分布式存储数据。通过 MD5 作哈希值。 将1 ~ 264-1 想象成一个环，将 [0 , 264 / 3] 分给节点1，[264 / 3 , 264 / 3 * 2] 分给节点2，[264 / 3 * 2 , 264] 分给节点3。 当增加节点时，比如节点放在节点2和节点3正中间。只用从节点2上移动 264 / 3 / 2 个数据到新节点上。 问题1：开始如何均分数据 问题2：增加减少节点导致负载不均衡 虚拟节点技术 ：按比例去抢环，增加节点时，按比例从每个节点拿数据放到新节点。删除节点也按比例分配到别的节点。 一致性哈希按比例同时可以解决：性能强的机器数据量大，性能弱的机器数据少。 【题目】: 岛问题 【题目】矩阵中只有0和1两种值，每个位置都可以和自己的上下左右四个位置相连，相邻的一片1叫做一个岛，求一个矩阵中有多少个岛？ 001010 111010 100100 000000 \u003e 这个矩阵中有三个岛 【解题】顺序遍历节点，遇到1则进入感染函数，将与该1一起的岛改成2，岛数++。 感染函数： public static void infect (int[][]m, int i, int j, int N, int M){ //(i,j)位置 N,M矩形长宽 if(i \u003c 0 || i \u003e= N || j \u003c 0 || j \u003e= M || m[i][j] != 1){ //i,j没越界，并且当前位置是1 return; } m[i][j] = 2; infect(m, i + 1, j, N, M); infect(m, i - 1, j, N, M); infect(m, i, j + 1, N, M); infect(m, i, j - 1, N, M); } 时间复杂度：O(N * M) 【进阶】如何设计一个并行算法解决这个问题 并查集： 解决两个集合查重与合并。使用 链表 和 hashSet 效率都不高。链表 合并的复杂度是 O(1)，查重是 O(n2)。使用 hashSet 查重 复杂度是 O(n)，合并 复杂度是 O(n)。 //找到头结点函数：判断集合是否是一个集合只用判断头结点是否相同。 //找头结点时同时优化：将该节点到头节点路径上的节点拍平，直接插到头结点上。当调用 findHead 函数次数越多，时间复杂度越接近 O(1) private Element\u003cV\u003e findHead(Element\u003cV\u003e element){ Stack\u003cElement\u003cV\u003e\u003e path = new Stack\u003c\u003e(); while (element != fatherMap.get(element)){ path.push(element); element = fatherMap.get(element); } while (!path.isEmpty()){ fatherMap.put(path.pop(), element); } return element; } //判断是否为同一集合函数 public boolean isSameSet(V a, V b){ if (elementMap.containsKey(a) \u0026\u0026 elementMap.containsKey(b)){ return findHead(elementMap.get(a)) == findHead(elementMap.get(b)); } return false; } //合并集合函数, 小集合插到大集合尾部 public void union(V a, V b){ if (elementMap.containsKey(a) \u0026\u0026 elementMap.containsKey(b)){ ... } } 并行解决方案： 如有两个线程，将岛图切分成 2 块，分别进行顺序遍历感染。这样统计出分散的岛视为不同的集合，将这些集合进行并查集合并操作，算出合并后的岛数。 ","date":"2020-01-01","objectID":"/20200101_leetcode/:15:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 贪心思想 笔试面试都很少出现 概念：保证每次操作都是局部最优的，并且最后得到的结果是全局最优的。 因为变化太多，笔试面试都很少出现。 题目一：分配饼干 Assign Cookies (Easy) Input: grid[1,3], size[1,2,4] Output: 2 题目描述：每个孩子都有一个满足度 grid，每个饼干都有一个大小 size，只有饼干的大小大于等于一个孩子的满足度，该孩子才会获得满足。求解最多可以获得满足的孩子数量。 解题： public int findContentChildren(int[] grid, int[] size) { if (grid == null || size == null) return 0; Arrays.sort(grid); Arrays.sort(size); int gi = 0, si = 0; while (gi \u003c grid.length \u0026\u0026 si \u003c size.length) { if (grid[gi] \u003c= size[si]) { gi++; } si++; } return gi; } ","date":"2020-01-01","objectID":"/20200101_leetcode/:16:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"SQL 题目一：登录日志，计算每个人连续登录的最大天数(断一天也算连续) val tx_sql = spark.createDataFrame(Seq( (\"1002\",\"2021-08-01\"), (\"1002\",\"2021-08-02\"), (\"1002\",\"2021-08-04\"), (\"1002\",\"2021-08-05\"), (\"1002\",\"2021-08-06\"), (\"1003\",\"2021-08-01\"), (\"1003\",\"2021-08-04\"), (\"1003\",\"2021-08-05\") )).toDF(\"id\",\"time\") tx_sql.createOrReplaceTempView(\"tx_sql\") 思路一：等差数列 开窗函数，按照id分组同时按照时间排序，求rank select id,time,rank() over(partition by id order by time) rk from tx_sql 将每行日期减去rk值，如果之前是连续的日期，则相减之后为相同日期 select if,time,data_sub(time, rk) flag from (select id,time,rank() over(partition by id order by time) rk from tx_sql) t1 计算绝对连续的天数，相同的日期个数即为连续天数 select id,flag,count(*) days from (select id,time,date_sub(time, rk) flag from (select id,time,rank() over(partition by id order by time) rk from tx_sql)t1)t2 group by id,flag 再计算连续问题，使用相同方法再计算一次 select id,flag,days,rank() over(partition by id order by flag) newFlag from ()t3 日期差1则可以相加 select id,flag,sum(days)+count(*)-1 days from t5 group by id,flag 取最大值 思路二：数据 lag 下移做差值 将每行数据下移 select id,time,lag(1,time,'1970-01-01') over(partition by id order by time) lagDt from tx 做差值 select id,time,datediff(time,lagDt) dtDiff from t1 差值为 1或2 都视为连续，大于2则为断开，分到另一组 select id,time,sum(if(dtDiff \u003e 2, 1, 0)) over(partition by id order by time) flag from t2 计算每一组的最大最小值差加一 select id,flag,datediff(max(dt),min(dt)) + 1 from t3 group by id,flag 求连续天数的最大值 TIPS HiveOnSpark bug: Datediff over 子查询 =\u003e Nullpoint 解决方案： 换 MR 引擎 将时间字段由 String 类型改为 Date 类型 ","date":"2020-01-01","objectID":"/20200101_leetcode/:17:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200101_leetcode/"},{"categories":["Technology"],"content":"Article description.","date":"2019-12-29","objectID":"/20191229_copy-in-java/","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Cloning is a process of creating an exact copy of an existing object in the memory. In java, clone() method of java.lang.Object class is used for cloning process. This method creates an exact copy of an object on which it is called through field-by-field assignment and returns the reference of that object. Not all the objects in java are eligible for cloning process. The objects which implement Cloneable interface are only eligible for cloning process. Cloneable interface is a marker interface which is used to provide the marker to cloning process. Click here to see more info on clone() method in java. ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:0:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Preface Object 类中有方法clion()，具体方法如下： protected native Object clone() throws CloneNotSupportedException; 该方法由 protected 修饰，java中所有类默认是继承Object类的，重载后的clone()方法为了保证其他类都可以正常调用，修饰符需要改成public。 该方法是一个native方法，被native修饰的方法实际上是由非Java代码实现的，效率要高于普通的java方法。 该方法的返回值是Object对象，因此我们需要强转成我们需要的类型。 该方法抛出了一个CloneNotSupportedException异常，意思就是不支持拷贝，需要我们实现Cloneable接口来标记，这个类支持拷贝。 为了演示，我们新建两个实体类Dept 和 User，其中User依赖了Dept，实体类代码如下： Dept 类： @Data @AllArgsConstructor @NoArgsConstructor public class Dept { private int deptNo; private String name; } User 类： @Data @AllArgsConstructor @NoArgsConstructor public class User { private int age; private String name; private Dept dept; } ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:1:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Shallow Copy 对于基本类型的的属性，浅拷贝会将属性值复制给新的对象，而对于引用类型的属性，浅拷贝会将引用复制给新的对象。而像String，Integer这些引用类型，都是不可变的，拷贝的时候会创建一份新的内存空间来存放值，并且将新的引用指向新的内存空间。不可变类型是特殊的引用类型，我们姑且认为这些被final标记的引用类型也是复制值。 shallowCopy 浅拷贝功能实现 @Data @AllArgsConstructor @NoArgsConstructor public class User implements Cloneable{ private int age; private String name; private Dept dept; @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } } 如何验证我们的结论呢？首先对比被拷贝出的对象和原对象是否相等，不等则说明是新拷贝出的一个对象。其次修改拷贝出对象的基本类型属性，如果原对象的此属性发生了修改，则说明基本类型的属性是同一个，最后修改拷贝出对象的引用类型对象即Dept属性，如果原对象的此属性发生了改变，则说明引用类型的属性是同一个。清楚测试原理后，我们写一段测试代码来验证我们的结论。 public static void main(Strign[] args) thows Exception{ Dept dept = new Dept(12,\"市场部\"); User user = new User(18,\"Java\", dept); User user1 = (User)user.clone(); System.out.println(user == user1); user1.setAge(20); System.out.println(user); System.out.println(user1); dept.setName(\"研发部\"); System.out.println(user); System.out.println(user1); } 运行结果如下： false User{age=18, name='Java', dept=Dept{deptNo=12, name='市场部'}} User{age=20, name='Java', dept=Dept{deptNo=12, name='市场部'}} User{age=18, name='Java', dept=Dept{deptNo=12, name='研发部'}} User{age=20, name='Java', dept=Dept{deptNo=12, name='研发部'}} ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:2:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Deep Copy 相较于浅拷贝而言，深拷贝除了会将基本类型的属性复制外，还会将引用类型的属性也会复制。 DeepCopy 深拷贝功能实现 在拷贝user的时候，同事将user中的dept属性进行拷贝。 dept 类： @Data @AllArgsConstructor @NoArgsXonstructor public class Dept implements Cloneable{ private int deptNo; private String name; @Override public Object clone() throws CloneNotSupportedException{ return super.clone(); } } user 类： @Data @AllArgsConstructor @NoArgsConstructor public class User implements Cloneable{ private int age; private String name; private Dept dept; @Override protected Object clone() throws CloneNotSupportedException { User user = (User) super.clone(); user.dept = (Dept) dept.clone(); return user; } } 使用浅拷贝的测试代码继续测试，运行结果如下： false User{age=18, name='Java旅途', dept=Dept{deptNo=12, name='市场部'}} User{age=20, name='Java旅途', dept=Dept{deptNo=12, name='市场部'}} User{age=18, name='Java旅途', dept=Dept{deptNo=12, name='研发部'}} User{age=20, name='Java旅途', dept=Dept{deptNo=12, name='市场部'}} 除此之外，还可以利用反序列化实现深拷贝，先将对象序列化成字节流，然后再将字节流序列化成对象，这样就会产生一个新的对象。 ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:3:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Life"],"content":"Article description.","date":"2019-12-18","objectID":"/20191218_2019-summary-video/","tags":["Life","Video","USA","Graduation"],"title":"Memories of 2019","uri":"/20191218_2019-summary-video/"},{"categories":["Life"],"content":"Photos and videos recorded in 2019. ","date":"2019-12-18","objectID":"/20191218_2019-summary-video/:0:0","tags":["Life","Video","USA","Graduation"],"title":"Memories of 2019","uri":"/20191218_2019-summary-video/"},{"categories":["Technology"],"content":"Article description.","date":"2019-11-26","objectID":"/20191126_git-contribution/","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Contribution graph shows activity from public repositories. You can choose to show activity from both public and private repositories, with specific details of your activity in private repositories anonymized. ","date":"2019-11-26","objectID":"/20191126_git-contribution/:0:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Update local git config git config --global user.name “github’s Name” git config --global user.email \"github@*.com\" ","date":"2019-11-26","objectID":"/20191126_git-contribution/:1:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Update commit history If you do not want to waste your commit history. we can use ‘git log’ to see the git record git log we need to edit all of the history of commit and push. git filter-branch -f --env-filter ' if [ \"$GIT_AUTHOR_NAME\" = \"oldName\" ] then export GIT_AUTHOR_NAME=\"newName\" export GIT_AUTHOR_EMAIL=\"newEmail\" fi ' HEAD git filter-branch -f --env-filter ' if [ \"$GIT_COMMITTER_NAME\" = \"oldName\" ] then export GIT_COMMITTER_NAME=\"newName\" export GIT_COMMITTER_EMAIL=\"newEmail\" fi ' HEAD 如果无差别把所有都改的话去掉if..fi git filter-branch -f --env-filter \" GIT_AUTHOR_NAME='newName'; GIT_AUTHOR_EMAIL='newEmail'; GIT_COMMITTER_NAME='newName'; GIT_COMMITTER_EMAIL='newEmail' \" HEAD ","date":"2019-11-26","objectID":"/20191126_git-contribution/:2:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Update Git push 你这里将你本地git的账户和邮箱重新设置了,但是github并没有那么智能就能判断你是原来你系统默认的用户. 也就是说你新配置的用户和你默认的被github识别成两个用户. 这样你以后操作的时候commit 或者 push的时候有可能产生冲突. Solution: 使用强制push的方法: git push -u origin master -f 这样会使远程修改丢失，一般是不可取的，尤其是多人协作开发的时候。 push前先将远程repository修改pull下来 git pull origin master git push -u origin master 若不想merge远程和本地修改，可以先创建新的分支： git branch [name] #然后push git push -u origin [name] ","date":"2019-11-26","objectID":"/20191126_git-contribution/:3:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Article description.","date":"2019-10-07","objectID":"/20191007_u32/","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"C/C++ provides various data types that can be used in your programs.In general, you’d commonly use: int for most variables and “countable” things (for loop counts, variables, events). char for characters and strings. float for general measurable things (seconds, distance, temperature). uint32 for bit manipulations, especially on 32-bit registers. int for most variables and “countable” things (for loop counts, variables, events) char for characters and strings float for general measurable things (seconds, distance, temperature) uint32_t for bit manipulations, especially on 32-bit registers ","date":"2019-10-07","objectID":"/20191007_u32/:0:0","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Integer Data Types C type alias Bits Sign Range char int8 8 Signed -128 .. 127 unsigned char uint8 8 Unsigned 0 .. 255 short int16 16 Signed -32,768 .. 32,767 unsigned short uint16 16 Unsigned 0 .. 65,535 int int32 32 Signed -2,147,483,648 .. 2,147,483,647 unsigned int uint32 32 Unsigned 0 .. 4,294,967,295 long long int64 64 Signed -9,223,372,036,854,775,808 .. 9,223,372,036,854,775,807 unsigned long long uint64 64 Unsigned 0 .. 18,446,744,073,709,551,615 ","date":"2019-10-07","objectID":"/20191007_u32/:0:1","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Floating Point Data Types C type IEE754 Name Bits Range float Single Precision 32 -3.4E38 .. 3.4E38 double Double Precision 64 -1.7E308 .. 1.7E308 ","date":"2019-10-07","objectID":"/20191007_u32/:0:2","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Print format %[flags][width][.prec][length]type %[标志][最小宽度][.精度][类型长度]类型。 1. Type 字符 对应数据类型 含义 示例 d/i int 输出十进制有符号32bits整数，i是老式写法 printf(\"%i\",123);输出123 o unsigned int 无符号8进制(octal)整数(不输出前缀0) printf(\"0%o\",123);输出0173 u unsigned int 无符号10进制整数 printf(\"%u\",123);输出123 x/X unsigned int 无符号16进制整数，x对应的是abcdef，X对应的是ABCDEF（不输出前缀0x) printf(\"0x%x 0x%X\",123,123);输出0x7b 0x7B f/lf float(double) 单精度浮点数用f,双精度浮点数用lf(printf可混用，但scanf不能混用) printf(\"%.9f %.9lf\",0.000000123,0.000000123);输出0.000000123 0.000000123。注意指定精度，否则printf默认精确到小数点后六位 F float(double) 与f格式相同，只不过 infinity 和 nan 输出为大写形式。 例如printf(\"%f %F %f %F\\n\",INFINITY,INFINITY,NAN,NAN);输出结果为inf INF nan NAN e/E float(double) 科学计数法，使用指数(Exponent)表示浮点数，此处”e”的大小写代表在输出时“e”的大小写 printf(\"%e %E\",0.000000123,0.000000123);输出1.230000e-07 1.230000E-07 g float(double) 根据数值的长度，选择以最短的方式输出，%f或%e printf(\"%g %g\",0.000000123,0.123);输出1.23e-07 0.123 G float(double) 根据数值的长度，选择以最短的方式输出，%f或%E printf(\"%G %G\",0.000000123,0.123);输出1.23E-07 0.123 c char 字符型。可以把输入的数字按照ASCII码相应转换为对应的字符 printf(\"%c\\n\",64)输出A s char* 字符串。输出字符串中的字符直至字符串中的空字符（字符串以空字符’\\0‘结尾） printf(\"%s\",\"测试test\");输出：测试test S wchar_t* 宽字符串。输出字符串中的字符直至字符串中的空字符（宽字符串以两个空字符’\\0‘结尾） setlocale(LC_ALL,\"zh_CN.UTF-8\"); wchar_t wtest[]=L\"测试Test\"; printf(\"%S\\n\",wtest); 输出：测试test p void* 以16进制形式输出指针 printf(\"%010p\",\"lvlv\");输出：0x004007e6 n int* 什么也不输出。%n对应的参数是一个指向signed int的指针，在此之前输出的字符数将存储到指针所指的位置 int num=0; printf(\"lvlv%n\",\u0026num); printf(\"num:%d\",num); 输出:lvlvnum:4 m 无 打印errno值对应的出错内容 printf(\"%m\\n\"); a/A float(double) 十六进制p计数法输出浮点数，a为小写，A为大写 printf(\"%a %A\",15.15,15.15);输出：0x1.e4ccccccccccdp+3 0X1.E4CCCCCCCCCCDP+3 2. Flags 字符 名称 说明 - 减号 结果左对齐，右边填空格。默认是右对齐，左边填空格。 + 加号 输出符号(正号或负号) space 空格 输出值为正时加上空格，为负时加上负号 # 井号 type是o、x、X时，增加前缀0、0x、0X。 type是a、A、e、E、f、g、G时，一定使用小数点。默认的，如果使用.0控制不输出小数部分，则不输出小数点。 type是g、G时，尾部的0保留。 0 数字零 将输出的前面补上0，直到占满指定列宽为止（不可以搭配使用“-”） example: printf(\"%5d\\n\",1000); //默认右对齐,左边补空格 //output:1000 printf(\"%-5d\\n\",1000); //左对齐,右边补空格 //output:1000 printf(\"%+d %+d\\n\",1000,-1000); //输出正负号 //output:+1000 -1000 printf(\"% d % d\\n\",1000,-1000); //正号用空格替代，负号输出 //output:1000 -1000 printf(\"%x %#x\\n\",1000,1000); //输出0x //output:3e8 0x3e8 printf(\"%.0f %#.0f\\n\",1000.0,1000.0)//当小数点后不输出值时依然输出小数点 //output:1000 1000. printf(\"%g %#g\\n\",1000.0,1000.0); //保留小数点后后的0 //output:1000 1000.00 printf(\"%05d\\n\",1000); //前面补0 //01000 3. width 输出最小宽度 width 描述 示例 数值 十进制整数 printf(\"%06d\",1000);输出:001000 * 星号。不显示指明输出最小宽度，而是以星号代替，在printf的输出参数列表中给出 printf(\"%0*d\",6,1000);输出:001000 4. precision .precision 描述 .数值 十进制整数。 (1)对于整型（d,i,o,u,x,X）,precision表示输出的最小的数字个数，不足补前导零，超过不截断。(2)对于浮点型（a, A, e, E, f ），precision表示小数点后数值位数，默认为六位，不足补后置0，超过则截断。 (3)对于类型说明符g或G，表示可输出的最大有效数字。 (4)对于字符串（s），precision表示最大可输出字符数，不足正常输出，超过则截断。 precision不显示指定，则默认为0 .* 以星号代替数值，类似于width中的*，在输出参数列表中指定精度。 5. length 类型长度指明待输出数据的长度。因为相同类型可以有不同的长度，比如整型有16bits的short int，32bits的int，也有64bits的long int，浮点型有32bits的单精度float和64bits的双精度double。为了指明同一类型的不同长度，于是乎，类型长度（length）应运而生，成为格式控制字符串的一部分。 typeTable ","date":"2019-10-07","objectID":"/20191007_u32/:0:3","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Article description.","date":"2019-08-25","objectID":"/20190825_struct-size/","tags":["C"],"title":"The size of structure in C","uri":"/20190825_struct-size/"},{"categories":["Technology"],"content":"The sizeof for a struct is not always equal to the sum of sizeof of each individual member. This is because of the padding added by the compiler to avoid alignment issues. Padding is only added when a structure member is followed by a member with a larger size or at the end of the structure. Different compilers might have different alignment constraints as C standards state that alignment of structure totally depends on the implementation. Case 1: struct A { // sizeof(int) = 4 int x; // Padding of 4 bytes // sizeof(double) = 8 double z; // sizeof(short int) = 2 short int y; // Padding of 6 bytes }; Output: ​ Size of struct: 24 struct_sizeof_ex1 The red portion represents the padding added for data alignment and the green portion represents the struct members. In this case, x (int) is followed by z (double), which is larger in size as compared to x. Hence padding is added after x. Also, padding is needed at the end for data alignment. Case 2: struct B { // sizeof(double) = 8 double z; // sizeof(int) = 4 int x; // sizeof(short int) = 2 short int y; // Padding of 2 bytes }; Output: ​ Size of struct: 16 struct_sizeof_ex2 In this case, the members of the structure are sorted in decreasing order of their sizes. Hence padding is required only at the end. Case 3: struct C { // sizeof(double) = 8 double z; // sizeof(short int) = 2 short int y; // Padding of 2 bytes // sizeof(int) = 4 int x; }; Output: ​ Size of struct: 16 struct_sizeof_ex3 In this case, y (short int) is followed by x (int) and hence padding is required after y. No padding is needed at the end in this case for data alignment. C language doesn’t allow the compilers to reorder the struct members to reduce the amount of padding. In order to minimize the amount of padding, the struct members must be sorted in a descending order (similar to the case 2). ","date":"2019-08-25","objectID":"/20190825_struct-size/:0:0","tags":["C"],"title":"The size of structure in C","uri":"/20190825_struct-size/"},{"categories":["Technology"],"content":"Article description.","date":"2019-07-17","objectID":"/20190717_typedef/","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"A typedef is a C keyword that defines a new name for a data type, including internal data types (int, char, etc.) and custom data types (struct, etc.). A typedef is itself a type of stored class keyword that cannot appear in the same expression as the keywords auto, extern, static, register, etc. ","date":"2019-07-17","objectID":"/20190717_typedef/:0:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"1. 概述 typedef为C语言的关键字，作用是为一种数据类型定义一个新名字，这里的数据类型包括内部数据类型（int，char等）和自定义的数据类型（struct等）。 typedef本身是一种存储类的关键字，与auto、extern、static、register等关键字不能出现在同一个表达式中。 ","date":"2019-07-17","objectID":"/20190717_typedef/:1:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"2. 作用及用法 ","date":"2019-07-17","objectID":"/20190717_typedef/:2:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"2.1 typedef的用法 使用typedef定义新类型的方法：在传统的变量声明表达式里用（新的）类型名替换变量名，然后把关键字typedef加在该语句的开头就行了。 下面以两个示例，描述typedef的用法步骤。 示例1： int a; ———— 传统变量声明表达式 typedef int myint_t; ———— 使用新的类型名myint_t替换变量名a。在语句开头加上typedef关键字，myint_t就是我们定义的新类型 示例2： void (*pfunA)(int a); ———— 传统变量（函数）声明表达式 typedef void (*PFUNA)(int a); ———— 使用新的类型名PFUNA替换变量名pfunA。在语句开头加上typedef关键字，PFUNA就是我们定义的新类型 ","date":"2019-07-17","objectID":"/20190717_typedef/:2:1","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"2.2 typedef的作用 typedef的作用有以下几点： 1）typedef的一个重要用途是定义机器无关的类型。例如，定义一个叫REAL的浮点类型，该浮点类型在目标机器上可以获得最高的精度： typedef long double REAL; 如果在不支持 long double 的机器上运行相关代码，只需要对对应的typedef语句进行修改，例如： typedef double REAL; 或者： typedef float REAL; 2）使用typedef为现有类型创建别名，给变量定义一个易于记忆且意义明确的新名字。 例如: typedef unsigned int UINT 3）使用typedef简化一些比较复杂的类型声明。 例如： typedef void (*PFunCallBack)(char* pMsg, unsigned int nMsgLen); 上述声明引入了PFunCallBack类型作为函数指针的同义字，该函数有两个类型分别为char*和unsigned int参数，以及一个类型为int的返回值。通常，当某个函数的参数是一个回调函数时，可能会用到typedef简化声明。 例如，承接上面的示例，我们再列举下列示例： RedisSubCommand(const string\u0026 strKey, PFunCallBack pFunCallback, bool bOnlyOne); 注意：类型名PFunCallBack与变量名pFunCallback的大小写区别。 RedisSubCommand函数的参数是一个PFunCallBack类型的回调函数，返回某个函数（pFunCallback）的地址。在这个示例中，如果不用typedef，RedisSubCommand函数声明如下： RedisSubCommand(const string\u0026 strKey, void (*pFunCallback)(char* pMsg, unsigned int nMsgLen), bool bOnlyOne); 从上面两条函数声明可以看出，不使用typedef的情况下，RedisSubCommand函数的声明复杂得多，不利于代码的理解，并且增加的出错风险。 所以，在某些复杂的类型声明中，使用typedef进行声明的简化是很有必要的。 ","date":"2019-07-17","objectID":"/20190717_typedef/:2:2","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"3. typedef与#define 两者的区别如下： #define进行简单的进行字符串替换。 #define宏定义可以使用#ifdef、#ifndef等来进行逻辑判断，还可以使用#undef来取消定义。 typedef是为一个类型起新名字。typedef符合（C语言）范围规则，使用typedef定义的变量类型，其作用范围限制在所定义的函数或者文件内（取决于此变量定义的位置），而宏定义则没有这种特性。 通常，使用typedef要比使用#define要好，特别是在有指针的场合里。 下面列举几个示例。 3.1 示例1 代码如下： typedef　char*　pStr1; #define　pStr2　char*　pStr1　s1, s2; pStr2　s3, s4; 在上述的变量定义中，s1、s2、s3都被定义为char类型；但是s4则定义成了char类型，而不是我们所预期的指针变量char，这是因为#define只做简单的字符串替换，替换后的相关代码等同于为： char*　s3, s4; 而使用typedef为char*定义了新类型pStr1后，相关代码等同于为： char *s3, *s4; 3.1 示例2 代码如下： typedef char *pStr; char string[5]=\"test\"; const char *p1=string; const pStr p2=string; p1++; p2++; error:increment of read-only variable 'p2' 根据错误信息，能够看出p2为只读的常量了，所以p2++出错了。这个问题再一次提醒我们：typedef和#define不同，typedef不是简单的文本替换，上述代码中const pStr p2并不等于const char * p2，pStr是作为一个类型存在的，所以const pStr p2实际上限制了pStr类型的p2变量，对p2常量进行了只读限制。也就是说，const pStr p2和pStr const p2本质上没有区别（可类比const int p2和int const p2），都是对变量p2进行只读限制，只不过此处变量p2的数据类型是我们自己定义的（pStr），而不是系统固有类型（如int）而已。 所以，const pStr p2的含义是：限定数据类型为char *的变量p2为只读，因此p2++错误。 注意：在本示例中，typedef定义的新类型与编译系统固有的类型没有差别。 ","date":"2019-07-17","objectID":"/20190717_typedef/:3:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"Article description.","date":"2019-06-11","objectID":"/20190611_conditional-compilation/","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"Conditional compilation is static compilation of code based on the actual definition of macros (some kind of condition). Compilation conditions can be determined based on the value of an expression or whether a particular macro is defined. C language conditional compilation related precompilation instructions, including #define, #undef, #ifdef, #ifndef, #if, #elif, #else, #endif, defined. ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:0:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"预编译指令 #define 定义一个预处理宏 #undef 取消宏的定义 #if 编译预处理中的条件命令，相当于C语法中的if语句 #ifdef 判断某个宏是否被定义，若已定义，执行随后的语句 #ifndef 与#ifdef相反，判断某个宏是否未被定义 #elif 若#if, #ifdef, #ifndef或前面的#elif条件不满足，则执行#elif之后的语句，相当于C语法中的else-if #else 与#if, #ifdef, #ifndef对应, 若这些条件不满足，则执行#else之后的语句，相当于C语法中的else #endif #if, #ifdef, #ifndef这些条件命令的结束标志. #defined 与#if, #elif配合使用，判断某个宏是否被定义 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:1:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"条件编译 条件编译是根据实际定义宏（某类条件）进行代码静态编译的手段。可根据表达式的值或某个特定宏是否被定义来确定编译条件。 最常见的条件编译是防止重复包含头文件的宏，形式跟下面代码类似： #ifndef ABCD_H #define ABCD_H // ... some declaration codes #endif // #ifndef ABCD_H 在实现文件中通常有如下类似的定义： #ifdef _DEBUG // ... do some operations #endif #ifdef _WIN32 // ... use Win32 API #endif ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:2:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"预编译指令应用举例 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"1. #define、#undef #define命令定义一个宏: #define MACRO_NAME[(args)] [tokens[(opt)]] 之后出现的MACRO_NAME将被替代为所定义的标记(tokens)。宏可带参数，而后面的标记也是可选的。 宏定义，按照是否带参数通常分为对象宏、函数宏两种。 对象宏: 不带参数的宏被称为\"对象宏(objectlike macro)\"。对象宏多用于定义常量、通用标识。例如： // 常量定义 #define MAX_LENGTH 100 // 通用标识，日志输出宏 #define SLog printf // 预编译宏 #define _DEBUG 函数宏：带参数的宏。利用宏可以提高代码的运行效率: 子程序的调用需要压栈出栈, 这一过程如果过于频繁会耗费掉大量的CPU运算资源。 所以一些代码量小但运行频繁的代码如果采用带参数宏来实现会提高代码的运行效率。但多数c++程序不推荐使用函数宏，调试上有一定难度，可考虑使用c++的inline代替之。例如： // 最小值函数 #define MIN(a,b) ((a)\u003e(b)? (a):(b)) // 安全释放内存函数 #define SAFE_DELETE(p) {if(NULL!=p){delete p; p = NULL;}} #undef可以取消宏定义，与#define对应。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:1","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"2. defined defined用来测试某个宏是否被定义。defined(name): 若宏被定义，则返回1，否则返回0。 它与#if、#elif、#else结合使用来判断宏是否被定义，乍一看好像它显得多余, 因为已经有了#ifdef和#ifndef。defined可用于在一条判断语句中声明多个判别条件；#ifdef和#ifndef则仅支持判断一个宏是否定义。 #if defined(VAX) \u0026\u0026 defined(UNIX) \u0026\u0026 !defined(DEBUG) 和#if、#elif、#else不同，#ifdef、#ifndef、defined测试的宏可以是对象宏，也可以是函数宏。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:2","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"3. #ifdef、#ifndef、#else、#endif 条件编译中相对常用的预编译指令。模式如下： #ifdef ABC // ... codes while definded ABC #elif (CODE_VERSION \u003e 2) // ... codes while CODE_VERSION \u003e 2 #else // ... remained cases #endif #ifdef用于判断某个宏是否定义，和#ifndef功能正好相反，二者仅支持判断单个宏是否已经定义，上面例子中二者可以互换。如果不需要多条件预编译的话，上面例子中的#elif和#else均可以不写。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:3","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"4. #if、#elif、#else、#endif 在判断某个宏是否被定义时，应当避免使用#if，因为该宏的值可能就是被定义为0。而应当使用#ifdef或#ifndef。 注意: #if、#elif之后的宏只能是对象宏。如果宏未定义，或者该宏是函数宏，则编译器可能会有对应宏未定义的警告。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:4","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"Article description.","date":"2019-05-10","objectID":"/20190510_pythonintroduction/","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. Python is often described as a “batteries included” language due to its comprehensive standard library. ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:0:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"1. 数据类型转换 整型 浮点 bool 复数 string list tuple set dict 整型 \\ 加.0 int(0) 加0j 任意 × × × × 浮点 去小数 \\ 0.0 加0j 数据 × × × × bool true1 false0 true1.0 false0.0 \\ true1+0j 可以 × × × × 复数 × × 0j \\ 转换 × × × × string 纯数字 纯数字 '' 纯数\u0026+0j \\ 每个字符转成每个值 每个字符转成每个值 每个字符转成每个值+去重 × list × × [] × 成为 \\ 内容不变 随机+去重 有2个数据的二级列表 tuple × × () × str 内容不变 \\ 随机+去重 有2个数据的二级列表 set × × set() × 格式 内容不变+随机 内容不变+随机 \\ 有2个数据的二级列表 dict × × {} × 数据 仅保留键 仅保留键 仅保留键 \\ ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:1:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"2. 身份运算 判断地址是否相同 x is y x is not y 1.字符串：字符串值相同，ID相同。 2.列表、字典、集合：无论什么情况ID都不同 ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:2:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"3. 成员检测运算 val1 in val2 val1 not in val2 //检测一个数据是否在容器中 ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:3:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"4. 流程控制 1. if ___: elif ___: elif ___: else: 2. while ___: else: 3. for x in 容器: ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:4:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"5. 函数 def func( name, sex = “male”, *args, like = “乒乓”, **kwargs); ↓ ↓ ↓ ↓ ↓ ↓ func( “刘佳锐”, “男”, “乐色”, “垃圾”, like = “羽毛球”, skin = “yellow”, hobby = “hello”) print(locals()) //获取当前作用域的局部变量 print(globals()) //获取当前作用域的全局变量 ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:5:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"6. 迭代器 能被next()函数调用并不断返回下一个值的对象 特征：迭代器会生成惰性序列，通过计算把值依次返回 优点：需要数据的时候，一次取一个可以很大节省内存 检测： from collections import Iterable, Iterator isinstance()判断数据类型，返回bool值 print(isinstance(list1,list1)) Iter：使用iter可把迭代数据变为迭代器 list1 = [1,2,3] result = iter(list1) print(next(result)) ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:6:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"Article description.","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"Performance optimization methods and ideas for large C projects. Performance optimization strategies for x86 projects that encounter performance bottlenecks when porting to low performance processors. ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:0:0","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"通常优化方法 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:0","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"1. 宏定义或内联 短的、调用频繁的函数改为宏定义或内联函数，减少调用层级 可能编译器已经做了部分优化，效果不一定明显 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:1","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"2. 固定次数的短循环展开 循环语句如果循环次数已知，且是短循环，可以将语句展开 编译器可能已做优化，效果不一定明显 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:2","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"3. 减少内存分配和释放的次数 频繁使用的变量能用全局变量的尽量不用局部变量 函数体内部的局部变量，如果大小不是特别大，尽量不用动态分配空间 数据结构中，尽量不用指针变量 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:3","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"4. 移位代替乘除 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:4","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"5. 条件语句优化 根据分支被执行的频率将频繁执行的分支放在前面部分 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:5","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"6. 数据结构/算法 数据结构中尽量减少需要动态分配空间的指针，改用联合、结构体或固定大小的缓存区 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:6","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"7. 使用并行程序 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:7","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"代码重构原则 维护一套代码，使用宏定义控制 小步前进，重构一部分进行充分测试后再重构下一部分 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:2:0","tags":["C"],"title":"C Project performance optimization","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"Article description.","date":"2019-03-04","objectID":"/20190304_gdb-debug/","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"GDB is a powerful program debugging tool based on command line under UNIX/LINUX operating system released by GNU Source Organization. For a C/C ++ programmer working on Linux, GDB is an essential tool. ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:0:0","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"gdb的使用 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:0","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"启动gdb 编译一个测试程序，-g表示可以调试 gcc -g demo.c -o demo 启动gdb gdb test 或者 gdb -q test //表示不打印gdb版本信息，界面较为干净； ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:1","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"查看源码 list ：简记为 l ，其作用就是列出程序的源代码，默认每次显示10行。 list 行号：将显示当前文件以“行号”为中心的前后10行代码，如：list 12 list 函数名：将显示“函数名”所在函数的源代码，如：list main list ：不带参数，将接着上一次 list 命令的，输出下边的内容。 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:2","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"运行程序 run：简记为 r ，其作用是运行程序，当遇到断点后，程序会在断点处停止运行，等待用户输入下一步的命令。 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:3","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"设置参数 set args 参数1 参数2 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:4","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"设置断点 break n （简写b n）:在第n行处设置断点.可以带上代码路径和代码名称： b demo.cpp:578 b fn1 if a＞b：条件断点设置 break func（break缩写为b）：在函数func()的入口处设置断点，如：break cb_button delete 断点号n：删除第n个断点 disable 断点号n：暂停第n个断点 enable 断点号n：开启第n个断点 clear 行号n：清除第n行的断点 info b （info breakpoints）：显示当前程序的断点设置情况 delete breakpoints：清除所有断点 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:5","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"单步执行 continue （简写c ）：继续执行，到下一个断点处（或运行结束） next：（简写 n），单步跟踪程序，当遇到函数调用时，也不进入此函数体；此命令同 step 的主要区别是，step 遇到用户自定义的函数，将步进到函数中去运行，而 next 则直接调用函数，不会进入到函数体内。 step （简写s）：单步调试如果有函数调用，则进入函数；与命令n不同，n是不进入调用的函数的 until：当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。 until+行号： 运行至某行，不仅仅用来跳出循环 finish： 运行程序，直到当前函数完成返回，并打印函数返回时的堆栈地址和返回值及参数值等信息。 call 函数(参数)：调用程序中可见的函数，并传递“参数”，如：call gdb_test(55) ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:6","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"查看变量 print 表达式：简记为 p ，其中“表达式”可以是任何当前正在被测试程序的有效表达式，比如当前正在调试C语言的程序，那么“表达式”可以是任何C语言的有效表达式，包括数字，变量甚至是函数调用。 print a：将显示整数 a 的值 print ++a：将把 a 中的值加1,并显示出来 print name：将显示字符串 name 的值 print gdb_test(22)：将以整数22作为参数调用 gdb_test() 函数 print gdb_test(a)：将以变量 a 作为参数调用 gdb_test() 函数 display 表达式：在单步运行时将非常有用，使用display命令设置一个表达式后，它将在每次单步进行指令后，紧接着输出被设置的表达式及值。如： display a watch 表达式：设置一个监视点，一旦被监视的“表达式”的值改变，gdb将强行终止正在被调试的程序。如： watch a whatis ：查询变量或函数 info function： 查询函数 扩展info locals： 显示当前堆栈页的所有变量 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:7","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"查看运行信息 where/bt ：当前运行的堆栈列表 bt backtrace 显示当前调用堆栈 up/down 改变堆栈显示的深度 set args 参数:指定运行时的参数 show args：查看设置好的参数 info program： 来查看程序的是否在运行，进程号，被暂停的原因。 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:8","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"分割窗口 layout：用于分割窗口，可以一边查看代码，一边测试 layout src：显示源代码窗口 layout asm：显示反汇编窗口 layout regs：显示源代码/反汇编和CPU寄存器窗口 layout split：显示源代码和反汇编窗口 Ctrl + L：刷新窗口 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:9","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"退出gdb quit：简记为 q ，退出gdb ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:10","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"Article description.","date":"2019-02-03","objectID":"/20190203_huge-pages/","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"Reduce page tables by increasing the size of operating system pages to avoid fast table misses. Large page memory optimizer is designed for malloc mechanism, which means allocating large pages to increase TLB hit ratio. ","date":"2019-02-03","objectID":"/20190203_huge-pages/:0:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"原理 大页内存的原理涉及到操作系统的虚拟地址到物理地址的转换过程。操作系统为了能同时运行多个进程，会为每个进程提供一个虚拟的进程空间，在32位操作系统上，进程空间大小为4G，64位系统为2^64（实际可能小于这个值）。事实上，每个进程的进程空间都是虚拟的，这和物理地址还不一样。两个进行访问相同的虚拟地址，但是转换到物理地址之后是不同的。这个转换就通过页表来实现，涉及的知识是操作系统的分页存储管理。 分页存储管理将进程的虚拟地址空间，分成若干个页，并为各页加以编号。相应地，物理内存空间也分成若干个块，同样加以编号。页和块的大小相同。 在操作系统中设置有一个页表寄存器，其中存放了页表在内存的始址和页表的长度。进程未执行时，页表的始址和页表长度放在本进程的PCB中；当调度程序调度该进程时，才将这两个数据装入页表寄存器。 当进程要访问某个虚拟地址中的数据时，分页地址变换机构会自动地将有效地址（相对地址）分为页号和页内地址两部分，再以页号为索引去检索页表，查找操作由硬件执行。若给定的页号没有超出页表长度，则将页表始址与页号和页表项长度的乘积相加，得到该表项在页表中的位置，于是可以从中得到该页的物理块地址，将之装入物理地址寄存器中。与此同时，再将有效地址寄存器中的页内地址送入物理地址寄存器的块内地址字段中。这样便完成了从虚拟地址到物理地址的变换。 由于页表是存放在内存中的，这使CPU在每存取一个数据时，都要两次访问内存。第一次时访问内存中的页表，从中找到指定页的物理块号，再将块号与页内偏移拼接，以形成物理地址。第二次访问内存时，才是从第一次所得地址中获得所需数据。因此，采用这种方式将使计算机的处理速度降低近1/2。 为了提高地址变换速度，可在地址变换机构中，增设一个具有并行查找能力的特殊高速缓存，也即快表（TLB），用以存放当前访问的那些页表项。具有快表的地址变换机构如图四所示。由于成本的关系，快表不可能做得很大，通常只存放16~512个页表项。 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:1:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"大页内存的配置和使用 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"1. 安装libhugetlbfs库 libhugetlbfs库实现了大页内存的访问。安装可以通过apt-get或者yum命令完成，如果系统没有该命令，还可以git clone, 然后make生成libhugetlbfs.so文件. 直接使用makefile进行编译：make BUILDTYPE=NATIVEONLY一定要加最后的参数 BUILDTYPE=NATIVEONLY否则会遇见各种错误 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:1","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"2. 配置grub启动文件 具体就是在kernel选项的最后添加几个启动参数：transparent_hugepage=never default_hugepagesz=1G hugepagesz=1Ghugepages=123 这四个参数中，最重要的是后两个，hugepagesz用来设置每页的大小，我们将其设置为1G，其他可选的配置有4K，2M（其中2M是默认）。 vim /boot/grub/grub.cfg 修改完grub.conf后，重启系统。然后运行命令查看大页设置是否生效 cat /proc/meminfo|grep Huge ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:2","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"3. mount 执行mount，将大页内存映像到一个空目录。可以执行下述命令： mount -t hugetlbfs hugetlbfs /mnt/huge ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:3","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"4. 运行应用程序 为了能启用大页，不能按照常规的方法启动应用程序，需要按照下面的格式启动： HUGETLB_MORECORE=yes LD_PRELOAD=libhugetlbfs.so ./your_program 这种方法会加载libhugetlbfs库，用来替换标准库。具体的操作就是替换标准的malloc为大页的malloc。此时，程序申请内存就是大页内存了。 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:4","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"大页内存的使用场景 任何优化手段都有它适用的范围，大页内存也不例外。只有耗费的内存巨大、访存随机而且访存是瓶颈的程序大页内存才会带来很明显的性能提升。 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:3:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"Article description.","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"In development, we just need to know that lib is needed at compile time and DLL is needed at run time. If you want to compile source code, lib is all you need. If you want dynamically connected programs to run, you need only a DLL. This article will more clearly understand the difference, generation, use of the two. ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:0:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"lib和dll区别 首先介绍静态库（静态链接库）、动态库（动态链接库）的概念，首先两者都是代码共享的方式。 静态库：在链接步骤中，连接器将从库文件取得所需的代码，复制到生成的可执行文件中，这种库称为静态库，其特点是可执行文件中包含了库代码的一份完整拷贝；缺点就是被多次使用就会有多份冗余拷贝。即静态库中的指令都全部被直接包含在最终生成的 EXE 文件中了。在vs中新建生成静态库的工程，编译生成成功后，只产生一个.lib文件。 动态库：动态链接库是一个包含可由多个程序同时使用的代码和数据的库，DLL不是可执行文件。动态链接提供了一种方法，使进程可以调用不属于其可执行代码的函数。函数的可执行代码位于一个 DLL 中，该 DLL 包含一个或多个已被编译、链接并与使用它们的进程分开存储的函数。在vs中新建生成动态库的工程，编译成功后，产生一个.lib文件和一个.dll文件。 静态库和动态库中的lib有什么区别呢？ 静态库中的lib：该LIB包含函数代码本身（即包括函数的索引，也包括实现），在编译时直接将代码加入程序当中。 动态库中的lib：该LIB包含了函数所在的DLL文件和文件中函数位置的信息（索引），函数实现代码由运行时加载在进程空间中的DLL提供。 总之，lib是编译时用到的，dll是运行时用到的。如果要完成源代码的编译，只需要lib；如果要使动态链接的程序运行起来，只需要dll。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:1:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"静态链接 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"1. 为什么进行静态链接 在我们的实际开发中，不可能将所有代码放在一个源文件中，所以会出现多个源文件，而且多个源文件之间不是独立的，而会存在多种依赖关系，如一个源文件可能要调用另一个源文件中定义的函数，但是每个源文件都是独立编译的，即每个*.c文件会形成一个*.o文件，为了满足前面说的依赖关系，则需要将这些源文件产生的目标文件进行链接，从而形成一个可以执行的程序。这个链接的过程就是静态链接 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:1","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"2. 静态链接的原理 由很多目标文件进行链接形成的是静态库，反之静态库也可以简单地看成是一组目标文件的集合，即很多目标文件经过压缩打包后形成的一个文件。 以下面这个图来简单说明一下从静态链接到可执行文件的过程，根据在源文件中包含的头文件和程序中使用到的库函数，如stdio.h中定义的printf()函数，在libc.a中找到目标文件printf.o(这里暂且不考虑printf()函数的依赖关系)，然后将这个目标文件和我们hello.o这个文件进行链接形成我们的可执行文件。 graph TD A[Christmas] --\u003e|Get money| B(Go shopping) B --\u003e C{Let me think} C --\u003e|One| D[Laptop] C --\u003e|Two| E[iPhone] C --\u003e|Three| F[fa:fa-car Car] 链接器在链接静态链接库的时候是以目标文件为单位的。比如我们引用了静态库中的printf()函数，那么链接器就会把库中包含printf()函数的那个目标文件链接进来，如果很多函数都放在一个目标文件中，很可能很多没用的函数都被一起链接进了输出结果中。由于运行库有成百上千个函数，数量非常庞大，每个函数独立地放在一个目标文件中可以尽量减少空间的浪费，那些没有被用到的目标文件就不要链接到最终的输出文件中。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:2","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"3. 静态链接的优缺点 静态链接的缺点很明显，一是浪费空间，因为每个可执行程序中对所有需要的目标文件都要有一份副本，所以如果多个程序对同一个目标文件都有依赖，如多个程序中都调用了printf()函数，则这多个程序中都含有printf.o，所以同一个目标文件都在内存存在多个副本；另一方面就是更新比较困难，因为每当库函数的代码修改了，这个时候就需要重新进行编译链接形成可执行程序。但是静态链接的优点就是，在可执行程序中已经具备了所有执行程序所需要的任何东西，在执行的时候运行速度快。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:3","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"动态链接 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"1. 为什么进行动态链接 动态链接出现的原因就是为了解决静态链接中提到的两个问题，一方面是空间浪费，另外一方面是更新困难。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:1","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"2. 动态链接的原理 动态链接的基本思想是把程序按照模块拆分成各个相对独立部分，在程序运行时才将它们链接在一起形成一个完整的程序，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文件。下面简单介绍动态链接的过程： 假设现在有两个程序program1.o和program2.o，这两者共用同一个库lib.o,假设首先运行程序program1，系统首先加载program1.o，当系统发现program1.o中用到了lib.o，即program1.o依赖于lib.o，那么系统接着加载lib.o，如果program1.o和lib.o还依赖于其他目标文件，则依次全部加载到内存中。当program2运行时，同样的加载program2.o，然后发现program2.o依赖于lib.o，但是此时lib.o已经存在于内存中，这个时候就不再进行重新加载，而是将内存中已经存在的lib.o映射到program2的虚拟地址空间中，从而进行链接（这个链接过程和静态链接类似）形成可执行程序。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:2","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"3. 动态链接的优缺点 动态链接的优点显而易见，就是即使需要每个程序都依赖同一个库，但是该库不会像静态链接那样在内存中存在多分，副本，而是这多个程序在执行时共享同一份副本；另一个优点是，更新也比较方便，更新时只需要替换原来的目标文件，而无需将所有的程序再重新链接一遍。当程序下一次运行时，新版本的目标文件会被自动加载到内存并且链接起来，程序就完成了升级的目标。但是动态链接也是有缺点的，因为把链接推迟到了程序运行时，所以每次执行程序都需要进行链接，所以性能会有一定损失。 ​ 据估算，动态链接和静态链接相比，性能损失大约在5%以下。经过实践证明，这点性能损失用来换区程序在空间上的节省和程序构建和升级时的灵活性是值得的。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:3","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"read binary file to char *bytes in c.","date":"2018-12-20","objectID":"/20181220_read-binary-file/","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"Read the binary file (any file will do; this article uses binary as an example) and read the entire contents of the binary file into a char* string. With fseek() and fread() functions to achieve file reading advanced methods. 需求 使用fwrite(dbdata, dblength, 1,fp)把字节流写入二进制文件。在新程序读取二进制文件遇到问题：二进制内容不能向文本一样行读取，也不知道二进制文件长度，在fread()函数中无从下手。 ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:0:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"1.creat FILE pointer and set mode as ‘rb’ FILE *f = fopen(inputFN, \"rb\"); ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:1:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"2.check the FILE pointer is not null if (!f) { fprintf(stderr, \"ERROR: unable to open file \\\"%s\\\": %s\\n\", inputFN,strerror(errno)); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:2:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"3.use fseek/ftell to get data length fseek(f,0,SEEK_END) put the pointer to the end of the file. ftell(f) can get the current offset. then use fseek(f,0,SEEK_SET) put the pointer to the start of file. if (fseek(f, 0, SEEK_END) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } long datalen = ftell(f); if (dataLen \u003c 0) { fprintf(stderr, \"ERROR: ftell() failed: %s\\n\", strerror(errno)); fclose(f); return NULL; } if (fseek(f, 0, SEEK_SET) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:3:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"4.check the datalen if ((unsigned long)dataLen \u003e UINT_MAX) { dataLen = UINT_MAX; printf(\"WARNING: clipping data to %ld bytes\\n\", dataLen); } else if (dataLen == 0) { fprintf(stderr, \"ERROR: input file \\\"%s\\\" is empty\\n\", inputFN); fclose(f); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:4:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"5.malloc memory to char *inputData char *inputData = static_cast\u003cchar *\u003e(malloc(dataLen)); if (!inputData) { fprintf(stderr, \"ERROR: unable to malloc %ld bytes\\n\", dataLen); fclose(f); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:5:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"6.read the bin data // create a pointer p to point the begin of the inputData char *p = inputData; // create a bytesLeft to record the moving of offset size_t bytesLeft = dataLen; while (bytesLeft) { //fread will return the bytes of read size_t bytesRead = fread(p, 1, bytesLeft, f); bytesLeft -= bytesRead; p += bytesRead; if (ferror(f) != 0) { fprintf(stderr, \"ERROR: fread() failed\\n\"); free(inputData); fclose(f); return NULL; } } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:6:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"7.close the File stream fclose(f); ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:7:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"8.return length \u0026 inputData //change the parameter of \u0026length *length = (unsigned int)dataLen; return inputData; ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:8:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"9.完整代码 FILE *f = fopen(inputFN, \"rb\"); if (!f) { fprintf(stderr, \"ERROR: unable to open file \\\"%s\\\": %s\\n\", inputFN,strerror(errno)); return NULL; } if (fseek(f, 0, SEEK_END) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } long dataLen = ftell(f); if (dataLen \u003c 0) { fprintf(stderr, \"ERROR: ftell() failed: %s\\n\", strerror(errno)); fclose(f); return NULL; } if (fseek(f, 0, SEEK_SET) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } if ((unsigned long)dataLen \u003e UINT_MAX) { dataLen = UINT_MAX; printf(\"WARNING: clipping data to %ld bytes\\n\", dataLen); } else if (dataLen == 0) { fprintf(stderr, \"ERROR: input file \\\"%s\\\" is empty\\n\", inputFN); fclose(f); return NULL; } char *inputData = static_cast\u003cchar *\u003e(malloc(dataLen)); if (!inputData) { fprintf(stderr, \"ERROR: unable to malloc %ld bytes\\n\", dataLen); fclose(f); return NULL; } char *p = inputData; size_t bytesLeft = dataLen; while (bytesLeft) { size_t bytesRead = fread(p, 1, bytesLeft, f); bytesLeft -= bytesRead; p += bytesRead; if (ferror(f) != 0) { fprintf(stderr, \"ERROR: fread() failed\\n\"); free(inputData); fclose(f); return NULL; } } fclose(f); *length = (unsigned int)dataLen; ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:9:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"Article description.","date":"2018-11-18","objectID":"/20181118_sqlcipher/","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"SQLCipher is based on SQLite, and thus, the majority of the accessible API is identical to the C/C++ interface for SQLite 3. However, SQLCipher does add a number of security specific extensions in the form of PRAGMAs, SQL Functions and C Functions. ","date":"2018-11-18","objectID":"/20181118_sqlcipher/:0:0","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"1. Build SQLcipher from source $ git clone https://github.com/sqlcipher/sqlcipher.git $ cd sqlcipher $ ./configure –enable-tempstore=yes CFLAGS=\"-DSQLITE_HAS_CODEC -DSQLITE_TEMP_STORE=2\" LDFLAGS=\"-lcrypto\" $ make $ make install #if you want to do a system wide install of SQLCipher Mark the output of make install, especially the following lines: libtool: install: /usr/bin/install -c .libs/libsqlcipher.a /usr/local/lib/libsqlcipher.a /usr/bin/install -c -m 0644 sqlite3.h /usr/local/include/sqlcipher these are the folders of SQLcipher headers and the library necessary when building proper C project. ","date":"2018-11-18","objectID":"/20181118_sqlcipher/:1:0","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"2. Building minimal C project example In SQLite_example.c put the following lines: #include \"sqlite3.h\" //We want to SQLCipher extension, rather then a system wide SQLite header rc = sqlite3_open(\"test.db\",\u0026db); //open SQLite database test.db rc = sqlite3_key(db,\"1q2w3e4r\",8); //apply encryption to previously opened database Build you example: $gcc SQLite_example.c -o SQLtest -I /path/to/local/folder/with/sqlcipher/header/files/ -L /path/to/local/folder/with/sqlcipher/library.a -l sqlcipher e.g. with paths extracted from the output of $make install $gcc SQLite_example.c -o SQLtest -I /usr/local/include/sqlcipher -L /usr/local/lib/libsqlcipher.a -lsqlcipher Finally, make sure that your SQLCipher library is in the system wide library path e.g. for (Arch)Linux: $ export LD_LIBRARY_PATH=/usr/local/lib/ Run your test code ((Arch)Linux): $ ./SQLtest ","date":"2018-11-18","objectID":"/20181118_sqlcipher/:2:0","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"Article description.","date":"2018-10-17","objectID":"/20181017_sql-introduction/","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"In computer programming, create, read, update, and delete (CRUD) are the four basic functions of persistent storage. Alternate words are sometimes used when defining the four basic functions of CRUD, such as retrieve instead of read, modify instead of update, or destroy instead of delete. CRUD is also sometimes used to describe user interface conventions that facilitate viewing, searching, and changing information, often using computer-based forms and reports. ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:0","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"1. 查询语句 select … from … where … group by … having … order by … limit … 次序 4 1 2 3 5 6 7 ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:1","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"2. Group by GROUP BY 语句用于结合聚合函数，根据一个或多个列对结果集进行分组。 实例： mysql\u003e SELECT * FROM access_log; +-----+---------+-------+------------+ | aid | site_id | count | date | +-----+---------+-------+------------+ | 1 | 1 | 45 | 2016-05-10 | | 2 | 3 | 100 | 2016-05-13 | | 3 | 1 | 230 | 2016-05-14 | | 4 | 2 | 10 | 2016-05-14 | | 5 | 5 | 205 | 2016-05-14 | | 6 | 4 | 13 | 2016-05-15 | | 7 | 3 | 220 | 2016-05-15 | | 8 | 5 | 545 | 2016-05-16 | | 9 | 3 | 201 | 2016-05-17 | +-----+---------+-------+------------+ 9 rows in set (0.00 sec) SELECT site_id, SUM(access_log.count) AS nums FROM access_log GROUP BY site_id; +---------+------+ | site_id | nums | +---------+------+ | 1 | 275 | | 2 | 10 | | 3 | 521 | | 4 | 13 | | 5 | 750 | +---------+------+ ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:2","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"3. 聚集函数 1 2 3 4 5 6 count sum max min avg group_concat ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:3","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"4. Having HAVING 子句可以让我们筛选分组后的各组数据。 查询每个班中人数大于2的班级号： select count(1) as n, classid from stu group by classid having n\u003e2; or select classid from stu group by classid having count(1)\u003e2; ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:4","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"5. Order by ... order by n, classid; 1.先按n排序 2.在不改n排序的情况下排classid ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:5","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"6. Limit ... limit 1, 10; # 检索记录行 2-10 ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:6","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"7. Join stu join class on classid = class.id # join 会把左表的每一行分别与右表每一行拼接 # on 做筛选 ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:7","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"Article description.","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefiles define a set of rules that specify which files need to be compiled first, which files need to be compiled later, which files need to be recompiled, and even more complex functional operations, because makefiles are like Shell scripts that also execute operating system commands. One of the benefits of Makefiles is that they are “automatically compiled”. The entire project is automatically compiled, greatly improving the efficiency of software development. ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:0:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"关于程序的编译和链接 无论是C还是C++，首先要把源文件编译成中间代码文件，在Windows下也就是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做编译（compile）。然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。 编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。 链接时，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。 总结一下，源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。需要指定函数的ObjectFile. ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:1:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile 介绍 make命令执行时，需要一个 Makefile 文件，以告诉make命令需要怎么样的去编译和链接程序。 首先，我们用一个示例来说明Makefile的书写规则。以便给大家一个感兴认识。这个示例来源于GNU的make使用手册，在这个示例中，我们的工程有8个C文件，和3个头文件，我们要写一个Makefile来告诉make命令如何编译和链接这几个文件。我们的规则是： 如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。 如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。 如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. Makefile 规则 target... : prerequisites ... command ... target也就是一个目标文件，可以是Object File，也可以是执行文件。还可以是一个标签（Label） prerequisites就是，要生成那个target所需的文件或是目标。 command也就是make需要执行的命令。(任意的shell命令) 这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在command中。也就是说，prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2. $@, $^, $\u003c 含义 $@–目标文件 $^–所有的依赖文件 $\u003c–第一个依赖文件 一个示例 如果一个工程有3个头文件，和8个C文件，我们为了完成前面所述的那三个规则，我们的Makefile应该是下面的这个样子的。 edit : main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o main.o : main.c defs.h cc -c main.c kbd.o : kbd.c defs.h command.h cc -c kbd.c command.o : command.c defs.h command.h cc -c command.c display.o : display.c defs.h buffer.h cc -c display.c insert.o : insert.c defs.h buffer.h cc -c insert.c search.o : search.c defs.h buffer.h cc -c search.c files.o : files.c defs.h buffer.h command.h cc -c files.c utils.o : utils.c defs.h cc -c utils.c clean : rm edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o 反斜杠（\\）是换行符的意思。这样比较便于Makefile的易读。 在这个makefile中，目标文件（target）包含：执行文件edit和中间目标文件（*.o），依赖文件（prerequisites）就是冒号后面的那些 .c 文件和 .h文件。每一个 .o 文件都有一组依赖文件，而这些 .o 文件又是执行文件 edit 的依赖文件。依赖关系的实质上就是说明了目标文件是由哪些文件生成的，换言之，目标文件是哪些文件更新的。 在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个Tab键作为开头。记住，make并不管命令是怎么工作的，他只管执行所定义的命令。make会比较targets文件和prerequisites文件的修改日期，如果prerequisites文件的日期要比targets文件的日期要新，或者target不存在的话，那么，make就会执行后续定义的命令。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. make是如何工作的 在默认的方式下，也就是我们只输入make命令。那么， make会在当前目录下找名字叫“Makefile”或“makefile”的文件。 如果找到，它会找文件中的第一个目标文件（target），在上面的例子中，他会找到“edit”这个文件，并把这个文件作为最终的目标文件。 如果edit文件不存在，或是edit所依赖的后面的 .o 文件的文件修改时间要比edit这个文件新，那么，他就会执行后面所定义的命令来生成edit这个文件。 如果edit所依赖的.o文件也存在，那么make会在当前文件中找目标为.o文件的依赖性，如果找到则再根据那一个规则生成.o文件。 当然，C文件和H文件是存在的啦，于是make会生成 .o 文件，然后再用 .o 文件声明make的终极任务，也就是执行文件edit了。 这就是整个make的依赖性，make会一层又一层地去找文件的依赖关系，直到最终编译出第一个目标文件。在找寻的过程中，如果出现错误，比如最后被依赖的文件找不到，那么make就会直接退出，并报错，而对于所定义的命令的错误，或是编译不成功，make根本不理。make只管文件的依赖性 于是在我们编程中，如果这个工程已被编译过了，当我们修改了其中一个源文件，比如file.c，那么根据我们的依赖性，我们的目标file.o会被重编译（也就是在这个依性关系后面所定义的命令），于是file.o的文件也是最新的啦，于是file.o的文件修改时间要比edit要新，所以edit也会被重新链接了（详见edit目标文件后定义的命令）。 而如果我们改变了“command.h”，那么，kdb.o、command.o和files.o都会被重编译，并且，edit会被重链接。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. makefile中使用变量 以edit的规则为例： edit:main.o kbd.o command.o display.o \\ insert.o search.o files.o utills.o cc -o edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o 看到[.o]文件的字符串被重复了两次，如果我们的工程需要加入新的[.o]文件，那么需要在两个地方加。因为此时的makefile并不复杂。当makefile变得复杂我们就有可能忘掉某个地方，而导致编译失败。所以为了makefile的易维护，在makefile中可以使用变量。 比如我们声明一个变量，objects, OBJECTS,objs,OBJS,或OBJ，表示obj文件。在makefile一开始就定义 objects = main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o 于是，就可以方便的在makefile中以\"$(objects)“的方式来使用这个变量了。改良后的makefile如下： objects = main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o edit : $(objects) cc -o edit $(objects) main.o : main.c defs.h cc -c main.c command.o : command.c defs.h command.h cc -c command.c display.o : display.c defs.h buffer.h cc -c display.c insert.o : insert.c defs.h buffer.h cc -c insert.c search.o : search.c defs.h buffer.h cc -c search.c files.o : files.c defs.h buffer.h command.h cc -c files.c utils.o : utils.c defs.h cc -c utils.c clean : rm edit $(objects) 于是如果有新的 .o 文件加入，只需修改一下 objects 变量就可以了。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"5. make自动推导 GNU的make很强大，它可以自动推导文件以及文件依赖关系后面的命令，于是我们没有必要在每个[.o]文件写上依赖关系。make会自动识别，并自己推导命令。 只要make找到一个[.o]文件，它就会自动的把[.c]文件加在依赖关系中，如果make找到一个whatever.o，那么whatever.c等whatever.o的依赖文件，并且cc -c whatever.c也会被推导出来。于是makefile可以又一次简化。 objects = main.o kbd.o command.o display.o \\ insert.o seaerch.o files.o utils.o edit : $(objects) cc -o edit $(objects) main.o : defs.h kbd.o : defs.h command.h command.o : defs.h command.h display.o : defs.h buffer.h insert.o : defs.h buffer.h search.o : defs.h buffer.h files.o : defs.h buffer.h command.h utils.o : defs.h .PHONY : clean clean : rm edit $(objects) 这就是make的“隐晦规则”。 .PHONY表示，clean是个伪目标文件 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:5","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"6. makefile的收缩 收拢起来 objects = main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o edit : $(objects) cc -o edit $(objects) $(objects) : defs.h kbd.o command.o files.o : command.h display.o insert.o search.o files.o : buffer.h .PHONY : clean clean: rm edit $(objects) 虽然makefile变得很简单，但我们的文件依赖关系会显得凌乱，新增.o文件不好管理 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:6","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"7. 清空目标文件的规则 每个Makefile中都应该写一个清空目标文件(.o和执行文件)的规则。 一般的风格是： clean: rm edit $(objects) 更稳健的做法是： .PHONY: clean clean: -rm edit $(objects) .PHONY表示clean是一个“伪目标”，向make说明，不管是否有这个文件，这个目标就是“伪目标”。只要有这个声明，不管是否有“clean”文件，要运行“clean”这个目标，只有“make clean”这样。 而在rm命令前面加了一个减号表示也许某些文件出现问题，但不用管，继续往后执行。 当然，clean的规则不要放在文件的开头，不然，这就会变成make的默认目标。 “clean从来都是放在文件的最后” ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:7","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile总述 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. Makefile里有什么 Makefile主要包含了五个东西：显示规则，隐晦规则，变量定义，文件指示和注释。 显示规则：显示规则说明了如何生成一个或多个的目标文件。这是由Makefile书写者明显指出要生成的文件、文件的依赖文件、生成的命令。 隐晦规则：由于我们的make有自动推导功能，所以隐晦的规则可以让我们比较粗糙地简略书写Makefile，这是make支持的。 变量的定义：在Makefile中我们要定义一系列的变量，变量一般都是字符串，这个有点像C语言中的宏，当Makefile被执行时，其中的变量都会被扩展到相应的引用位置上。 文件指示：其中包括三个部分，一个是在Makefile中引用另一个Makefile，就像C中的#include一样；另一个是指根据某些情况指定Makefile中的有效部分，就像C语言中的预编译#if一样；还有就是定义一个多行的命令。 注释：Makefile中只有行注释，和UNIX的Shell脚本一样，其注释是用“#”字符，这个就像C/C++中的“//”一样。如果要在Makefile中使用“#”字符，可以用反斜框进行转义，如：“#”。 最后，在Makefile中的命令，必须要以[Tab]键开始。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2. Makefile的文件名 默认的情况下，make命令会在当前目录下按顺序找寻文件名为**“GNUmakefile”、“makefile”、“Makefile”**的文件，找到了解释这个文件。在这三个文件名中，最好使用“Makefile”这个文件名，因为，这个文件名第一个字符为大写，这样有一种显目的感觉。最好不要用“GNUmakefile”，这个文件是GNU的make识别的。有另外一些make只对全小写的“makefile”文件名敏感，但是基本上来说，大多数的make都支持“**makefile”和“Makefile”**这两种默认文件名。 当然，可以使用别的文件名来书写Makefile，比如：“Make.Linux”，“Make.Solaris”，“Make.AIX”等，如果要****指定特定的Makefile，可以使用make的“-f”和“–file”参数****，如：make -f Make.Linux或make –file Make.AIX。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. 引用其他的Makefile 在Makefile使用include关键字可以把别的Makefile包含进来，这很像C语言的#include，被包含的文件会原模原样的放在当前文件的包含位置。include的语法是： include filename #filename可以是当前操作系统Shell的文件模式（可以保含路径和通配符） 在include前面可以有一些空字符，但是绝不能是[Tab]键开始。include和可以用一个或多个空格隔开。 举个例子，有这样几个Makefile：a.mk、b.mk、c.mk，还有一个文件叫foo.make，以及一个变量$(bar)，其包含了e.mk和f.mk，那么，下面的语句： include foo.make *.mk $(bar) #等价于： include foo.make a.mk b.mk c.mk e.mk f.mk make命令开始时，会把找寻include所指出的其它Makefile，并把其内容安置在当前的位置。就好像C/C++的#include指令一样。如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，如果当前目录下没有找到，那么，make还会在下面的几个目录下找： 如果make执行时，有“-I”或“–include-dir”参数，那么make就会在这个参数所指定的目录下去寻找。 如果目录/include（一般是：/usr/local/bin或/usr/include）存在的话，make也会去找。 如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成makefile的读取，make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。如果想让make不理那些无法读取的文件，而继续执行，可以在include前加一个减号“-”。如： -include\u003cfilename\u003e 其表示，无论include过程中出现什么错误，都不要报错继续执行。和其它版本make兼容的相关命令是sinclude，其作用和这一个是一样的。这个变量中的值是其它的Makefile，用空格分隔。只是，它和include不同的是，从这个环境变量中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. 环境变量Makefiles 如果当前环境中定义了环境变量Makefiles，那么make会把这个变量中的值做一个类似于include的动作。这个变量中的值是其它的Makefile，用空格分隔。只是，它和include不同的是，从这个环境变中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。 但还是建议不要使用这个环境变量，因为只要这个变量一旦被定义，那么当使用make时，所有的Makefile都会受到它的影响，这绝不是想看到的。在这里提这个事，只是为了告诉大家，也许有时候Makefile出现了怪事，那么可以看看当前环境中有没有定义这个变量。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"5. make的工作方式 GNU的make工作时执行步骤如下： 读入所有的makefile 读入被include的其他makefile 初始化文件中的变量 推导隐晦规则，并分析所有规则 为所有的目标文件创建依赖关系链 根据依赖关系，决定哪些目标文件重新生成 执行生成命令 1-5步为第一个阶段，6-7为第二个阶段。 第一个阶段中，如果定义被使用了，那么make会把其展开在使用的位置。但make并不会完全马上展开，如果变量出现在依赖关系的规则中，那么仅当这条依赖被决定要使用了，变量才会在内部展开 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:5","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile书写规则 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. Makefile规则 规则包含两个部分，一个是依赖关系，一个是生成目标的方法。 在Makefile中，规则的顺序是很重要的，因为，Makefile中只应该有一个最终目标，其它的目标都是被这个目标所连带出来的，所以一定要让make知道你的最终目标是什么。一般来说，定义在Makefile中的目标可能会有很多，但是第一条规则中的目标将被确立为最终的目标。如果第一条规则中的目标有很多个，那么，第一个目标会成为最终的目标。make所完成的也就是这个目标。 规则举例 foo.o: foo.c defs.h # foo模块 cc -c -g foo.c foo.o是我们的目标，foo.c和defs.h是目标所依赖的源文件，而只有一个命令“cc -c -g foo.c”（以Tab键开头） 文件的依赖关系，foo.o依赖于foo.c和defs.h的文件，如果foo.c和defs.h的文件日期要比foo.o文件日期要新，或是foo.o不存在，那么依赖关系发生。 如果生成（或更新）foo.o文件。也就是那个cc命令，其说明了，如何生成foo.o这个文件。（当然foo.c文件include了defs.h文件） ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2. 规则的语法 targets : prerequisites command #或： targets : prerequisites ; command command targets是文件名，以空格分开，可以使用通配符。一般来说，我们的目标基本上是一个文件，但也有可能是多个文件。 command是命令行，如果其不与“target:prerequisites”在一行，那么，必须以[Tab键]开头，如果和prerequisites在一行，那么可以用分号做为分隔 prerequisites也就是目标所依赖的文件（或依赖目标）。如果其中的某个文件要比目标文件要新，那么，目标就被认为是“过时的”，被认为是需要重生成的。 如果命令太长，你可以使用反斜框（‘\\’）作为换行符。make对一行上有多少个字符没有限制。规则告诉make两件事，文件的依赖关系和如何成成目标文件。 一般来说，make会以UNIX的标准Shell，也就是/bin/sh来执行命令。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. 在规则中使用通配符 make支持三各通配符：“*”，“?”和“[…]”。这是和Unix的B-Shell是相同的。 “~”：波浪号（“~”）字符在文件名中也有比较特殊的用途。如果是“~/test”，这就表示当前用户的$HOME目录下的test目录。而“~hchen/test”则表示用户hchen的宿主目录下的test目录。（这些都是Unix下的小知识了，make也支持）而在Windows或是MS-DOS下，用户没有宿主目录，那么波浪号所指的目录则根据环境变量“HOME”而定。 “*”：通配符代替了你一系列的文件，如“.c”表示所以后缀为c的文件。一个需要我们注意的是，如果我们的文件名中有通配符，如：“”，那么可以用转义字符“\\”，如“*”来表示真实的“*”字符，而不是任意长度的字符串。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. 静态模式 静态模式可以更加容易地定义多目标的规则，可以让我们的规则变得更加的有弹性和灵活。语法： \u003ctargets...\u003e: \u003ctarget-pattern\u003e: \u003cprereq-patterns ...\u003e \u003ccommands\u003e targets定义了一系列的目标文件，可以有通配符。是目标的一个集合。 target-parrtern是指明了targets的模式，也就是的目标集模式。 prereq-parrterns是目标的依赖模式，它对target-parrtern形成的模式再进行一次依赖目标的定义。 说明: 如果我们的定义成“%.o”，意思是我们的集合中都是以“.o”结尾的。 而如果我们的定义成“%.c”，意思是对所形成的目标集进行二次定义，其计算方法是，取模式中的“%”（也就是去掉了[.o]这个结尾），并为其加上[.c]这个结尾，形成的新集合。 所以，我们的“目标模式”或是“依赖模式”中都应该有“%”这个字符 实例： objects = foo.o bar.o all: $(objects) $(objects): %.o: %.c $(CC) -c $(CFLAGS) $\u003c -o $@ 例子中，指明了目标从$objects中获取 “%.o”表明要所有以“.o”结尾的目标，也就是\"foo.o bar.o”，也就是变量$object集合的模式 而依赖模式“%.c”则取模式“%.o”的“%”，也就是“foobar”，并为其加下“.c”的后缀，于是，我们的依赖目标就是“foo.c bar.c” 而命令中的“$\u003c”和“$@”则是自动化变量，“$\u003c”表示所有的依赖目标集（也就是“foo.c bar.c”），“$@”表示目标集（也褪恰癴oo.o bar.o”） 上面的规则展开后等价于： foo.o : foo.c $(CC) -c $(CFLAGS) foo.c -o foo.o bar.o : bar.c $(CC) -c $(CFLAGS) bar.c -o bar.o ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile书写命令 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. 显示命令 通常，make会把其要执行的命令行在命令执行前输出到屏幕上。当我们用“@”字符在命令行前，那么，这个命令将不被make显示出来，最具代表性的例子是，我们用这个功能来像屏幕显示一些信息。如： @echo 正在编译XXX模块…… ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2.赋值命令 # = 是最基本的赋值 # := 是覆盖之前的值 # ?= 是如果没有被赋值过就赋予等号后面的值 # += 是添加等号后面的值 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. 指定目标 1. “all” 这个伪目标是所有目标的目标，其功能一般是编译所有的目标。 2. “clean” 这个伪目标功能是删除所有被make创建的文件。 3. “install” 这个伪目标功能是安装已编译好的程序，其实就是把目标执行文件拷贝到指定的目标中去。 4. “print” 这个伪目标的功能是例出改变过的源文件。 5. “tar” 这个伪目标功能是把源程序打包备份。也就是一个tar文件。 6. “dist” 这个伪目标功能是创建一个压缩文件，一般是把tar文件压成Z文件。或是gz文件。 7. “TAGS” 这个伪目标功能是更新所有的目标，以备完整地重编译使用。 8. “check”和“test” 这两个伪目标一般用来测试makefile的流程。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. 自动化变量 $@表示规则中的目标文件集。在模式规则中，如果有多个目标，那么，\"$@“就是匹配于目标中模式定义的集合。 $%仅当目标是函数库文件中，表示规则中的目标成员名。例如，如果一个目标是\"foo.a(bar.o)\"，那么，\"$%“就是\"bar.o”，\"$@“就是\"foo.a”。如果目标不是函数库文件（Unix下是 [.a]，Windows下是[.lib]），那么，其值为空。 $\u003c依赖目标中的第一个目标名字。如果依赖目标是以模式（即”%\"）定义的，那么\"$\u003c“将是符合模式的一系列的文件集。注意，其是一个一个取出来的。 $?所有比目标新的依赖目标的集合。以空格分隔。 $^所有的依赖目标的集合。以空格分隔。如果在依赖目标中有多个重复的，那个这个变量会去除重复的依赖目标，只保留一份。 $+这个变量很像”$^\"，也是所有依赖目标的集合。只是它不去除重复的依赖目标。 $*这个变量表示目标模式中\"%“及其之前的部分。如果目标是\"dir/a.foo.b”，并且目标的模式是\"a.%.b\"，那么，\"$*“的值就是\"dir /a.foo”。这个变量对于构造有关联的文件名是比 较有较。如果目标中没有模式的定义，那么\"$*“也就不能被推导出，但是，如果目标文件的后缀是 make所识别的，那么”$*“就是除了后缀的那一部分。例如：如果目标是\"foo.c” ，因为\".c\"是make所能识别的后缀名，所以，\"$*“的值就是\"foo”。这个特性是GNU make的，很有可能不兼容于其它版本的make，所以，你应该尽量避免使用\"$*\"，除非是在隐含规则或是静态模式中。如果目标中的后缀是make所不能识别的，那么\"$*“就是空值。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Article description.","date":"2018-08-12","objectID":"/20180812_hyperscan/","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"Hyperscan is a high-performance regular expression matching library from Intel. It is based on the X86 platform based on PCRE prototype development. While supporting most of the syntax of PCRE, Hyperscan adds specific syntax and working modes to ensure its usefulness in real-world network scenarios. ","date":"2018-08-12","objectID":"/20180812_hyperscan/:0:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"1. 概述 Hyperscan demo中使用libpcap从pcap文件中读取数据包，并根据一个规则文件中指定的多个正则表达式对报文进行匹配，并输出匹配结果和一些统计信息。Hyperscan增加了特定的语法和工作模式来保证其在真实网络场景下的实用性。与此同时，大量高效算法及IntelSIMD*指令的使用实现了Hyperscan的高性能匹配。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:1:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"2. KeyWords \u0026 KeyFunc Patterns: 规则，用来匹配关键词的规则，支持PCRE的大部分语法（正则表达式c十六进制.etc） id: 与规则绑定,一个规则对应一个不同的id，匹配命中时返回pattern对应的id flags: 与规则绑定,对pattern进行特殊操作，如：与或非逻辑运算(绑定多个pattern),忽略大小写，多行匹配，单次匹配.etc hs_compile_*(): 将patterns生成无向连通图(database)，匹配时把数据往连通图里迭代遍历 Scratch()：在扫描数据时，Hyperscan需要少量的临时内存来存储动态内部数据。但database的数量太大了，无法装入堆栈，特别是对于嵌入式应用程序，而且动态分配内存过于昂贵，因此必须为扫描函数提供预先分配的“Scratch”空间。 Serialization(): 将生成的database序列化成二进制文件，再由凡序列化拿到databse。 Scan(): 匹配，将需要匹配的数据放入database无向连通图中匹配，命中后调用回调函数。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:2:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"3. 原理 Hyperscan以自动机理论为基础，其工作流程主要分成两个部分：编译期(compiletime)和运行期(run-time)。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:3:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"3.1编译期 Hyperscan 自带C++编写的正则表达式编译器。如图1所示，它将正则表达式作为输入，针对不同的平台，用户定义的模式及特殊语法，经过复杂的图分析及优化过程，生成对应的数据库。另外，生成的数据库可以被序列化后保存在内存中，以供运行期提取使用。 Hyperscan-fig-1 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:3:1","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"3.2运行期 Hyperscan的运行期是通过C语言来开发的。图2展示了Hyperscan在运行期的主要流程。用户需要预先分配一段内存来存储临时匹配状态信息，之后利用编译生成的数据库调用Hyperscan内部的匹配引擎(NFA, DFA等)来对输入进行模式匹配。Hyperscan在引擎中使用Intel处理器所具有的SIMD指令进行加速。同时，用户可以通过回调函数来自定义匹配发生后采取的行为。由于生成的数据库是只读的，用户可以在多个CPU核或多线程场景下共享数据库来提升匹配扩展性。 Hyperscan-fig-2 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:3:2","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4. Hyperscan伪代码 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.1 编译 函数buildDatabase用来编译规则文件中的多个正则表达式，参数mode指定了是BLOCK,STREAM,或者向量模式。 static hs_database_t *buildDatabase(const vector\u003cconst char *\u003e \u0026expressions, const vector\u003cunsigned\u003e flags, const vector\u003cunsigned\u003e ids, unsigned int mode) { hs_database_t *db; hs_compile_error_t *compileErr; hs_error_t err; Clock clock; clock.start(); err = hs_compile_multi(expressions.data(), flags.data(), ids.data(), expressions.size(), mode, nullptr, \u0026db, \u0026compileErr); clock.stop(); if (err != HS_SUCCESS) { if (compileErr-\u003eexpression \u003c 0) { // The error does not refer to a particular expression. cerr \u003c\u003c \"ERROR: \" \u003c\u003c compileErr-\u003emessage \u003c\u003c endl; } else { cerr \u003c\u003c \"ERROR: Pattern '\" \u003c\u003c expressions[compileErr-\u003eexpression] \u003c\u003c \"' failed compilation with error: \" \u003c\u003c compileErr-\u003emessage \u003c\u003c endl; } // As the compileErr pointer points to dynamically allocated memory, if // we get an error, we must be sure to release it. This is not // necessary when no error is detected. hs_free_compile_error(compileErr); exit(-1); } //... } 其中的核心代码是hs_compile_multi的调用，此函数用来编译多个正则表达式，从代码可见除了mode参数，BLOCK和STREAM模式都使用这一API。它的原型是 hs_error_t hs_compile_multi(const char *const * expressions, const unsigned int * flags, const unsigned int * ids, unsigned int elements, unsigned int mode, const hs_platform_info_t * platform, hs_database_t ** db, hs_compile_error_t ** error) 其中，expressions是多个正则表达式字符串，flags和ids分别是expressions对应的flag和id数组；elements是表达式字符串的个数；其余参数与上一个例子中提到的hs_compile的参数涵义相同。 这里要注意的一个事情是参数ids，它是正则表达式的ID数组。每个表达式都有一个唯一ID，这样命中的时候匹配回调函数可以得到此ID，告诉调用者哪个表达式命中了。如果ids传入NULL，则所有表达式的ID都为0。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:1","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.2 准备匹配临时数据 为接下来的匹配分配足够的临时数据空间(scratch space） public: Benchmark(const hs_database_t *streaming, const hs_database_t *block) : db_streaming(streaming), db_block(block), scratch(nullptr), matchCount(0) { // Allocate enough scratch space to handle either streaming or block // mode, so we only need the one scratch region. hs_error_t err = hs_alloc_scratch(db_streaming, \u0026scratch); if (err != HS_SUCCESS) { cerr \u003c\u003c \"ERROR: could not allocate scratch space. Exiting.\" \u003c\u003c endl; exit(-1); } // This second call will increase the scratch size if more is required // for block mode. err = hs_alloc_scratch(db_block, \u0026scratch); if (err != HS_SUCCESS) { cerr \u003c\u003c \"ERROR: could not allocate scratch space. Exiting.\" \u003c\u003c endl; exit(-1); } } ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:2","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.3 匹配 4.3.1 BLOCK模式 // Scan each packet (in the ordering given in the PCAP file) through // Hyperscan using the block-mode interface. void scanBlock() { for (size_t i = 0; i != packets.size(); ++i) { const std::string \u0026pkt = packets[i]; hs_error_t err = hs_scan(db_block, pkt.c_str(), pkt.length(), 0, scratch, onMatch, \u0026matchCount); if (err != HS_SUCCESS) { cerr \u003c\u003c \"ERROR: Unable to scan packet. Exiting.\" \u003c\u003c endl; exit(-1); } } } 其中，db就是上一步编译的databas；data和length分别是要匹配的数据和数据长度；flags用来在未来版本中控制函数行为，目前未使用；scratch是匹配时要用的临时数据，之前已经分配好；onEvent非常关键，即匹配时调用的回调函数，由用户指定；context是用户自定义指针。 4.3.2 匹配回调函数 匹配回调函数的原型是 typedef (* match_event_handler)(unsigned int id, unsigned long long from, unsigned long long to, unsigned int flags, void *context) 其中，id是命中的正则表达式的ID，对于使用hs_compile编译的唯一表达式来说，此值为0；如果在编译时指定了相关模式选项(hs_compile中的mode参数），则此值将会设为匹配特征的起始位置，否则会设为0；to是命中数据的下一个字节的偏移；flags目前未用；context是用户自定义指针。 返回值为非0表示停止匹配，否则继续；在匹配的过程中，每次命中时都将同步调用匹配回调函数，直到匹配结束。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:3","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.4 清理资源 包括关闭流（hs_close_stream）、释放database等。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:4","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"Article description.","date":"2018-06-10","objectID":"/20180610_c-getline/","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"Getline ()/get()/read() will read one more line. The cause may be a problem with the file itself or the getline() function. You can judge getLine ()/get()/read() while checking, and then process the data if you get it. Insert Lead paragraph here. ","date":"2018-06-10","objectID":"/20180610_c-getline/:0:0","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"1. 问题原因 ","date":"2018-06-10","objectID":"/20180610_c-getline/:1:0","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"1. 问题1: 文件末尾存在回车 while (!feof(fp)) { fgets(buffer,256,fp); j++; } feof（）这个函数是用来判断指针是否已经到达文件尾部的。若fp已经指向文件末尾，则feof（fp）函数值为“真”，即返回非零值；否则返回0。 如果文件还有换行或者空格的时候， 他会继续循环。 ","date":"2018-06-10","objectID":"/20180610_c-getline/:1:1","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"2.问题2: getline(s,1024,’\\n’)函数 while(!feof(s)) { infile.getline(s,1024,'\\n'); } 最后语句infile.getline(s,1024,’\\n’)未读到内容，出错后，变量s的内容并没改变，程序仍可继续执行，使s中的原数据再使用了一次。 ","date":"2018-06-10","objectID":"/20180610_c-getline/:1:2","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"2. 解决方法 ","date":"2018-06-10","objectID":"/20180610_c-getline/:2:0","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"1. fgets放到while里判断 while (fgets(buffer,256,fp)) { j++; } ","date":"2018-06-10","objectID":"/20180610_c-getline/:2:1","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"2.getline放到while里判断 while(infile.getline(s,1024,'\\n')) { ....... } 即infile.getline(s,1024,’\\n’)正确读到数据后再处理。 同理，对get()/read()等都类似处理。 ","date":"2018-06-10","objectID":"/20180610_c-getline/:2:2","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"Article description.","date":"2018-05-07","objectID":"/20180507_mxml/","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"MXML (Minimal XML) is a small, fast and versatile library that reads a whole XML file and puts it in a DOM tree. ","date":"2018-05-07","objectID":"/20180507_mxml/:0:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"1.XML XML特点和作用 : XML 指可扩展标识语言（ eXtensible Markup Language） XML 的设计宗旨是传输数据， 而非显示数据 XML 标签没有被预定义。 您需要自行定义标签。 作为一种通用的数据存储和通信格式被广泛应用。 描述的数据作为一棵树型的结构而存在。 \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cbooks\u003e \u003cbook id=\"001\"\u003e \u003cauthor\u003ewanger\u003c/author\u003e \u003c/book\u003e \u003c/books\u003e ","date":"2018-05-07","objectID":"/20180507_mxml/:1:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"2.mxml写入操作 mxml既可以创建写入xml文件，也可以解析xml文件数据 写入示例: #include\"mxml.h\" int main() { // create a new xml mxml_node_t *xml = mxmlNewXML(\"1.0\"); //add a node name books mxml_node_t *books = mxmlNewElement(xml,\"books\"); //add a node under books name book mxml_node_t *book = mxmlNewElement(books,\"book\"); //set attr mxmlElementSetAttr(book,\"id\",\"001\"); mxml_node_t *author = mxmlNewElement(book,\"author\"); mxmlNewText(author,0,\"wanger\"); FILE* fp = fopen(\"book.xml\",\"wb\"); mxmlSaveFile(xml,fp,MXML_NO_CALLBACK); fclose(fp); mxmlDelete(xml); return 0; } ","date":"2018-05-07","objectID":"/20180507_mxml/:2:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"3.mxml解析操作 解析步骤： 步骤： 打开一个xml文件 把文件加载到内存中，mxml_node_t mxmlLoadFile(mxml_node_t *top, FILE *fp,mxml_type_t (*cb)(mxml_node_t )); 查找待提取的节点标签：mxml_node_t *mxmlFindElement(mxml_node_t *node, mxml_node_t *top,const char *name, const char *attr,const char *value, int descend); 获取标签属性：const char *mxmlElementGetAttr(mxml_node_t *node, const char *name); 获取标签的文本内容： const char *mxmlGetText(mxml_node_t *node, int *whitespace); 释放节点，关闭文件 示例： 解析的文本： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cbooks\u003e \u003cbook id=\"001\"\u003e \u003cauthor\u003ewanger\u003c/author\u003e \u003c/book\u003e \u003c/books\u003e 代码： #include\"mxml.h\" #include\u003cstdio.h\u003e int main() { FILE* fp = fopen(\"book.xml\",\"r\"); // jiazai xml mxml_node_t* xml = mxmlLoadFile(NULL,fp,MXML_NO_CALLBACK); mxml_node_t* book = NULL; mxml_node_t* author = NULL; // find note book = mxmlFindElement(xml,xml,\"book\",\"id\",NULL,MXML_DESCEND); //get attr author = mxmlFindElement(book,xml,\"author\",NULL,NULL,MXML_DESCEND); if(author == NULL) { printf(\"author error\\n\"); } else { printf(\"book id is:%s\\n\",mxmlElementGetAttr(book,\"id\")); printf(\"author is:%s\\n\",mxmlGetText(author,NULL)); book = mxmlFindElement(xml,xml,\"book\",\"id\",NULL,MXML_DESCEND); } mxmlDelete(xml); fclose(fp); return 0; } 结果： book id is:001 author is:wanger ","date":"2018-05-07","objectID":"/20180507_mxml/:3:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"4.mxml for循环解析长文件 xml文件: \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cproto-meta-dump\u003e \u003cproto name=\"AH\"\u003e \u003cproto-class\u003ecrypto\u003c/proto-class\u003e \u003cproto-suit\u003eTCP/IP\u003c/proto-suit\u003e \u003cproto-desc\u003e\u003c/proto-desc\u003e \u003cdump-name\u003e\u003c/dump-name\u003e \u003cmeta-data name=\"\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003etotMeta\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.spi\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003eSPI\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.sequence\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003eseqNum\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.length\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003epayLen\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.icv\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003eICV\u003c/dump-name\u003e \u003c/meta-data\u003e \u003c/proto\u003e ... \u003c/proto-meta-dump\u003e 解析代码： //open xml file FILE* fp = fopen(fileName,\"r\"); //create mxml tree head node mxml_node_t* tree = mxmlLoadFile(NULL,fp,MXML_NO_CALLBACK); //build head node:proto_meta_dump //第一层 mxml_node_t* proto = NULL; //第二层 mxml_node_t* class = NULL; mxml_node_t* suit = NULL; mxml_node_t* desc = NULL; mxml_node_t* name = NULL; //第三层 mxml_node_t* meta_data = NULL; mxml_node_t* dump_on = NULL; mxml_node_t* dump_format = NULL; mxml_node_t* dump_name = NULL; //遍历第一层proto，mxmlFindElement函数：寻找下一个proto节点 for(proto = mxmlFindElement(tree,tree,\"proto\",\"name\",NULL,MXML_DESCEND);proto!=NULL;proto = mxmlFindElement(proto,tree,\"proto\",\"name\",NULL,MXML_DESCEND)){ class = mxmlFindElement(proto,tree,\"proto-class\",NULL,NULL,MXML_DESCEND); suit = mxmlFindElement(proto,tree,\"proto-suit\",NULL,NULL,MXML_DESCEND); desc = mxmlFindElement(proto,tree,\"proto-desc\",NULL,NULL,MXML_DESCEND); name = mxmlFindElement(proto,tree,\"dump-name\",NULL,NULL,MXML_DESCEND); //拿到第一层数据 printf(\"|proto:%s\\n\",mxmlElementGetAttr(proto,\"name\")); printf(\" |----class:%s\\n\",mxmlGetText(class,NULL)); printf(\" |----suit:%s\\n\",mxmlGetText(suit,NULL)); printf(\" |----desc:%s\\n\",mxmlGetText(desc,NULL)); printf(\" |----name:%s\\n\",mxmlGetText(name,NULL)); //遍历第二层meta-data for(meta_data = mxmlFindElement(proto,proto,\"meta-data\",\"name\",NULL,MXML_DESCEND);meta_data!=NULL;meta_data = mxmlFindElement(meta_data,proto,\"meta-data\",\"name\",NULL,MXML_DESCEND)){ //拿到第二层数据 dump_on = mxmlFindElement(meta_data,proto,\"dump-on\",NULL,NULL,MXML_DESCEND); dump_id = mxmlFindElement(meta_data,proto,\"dump-id\",NULL,NULL,MXML_DESCEND); dump_format = mxmlFindElement(meta_data,proto,\"dump-format\",NULL,NULL,MXML_DESCEND); dump_name = mxmlFindElement(meta_data,proto,\"dump-name\",NULL,NULL,MXML_DESCEND); printf(...,mxmlGetText(...)); ... } } mxmlDelete(tree); fclose(fp); ","date":"2018-05-07","objectID":"/20180507_mxml/:4:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"Article description.","date":"2018-04-05","objectID":"/20180405_hashcode/","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"Hash table is a very important data structure, which is useful in many application scenarios. This paper will simply analyze the principle of hash table, and use C language to achieve a complete HashMap. ","date":"2018-04-05","objectID":"/20180405_hashcode/:0:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"1. 什么是HashMap？ 存储方式主要有两种线性存储和链式存储，常见的线性存储例如数组，常见的链式存储如链表、二叉树等。哈希表的存储主干为线性存储，这也是它在理想状态(无冲突)下时间复杂度为O(1)的关键所在。普通线性存储的存储内容与索引地址之间没有任何的联系，只能通过索引地址推算出存储内容，不能从存储内容推算出索引地址，是一个单向不可逆的过程，而HashMap存储的是一个\u003ckey, value\u003e的键值对，通过key和索引地址建立了一层关系，这层关系称之为哈希函数(或散列函数)，这样既可以通过key推算出索引地址，也可以通过推算出的索引地址直接定位到键值对，这是一个双向可逆的过程。需要注意的一点是HashMap并不直接暴露出键值对的索引地址，但是可以通过哈希函数推算出HashCode，其实HashCode就是真实的索引地址。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:1:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"2. 定义键值对结构 typedef struct entry { void * key; // 键 void * value; // 值 struct entry * next; // 冲突链表 }*Entry; #define newEntry() NEW(struct entry) #define newEntryList(length) (Entry)malloc(length * sizeof(struct entry)) 哈希冲突是指两个不同的key值得到了一个相同的HashCode，这种情况称之为哈希冲突，一个好的哈希函数很大程度上决定了哈希表的性能，不存在一种适合所有哈希表的哈希函数，在很多特定的情景下，需要有针对性的设计哈希函数才能达到理想的效果。当然啦，还是有一些优秀的哈希函数可以应对大多数情况的，对于性能要求不是很高的场景用这些就可以了。使用HashMap的时候难免会发生冲突，常用的方法主要分为两类：再散列法和链地址法。再散列法就是发生冲突时使用另一个哈希函数重新推算，一直到不冲突为止，这种方法有时候会造成数据堆积，就是元素本来的HashCode被其它元素再散列的HashCode占用，被迫再散列，如此恶性循环。链地址法就是在冲突的位置建立一个链表，将冲突值放在链表中，检索的时候需要额外的遍历冲突链表，本文采用的就是链地址法。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:2:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"3. 定义HashMap结构体 HashMap结构的存储本体是一个数组，建立一个Entry数组作为存储空间，然后根据传入的key计算出HashCode，当做数组的索引存入数据，读取的时候通过计算出的HashCode可以在数组中直接取出值。 size是当前存储键值对的数量，而listSize是当前数组的大小，仔细观察键值对结构会发现，数组的每一项其实都是冲突链表的头节点。因为冲突的存在，就有可能导致size大于listSize，当size大于listSize的时候一定发生了冲突，这时候就会扩容。 在结构体中放了一些常用的方法，因为C语言本身并没有类的概念，为了便于内部封装(经常会有一个方法调用另一个方法的时候)，可以让用户自定义一个方法而不影响其它方法的调用。举个简单的例子，put方法中调用了hashCode函数，如果想自定义一个hashCode方法，迫不得已还要再实现一个put方法，哪怕put中只改了一行代码。 结构体定义如下： // 哈希结构 typedef struct hashMap *HashMap; #define newHashMap() NEW(struct hashMap) // 哈希函数类型 typedef int(*HashCode)(HashMap, void * key); // 判等函数类型 typedef Boolean(*Equal)(void * key1, void * key2); // 添加键函数类型 typedef void(*Put)(HashMap hashMap, void * key, void * value); // 获取键对应值的函数类型 typedef void * (*Get)(HashMap hashMap, void * key); // 删除键的函数类型 typedef Boolean(*Remove)(HashMap hashMap, void * key); // 清空Map的函数类型 typedef void(*Clear)(HashMap hashMap); // 判断键值是否存在的函数类型 typedef Boolean(*Exists)(HashMap hashMap, void * key); typedef struct hashMap { int size; // 当前大小 int listSize; // 有效空间大小 HashCode hashCode; // 哈希函数 Equal equal; // 判等函数 Entry list; // 存储区域 Put put; // 添加键的函数 Get get; // 获取键对应值的函数 Remove remove; // 删除键 Clear clear; // 清空Map Exists exists; // 判断键是否存在 Boolean autoAssign; // 设定是否根据当前数据量动态调整内存大小，默认开启 }*HashMap; // 默认哈希函数 static int defaultHashCode(HashMap hashMap, void * key); // 默认判断键值是否相等 static Boolean defaultEqual(void * key1, void * key2); // 默认添加键值对 static void defaultPut(HashMap hashMap, void * key, void * value); // 默认获取键对应值 static void * defaultGet(HashMap hashMap, void * key); // 默认删除键 static Boolean defaultRemove(HashMap hashMap, void * key); // 默认判断键是否存在 static Boolean defaultExists(HashMap hashMap, void * key); // 默认清空Map static void defaultClear(HashMap hashMap); // 创建一个哈希结构 HashMap createHashMap(HashCode hashCode, Equal equal); // 重新构建 static void resetHashMap(HashMap hashMap, int listSize); HashMap的所有属性方法都有一个默认的实现，创建HashMap时可以指定哈希函数和判等函数(用于比较两个key是否相等)，传入NULL时将使用默认函数。这些函数都被设置为了static，在文件外不可访问。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:3:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"4. 哈希函数 int defaultHashCode(HashMap hashMap, let key) { IN_STACK; string k = (string)key; unsigned long h = 0; while (*k) { h = (h \u003c\u003c 4) + *k++; unsigned long g = h \u0026 0xF0000000L; if (g) { h ^= g \u003e\u003e 24; } h \u0026= ~g; } OUT_STACK; return h % hashMap-\u003elistSize; } key的类型为void *，是一个任意类型，HashMap本身也没有规定key值一定是string类型，上面的哈希函数只针对string类型，可以根据实际需要替换成其他。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:4:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"5. put函数 用于在哈希表中存入一个键值对，首先先推算出HashCode，然后判断该地址是否已经有数据，如果已有的key值和存入的key值相同，改变value即可，否则为冲突，需要挂到冲突链尾部，该地址没有数据时直接存储。实现如下： void resetHashMap(HashMap hashMap, int listSize) { if (listSize \u003c 8) return; // 键值对临时存储空间 Entry tempList = newEntryList(hashMap-\u003esize); HashMapIterator iterator = createHashMapIterator(hashMap); int length = hashMap-\u003esize; for (int index = 0; hasNextHashMapIterator(iterator); index++) { // 迭代取出所有键值对 iterator = nextHashMapIterator(iterator); tempList[index].key = iterator-\u003eentry-\u003ekey; tempList[index].value = iterator-\u003eentry-\u003evalue; tempList[index].next = NULL; } freeHashMapIterator(\u0026iterator); // 清除原有键值对数据 hashMap-\u003esize = 0; for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { Entry current = \u0026hashMap-\u003elist[i]; current-\u003ekey = NULL; current-\u003evalue = NULL; if (current-\u003enext != NULL) { while (current-\u003enext != NULL) { Entry temp = current-\u003enext-\u003enext; free(current-\u003enext); current-\u003enext = temp; } } } // 更改内存大小 hashMap-\u003elistSize = listSize; Entry relist = (Entry)realloc(hashMap-\u003elist, hashMap-\u003elistSize * sizeof(struct entry)); if (relist != NULL) { hashMap-\u003elist = relist; relist = NULL; } // 初始化数据 for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { hashMap-\u003elist[i].key = NULL; hashMap-\u003elist[i].value = NULL; hashMap-\u003elist[i].next = NULL; } // 将所有键值对重新写入内存 for (int i = 0; i \u003c length; i++) { Array x = tempList[i].value; hashMap-\u003eput(hashMap, tempList[i].key, tempList[i].value); } free(tempList); } void defaultPut(HashMap hashMap, let key, let value) { if (hashMap-\u003eautoAssign \u0026\u0026 hashMap-\u003esize \u003e= hashMap-\u003elistSize) { // 内存扩充至原来的两倍 // *注: 扩充时考虑的是当前存储元素数量与存储空间的大小关系，而不是存储空间是否已经存满， // 例如: 存储空间为10，存入了10个键值对，但是全部冲突了，所以存储空间空着9个，其余的全部挂在一个上面， // 这样检索的时候和遍历查询没有什么区别了，可以简单这样理解，当我存入第11个键值对的时候一定会发生冲突， // 这是由哈希函数本身的特性(取模)决定的，冲突就会导致检索变慢，所以这时候扩充存储空间，对原有键值对进行 // 再次散列，会把冲突的数据再次分散开，加快索引定位速度。 resetHashMap(hashMap, hashMap-\u003elistSize * 2); } int index = hashMap-\u003ehashCode(hashMap, key); if (hashMap-\u003elist[index].key == NULL) { hashMap-\u003esize++; // 该地址为空时直接存储 Array x = value; hashMap-\u003elist[index].key = key; hashMap-\u003elist[index].value = value; } else { Entry current = \u0026hashMap-\u003elist[index]; while (current != NULL) { if (hashMap-\u003eequal(key, current-\u003ekey)) { // 对于键值已经存在的直接覆盖 current-\u003evalue = value; return; } current = current-\u003enext; }; // 发生冲突则创建节点挂到相应位置的next上 Entry entry = newEntry(); entry-\u003ekey = key; entry-\u003evalue = value; entry-\u003enext = hashMap-\u003elist[index].next; hashMap-\u003elist[index].next = entry; hashMap-\u003esize++; } } put函数还有一个重要的功能，当size大于listSize时要主动扩容，这个判定条件看似有些不合理，当size大于listSize的时候可能因为冲突的存在，数组并没有存满，这时候就扩容不是浪费存储空间吗？事实确实如此，但这其实是为了加快检索速度一种妥协的办法，上文提到过，当size大于listSize时一定会发生冲突，因为哈希函数为了不越界，都会将计算出的HashCode进行取余操作，这就导致HashCode的个数一共就listSize个，超过这个个数就一定会冲突，冲突的越多，检索速度就越向O(n)靠拢，为了保证索引速度消耗一定的空间还是比较划算的，扩容时直接将容量变为了当前的两倍，这是考虑到扩容时需要将所有重新计算所有元素的HashCode，较为消耗时间，所以应该尽量的减少扩容次数。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:5:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"6. 其它函数 let defaultGet(HashMap hashMap, let key) { int index = hashMap-\u003ehashCode(hashMap, key); Entry entry = \u0026hashMap-\u003elist[index]; while (entry-\u003ekey != NULL \u0026\u0026 !hashMap-\u003eequal(entry-\u003ekey, key)) { entry = entry-\u003enext; } return entry-\u003evalue; } Boolean defaultRemove(HashMap hashMap, let key) { int index = hashMap-\u003ehashCode(hashMap, key); Entry entry = \u0026hashMap-\u003elist[index]; if (entry-\u003ekey == NULL) { return False; } Boolean result = False; if (hashMap-\u003eequal(entry-\u003ekey, key)) { hashMap-\u003esize--; if (entry-\u003enext != NULL) { Entry temp = entry-\u003enext; entry-\u003ekey = temp-\u003ekey; entry-\u003evalue = temp-\u003evalue; entry-\u003enext = temp-\u003enext; free(temp); } else { entry-\u003ekey = entry-\u003evalue = NULL; } result = True; } else { Entry p = entry; entry = entry-\u003enext; while (entry != NULL) { if (hashMap-\u003eequal(entry-\u003ekey, key)) { hashMap-\u003esize--; p-\u003enext = entry-\u003enext; free(entry); result = True; break; } p = entry; entry = entry-\u003enext; }; } // 如果空间占用不足一半，则释放多余内存 if (result \u0026\u0026 hashMap-\u003eautoAssign \u0026\u0026 hashMap-\u003esize \u003c hashMap-\u003elistSize / 2) { resetHashMap(hashMap, hashMap-\u003elistSize / 2); } return result; } Boolean defaultExists(HashMap hashMap, let key) { int index = hashMap-\u003ehashCode(hashMap, key); Entry entry = \u0026hashMap-\u003elist[index]; if (entry-\u003ekey == NULL) { return False; } if (hashMap-\u003eequal(entry-\u003ekey, key)) { return True; } if (entry-\u003enext != NULL) { do { if (hashMap-\u003eequal(entry-\u003ekey, key)) { return True; } entry = entry-\u003enext; } while (entry != NULL); return False; } else { return False; } } void defaultClear(HashMap hashMap) { for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { // 释放冲突值内存 Entry entry = hashMap-\u003elist[i].next; while (entry != NULL) { Entry next = entry-\u003enext; free(entry); entry = next; } hashMap-\u003elist[i].next = NULL; } // 释放存储空间 free(hashMap-\u003elist); hashMap-\u003elist = NULL; hashMap-\u003esize = -1; hashMap-\u003elistSize = 0; } HashMap createHashMap(HashCode hashCode, Equal equal) { HashMap hashMap = newHashMap(); hashMap-\u003esize = 0; hashMap-\u003elistSize = 8; hashMap-\u003ehashCode = hashCode == NULL ? defaultHashCode : hashCode; hashMap-\u003eequal = equal == NULL ? defaultEqual : equal; hashMap-\u003eexists = defaultExists; hashMap-\u003eget = defaultGet; hashMap-\u003eput = defaultPut; hashMap-\u003eremove = defaultRemove; hashMap-\u003eclear = defaultClear; hashMap-\u003eautoAssign = True; // 起始分配8个内存空间，溢出时会自动扩充 hashMap-\u003elist = newEntryList(hashMap-\u003elistSize); Entry p = hashMap-\u003elist; for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { p[i].key = p[i].value = p[i].next = NULL; } return hashMap; } ","date":"2018-04-05","objectID":"/20180405_hashcode/:6:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"7. Iterator接口 Iterator接口提供了遍历HashMap结构的方法，基本定义如下： // 迭代器结构 typedef struct hashMapIterator { Entry entry; // 迭代器当前指向 int count; // 迭代次数 int hashCode; // 键值对的哈希值 HashMap hashMap; }*HashMapIterator; #define newHashMapIterator() NEW(struct hashMapIterator) // 创建一个哈希结构 HashMap createHashMap(HashCode hashCode, Equal equal); // 创建哈希结构迭代器 HashMapIterator createHashMapIterator(HashMap hashMap); // 迭代器是否有下一个 Boolean hasNextHashMapIterator(HashMapIterator iterator); // 迭代到下一次 HashMapIterator nextHashMapIterator(HashMapIterator iterator); // 释放迭代器内存 void freeHashMapIterator(HashMapIterator * iterator); 实现如下: HashMapIterator createHashMapIterator(HashMap hashMap) { HashMapIterator iterator = newHashMapIterator(); iterator-\u003ehashMap = hashMap; iterator-\u003ecount = 0; iterator-\u003ehashCode = -1; iterator-\u003eentry = NULL; return iterator; } Boolean hasNextHashMapIterator(HashMapIterator iterator) { return iterator-\u003ecount \u003c iterator-\u003ehashMap-\u003esize ? True : False; } HashMapIterator nextHashMapIterator(HashMapIterator iterator) { if (hasNextHashMapIterator(iterator)) { if (iterator-\u003eentry != NULL \u0026\u0026 iterator-\u003eentry-\u003enext != NULL) { iterator-\u003ecount++; iterator-\u003eentry = iterator-\u003eentry-\u003enext; return iterator; } while (++iterator-\u003ehashCode \u003c iterator-\u003ehashMap-\u003elistSize) { Entry entry = \u0026iterator-\u003ehashMap-\u003elist[iterator-\u003ehashCode]; if (entry-\u003ekey != NULL) { iterator-\u003ecount++; iterator-\u003eentry = entry; break; } } } return iterator; } void freeHashMapIterator(HashMapIterator * iterator) { free(*iterator); *iterator = NULL; } ","date":"2018-04-05","objectID":"/20180405_hashcode/:7:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"8. 使用测试 #define Put(map, key, value) map-\u003eput(map, (void *)key, (void *)value); #define Get(map, key) (char *)map-\u003eget(map, (void *)key) #define Remove(map, key) map-\u003eremove(map, (void *)key) #define Existe(map, key) map-\u003eexists(map, (void *)key) int main() { HashMap map = createHashMap(NULL, NULL); Put(map, \"asdfasdf\", \"asdfasdfds\"); Put(map, \"sasdasd\", \"asdfasdfds\"); Put(map, \"asdhfgh\", \"asdfasdfds\"); Put(map, \"4545\", \"asdfasdfds\"); Put(map, \"asdfaasdasdsdf\", \"asdfasdfds\"); Put(map, \"asdasg\", \"asdfasdfds\"); Put(map, \"qweqeqwe\", \"asdfasdfds\"); printf(\"key: 4545, exists: %s\\n\", Existe(map, \"4545\") ? \"true\" : \"false\"); printf(\"4545: %s\\n\", Get(map, \"4545\")); printf(\"remove 4545 %s\\n\", Remove(map, \"4545\") ? \"true\" : \"false\"); printf(\"remove 4545 %s\\n\", Remove(map, \"4545\") ? \"true\" : \"false\"); printf(\"key: 4545, exists: %s\\n\", Existe(map, \"4545\") ? \"true\" : \"false\"); HashMapIterator iterator = createHashMapIterator(map); while (hasNextHashMapIterator(iterator)) { iterator = nextHashMapIterator(iterator); printf(\"{ key: %s, value: %s, hashcode: %d }\\n\", (char *)iterator-\u003eentry-\u003ekey, (char *)iterator-\u003eentry-\u003evalue, iterator-\u003ehashCode); } map-\u003eclear(map); freeHashMapIterator(\u0026iterator); return 0; } 运行结果： key: 4545, exists: true 4545: asdfasdfds remove 4545 true remove 4545 false key: 4545, exists: false { key: asdfasdf, value: asdfasdfds, hashcode: 2 } { key: asdhfgh, value: asdfasdfds, hashcode: 2 } { key: sasdasd, value: asdfasdfds, hashcode: 2 } { key: asdfaasdasdsdf, value: asdfasdfds, hashcode: 6 } { key: asdasg, value: asdfasdfds, hashcode: 7 } { key: qweqeqwe, value: asdfasdfds, hashcode: 9 } ","date":"2018-04-05","objectID":"/20180405_hashcode/:8:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"}]