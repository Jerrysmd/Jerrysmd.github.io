[{"categories":["Life"],"content":"第一章 商品的降生：市场的形成和商品拜物教 经济基础是商品流通的保证，正是因为经济的发展，大量商品得以在市场上流动，从而催生了拜物、符号工程和品牌推广这些概念，并最终彻底改变了人与物的关系。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:1:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"速度战胜了距离 资本必须努力战胜交通的考验，才能让商品在世界范围内流通并建立市场。它必须破坏时间和距离的关系，用最快的办法把商品从一个地方运到另一个地方。资本发展得越大，市场就越大，其流通的空间轨迹越大，对于市场空间的扩展需求就越强烈，从而需要加快时间、以战胜空间阻碍。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:1:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"商品带来了幻觉 商品的流通促进了劳动分工。 现在，居民既种植着当地的传统农作物，也享受着外来的产品。由于经济模式的变化，人们挣到了钱，也可以买到所需的物品。农民们在从事生产时，不再仅仅考虑自家的需求，而是优先选择生产一些卖得好的农产品。 消费者不了解商品的生产过程，也就无法衡量其成本、构造、所需劳动力以及生产背后的困难，人们只能以一种虚幻的方式去理解它们，在幻想中，商品仿佛不属于任何社会网络，而是独立地存在着。当商品被摆上商店货架供人们挑选时，它们显得遗世独立、纯粹得令人愉悦，它们进入了消费者的幻想，在人们欣赏的眼光下变成了奇妙之物。 这里牵扯出了市场营销的一个基本问题，也就是信任问题。人们为何会将生产的控制权交给不认识的大公司？ ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:1:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"品牌让产品重生 商品拜物的思想使交换价值不仅以事物本身的自然属性来决定，还掺杂了一系列社会和文化价值。这也是广告的常见表现手法，广告里的汽车不是一件实际生活中有使用价值的人类劳动产物，而是阳刚、刺激、地位和新潮的载体。消费者要想成为那样的人，无须做出任何自我提升的努力，只需要通过抽象化的购买和占有就能让自己拥有这些特质。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:1:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第二章 商品大观：百货商店和逛街 商品大观：百货商店和逛街 它建立起了人与商品感性的关系，不断吸引着人们的目光，为人们提供幻想，唤起人们的欲望，让“有闲人士”（flâneur）们爱上了“逛街”（shopping）。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:2:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"城市化和百货商店的出现 这意味着，随着城市的发展，公共视觉体验被商品化，参观带给人们乐趣，甚至成为娱乐活动的一种。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:2:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"商店陈设的技巧 一些百货商店会把最受欢迎的产品放在比较难找的地方，以吸引顾客探索。每过一段时间，货架就会重新布置一遍，造成一种刻意为之的混乱，让顾客多花费一些时间来搜寻商品，这样他们一不小心就会有新的发现。顾客逛的时间多了，也就有可能买得更多。 在这种氛围下，一些并没有使用价值或交换价值的商品也被赋予了极大的符号价值，仿佛购买它们就是获得了奢华，逃离了平庸。 巴黎乐蓬马歇百货商店（Le Bon Marché 无论是对异国情调、对旧时情怀还是对上流社会的模仿，这些空间和场景都不是静置不动的，百货商店会定期举办各类活动，给场景赋予活力。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:2:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"“逛街”概念的兴起 百货商店贩卖的是一种阶级身份，它们提供符合资产阶级生活方式的商品，并让人们认为购买某些衣服或家具就可以获得资产阶级的象征。通过购买一件商品，人们就可以假装加入了某个令人羡慕的群体。百货商店有意传播着这种阶级身份的体现。这种理念的受众群体是小资产阶级，他们经济实力或文化底蕴还不足以加入贵族或大资产阶级，因而百货商店所传达的这一切，就成了他们信奉的权威。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:2:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"一门“让人上头”的学科 百货商店之所以特别，是因为它是这些特性的集大成者，它集中了多种商业技巧和秘诀，并将其制度化，构成了一套商业模型。以前只有少部分精英能享受到的最讲究的商品销售方式，以一种亲民的方式进入了普通民众的生活。就这样，百货商店完成了大众商品文化建设的第一步，也为其打下了基础。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:2:4","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第三章 商品动力学：“同与不同”概念下的矩阵传播 19世纪资产阶级要想体现自己的阶级身份，靠的是符号物（objet-signe）的积累和展示。因此，资产阶级成了消费的领头羊，资产阶级的物质文化成了广义消费文化的起源。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:3:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"资产阶级物质文化及其重要功能 资产阶级物质文化及其重要功能 资产阶级信奉一套“占有体系”，物质的占有就是他们存在的方式，他们用财产、动产和不动产来证明自己的品味。这也让模糊的阶级边界变得清晰了。资产阶级注定要一直生活在竞争中，而竞争的主要体现就是他们居住的房子。 女人打扮得如仙女一般，既没有年龄，也没有姓名，她们就像资产阶级家里的陈设一样，被审美的变化所左右着。她的身体不再属于现实，而是和外物捆绑在了一起，被偶像化、理想化，也被操纵和控制着。房子和女人都变成了商品。 空间、布局、家具、摆设、生活用品，这些所有东西交织在一起，成为一门语言，表达着一家之主的社会地位。每件物品都是一个社会记号，是一个工具，让拥有它的人随时丈量与理想的距离。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:3:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"符号价值以及同与不同的张力 符号价值以及同与不同的张力 因此，鲍德里亚意识到在资本主义生产体系中消费对象的特征与拜物主义间的相似之处：“在商品背后隐含着的是生产关系的不透明性和劳动分工的现实性。不透明性使得人们无法掌握符号物真实的价值，于是只好由象征意义来决定它的交换逻辑。”在这种符号价值经济中，优势阶级控制了符号化的过程。因此，看似是人们选择着商品、商品给人们带来愉悦，但实际上这一切都服从着一种集体的社会逻辑，人并不是真的因为内在需求而消费，他们是被符号牵着鼻子走。为了维持自己的地位、为了守住他所属的阶级，他必须遵守这门消费的法则。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:3:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"附庸风雅者、丹迪主义者和波西米亚艺术家：原始消费者的三种形象 附庸风雅者、丹迪主义者和波西米亚艺术 人们对“本真”的向往成为消费文化的动力，这与现代工业是相辅相成的。只有当生产变得机械化、庞大化和拜物化时，“本真”才会被当作一种价值唤起。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:3:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"商品的潮起潮落 资产阶级本质上的自卑情结让他们产生通过符号物来补偿的倾向，这对消费的动态来说是不可或缺的。资产阶级是不确定也不完整的，他们必须不断地通过所拥有的物品来证明自己、通过物品来表明自己的身份，并带动了整个社会投入其中。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:3:4","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第四章 商品的幻影：图像在日常生活中的入侵和扩散 第四章 商品的幻影：图像在日常生活中的入侵和扩散 这是因为在19世纪末，图像成为商品的传播工具，帮助它迅速地征服了人们的感官，丰富了人们的想象力。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:4:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"从精神自足到理想社群 物品的获取以及各种其他商业行为都超出了人们直接所在的社会环境和生产范围。 空间的限制转化为人们“精神自足”的状态，他们无法想象自己生活范围以外的物质和体验。 图像的本质逐渐发生了变化，它的本身不再重要，它成了一个载体，成为一个可以展现遥远商品的媒介。 商品植入和大众媒体一直是共存的，就像印刷工艺的变革与生产力的发展密不可分一样，它们都在资本主义经济的推动下不断前行，不断发展。 打破了精神自足的印刷品给人们带来新意识、新视野，让人们产生新的共鸣，它是社会重组的基础，人们因此进入了一个由读者和观众组成的社群。人们的话题变得共通了，新闻、肥皂剧、趣闻、商品目录和教科书将人们的认知“同步”，形成了集体意识和记忆。这种共同的物质文化让那些偏远地区的人们可以与世界融为一体，商品的形象也被数以百万计的人复制和传播，于是再也没有人能逃脱这种幽灵般的存在了。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:4:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"从商店到杂志 杂志（magazine）这个词来源于“商店”（magasin），而“magasin”最初的意思是货物仓库 商店顾客等同于杂志读者，他们习惯了浏览新品，对形状、颜色和布局的变化越来越敏感，逐渐厌弃以前购买的陈旧物品，时刻被不断更新的产品诱惑。因此，杂志最终成为吸引消费者目光的重要工具，关注商品的变迁，研究风格语法，为市场提供服务。它引起人们对产品的关注，引领着人们的日常需求，并简化了商品在社会中的流通方式，加速了产品更新换代的节奏。 杂志和广告具有相同的目标：培育消费习惯和消费文化。广告旨在推广特定品牌和产品，而杂志则更普遍地灌输给人们日常的消费理念，从而间接为商家的利益服务。 媒体对大众发挥的三大功能：消费教育、社会想象的植入、商品平常化。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:4:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"从动态影像到白日梦 在电影中，我们也发现了前面描述的杂志的三个功能：消费教育、社会想象的植入、商品平常化。 我们从电影和杂志中都能观察到相同的“结构性缺席”（absences structurantes），即体力劳动、贫困、工会运动和移民这些社会现象很少被提及。银幕上的人物归属于一类“独特的中产阶级，不存在经济鸿沟，也没有社会压力”￼。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:4:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"从图像投影到“自我项目” 从图像投影到“自我项目” 图像的发展让人们将自己投射到原本无法想象的社会世界里，并因此感到痛苦。它通过给消费幻想赋予实体，让人们感到“只有自己例外”，感到自己是受限的、不完整的、贫穷的。它让人们越来越多地展开设想自我的可能性，不再把自我作为社群的一部分，而是作为一个独立的欲望主体。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:4:4","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第五章 消费心态：商品化带来的心理变化 消费心态：商品化带来的心理变化 曾经遵守苦行教义的人们，现在改为重视个人自由和自我表达。独立的自我被建立起来了，至少人们自己是这么认为的。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:5:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"集体秩序和集体精神 人们产生了更贴合消费的新心态，不再直接为生存而工作，而是致力于用他们的劳动赚钱，然后花着赚来的钱、通过市场满足自己的需要。人们逐渐逃离了公共秩序，逃离了邻居的监视，进入了城市，在匿名的环境里工作，并被华丽多彩的灯光、图像、娱乐和购物活动包围着。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:5:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"年轻人的出逃 年轻成为一种虚拟的观念，是可以通过消费来培养的，整个社会都在它的控制之下。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:5:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"消费心态及其成功因素 “新兴的现代社会观念如‘人可以建立自我’，其实是指‘通过消费建立自我’。也就是说，我们可以通过使用特定产品和特定服务来建立并展示我们的社会身份。于是，消费便成为给人们带来自主性、意义感、主观性、专属感和自由感的特权场所。” 爵士舞被视为人性的解放，动作不再是固定的，人们可以即兴舞蹈，也有了在舞蹈中互相触摸的自由。男人可以在舞蹈过程中将手放在女人身上，这让舞蹈成为性欲的隐蔽表达。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:5:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"欲望的正常化 在消费心态出现的世纪之交，支持现代化的人和反对现代化的人在新兴的媒体空间里相互对峙着。前者大肆赞扬现代性的闲散、自由和自然，后者则强调着道德恐慌，呼吁保留古老的精神。 从此，女性杂志不再谴责那些消费水平超过其社会地位的女人，转而谴责那些不懂风格语法、不会消费的女人。摆阔气不再被指责，而不懂品味、买不对东西、不懂符号价值的人则会被嫌弃。 过去，观众嘲笑不符合维多利亚时代标准的角色，而现在他们反过来嘲笑维多利亚时代的标准 弗洛伊德理论带来了各种新名词，同时也为消费者心态提供了“科学”支持。在他的理论下，无论是道德的行为还是不道德的行为，堕落的行为还是高尚的行为，都只是人本能的表现罢了。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:5:4","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"卷土重来的束缚 新的理想女性形象融入了市场，为了美丽，女人要将正确的符号物组合起来，好装扮自己。她们得到的，与其说是自由，不如说是象征性的自由。正如克里斯蒂娜·巴德（Christine Bard）所写：“现代女性消费者是在解放女性的伪装下诞生的。” ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:5:5","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第六章 社会工程：意识管理与商业秩序合法化 社会工程：意识管理与商业秩序合法化 这些大型公司在学会管理物流、品牌形象和员工的同时，还开发了意识管理的方式，并通过隐秘的宣传手段建立起它所依赖的经济秩序。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:6:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"操纵人群，建立受众 在群体的灵魂中占上风的，并不是对自由的要求，而是当奴才的欲望。他们是如此倾向于服从，因此不管谁自称是他们的主子，他们都会本能地表示臣服。 当下的观点认为，无论是从精神分析的角度还是行为主义的角度，人都并不能意识到自己行为的根本原因。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:6:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"用公关来讲故事 在人类社会，创造和破坏往往是共生的，随着新的市场参与者和新机构的不断涌现、不断强大，旧的市场参与者和机构则逐渐衰落并消失。 于是，专业的公关人员便在这样的背景下出现了。作为宣传专家，他们知道如何 “奉承和讨好”公众舆论。他们的工作就是维护好公司和公众的关系、缓解紧张的社会舆论，当然，这一切都是通过媒体完成的。 以AT\u0026T为首的大公司，其公关的一个理念就是，并不直接反驳攻击言论，而是给大众更有意思的想法，讲述另外的故事，使他们的存在和行为显得合理。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:6:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"和平时期的公关宣传 同盟国在战争期间进行的大规模舆论宣传活动证明了社会工程的有效性。战后，这一套机制也为大公司所用。 为了在不知不觉的情况下把观点强加于人，就要向公众提出解决方案，并通过传播恰当的信息，来“指导那些不谙世事的人” 民众之所以可以被摆布，是因为人们在生活中随时会不知不觉地产生先入为主的想法，并寻求心理捷径。 NAM的宣传原则是简单、避免使用复杂的展现方式和负面的表达，还要强调人民的利益。 这一切都证明了，要将一个想法强加于人们的头脑中，并不一定要让人们理解和掌握它，而是只需要充分地重复信息，使其融入人们周遭的环境，便能实现扎根的目的。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:6:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第七章 符号工程：广告的力量与弱点 符号工程：广告的力量与弱点 消费文化的建立有赖于图像的广泛传播和现代商业的发展。大众媒体的普及让拥有巨大资本的商家受益匪浅 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:7:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"大规模催眠的幻想 供给则必须积极制造相应的需求……它必须通过广告和宣传，与大量公众保持长期接触，以确保自己的产品可以获得持续性需求。只有抓住这种需求，运营成本高昂的工厂才有利可图。 受行为主义和精神分析的启发，商家们希望能找到人们无意识状态下做出选择的秘密，也就是人类认知的隐藏源泉。 伯内斯在纽约一年一度的复活节假期游行期间组织了一场“伪活动”。他邀请了十几位女权主义者一边抽着香烟一边参加游行，并把香烟称为“自由的火炬”。第二天，这个女权主义事件便出现在媒体上并引起很大的反响 女权主义者早在爱德华·伯内斯的营销事件之前，就早已把香烟作为解放女性和反对旧时代女性标准的象征了。 成功的广告公司可以操纵人类的动机和欲望，甚至让公众对那些他们不了解，也不想买的商品产生欲望。 迪希特经常把精神分析和符号学混合在一起，而且喜欢用神奇的性解释一切：吸烟就像口交，能带给人快感；芦笋长得像阴茎，因此具有性意义；对女性来说，烹饪就是一种生育仪式；木头具有感官美感，树木的纹理会唤起兴奋并吸引爱抚…… 对于消费者直接可获得的消费品而言，广告的影响就更加不稳定了。一名美国公民可能会相信国家对于俄罗斯的描述和宣传，因为他永远也不会真正接触到俄罗斯，俄罗斯对他来说是一个遥远而抽象的存在，可以受媒体图像摆布。但日常触手可及的番茄酱就不一样了，人们随时可以在商店买到、随时可以尝试，因此就不会随便听信广告对于它的宣传了。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:7:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"广告或联想的艺术 广告让商品得以象征个性、价值、属性、品质、社会地位等，广告可以告诉观众一件商品象征着什么，或者更确切地说，是商家希望这件商品能象征着什么。这种为商品增值的过程就是一种转移行为的过程，即把一些已经与特定人物、刻板印象和情境相关联的含义转移到商品上，也就是代言。 那个时期许多广告都运用了这种三段式（问题/耻辱—产品出现—解决/恢复）的脚本结构，它暗示消费者要意识到自己的错误和不足，还要通过采取正确的消费行为来“自我救赎”。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:7:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"资本现实主义及其作用 他们刻意逃避了集体主义文化，通过构建一种“不假思索”的文化，掩藏了某些想法和某些异议。因此，消费者的自由不过是在他人强加的一堆东西中进行选择的自由。 广告旨在让群众一直对他们当下的生活方式不满意，要让人们无法忍受丑陋的事物。因为令人看了以后觉得心满意足的广告是赚不到钱的，令人不满才能招揽更多顾客。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:7:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第八章 家庭中的消费主义：封闭的住宅与消费分工 一对夫妻不再受到各自的家庭制约，他们得以拥有属于自己的小家。而在这个亲密又个性化的空间里，消费也有了新的分工。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:8:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"家的变化 对资产阶级来说，家是远离商业和社会喧嚣的避风港，是一个“封闭的、虚幻的、永恒的宇宙”[插图]，也是一个远离外界的避难所。 生活方式取决于物质生存条件和集体关系，人们不能想怎样就怎样的。 由于人们的劳动地点从社群转移到市场，家庭也越来越少地参与社群的集体活动了，现代家庭的模式逐渐形成，它的规模逐渐变小，成为仅由父母和子女组成的紧密核心，大家逐渐习惯了“各自在家，各自为己”的生活方式。这意味着个人隐私、亲密关系和自我实现的意义被重视起来了。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:8:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"母亲——全家的消费总管 这种对贤妻良母的赞美、对她们倾情奉献的美化，在整个西方都极为广泛，现代社会更加强化了性别分工和各司其职的意识形态，男人和女人因此被各自束缚在不同的领域里。 然而，随着工业化和有薪就业的推进，雇佣的、外部的、男性工作与家庭的、内部的、女性工作之间产生了区隔。当男人远离家乡，到工厂和商业空间从事生产劳作、带来收入时，妇女却在花钱，尽管这是为维持家庭正常运行必要的开支。女人不再是生产者，而是成为负责家庭采购的“自雇者”。男人与女人的劳动空间彼此隔离，导致他们的经济地位和思想观念也发生了极大变化，这一切“破坏了婚姻中的两性平等，使女性成为仆人般的角色”[插图]。尽管女人和以前一样，完成着家中的劳作，但现在她们的地位却大不如前，毕竟，人们会认为花钱的人不如挣钱的人重要。 这些工具减少了家务劳动的时间，让妇女有更多时间照顾孩子和休闲。家庭劳动变得更加工业化了，这虽然增加了家庭生活成本，但极大地改变了家务所需的时间，以及家务工作的构成。由于基础设施、材料和工具的进步，人们对清洁度和舒适度的标准和要求也越来越高。 商品崇拜、劳动分工和工资制度都是家庭内部生产关系解体的原因。 母亲虽然只属于小小的一家之地，但是她所负责的住宅却是现代人倾注大量注意力的地方——它正日益被重新思考和安排。 现代商品陷入了一种“根本妥协”，即购买特定商品才能提高家庭地位和声望。人们很多时候都是在被迫的状态下必须要买一些东西，很少是因为有用或因为喜欢才买。 近几十年来，家务工作的不平等分配已显著减少，但西方国家的女性花在家务上的时间仍然是男性的2-6倍。￼虽然男人可能更多地承担了与房屋维护（修理、园艺）有关的工作，室内清洁、做饭和洗衣这类的工作还是主要由女性承担。人们对家中母亲的固有观点仍然是希望她们做好家务、养好孩子，她们仍然要对家庭幸福负责。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:8:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"孩子——家庭幸福的证据和指南针 孩子——家庭幸福的证据和指南针 这也归功于电视，一些人将其称为“保姆机”，因为很多家长在做家务的时候为了让孩子不吵闹，就让小孩自己看电视消遣，这就等于忙碌的母亲把小孩委托给一群商人来看管，而这些商人负责对小孩进行消费教育。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:8:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第九章 新消费精神：漫长的60年代和市场的重振 一种崇尚独特、彰显个性的反叛文化出现了。但这种“新精神”非但没有破坏商业秩序，反而通过宣扬富有个性的符号物，充当了消费主义的帮手。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:9:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"青年文化与从众焦虑 年轻人的文化就是商业的文化，他们的群体身份通过服饰美学和标志物来表达。青年们的不同小社群都有各自认同的消费代码，而这些代码形成了一种传达态度和信仰的语言。 在新兴的大众社会中，自主性和个人自由更加难以维护，这也许是年轻人有如此大情绪的原因。1950年代，生存焦虑无处不在，人们的生活在工业的影响下变得标准化。在大型公司的控制下，工作与消费各占据了人们生活的一半。现代社会是组织性极强的社会，破坏了不少个性。自1920年代以来，广告又为人们增添了更多的焦虑。微笑和整洁的家庭、奇妙的家用电器、完美的住宅、修剪整齐的草坪、锃亮的汽车，这些场景成了美好生活的模板，但也是一成不变且毫无个性的噩梦，人们的自主性被掩埋在了从众行为之下。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:9:1","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"压抑与自我探寻之间的斗争 压抑与自我探寻之间的斗争 人们要想拒绝做普通人，就要通过拥有某些东西来证明自己远离了庸俗。虽然这一切都属于反主流文化，也是反资产阶级美学的表现，但这些行为仍然遵守了资产阶级消费主义的基本原则，也就是为了让自己出众而消费。因此它们背后的机制是一样的，只是判断标准变了。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:9:2","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"对成规的反抗带来经济价值 对成规的反抗带来经济价值 在反主流文化体制下，人们追求的符号物从贬值到更新的流程越来越快，时尚周期也比以前更短了，这一切都进一步刺激了生产。所以说，人们反墨守成规的心态（anticonformise de masse）其实比标准化和统一的社会更适合资本积累。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:9:3","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"新消费精神的广告语言代谢 新消费精神的广告语言代谢 广告口号仍然存在，但它们现在强调的是自由，而不再是卓越，他们的言辞从“我们的产品是最好的”这类说法，改为了鼓励消费者“做真实的自己”。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:9:4","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Life"],"content":"第十章 超级消费者：呈指数增长的未来 超级消费者：呈指数增长的未来 在1960年之后，一切新的发展都只是在重复和放大已经发生的现象。 消费自由不过是全球化世界中一小部分精英所行使的特权。“所谓资本主义赋予人们的自由其实对于世界上的绝大多数人来说都是不存在的。”由于那些劳动力、工作和生产世界离人们较为遥远，造成了“结构性缺席”，让人们产生一种错觉，似乎全世界都持有同一种价值观，全世界都过着资产阶级的生活。对消费者自由的赞美其实是对生产的视而不见。 ","date":"2023-01-24","objectID":"/20230124_book-create-consumer/:10:0","tags":["Book"],"title":"Book summary: The Manufactured Consumer","uri":"/20230124_book-create-consumer/"},{"categories":["Technology"],"content":"Spring makes programming Java quicker, easier, and safer. Spring’s focus on speed, simplicity, and productivity has made it the world’s most popular Java framework. Spring works on microservices, reactive, cloud, web apps, serverless, event driven, and batch services. Note This is my Knowledge Transfer about Java and Spring Basic Conception Sharing. And this is my slide: Spring Sharing 1.Java 基本概念 2.Spring 简介 3.Spring 核心概念 IOC \u0026 DI \u0026 Bean 4.Bean 的配置、实例化、生命周期 5.DI 的注入方式、配置 6.Spring 注解开发 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:0:0","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"1.Java 基本概念 | 1.Java 基本概念 Java 和 Python 都是面向对象编程语言，因此都有类和对象的概念 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:1:0","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"类 在 Java 和 Python 中，类都是通过class关键字定义的 Python： class Person: def __init__(self, name, age): self.name = name self.age = age def introduce(self): print(\"My name is \" + self.name, end=\"\") print(\"I am \" + str(self.age) + \"old.\") Java： public class Person { private String name; private int age; public Person(String name, int age) { this.name = name; this.age = age; } public void introduce() { System.out.print(\"My name is \" + name); System.out.println(\"I am \" + age + \"old.\"); } } | 1.Java 基本概念 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:1:1","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"对象 在 Python 中，对象是通过类名调用构造函数创建的。在 Java 中，对象是通过 new 关键字创建的 Python： tom = Person(\"Tom\", 25) tom.introduce() # 输出 \"My name is Tom I am 25 old.\" Java： *Person tom = new Person(\"Tom\", 25); tom.introduce(); // 输出 \"My name is Tom I am 25 old.\" | 1.Java 基本概念 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:1:2","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"继承和多态 Python 中的继承通过在类定义时指定父类来实现继承，多态在python中通过duck-typing来实现。 Java 也支持类的继承和多态，通过 extends 关键字来实现继承，通过重写方法来实现多态。 Python： class Student(Person): def __init__(self, name, age, school): super().__init__(name, age) self.school = school def introduce(self): print(\"I am in \" + self.school) Java： public class Student extends Person { private String school; public Student(String name, int age, String school) { super(name, age); this.school = school; } @Override public void introduce() { System.out.println(\"I am in \" + school); } } | 1.Java 基本概念 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:1:3","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"接口和实现 Java 中的接口和 Python 中的抽象类有相似之处，但是抽象类可以实现一些方法，而接口则不能。 接口可以被多个类实现，而抽象类只能被单一类继承 Java 中的接口负责定义一组方法，具体的方法实现由实现类来完成 public interface BookDao { public void save(); } public class BookDaoImpl implements BookDao { public void save() { System.out.println(\"method\"); } } ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:1:4","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"2.Spring 简介 | 2.spring 简介 官网：spring.io Spring 形成了一种开发的生态圈。Spring 提供了若干个项目，每个项目用于完成特定的功能 | 2.spring 简介 spring framework 系统架构 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:2:0","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"3.Spring 核心概念 IOC \u0026 DI \u0026 Bean | 3.Spring 核心概念 业务层实现： public class ServiceImpl implements Service{ private Dao dao = new DaoImpl(); public void save(){ dao.save(); } } 数据层实现： public class DaoImpl implements Dao{ public void save(){ System.out.println(\"method1\"); } } public class DaoImpl2 implements Dao{ public void save(){ System.out.println(\"method2\"); } } – 问题：耦合度偏高 – 解决思想： 在使用对象时，在程序中不要主动使用 new 产生对象，转换为由外部提供对象 对象的创建控制权由程序移到外部，这种思想称为 IoC（Inversion of Control）控制反转 概念一：IoC（Inversion of Control）控制反转 | 3.Spring 核心概念 解决方案： 在使用对象时，在程序中不要主动使用 new 产生对象，转换为由外部提供对象 对象的创建控制权由程序移到外部，这种思想称为控制反转 IoC（Inversion of Control）控制反转 Spring 技术对 IoC 思想进行了实现 Spring 提供了一个容器，称为 IoC 容器 \\ Spring 容器，用来充当 IoC 思想中的外部 由主动 new 产生对象转化为由 IoC 容器提供对象 – IoC 容器负责对象的创建、初始化等一系列工作，被创建或被管理的对象在 IoC 容器中统称为 Bean 概念二：Spring Bean | 3.Spring 核心概念 业务层实现： public class ServiceImpl implements Service{ private Dao dao; public void save(){ bookDao.save(); } } 数据层实现： public class DaoImpl implements Dao{ public void save(){ System.out.println(\"method\"); } } ———————————— IoC 容器 ———————————— service dao – 在容器中建立 bean 与 bean 之间的依赖关系的整个过程称为依赖注入 概念三：DI（Dependency Injection）依赖注入 | 3.Spring 核心概念 目标：充分解耦 使用 IoC 容器管理 bean (IoC) 在 IoC 容器内将有依赖关系的 bean 进行关系绑定 (DI) 最终效果： 使用对象时不仅可以直接从 IoC 容器中获取，并且获取到的 bean 已经绑定了所有的依赖关系 | 3.Spring 核心概念 案例 demo： IoC 问题： 管理什么？（Service 和 Dao） 如何将被管理的对象告知 IoC 容器？（配置） 被管理的对象交给 IoC 容器，如何获取到 IoC 容器？（接口） 如何从 IoC 容器中获取 bean？（接口方法） DI 问题： Service 中使用 new 形式创建的 Dao 对象是否保留？（否） Service 中需要的 Dao 对象如何进入到 Service 中？（提供方法） Service 与 Dao 间的关系如何描述？（配置） ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:3:0","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"4.Bean 的配置、实例化、生命周期 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:4:0","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 4.Bean 的配置 类别 描述 名称 bean 功能 定义Spring核心容器管理的对象 格式 \u003cbeans\u003e\u003cbean\u003e\u003c/bean\u003e \u003c/beans\u003e 属性列表 id：bean的id，使用容器可以通过id值过去对应的bean，在一个容器中id值唯一 class：bean的类型，即配置的bean的全路径类名 范例 \u003cbean id=\"bookDao\" class=\"dao.impl.BookDaoImpl\"/\u003e \u003cbean id=\"bookService\" class=\"service.impl.BookServiceImpl\"\u003e\u003c/bean\u003e ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:4:1","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 4.Bean 的实例化 1.提供可访问的构造方法（常用） public class BookDaoImpl implements BookDao{ public BookDaoImpl(){ print(\"book constructor is running\"); } public void save(){ print(\"book dao save\"); } } 配置 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"\u003e\u003c/bean\u003e 2.静态工厂 public class OrderDaoFactory{ public static OrderDao getOrderDao(){ return new OrderDaoImpl(); } } 配置 \u003cbean id=\"orderDao\" class=\"com.jerry.factory.OrderDaoFactory\" factory-method=\"getOrderDao\"\u003e\u003c/bean\u003e 使用 main(){ UserDaoFactory userDaoFactory = new UserDaoFactory(); UserDao userDao = userDaoFactory.getUserDao(); userDao.save(); } 3.实例工厂 public class UserDaoFactory{ public UserDao getUserDao(){ return new UserDaoImpl(); } } 配置 \u003cbean id=\"userFactory\" class=\"com.jerry.factory.UserDaoFactory\"\u003e\u003c/bean\u003e \u003cbean id=\"userDao\" factory-method=\"getUserDao\" factory-bean=\"userFactory\"\u003e\u003c/bean\u003e 4.FactoryBean public class UserDaoFactoryBean implements FactoryBean\u003cUserDao\u003e{ //代替原始实例工厂中创建对象的方法 public UserDao getObject() throws Exception{ return new UserDaoImpl(); } public Class\u003c?\u003e getObjectType(){ return UserDao.class; } public boolean isSingleton(){ return true; } } 配置 \u003cbean id=\"userDao\" class=\"com.jerry.factory.UserDaoFactoryBean\"\u003e\u003c/bean\u003e ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:4:2","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 4.Bean 的生命周期 bean 的生命周期：bean 从创建到销毁的整体过程 bean 的生命周期控制方法： bean 的初始化 bean 的销毁 – 第一步：初始化容器 创建对象，内存分配 执行构造方法 执行属性注入 执行 bean 初始化方法 第二步：使用 bean 执行业务操作 第三步：关闭容器 执行 bean 销毁方法 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:4:3","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 4.Bean 的配置总结 bean 配置大全 \u003cbean id=\"bookDao\" name=\"dao bookDaoImpl daoImpl\" class=\"com.example.dao.impl.BookDaoImpl\" scope=\"singleton\" init-method=\"init\" destroy-method=\"destory\" autowire=\"byType\" factory-method=\"getInstance\" factory-bean=\"com.example.factory.BookDaoFactory\" lazy-init=\"true\" /\u003e ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:4:4","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"5.DI 的注入方式、配置 ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:5:0","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 5.DI 的注入方式 - 之 setter 注入 在 bean 中定义引用类型属性并提供可访问的 set 方法 public class BookServiceImpl implements BookService { private BookDao bookDao; public void setBookDao(BookDao bookDao){ this.bookDao = bookDao; } } 配置中使用 property 标签 ref 属性注入引用类型对象 配置中使用 property 标签 value 属性注入简单数据类型 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"/\u003e \u003cbean id=\"bookService\" class=\"com.jerry.service.impl.BookServiceImpl\"\u003e \u003cproperty name=\"bookDao\" ref=\"bookDao\"/\u003e \u003cproperty name=\"connectionNum\" value=\"10\"/\u003e \u003c/bean\u003e ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:5:1","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 5.DI 的注入方式 - 之 构造器注入 在 bean 中定义引用类型属性并提供可访问的构造方法 public class BookServiceImpl implements BookService { private BookDao bookDao; public BookServiceImpl(BookDao bookDao){ this.bookDao = bookDao; } } 配置中使用 constructor-arg 标签 ref 属性注入引用类型对象 配置中使用 constructor-arg 标签 value 属性注入简单数据类型 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"/\u003e \u003cbean id=\"bookService\" class=\"com.jerry.service.impl.BookServiceImpl\"\u003e \u003cconstructor-arg name=\"bookDao\" ref=\"bookDao\"/\u003e \u003cconstructor-arg name=\"connectionNum\" value=\"10\"/\u003e \u003c/bean\u003e ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:5:2","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 5.DI 的配置总结 注入依赖 配置大全 \u003cbean id=\"bookDao\" class=\"com.example.dao.impl.BookDaoImpl\"\u003e \u003cconstructor-arg name=\"bookDao\" ref=\"bookDao\"/\u003e \u003cconstructor-arg name=\"msg\" value=\"WARN\"/\u003e \u003cconstructor-arg index=\"3\" value=\"WARN\"/\u003e \u003cproperty name=\"bookDao\" ref=\"bookDao\"/\u003e \u003cproperty name=\"msg\" ref=\"WARN\"/\u003e \u003cproperty name=\"names\"\u003e \u003clist\u003e \u003cvalue\u003eexample\u003c/value\u003e \u003cref bean=\"dataSource\"/\u003e \u003c/list\u003e \u003c/property\u003e \u003c/bean\u003e ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:5:3","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"6.Spring 注解开发 | 6.注解 目的：简化开发 使用注解定义 bean： 使用 @Component(“bookDao”)定义bean @Component(\"bookDao\") public class BookDaoImpl implements BookDao {} @Component(\"bookService\") public class BookServiceImpl implements BookService {} 核心配置文件中通过组件扫描加载 bean \u003ccontext:component-scan base-package=\"com.example\"/\u003e ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:6:0","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 6.注解 使用注释依赖注入： 使用 @Autowired 注解开启自动装配模式（按类型） @Component(\"bookService\") public class BookServiceImpl implements BookService { @Autowired private BookDao bookDao; @Override public void save(){ System.out.println(\"service save running...\"); bookDao.save(); } //public void setBookDao(BookDao bookDao) { // this.bookDao = bookDao; //} } ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:6:1","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"| 6.注解 - XML 配置对比注解配置 功能- - - - - - - - - - - - - - - - XML 配置- - - - - - - - - - - - - - - - - - - - - - - - 注解- - - - - - - - - - - - - - - - - - - - - - - - 定义 bean bean 标签 - id 属性 - class 属性 @Component - @Controller - @Service - @Repository @ComponentScan 设置依赖注入 setter 注入（set 方法） - 引用 / 简单 构造器注入（构造方法） - 引用 / 简单 自动装配 @Autowired - @Qualifier @Value 配置第三方 bean bean 标签 静态工厂、实例工厂、FactoryBean @Bean 作用范围 scope 属性 @Scope 生命周期 标准接口 - init-method - destroy-method @PostConstructor @PreDestroy ","date":"2023-01-18","objectID":"/20230118_spring-java-sharing/:6:2","tags":["Java","Spring"],"title":"Spring Java Basic Conception Sharing","uri":"/20230118_spring-java-sharing/"},{"categories":["Technology"],"content":"Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters. Note This is my Knowledge Transfer about How to get start with Spark Java development. And this is my slide: ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:0:0","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"1.Spark 简介 ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:1:0","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"1. Spark 的背景 ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:1:1","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"2. Spark 的核心模块 Spark 的定义：一个通用的大数据分析引擎 Spark 的核心模块： ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:1:2","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"3. Spark 的生态组件 Spark 的生态： ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:1:3","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"2.Spark 代码快速上手 Maven 配置： \u003c?xml version=\"1.0\" encoding=\"UTF-8\"?\u003e \u003cproject xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"\u003e \u003cmodelVersion\u003e4.0.0\u003c/modelVersion\u003e \u003cgroupId\u003eorg.example\u003c/groupId\u003e \u003cartifactId\u003espark-demo\u003c/artifactId\u003e \u003cversion\u003e1.0-SNAPSHOT\u003c/version\u003e \u003cproperties\u003e \u003cmaven.compiler.source\u003e8\u003c/maven.compiler.source\u003e \u003cmaven.compiler.target\u003e8\u003c/maven.compiler.target\u003e \u003cscala.version\u003e2.13\u003c/scala.version\u003e \u003cspark.version\u003e3.3.1\u003c/spark.version\u003e \u003c/properties\u003e \u003cdependencies\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e \u003cartifactId\u003espark-core_${scala.version}\u003c/artifactId\u003e \u003cversion\u003e${spark.version}\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e \u003cartifactId\u003espark-sql_${scala.version}\u003c/artifactId\u003e \u003cversion\u003e${spark.version}\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e \u003cartifactId\u003espark-mllib_${scala.version}\u003c/artifactId\u003e \u003cversion\u003e${spark.version}\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.apache.spark\u003c/groupId\u003e \u003cartifactId\u003espark-graphx_${scala.version}\u003c/artifactId\u003e \u003cversion\u003e${spark.version}\u003c/version\u003e \u003c/dependency\u003e \u003cdependency\u003e \u003cgroupId\u003emysql\u003c/groupId\u003e \u003cartifactId\u003emysql-connector-java\u003c/artifactId\u003e \u003cversion\u003e8.0.22\u003c/version\u003e \u003c/dependency\u003e \u003c/dependencies\u003e \u003c/project\u003e ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:2:0","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"1. 离线批处理场景 需求一: 离线批处理场景 数据来源于葡萄牙银行电话调查的结果 原始数据中的关键列： lname fname province city age dur cam Santos Antonio Lisbon Porto 20 261 1 Silva Maria Porto Braga 57 149 1 Pereira Jorge Braga Faro 37 226 1 Martins Ana Faro Coimbra 40 151 1 … … … … .. … . 需求一: 离线批处理场景 需求列表： Spark 读取 CSV 文件 过滤掉通话时间少于50秒的数据 过滤掉年龄小于20的人 生成完整名字，使用逗号连接姓和名字段得出 生成详细地址字段，使用逗号连接州和城市字段得出 根据年龄将客户分组，并统计各组客户的客户总数、通话联系的平均次数、电话联系的平均时间 写入 mysql 数据库中 代码实现： import org.apache.spark.sql.*; import org.apache.spark.sql.expressions.UserDefinedFunction; import org.apache.spark.sql.types.DataTypes; import java.util.Properties; import static org.apache.spark.sql.functions.*; public class spark01_helloworld { public static void main(String[] args){ SparkSession sparkSession = new SparkSession.Builder() .appName(\"csv2db\") .master(\"local[*]\") .getOrCreate(); Dataset\u003cRow\u003e bankTable = sparkSession.read() .format(\"csv\") .option(\"header\", \"true\") .load(\"C:\\\\Users\\\\JiaRui\\\\Desktop\\\\bank-additional-full.csv\"); //TODO: 1.过滤掉通话时间少于50秒的数据 bankTable = bankTable.filter(\"duration \u003e= 50\"); //TODO: 2.过滤掉年龄小于20 bankTable.createOrReplaceTempView(\"bank_table\"); bankTable = sparkSession.sql(\"select * from bank_table where age \u003e= 20\"); //TODO: 3.生成完整名字，使用逗号连接姓和名字段得出 bankTable= bankTable.withColumn( \"name\", concat(bankTable.col(\"lname\"), lit(\", \"), bankTable.col(\"fname\")) ); //TODO: 4.生成详细地址字段，使用逗号连接州和城市字段得出 UserDefinedFunction mergeUdf = udf( (String x, String y) -\u003e x + \", \" + y, DataTypes.StringType ); sparkSession.udf().register(\"merge\", mergeUdf); bankTable = bankTable.withColumn(\"address\", call_udf(\"merge\",col(\"province\"),col(\"city\"))); //TODO: 5.根据年龄将客户分组，并统计各组客户的客户总数、通话联系的平均次数、电话联系的平均时间 Dataset\u003cRow\u003e reportTable = bankTable.groupBy(\"age\") .agg(count(\"age\").name(\"total customers\"), round(avg(\"campaign\"), 2).name(\"avg_calls\"), round(avg(\"duration\"), 2).name(\"avg_dur\")) .orderBy(\"age\"); String url = \"jdbc:mysql://172.20.121.222:3306/test\"; Properties properties = new Properties(); properties.setProperty(\"dirver\",\"com.mysql.cj.jdbc.Driver\"); properties.setProperty(\"user\",\"root\"); properties.setProperty(\"password\",\"123456\"); bankTable.write() .mode(SaveMode.Overwrite) .jdbc(url,\"bank_table\",properties); reportTable.write() .mode(SaveMode.Overwrite) .jdbc(url,\"report_table\",properties); sparkSession.stop(); } } ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:2:1","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"2. 实时流处理场景 需求二: 实时流处理场景 数据源是股票价格数据的实时数据，每分钟更新 样例数据： key: 000002.SZ value: 1502126681, 22.71, 21.54, 22.32, 22.17 其中每个字段分别是股票代码、事件时间戳、现价、买入价、卖出价、成交均价。 需求列表： 读取 kafka 数据 记录每支股票每半个小时的最高价和最低价 写入 Hbase 数据库中 代码实现： import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.streaming.Trigger; import static org.apache.spark.sql.functions.*; public class spark02_streaming { public static void main(String[] args) throws Exception { SparkSession sparkSession = SparkSession .builder() .master(\"local[*]\") .appName(\"StockCCI\") .getOrCreate(); //分别设置window长度、容忍最大晚到时间和触发间隔 String windowDuration = \"30 minutes\"; String waterThreshold = \"5 minutes\"; String triggerTime = \"1 minutes\"; sparkSession.readStream() .format(\"kafka\") .option(\"kafka.bootstrap.servers\",\"broker1:port1,broker2:port2\") .option(\"subcribe\",\"stock\") .load() .selectExpr( \"CAST(key AS STRING)\", \"CAST(value AS STRING)\".split(\",\")[0], \"CAST(value AS STRING)\".split(\",\")[1], \"CAST(value AS STRING)\".split(\",\")[2], \"CAST(value AS STRING)\".split(\",\")[3], \"CAST(value AS STRING)\".split(\",\")[4]) .toDF(\"companyno\",\"timestamp\",\"price\",\"bidprice\",\"sellpirce\",\"avgprice\") .selectExpr( \"CAST(companyno AS STRING)\", \"CAST(timestamp AS TIMESTAMP)\", \"CAST(price AS DOUBLE)\", \"CAST(bidprice AS DOUBLE)\", \"CAST(sellpirce AS DOUBLE)\", \"CAST(avgprice AS DOUBLE)\" ) //设定水位 .withWatermark(\"timestamp\",waterThreshold) .groupBy(\"timestamp\", windowDuration, \"companyno\") //求出最高价和最低价 .agg( max(col(\"price\")).as(\"max_price\"), min(col(\"price\")).as(\"min_price\") ) .writeStream() .outputMode(\"append\") .trigger(Trigger.ProcessingTime(triggerTime)) .format(\"HBaseWriter\") .start() .awaitTermination(); } } ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:2:2","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"3. 机器学习场景 需求三: 机器学习场景 数据源是文本和打分标签 训练数据： id text label 0L “a b c spark” 1.0 1L “b d” 0.0 2L “spark f g h” 1.0 3L “hadoop mapr” 0.0 测试数据： id “text” 4L “spark i j” 5L “l m n” 6L “spark hadoop spark” 7L “apache hadoop” 需求三: 机器学习场景 需求列表： 将文档分词 将分词的结果转换为词向量 学习模型 预测（是否为垃圾邮件） 代码实现： import org.apache.spark.ml.Pipeline; import org.apache.spark.ml.PipelineModel; import org.apache.spark.ml.PipelineStage; import org.apache.spark.ml.classification.LogisticRegression; import org.apache.spark.ml.feature.HashingTF; import org.apache.spark.ml.feature.Tokenizer; import org.apache.spark.sql.Dataset; import org.apache.spark.sql.Row; import org.apache.spark.sql.SparkSession; import org.apache.spark.sql.types.DataTypes; import org.apache.spark.sql.types.Metadata; import org.apache.spark.sql.types.StructField; import org.apache.spark.sql.types.StructType; public class spark03_mlpipeline { public static void main(String[] args) { SparkSession sparkSession = SparkSession .builder() .master(\"local[*]\") .appName(\"mlpipeline\") .getOrCreate(); Dataset\u003cRow\u003e training = sparkSession.read() .format(\"csv\") .option(\"header\", \"true\") .schema(new StructType(new StructField[] { new StructField(\"id\", DataTypes.LongType, false, Metadata.empty()), new StructField(\"text\", DataTypes.StringType, false, Metadata.empty()), new StructField(\"label\", DataTypes.DoubleType, false, Metadata.empty()) })) .load(\"C:\\\\Users\\\\JiaRui\\\\Desktop\\\\training.csv\"); Tokenizer tokenizer = new Tokenizer() .setInputCol(\"text\") .setOutputCol(\"words\"); HashingTF hashingTF = new HashingTF() .setNumFeatures(1000) .setInputCol(tokenizer.getOutputCol()) .setOutputCol(\"features\"); LogisticRegression lr = new LogisticRegression() .setMaxIter(10) .setRegParam(0.001); Pipeline pipeline = new Pipeline() .setStages(new PipelineStage[]{tokenizer,hashingTF,lr}); PipelineModel model = pipeline.fit(training); Dataset\u003cRow\u003e test = sparkSession.read() .format(\"csv\") .option(\"header\", \"true\") .schema(new StructType(new StructField[]{ new StructField(\"id\", DataTypes.LongType, false, Metadata.empty()), new StructField(\"text\", DataTypes.StringType, false, Metadata.empty()) })) .load(\"C:\\\\Users\\\\JiaRui\\\\Desktop\\\\test.csv\"); Dataset\u003cRow\u003e res = model.transform(test) .select(\"id\", \"text\", \"probability\", \"prediction\"); res.show(); } } ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:2:3","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"4. 图计算场景 需求四: 图计算场景 需求：使用 GraphX 实现了 PageRank 得分。 代码实现： import org.apache.spark.api.java.JavaRDD; import org.apache.spark.graphx.Graph; import org.apache.spark.graphx.lib.PageRank; import org.apache.spark.sql.SparkSession; import org.apache.spark.storage.StorageLevel; import scala.Tuple2; import scala.reflect.ClassTag; public class spark04_grahpx { public static void main(String[] args) { SparkSession sparkSession = new SparkSession .Builder() .appName(\"pageRank\") .master(\"local[*]\") .getOrCreate(); sparkSession.sparkContext().setLogLevel(\"WARN\"); // URL neighbor URL // URL neighbor URL // ... JavaRDD\u003cString\u003e lines = sparkSession.read() .textFile(\"C:\\\\Users\\\\JiaRui\\\\Desktop\\\\pageRank.csv\") .javaRDD(); JavaRDD\u003cTuple2\u003cObject, Object\u003e\u003e edges = lines.map(s -\u003e { String[] parts = s.split(\"\\\\s+\"); return new Tuple2\u003c\u003e(parts[0], parts[1]); }); Graph\u003cObject, Object\u003e graph = Graph.fromEdgeTuples(edges.rdd(), 1, null, StorageLevel.MEMORY_AND_DISK(), StorageLevel.MEMORY_AND_DISK(), ClassTag.Any()); Graph\u003cObject, Object\u003e rankedGraph = PageRank.runUntilConvergence(graph, 0.001, 0.001, ClassTag.Any(), ClassTag.Any()); JavaRDD\u003cTuple2\u003cObject, Object\u003e\u003e pageRankValues = rankedGraph.vertices().toJavaRDD(); pageRankValues.foreach(x -\u003e System.out.println(x)); } } ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:2:4","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"3.Spark 数据处理与分析场景 ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:3:0","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"4.Spark 分布式计算框架和编程模型 MapReduce ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:4:0","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"5.Spark 弹性数据集 RDD 和结构化数据 Dataset //读取 json 文件 spark.read.json(\"examples/src/main/resources/people.json\"); //读取 csv 文件 spark.read.csv(\"examples/src/main/resources/people.csv\"); //读取 parquet 文件 spark.read.parquet(\"examples/src/main/resources/people.parquet\"); //读取 orc 文件 spark.read.orc(\"examples/src/main/resources/people.orc\"); //读取文本文件 spark.read.text(\"examples/src/main/resources/people.csv\"); //通过 JDBC 连接外部数据库生成 spark.read.format(\"jdbc\") .option(\"url\",\"jdbc:driver\") .option(\"user\",\"\") .option(\"password\",\"\") .load(); //条件查询 df.select(\"age\").where(\"name is not null and age \u003e 10\").foreach(println(_)); //分组统计 df.groupBy(\"age\").count().foreach(println(_)); //连接操作: //支持 inner/cross/outer/full/full_outer/left/left_outer/cross/left_semi/left_anti等 leftDF.join(rightDF, leftDF(\"pid\") == rightDF(\"fid\"),\"left_outer\"); ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:5:0","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Technology"],"content":"6.Spark抽象概念、架构与运行环境 | 基于某个运行环境初始化 SparkSession SparkSession sparkSession = new SparkSession .Builder() .appName(\"newApp\") .master(\"yarn-client\") .config(\"spark.executor.instances\", \"10\") .config(\"spark.executor.memory\", \"10g\") .getOrCreate(); ","date":"2022-12-31","objectID":"/20221231_spark-sharing/:6:0","tags":["Java","Spark"],"title":"Spark Java Sharing","uri":"/20221231_spark-sharing/"},{"categories":["Life"],"content":"Photos and videos recorded in 2022. ","date":"2022-12-18","objectID":"/20221218_2022-summary-video/:0:0","tags":["Life","Video","Shang hai"],"title":"Memories of 2022","uri":"/20221218_2022-summary-video/"},{"categories":["Technology"],"content":"The system will be flexible and allow users to specify their own set of rules for price movements and triggers for notifications. This is a useful tool for anyone interested in keeping track of their investments and making informed decisions in the stock market. ","date":"2022-11-14","objectID":"/20221114_stock-spy/:0:0","tags":["Python","Pandas"],"title":"Stock Monitoring and Notification Project","uri":"/20221114_stock-spy/"},{"categories":["Technology"],"content":"Introduction In this project, we will be developing functions to retrieve stock data from Tencent and Sina using the Python programming language. The functions will be able to retrieve both daily and minute-level data, and will allow users to specify the stock code, time range, and frequency of the data. ","date":"2022-11-14","objectID":"/20221114_stock-spy/:1:0","tags":["Python","Pandas"],"title":"Stock Monitoring and Notification Project","uri":"/20221114_stock-spy/"},{"categories":["Technology"],"content":"Design ","date":"2022-11-14","objectID":"/20221114_stock-spy/:2:0","tags":["Python","Pandas"],"title":"Stock Monitoring and Notification Project","uri":"/20221114_stock-spy/"},{"categories":["Technology"],"content":"Tencent For the Tencent functions, we will be using the requests library to make HTTP requests to the Tencent stock data API. The API will return the stock data in JSON format, which we will then parse and convert into a Pandas dataframe for easier manipulation. The get_price_day_tx function will be used to retrieve daily data, while the get_price_min_tx function will be used to retrieve minute-level data. Users will be able to specify the stock code, end date (optional), number of periods to retrieve, and the frequency of the data (e.g. daily, weekly, monthly). ","date":"2022-11-14","objectID":"/20221114_stock-spy/:2:1","tags":["Python","Pandas"],"title":"Stock Monitoring and Notification Project","uri":"/20221114_stock-spy/"},{"categories":["Technology"],"content":"Sina For the Sina function, we will be using the pandas_datareader library to retrieve stock data from the Sina API. The get_price_sina function will allow users to specify the stock code, start and end dates, and the frequency of the data. ","date":"2022-11-14","objectID":"/20221114_stock-spy/:2:2","tags":["Python","Pandas"],"title":"Stock Monitoring and Notification Project","uri":"/20221114_stock-spy/"},{"categories":["Technology"],"content":"Implementation The Tencent functions will follow the steps outlined below: Define the function and parse the input parameters. Make the HTTP request to the Tencent API using the requests library. Parse the JSON response and extract the relevant data. Convert the data into a Pandas dataframe. Return the dataframe to the user. The Sina function will follow these steps: Define the function and parse the input parameters. Use the pandas_datareader library to retrieve the stock data from the Sina API. Return the data to the user. ","date":"2022-11-14","objectID":"/20221114_stock-spy/:3:0","tags":["Python","Pandas"],"title":"Stock Monitoring and Notification Project","uri":"/20221114_stock-spy/"},{"categories":["Technology"],"content":"Code 使用 requests 从接口获取数据 使用 pandas 将数据转化成表格 import datetime import json import requests import pandas as pd # 腾讯日线 def get_price_day_tx(code, end_date='', count=10, frequency='1d'): # 日线获取 unit = 'week' if frequency in '1w' else 'month' if frequency in '1M' else 'day' # 判断日线，周线，月线 if end_date: end_date = end_date.strftime('%Y-%m-%d') if isinstance(end_date, datetime.date) else \\ end_date.split(' ')[0] end_date = '' if end_date == datetime.datetime.now().strftime('%Y-%m-%d') else end_date # 如果日期今天就变成空 URL = f'http://web.ifzq.gtimg.cn/appstock/app/fqkline/get?param={code},{unit},,{end_date},{count},qfq' st = json.loads(requests.get(URL).content); ms = 'qfq' + unit; stk = st['data'][code] buf = stk[ms] if ms in stk else stk[unit] # 指数返回不是qfqday,是day df = pd.DataFrame(buf, columns=['time', 'open', 'close', 'high', 'low', 'volume'], dtype='float') df.time = pd.to_datetime(df.time); df.set_index(['time'], inplace=True); df.index.name = '' # 处理索引 return df # 腾讯分钟线 def get_price_min_tx(code, end_date=None, count=10, frequency='1d'): # 分钟线获取 ts = int(frequency[:-1]) if frequency[:-1].isdigit() else 1 # 解析K线周期数 if end_date: end_date = end_date.strftime('%Y-%m-%d') if isinstance(end_date, datetime.date) else \\ end_date.split(' ')[0] URL = f'http://ifzq.gtimg.cn/appstock/app/kline/mkline?param={code},m{ts},,{count}' st = json.loads(requests.get(URL).content); buf = st['data'][code]['m' + str(ts)] df = pd.DataFrame(buf, columns=['time', 'open', 'close', 'high', 'low', 'volume', 'n1', 'n2']) df = df[['time', 'open', 'close', 'high', 'low', 'volume']] df[['open', 'close', 'high', 'low', 'volume']] = df[['open', 'close', 'high', 'low', 'volume']].astype('float') df.time = pd.to_datetime(df.time); df.set_index(['time'], inplace=True); df.index.name = '' # 处理索引 df['close'][-1] = float(st['data'][code]['qt'][code][3]) # 最新基金数据是3位的 return df # sina新浪全周期获取函数，分钟线 5m,15m,30m,60m 日线1d=240m 周线1w=1200m 1月=7200m def get_price_sina(code, end_date='', count=10, frequency='60m'): # 新浪全周期获取函数 frequency = frequency.replace('1d', '240m').replace('1w', '1200m').replace('1M', '7200m'); mcount = count ts = int(frequency[:-1]) if frequency[:-1].isdigit() else 1 # 解析K线周期数 if (end_date != '') \u0026 (frequency in ['240m', '1200m', '7200m']): end_date = pd.to_datetime(end_date) if not isinstance(end_date, datetime.date) else end_date # 转换成datetime unit = 4 if frequency == '1200m' else 29 if frequency == '7200m' else 1 # 4,29多几个数据不影响速度 count = count + (datetime.datetime.now() - end_date).days // unit # 结束时间到今天有多少天自然日(肯定 \u003e交易日) # print(code,end_date,count) URL = f'http://money.finance.sina.com.cn/quotes_service/api/json_v2.php/CN_MarketData.getKLineData?symbol={code}\u0026scale={ts}\u0026ma=5\u0026datalen={count}' dstr = json.loads(requests.get(URL).content); # df=pd.DataFrame(dstr,columns=['day','open','high','low','close','volume'],dtype='float') df = pd.DataFrame(dstr, columns=['day', 'open', 'high', 'low', 'close', 'volume']) df['open'] = df['open'].astype(float); df['high'] = df['high'].astype(float); # 转换数据类型 df['low'] = df['low'].astype(float); df['close'] = df['close'].astype(float); df['volume'] = df['volume'].astype(float) df.day = pd.to_datetime(df.day); df.set_index(['day'], inplace=True); df.index.name = '' # 处理索引 if (end_date != '') \u0026 (frequency in ['240m', '1200m', '7200m']): return df[df.index \u003c= end_date][ -mcount:] # 日线带结束时间先返回 return df def get_price(code, end_date='', count=10, frequency='1d', fields=[]): # 对外暴露只有唯一函数，这样对用户才是最友好的 xcode = code.replace('.XSHG', '').replace('.XSHE', '') # 证券代码编码兼容处理 xcode = 'sh' + xcode if ('XSHG' in code) else 'sz' + xcode if ('XSHE' in code) else code if frequency in ['1d', '1w', '1M']: # 1d日线 1w周线 1M月线 try: return get_price_sina(xcode, end_date=end_date, count=count, frequency=frequency) # 主力 except: return get_price_day_tx(xcode, end_date=end_date, count=count, frequency=frequency) # 备用 if frequency in ['1m', '5m', '15m', '30m', '60m']: # 分钟线 ,1m只有腾讯接口 5分钟5m 60分钟60m if frequency in '1m': return get_price_min_tx(xcode, end_date=end_dat","date":"2022-11-14","objectID":"/20221114_stock-spy/:4:0","tags":["Python","Pandas"],"title":"Stock Monitoring and Notification Project","uri":"/20221114_stock-spy/"},{"categories":["Technology"],"content":"A Chrome extension is a small software program that is designed to add functionality to the Google Chrome web browser. Extensions can be developed using web technologies such as HTML, CSS, and JavaScript. Therefore, JavaScript is often used in the development of Chrome extensions to add interactivity and to implement the logic of the extension. ","date":"2022-09-29","objectID":"/20220929_chrome-extension-project/:0:0","tags":["JavaScript","Browser Extension"],"title":"Chrome Extension Project: Weibo Advertising Filter","uri":"/20220929_chrome-extension-project/"},{"categories":["Technology"],"content":"Introduction Github Project https://github.com/Jerrysmd/weibo-content-filter This is a Chrome extension that serves as a new version of the Weibo plugin. It removes recommended, promoted, and advertisement Weibo from the timeline. With this extension, users can enjoy a cleaner and more streamlined Weibo experience. The extension is designed to improve the user’s browsing experience by removing unnecessary content and focusing on the content that the user wants to see. It is a simple and easy-to-use tool that can greatly enhance the way users interact with Weibo. ","date":"2022-09-29","objectID":"/20220929_chrome-extension-project/:1:0","tags":["JavaScript","Browser Extension"],"title":"Chrome Extension Project: Weibo Advertising Filter","uri":"/20220929_chrome-extension-project/"},{"categories":["Technology"],"content":"Requirements analysis In order to develop a Chrome extension that effectively removes recommended, promoted, and advertisement Weibo from the timeline, we conducted a thorough requirements analysis to identify the needs and expectations of the target audience. This included gathering feedback from potential users, researching similar extensions on the market, and analyzing the technical requirements for implementing the desired functionality. Based on this analysis, we identified the following functional requirements for the extension: The extension should be able to identify and remove recommended, promoted, and advertisement Weibo from the timeline. The extension should allow the user to customize which types of Weibo they want to remove (e.g. recommended only, promoted only, both recommended and promoted). The extension should have a user-friendly interface that allows the user to easily enable or disable the extension. In addition to these functional requirements, we also identified the following non-functional requirements: The extension should be lightweight and not significantly impact the performance of the browser. The extension should be compatible with the latest version of Google Chrome. The extension should be easy to install and use for the average user. By considering these requirements, we were able to create a clear vision for the extension and develop a plan for its development. ","date":"2022-09-29","objectID":"/20220929_chrome-extension-project/:2:0","tags":["JavaScript","Browser Extension"],"title":"Chrome Extension Project: Weibo Advertising Filter","uri":"/20220929_chrome-extension-project/"},{"categories":["Technology"],"content":"Design In order to meet the requirements and deliver a high-quality Chrome extension, we have developed the following design for the extension: User interface: The extension will have a simple and intuitive user interface that allows the user to easily enable or disable the extension, as well as customize the types of Weibo that they want to remove. The interface will consist of a toggle button and a drop-down menu, both of which can be accessed from the extension icon in the Chrome toolbar. Weibo identification: To identify recommended, promoted, and advertisement Weibo, the extension will use a combination of CSS selectors and JavaScript code. The selectors will be used to locate the relevant elements on the page, and the JavaScript code will be used to determine whether the elements should be removed based on the user’s preferences. Performance optimization: To ensure that the extension does not significantly impact the performance of the browser, we will optimize the code for efficiency and minimize the number of DOM manipulations. We will also implement caching and other performance-enhancing techniques as necessary. Overall, our design aims to create a simple and effective tool that improves the user’s experience on Weibo by removing unwanted content from the timeline. ","date":"2022-09-29","objectID":"/20220929_chrome-extension-project/:3:0","tags":["JavaScript","Browser Extension"],"title":"Chrome Extension Project: Weibo Advertising Filter","uri":"/20220929_chrome-extension-project/"},{"categories":["Technology"],"content":"Implementation manifest.json：Chrome extension settings { \"name\": \"大眼夹（新版微博）\", \"short_name\": \"大眼夹\", \"version\": \"0.1\", \"manifest_version\": 3, \"description\": \"新版微博非官方插件，去除时间线上的推荐、推广和广告微博。\", \"icons\": { \"48\": \"weiboFilter.png\", \"128\": \"weiboFilter.large.png\" }, \"content_scripts\": [ { \"matches\": [ \"https://weibo.com/*\", \"https://www.weibo.com/*\", \"https://d.weibo.com/*\", \"http://d.weibo.com/*\", \"http://weibo.com/*\", \"http://www.weibo.com/*\" ], \"js\": [\"main.js\"], \"run_at\": \"document_end\" } ], \"web_accessible_resources\": [{ \"resources\": [\"weiboClean.js\"], \"matches\": [\"\u003call_urls\u003e\"] }], \"permissions\": [ \"storage\" ] } main.js // 借助自定义事件实现page script（注入页面的主程序）与content script（运行在沙箱中） // 之间的异步通讯，使前者可以间接调用chrome.* API和GM_* API console.log('I am in weiboFilter.js'); document.addEventListener('wbpGet', function (event) { event.stopPropagation(); var name = event.detail.name; var post = function (value) { // 注意：不能在此处直接调用callback，否则回调函数将在本程序所在的沙箱环境中运行，在Chrome 27及更高版本下会出错 // 在Greasemonkey（Firefox扩展）环境下也不能通过detail直接传对象，只能送string或array // 详见https://developer.mozilla.org/en-US/docs/Web/API/CustomEvent#Specification document.dispatchEvent(new CustomEvent('wbpPost', { detail: event.detail.id + '=' + (value || '') })); }; if (event.detail.sync) { // 一次性读取所有设置 chrome.storage.sync.get(null, function (items) { var i = 0, value = ''; while ((name + '_' + i) in items) { value += items[name + '_' + (i++)]; } post(i ? value : event.detail.defVal); }); } else { // 注意：使用chrome.storage.StorageArea.get()时，如果通过{name:defVal}的形式传送默认值， // 且defVal为null或undefined，即使name存在也会返回空对象{}，详见crbug.com/145081 chrome.storage.local.get(name, function (items) { post(name in items ? items[name] : event.detail.defVal); }); } }); document.addEventListener('wbpSet', function (event) { event.stopPropagation(); var data = {}, name = event.detail.name, value = event.detail.value; data[name] = value; if (event.detail.sync) { // 将设置保存到同步存储 // 一次性读取所有设置 chrome.storage.sync.get(null, function (items) { var data = {}, i = 0, errorHandler = function () { if (chrome.runtime \u0026\u0026 chrome.runtime.lastError) { console.error('Error writing to storage.sync: ' + chrome.runtime.lastError.message); } }, partLength = Math.round(chrome.storage.sync.QUOTA_BYTES_PER_ITEM * 0.8); // chrome.storage.sync的存储条数（MAX_ITEMS=512）与单条长度（QUOTA_BYTES_PER_ITEM=4,096） // 均受限制，当设置过长时需要拆分；chrome.storage.storageArea.set()会对输入项 // 做一次JSON.stringify()，且Unicode字符将被转换为\\uXXXX的转义形式，都会导致字符串膨胀 // 因此拆分时需要事先留出一定裕度（此处保留20%），并将字符串转为Unicode转义格式 for (var j = 0, l = 0, s = '', u; j \u003c value.length; ++j) { if (value.charCodeAt(j) \u003c 0x80) { // ASCII字符 s += value.charAt(j); ++l; } else { // Unicode字符 u = value.charCodeAt(j).toString(16); s += '\\\\u' + (u.length === 2 ? '00' + u : u.length === 3 ? '0' + u : u); l += 6; } if (l \u003e= partLength) { data[name + '_' + (i++)] = s; l = 0; s = ''; } } if (l \u003e 0) { data[name + '_' + (i++)] = s; } // 保存新的设置 chrome.storage.sync.set(data, errorHandler); // 清除多余的旧设置块 var keys = []; while ((name + '_' + i) in items) { keys.push(name + '_' + (i++)); } chrome.storage.sync.remove(keys, errorHandler); }); } else { // 将设置保存到本地存储 chrome.storage.local.set(data); } }); //#endif // 将脚本注入页面环境 var script = document.createElement('script'); script.setAttribute('type', 'text/javascript'); script.src = chrome.runtime.getURL(\"/weiboClean.js\"); document.head.appendChild(script); weiboClean.js document.addEventListener('DOMNodeInserted', function (event) { var node = event.target; if (node.tagName !== 'DIV') { return; } document.querySelectorAll('#scroller \u003e div.vue-recycle-scroller__item-wrapper \u003e div').forEach(function (div) { //console.log(div); if(div.querySelectorAll('i[title=\"负反馈\"').length \u003e 0){ div.style.display = 'none'; }else{ div.style.display = ''; } }); }); ","date":"2022-09-29","objectID":"/20220929_chrome-extension-project/:4:0","tags":["JavaScript","Browser Extension"],"title":"Chrome Extension Project: Weibo Advertising Filter","uri":"/20220929_chrome-extension-project/"},{"categories":["Technology"],"content":"Testing Detecting the GUCCI advertising，add the attribute of display: none Test ","date":"2022-09-29","objectID":"/20220929_chrome-extension-project/:5:0","tags":["JavaScript","Browser Extension"],"title":"Chrome Extension Project: Weibo Advertising Filter","uri":"/20220929_chrome-extension-project/"},{"categories":["Technology"],"content":"Deployment Deployment ","date":"2022-09-29","objectID":"/20220929_chrome-extension-project/:6:0","tags":["JavaScript","Browser Extension"],"title":"Chrome Extension Project: Weibo Advertising Filter","uri":"/20220929_chrome-extension-project/"},{"categories":["Technology"],"content":"Spring MVC is a module in the Spring framework that helps you build web applications. It is a framework that helps you build web applications in a clean and modular way, by providing a structure for request handling and a model-view-controller design pattern. ","date":"2022-09-14","objectID":"/20220914_springmvc-intro/:0:0","tags":["Java","Spring","SpringMVC"],"title":"SpringMVC Introduction","uri":"/20220914_springmvc-intro/"},{"categories":["Technology"],"content":"SpringMVC ","date":"2022-09-14","objectID":"/20220914_springmvc-intro/:1:0","tags":["Java","Spring","SpringMVC"],"title":"SpringMVC Introduction","uri":"/20220914_springmvc-intro/"},{"categories":["Technology"],"content":"对比 Servlet 对比 SpringMVC 和 Servlet，实现相同的功能。 实现对 User 模块增删改查的模拟操作。 Servlet 实现 com.jerry.servlet.UserSaveServlet.java: package com.jerry.servlet; import ...; @WebServlet(\"/user/save\") public class UserSaveServlet extends HttpServlet{ @Override protected void doGet(HttpServletRequest req, HeepServletResponse resp) throws ServletException, IOException{ String name = req.getParameter(\"name\"); println(\"servlet save name：\" + name); resp.setContenType(\"text/json;charset=utf-8\"); PrintWriter pw = resp.getWriter(); pw.write(\"{'module':'servlet save'}\"); } @Override protected void doPost(HttpServletRequest req, HeepServletResponse resp) throws ServletException, IOException{ this.doGet(req,resp); } } com.jerry.servlet.UserSelectServlet.java: 和 Save 功能类似实现方式 com.jerry.servlet.UserUpdateServlet.java: 和 Save 功能类似实现方式 com.jerry.servlet.UserDeleteServlet.java: 和 Save 功能类似实现方式 SpringMVC 实现 com.jerry.springmvc.UserController.java: package com.jerry.springmvc; import ...; @Controller public class UserController{ @RequestMapping(\"/save\") @ResponseBody public String save(String name){ println(\"springmvc save name：\" + name); } @RequestMapping(\"/select\") @ResponseBody public String select(String name){ println(\"springmvc select name：\" + name); } @RequestMapping(\"/update\") @ResponseBody public String update(String name){ println(\"springmvc update name：\" + name); } @RequestMapping(\"/delete\") @ResponseBody public String delete(String name){ println(\"springmvc delete name：\" + name); } } ","date":"2022-09-14","objectID":"/20220914_springmvc-intro/:1:1","tags":["Java","Spring","SpringMVC"],"title":"SpringMVC Introduction","uri":"/20220914_springmvc-intro/"},{"categories":["Technology"],"content":"概述 SpringMVC 与 Servlet 技术功能等同，都属于 web 层开发技术 优点 使用简单，相比 Servlet 开发便捷 灵活性强 ","date":"2022-09-14","objectID":"/20220914_springmvc-intro/:1:2","tags":["Java","Spring","SpringMVC"],"title":"SpringMVC Introduction","uri":"/20220914_springmvc-intro/"},{"categories":["Technology"],"content":"Demo com.jerry.controller.UserController //使用 @Controller 定义 bean @Controller public class UserController{ //设置当前操作的访问路径 @RequestMapping(\"/save\") //设置当前操作的返回值 @ResponseBody public String save(){ return \"{'module':'springmvc'}\"; } } 工作流程分析： 启动服务器初始化过程 单词请求过程 ","date":"2022-09-14","objectID":"/20220914_springmvc-intro/:1:3","tags":["Java","Spring","SpringMVC"],"title":"SpringMVC Introduction","uri":"/20220914_springmvc-intro/"},{"categories":["Technology"],"content":"Bean 加载控制 ","date":"2022-09-14","objectID":"/20220914_springmvc-intro/:1:4","tags":["Java","Spring","SpringMVC"],"title":"SpringMVC Introduction","uri":"/20220914_springmvc-intro/"},{"categories":["Technology"],"content":"请求与相应 请求映射路径 名称：@RequestMapping 类型：方法注解 \\ 类注解 位置：SpringMVC 控制器方法定义上方 作用：设置当前控制器方法请求访问路径，如果设置在类上统一设置当前控制器方法请求访问路径前缀 范例： com.jerry.controller.UserController @Controller @RequestMapping(\"/user\") public class UserController{ @RequestMapping(\"/save\") @ResponseBody public String save(){ return \"{'module':'springmvc'}\"; } } Get \u0026 Post","date":"2022-09-14","objectID":"/20220914_springmvc-intro/:1:5","tags":["Java","Spring","SpringMVC"],"title":"SpringMVC Introduction","uri":"/20220914_springmvc-intro/"},{"categories":["Technology"],"content":"Spring makes programming Java quicker, easier, and safer. Spring’s focus on speed, simplicity, and productivity has made it the world’s most popular Java framework. Spring works on microservices, reactive, cloud, web apps, serverless, event driven, and batch services. ","date":"2022-08-05","objectID":"/20220805_spring-intro/:0:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"Framework spring-overview 上层依赖于下层 Core Container 核心容器，管理对象 AOP 面向切面编程 Aspects AOP 思想实现 Data Access / Integration 数据访问 / 集成 Transactions 事务 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:1:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"核心容器 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:2:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"IoC \u0026 DI IoC (Inversion of Control) 控制反转 DI (Dependency Injection) 依赖注入 场景： //业务层实现 public class ServiceImpl implements Service{ //数据层更新，业务层也需更新。然后重新编译，重新测试，重新部署，重新发布。 private Dao dao = new DaoImpl(); public void save(){ bookDao.save(); } } //数据层实现 public class DaoImpl implements Dao{ public void save(){ //...method1 } } public class DaoImpl2 implements Dao{ public void save(){ //...method2 } } 现状：耦合度高。数据层更新，业务层也需更新。然后重新编译，重新测试，重新部署，重新发布。 解决方案： IoC (Inversion of Control) 控制反转 使用对象时，在程序中不要主动使用 new 产生对象，转换由外部提供对象。此思想称为控制反转 IoC。 Spring 对 IoC 思想进行了实现： Spring 提供了一个容器，称为 IoC 容器，用来作为提供对象的**“外部”**。就是 Spring Framework 中的核心容器。 IoC 容器负责对象的创建，初始化等一系列工作，被创建或被管理的对象在 IoC 容器中统称为 Bean。 DI (Dependency Injection) 依赖注入 在容器中建立 bean 与 bean 之间的依赖关系的整个过程，称为依赖注入。 总结 目标：充分解耦 使用 IoC 容器管理 bean （IoC） 在 IoC 容器内将有依赖关系的 bean 进行关系绑定（DI） 最终效果 使用对象时不仅可以直接 IoC Intro Case IoC 管理什么？（Service、Dao、Service和Dao 关系） 如何将被管理的对象告知 IoC 容器？（配置） 被管理的对象交给 IoC 容器，如何获取到 IoC 容器？（接口） IoC 容器得到后，如何从容器中获取 bean？（接口方法） IoC Intro Case 步骤（XML） 第一步：导入 spring-context \u003c!--Mavern文件：pom.xml--\u003e \u003cdependency\u003e \u003cgroupId\u003eorg.springframework\u003c/groupId\u003e \u003cartifactId\u003espring-context\u003c/artifactId\u003e \u003cversion\u003e*.*.*.RELEASE\u003c/version\u003e \u003c/dependency\u003e 第二步：定义 Spring 管理的类和接口 public interface BookService{ public void save(); } public class BookServiceImpl implements BookService{ private BookDao bookDao = new BookDaoImpl(); public void save(){ bookDao.save(); } } 第三步：创建 Spring 配置文件，配置对应类作为 Spring 管理的 bean \u003c!--配置文件：applicationContext.xml--\u003e \u003c!--第三步：新建 applicationContext.xml 文件，配置 bean: bean 标签标识配置 bean id 属性标示 bean 的名字 class 属性标示给 bean 定义类型--\u003e \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"/\u003e \u003cbean id=\"bookService\" class=\"com.jerry.service.impl.BookServiceImpl\"/\u003e 第四步：初始化 IoC 容器（Spring 容器），通过容器获取 bean //App2.java package com.jerry; public class App2{ public static void main(String[] args){ // 获取 IoC 容器，根据配置文件创建 IoC ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); // 获取 bean，与配置文件的 id 对应 BookService bookService = (BookService) ctx.getBean(\"bookService\"); bookService.save();// Service层是由一个或多个Dao层操作组成的。 } } DI Intro Case 基于 IoC 管理 bean（基于 IoC Intro Case） Service 中使用 new 形式创建 Dao 对象？（否，使用 new 耦合性仍然很高） Service 中需要的 Dao 对象如何进入到 Service 中？（提供方法） Service 与 Dao 间的关系如何描述，Spring 如何知道该关系？（配置） DI Intro Case 步骤（XML） 第一步：删除业务层中使用 new 的方式创建的 dao 对象，提供依赖对象对应的 setter 方法 public class BookServiceImpl implements BookService{ // 5.删除业务层中使用 new 的方式创建的 dao 对象 private BookDao bookDao; public void save(){ bookDao.save(); } // 6.提供对应的 set 方法 public void setBookDao(bookDao bookDao){ this.bookDao = bookDao; } } 第二步：配置 service 与 dao 之间的关系 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"/\u003e \u003cbean id=\"bookService\" class=\"com.jerry.service.impl.BookServiceImpl\"\u003e \u003c!--7.配置 server 与 dao 的关系--\u003e \u003c!--name 属性表示配置哪一个具体的属性--\u003e \u003c!--ref 属性表示参照哪一个 bean，与 bean id 对应--\u003e \u003cproperty name=\"bookDao\" ref=\"bookDao\"\u003e\u003c/property\u003e \u003c/bean\u003e ","date":"2022-08-05","objectID":"/20220805_spring-intro/:2:1","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"Bean Bean 配置 bean 基础配置 类别 描述 属性列表 id：使用容器可以通过 id 获取对应的 bean class：bean 的类型，即配置 bean 的全路径类名 范例 \u003cbean id=“bookDao” class=“com.jerry.dao.impl.BookDaoImpl”/\u003e\u003cbean id=“bookService” class=“com.jerry.service.impl.BookServiceImpl”/\u003e bean 作用范围配置 类别 描述 属性 scope 功能 定义 bean 的作用范围，可选范围如下 1. singleton：单例（默认） 2. prototype：非单例 范例 \u003cbean id=“bookDao” class=“com.jerry.dao.impl.BookDaoImpl”/\u003e 为什么 bean 默认为单例？ 这些对象复用没有问题，所以默认为单例，节省内存。 适合交给容器进行管理的 bean 适合交给容器管理的 bean 不适合交给容器管理的 bean 表现层对象 封装实体的域对象（记录成员变量的对象） 业务层对象 数据层对象 工具类对象 Bean 实例化 Bean 的四种实例化 方法一：构造方法实例化（常用） 提供可访问的构造方法 public class BookDaoImpl implements BookDao{ public BookDaoImpl(){ print(\"book constructor is running\"); } public void save(){ print(\"book dao save\"); } } 配置 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"\u003e\u003c/bean\u003e 无参构造方法如果不存在，将抛出异常 BeanCreateionException 方法二：静态工厂实例化 静态工厂 public class OrderDaoFactory{ public static OrderDao getOrderDao(){ return new OrderDaoImpl(); } } 配置 \u003cbean id=\"orderDao\" class=\"com.jerry.factory.OrderDaoFactory\" factory-method=\"getOrderDao\"\u003e\u003c/bean\u003e 使用 main(){ UserDaoFactory userDaoFactory = new UserDaoFactory(); UserDao userDao = userDaoFactory.getUserDao(); userDao.save(); } 方法三：使用实例工厂实例化 实例工厂 public class UserDaoFactory{ public UserDao getUserDao(){ return new UserDaoImpl(); } } 配置 \u003cbean id=\"userFactory\" class=\"com.jerry.factory.UserDaoFactory\"\u003e\u003c/bean\u003e \u003cbean id=\"userDao\" factory-method=\"getUserDao\" factory-bean=\"userFactory\"\u003e\u003c/bean\u003e 方法四：使用 FactoryBean 实例化 bean（重要） FactoryBean public class UserDaoFactoryBean implements FactoryBean\u003cUserDao\u003e{ //代替原始实例工厂中创建对象的方法 public UserDao getObject() throws Exception{ return new UserDaoImpl(); } public Class\u003c?\u003e getObjectType(){ return UserDao.class; } public boolean isSingleton(){ return true; } } 配置 \u003cbean id=\"userDao\" class=\"com.jerry.factory.UserDaoFactoryBean\"\u003e\u003c/bean\u003e Bean 的生命周期 bean 生命周期控制方法 方法一：配置文件管理控制方法 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\" init-method=\"init\" destroy-method=\"destory\"\u003e\u003c/bean\u003e 方法二：使用接口控制（了解） //实现 InitializingBean，DisposableBean 接口 public class BookServiceImpl implements BookService, InitializingBean, DisposableBean{ public void save(){} public void afterPropertiesSet() throws Exception{} public void destroy() throws Exception{} } 生命周期 初始化容器 创建对象（内存分配） 执行构造方法 执行属性注入（set 操作） 执行 bean 初始化方法 使用 bean 执行业务操作 销毁容器 执行 bean 销毁方法 关闭容器 ConfigurableApplicationContext close() registerShutdownHook() 依赖注入方式 向一个类中传递数据的方式有几种？ 普通方法（set 方法） 构造方法 依赖注入描述了在容器中建立 bean 与 bean 之间依赖关系的过程，如果 bean 运行需要的是数字或字符串呢？ 引用类型 简单类型（基本数据类型与 String） 依赖注入方式 setter 注入 简单类型 引用类型 构造器注入 简单类型 引用类型 Setter 注入 - 引用类型 在 bean 中定义引用类型属性并提供可访问的 set 方法 public class BookServiceImpl implements BookService { private BookDao bookDao; public void setBookDao(BookDao bookDao){ this.bookDao = bookDao; } } 配置中使用 property 标签 ref 属性注入引用类型对象 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"/\u003e \u003cbean id=\"bookService\" class=\"com.jerry.service.impl.BookServiceImpl\"\u003e \u003c!--配置 server 与 dao 的关系--\u003e \u003c!--name 属性表示配置哪一个具体的属性--\u003e \u003c!--ref 属性表示参照哪一个 bean，与 bean id 对应--\u003e \u003cproperty name=\"bookDao\" ref=\"bookDao\"\u003e\u003c/property\u003e \u003c/bean\u003e Setter 注入 - 简单类型 配置文件注入值 \u003cbean id=\"userkDao\" class=\"com.jerry.dao.impl.UserDaoImpl\"/\u003e \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"\u003e \u003cproperty name=\"databaseName\" value=\"mysql\"\u003e\u003c/property\u003e \u003cproperty name=\"connectionNum\" value=\"10\"\u003e\u003c/property\u003e \u003c/bean\u003e \u003cbean id=\"bookService\" class=\"com.jerry.service.impl.BookServiceImpl\"\u003e \u003cproperty name=\"bookDao\" ref=\"bookDao\"\u003e\u003c/property\u003e \u003cproperty name=\"userDao\" ref=\"userDao\"\u003e\u003c/property\u003e \u003c/bean\u003e 在 bean 中定义引用类型属性并提供可访问的 set 方法 public class BookDaoImpl implements BookDao{ private int connectionNum; private String databaseName; public void setConnectionNum(int connectionNum){ this.connectionNum = connectionNum; } public void setDatabaseName(String databaseName){ this.databaseName = databaseName; } public void save(){ System.out.println(\"book dao save\" + databaseName + connectionNum); } } 构造器注入 - 引用类型（了解） 在 bean 中定义引用类型属性并提供可访问的构造方法 配置中使用 constructor-arg 标签 ref 属性注入引用类型对象 依赖注入方式选择 使用 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:2:2","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"核心容器总结 容器相关： BeanFactory 是 IoC 容器的顶层接口，初始化 BeanFactory 对象时，加载的 bean 延迟加载 ApplicationContext 接口是 Spring 容器的核心接口，初始化时 bean 立即加载 ApplicationContext 接口提供基础的 bean 操作相关方法，通过其他接口扩展其功能 ApplicationContext 接口常用初始化类 ClassPathXmlApplicationContext FileSystemXmlApplicationContext Bean 相关： \u003cbean /\u003e id bean 的 id name bean 别名 class bean 类型，静态工厂类，FactoryBean 类 scope 控制 bean 的实例数量 init-method 生命周期初始化方法 destroy-method 生命周期销毁方法 autowire 自动装配类型 factory-method bean 工厂方法，应用于静态工厂或实例工厂 factory-bean 实例工厂 bean lazy-init 控制 bean 延迟加载 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:2:3","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"注解 @Component(\"bookDao\") public class BookDaoImpl implements BookDao{ public void save(){ print(\"boook dao save\"); } } \u003ccontext:component-scan base-package=\"com.jerry\"/\u003e \u003c!--扫描 com.jerry 下的所有的包--\u003e @Component 注解代表了原来使用 applicationContext.xml 中的 \u003cbean id=\"bookDao\" class=\"com.jerry.dao.impl.BookDaoImpl\"/\u003e ","date":"2022-08-05","objectID":"/20220805_spring-intro/:3:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"衍生注解定义 bean Spring 提供 @Component 注解的三个衍生注解，和 @Component 功能一样，只是方便理解。 @Controller：用于表现层 bean 定义 @Service：用于业务层 bean 定义 @Repository：用于数据层 bean 定义 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:3:1","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"纯注解开发 Spring 3.0 开始了纯注解开发模式，使用 Java 类替代配置文件，开启了 Spring 快速开发 Java 类代替 Spring 核心配置文件 @Configuration @ComponentScan(\"com.jerry\") public class SpringConfig{...} 两个注解完全代替了原本的 applicationContext.xml 文件，不需再使用配置文件 读取 Spring 核心配置文件初始化容器对象切换为读取 Java 配置类初始化容器对象 ApplicationContext ctx = new ClassPathXmlApplicationContext(\"applicationContext.xml\"); ApplicationContext ctx = new AnnotationConfigApplicationContext(SpringConfig.class); Bean 的作用范围 使用 @Scope 定义 bean 作用范围 使用 @PostConstruct、@PreDestroy 定义 bean 生命周期 @Repository @Scope(\"singletion\") public class BookDaoImpl implements BookDao{ public BookDaoImpl(){print(\"...\")} @PostConstruct public void init(){print(\"...\")} @PreDestroy public void destroy(){print(\"...\")} } 依赖注入 使用 @Autowired 注解开启自动装配模式（按类型） @Service public class BookServiceImpl implements BookService{ //////////////////////////////// @Autowired private BookDao bookDao; //@Autowired 代替了 setter 方法 //public void setBookDao(BookDao bookDao){ // this.bookDao = bookDao; //} //////////////////////////////// public void save(){ bookDao.save(); } } 注意： 自动装配基于反射设计创建对象并暴力反射对应属性为私有属性初始化数据，因此无需提供 setter 方法 自动装配建议使用无参构造方法创建对象（默认），如果不提供对应构造方法，请提供唯一的构造方法 使用 @Qualifier 注解开启指定名称装配 bean @Autowired @Qualifier(\"bookDao\") private BookDao bookDao; 使用 @Value 实现简单类型注入 @Repository(\"bookDao\") public class BookDaoImpl implements BookDao{ @Value(\"100\") private String connectionNum; } @Repository(\"bookDao\") public class BookDaoImpl implements BookDao{ @Value(\"${connectionNum}\") private String connectionNum; } 第三方 bean 管理 将独立的配置类加入核心配置 导入式 public class JdbcConfig{ @Bean public DataSource dataSource(){ DruidDataSource ds = new DruidDataSource(); //相关配置 return ds; } } 使用 @Import 注解手动加入配置类到核心配置，此注解只能添加一次，多个数据使用数组格式 @Configuration @Import(JdbcConfig.class) public class SpringConfig{} 第三方 bean 依赖注入 简单类型依赖注入 public class JdbcConfig{ @Value(\"com.mysql.jdbc.Driver\") private String driver; @Value(\"jdbc:mysql://localhost:3306/spring_db\") private String url; //... @Bean public DataSource dataSource(){ DruidDataSource ds = new DruidDataSource(); ds.setDriverClassName(driver); ds.setUrl(url); return ds; } } 引用类型依赖注入 @Bean public DataSource dataSource(BookService bookService){ print(bookService); DruidDataSource ds = new DruidDataSource(); //属性设置 return ds; } 引用类型注入只需要为 bean 定义方法设置形参即可，容器会根据类型自动装备对象。 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:3:2","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"注解总结 XML 配置对比注解配置 功能 XML 配置 注解 定义 bean bean 标签 * id 属性 * class 属性 @Component * @Controller * @Service * @Repository @ComponentScan 设置依赖注入 setter 注入（set 方法） * 引用 / 简单 构造器注入（构造方法） * 引用 / 简单 自动装配 @Autowired * @Qualifier @Value 配置第三方 bean bean 标签 静态工厂、实例工厂、FactoryBean @Bean 作用范围 scope 属性 @Scope 生命周期 标准接口 * init-method * destroy-method @PostConstructor @PreDestroy ","date":"2022-08-05","objectID":"/20220805_spring-intro/:3:3","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"Spring 整合 MyBatis MyBatis is a popular, open-source persistence framework for Java that simplifies the process of working with databases. MyBatis 程序核心对象分析 //1. 创建 SqlSessionFactoryBuilder 对象 SQLSessionFactoryBuilder sqlSessionFactoryBuilder = new SqlSessionFactoryBuilder(); //2. 加载 SqlMapConfig.xml 配置文件 InputStream inputStream = Resources.getResourceAsStream(\"SqlMapConfig.xml\"); //3. 创建 SqlSessionFactory 对象 SqlSessionFactory sqlSessionFactory = sqlSeeesionFactoryBuilder.build(inputStream); //4. 获取 SqlSession SqlSession sqlSession = sqlSessionFactory.openSession(); //5. 执行 SqlSession 对象执行查询，获取结果 User AccountDao accountDao = sqlSession.getMapper(AccountDao.class); Account ac = accountDao.findById(2); println(ac); //6. 释放资源 sqlSession.cloase(); 使用 bean 替换原始的 mybatis-config.xml 中的环境配置 @Bean public SqlSessionFactoryBean sqlSessionFactory(DataSource dataSource){ SqlSesssionFactoryBean ssfb = new SqlSessionFactoryBean(); ssfb.setTypeAliasesPackage(\"com.jerry.domain\"); ssfb.setDataSource(dataSource); return ssfb; } 使用 bean 替换原始的 mybatis-config.xml 中的 mapper 配置 @Bean public MapperScannerConfigurer mapperScannerConfigurer(){ MapperScannerConfigurer msc = new MapperScannerConfigurer(); msc.setBasePackage(\"com.jerry.dao\"); return msc; } ","date":"2022-08-05","objectID":"/20220805_spring-intro/:4:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"Spring 整合 JUnit 使用 Spring 整合 JUnit 专用的类加载器 @RunWith(SpringJUnit4ClassRunner.class) @ContextConfiguration(classes = SpringConfig.class) public class BookServiceTest{ @Autowired private BookService bookService; @Test public void testSave(){ bookService.save(); } } ","date":"2022-08-05","objectID":"/20220805_spring-intro/:5:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"AOP AOP(Aspect Oriented Programming)面向切面编程，一种编程范式，指导开发者如何组织程序结构 作用：在不惊动原始设计的基础上为其进行功能增强 Spring 理念：无侵入式 AOP 核心概念 连接点（JoinPoint）：程序执行过程中的任意位置，粒度为执行方法、抛出异常、设置变量等 在 SpringAOP 中，理解为方法的执行 切入点（Pointcut）：匹配连接点的式子 在 SpringAOP 中，一个切入点可以只描述一个具体方法，也可以匹配多个方法 一个具体方法：com.jerry.dao 包下的 BookDao 接口中的无形参无返回值的 save 方法 匹配多个方法：所有的 save 方法，所有的 get 开头的方法，所有以 Dao 结尾的接口中的任意方法，所有带一个参数的方法 通知（Advice）：在切入点处执行的操作，也就是共性功能 在 SpringAOP 中，功能最终以方法的形式呈现 通知类：定义通知的类 切面（Aspect）：描述通知与切入点的对应关系 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:6:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"实例 案例设定：测定接口执行效率 简化设定：在接口执行前输出当前系统时间 开发模式：XML 或 注解 思路分析： 导入坐标（pom.xml） 制作连接点方法（原始操作，Dao 接口与实现类） 制作共性功能（通知类与通知） 定义切入点 绑定切入点与通知关系（切面） public class MyAdvice{ @Pointcut(\"execution(void com.jerry.dao.BookDao.update())\") private void pt(){} @Before(\"pt()\") public void before(){ println(\"before the func\"); } } ","date":"2022-08-05","objectID":"/20220805_spring-intro/:6:1","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"AOP 工作流程 Spring 容器启动 读取所有切面配置中的切入点 初始化 bean，判定 bean 对应的类中的方法是否匹配到任意切入点 匹配失败，创建对象 匹配成功，创建原始对象（目标对象）的代理对象 获取 bean 执行方法 获取 bean，调用方法并执行，完成操作 获取的 bean 是代理对象时，根据代理对象的运行模式运行原始方法与增强的内容，完成操作 SpringAOP本质：代理模式 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:6:2","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"AOP 切入点表达式 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:6:3","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"AOP 通知类型 AOP 通知共分为 5 种类型 前置通知 后置通知 环绕通知（重点） 返回后通知（了解） 抛出异常后通知（了解） ","date":"2022-08-05","objectID":"/20220805_spring-intro/:6:4","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"Demo 统计执行时间 统计一个方法万次执行时间 @Aspect public class ProjectAdvice{ @Pointcut(\"execution(* com.jerry.service.*Service.*(..))\") private void servicePt(){} @Around(\"servicePt()\") public void runSpeed(ProceedingJoinPoint pjp) throws Throwable{ //获取执行签名信息 Signature signature = pjp.getSignature(); //通过签名获取执行类型（接口名） String className = signature.getDeclaringTypeName(); //通过签名获取执行操作名称（方法名） String methodName = signature.getName(); long start = System.currentTimeMillis(); for(int i = 0; i \u003c 10000; i++){ pjp.proceed(); } long end = System.currentTimeMillis(); println(\"业务层接口万次执行时间：\"+ className + methodName + \"：\" + (end - start) + \"ms\"); } } ","date":"2022-08-05","objectID":"/20220805_spring-intro/:6:5","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"AOP 通知获取数据 获取切入点方法的参数 JoinPoint：适用于前置、后置、返回后、抛出异常后通知 ProceedJoinPoint：适用于环绕通知 获取切入点返回值 返回后通知 环绕通知 获取切入点方法运行异常信息 抛出异常后通知 环绕通知 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:6:6","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"事务 事务作用：在数据层保障一系列的数据库操作同时成功同时失败 Spring 事务作用：在数据层或业务层保障一系列的数据库操作同时成功同时失败 public interface PlatformTransactionManager{ void commit(TransactionStatus status) throws TransactionException; void roolback(TransactionStatus status) throws TransactionException; } public class DataSourceTransactionManager{ //... } ","date":"2022-08-05","objectID":"/20220805_spring-intro/:7:0","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"Demo 银行转账 数据层提供基础操作，指定账户减钱（outMoney），指定账户加钱（inMoney） 业务层提供转账操作（transfer），调用减钱与加钱的操作 基于 Spring 整合 MyBatista 环境搭建 步骤： 在业务层接口上添加事务管理 public interface AccountService{ @Transactional public void transer(String out, String in, Double money); } 注意事项 Spring 注解式事务通常添加在业务层接口中而不会添加到业务层实现类中，降低耦合 设置事务管理器 @Bean public PlatformTransactionManager transactionManager(DataSource dataSource){ DataSourceTransactionManager ptm = new DataSourceTransactionManager(); ptm.setDataSource(dataSource); return ptm; } 注意事项 事务管理器要根据实现技术进行选择 MyBatis框架使用的是 JDBC 事务 开启注解式事务驱动 @Configuration @ComponentScan(\"com.jerry\") @PropertySource(\"classpath:jdbc.properties\") @Import({JdbcConfig.class, MybatisConfig.class}) @EnableTransactionMangement public class SpringConfig{} ","date":"2022-08-05","objectID":"/20220805_spring-intro/:7:1","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"事务传播行为 场景：交易记录后并写入日志。 问题：因为交易和写入日志默认为同一事务，如果交易出错，也不会写入日志。 解决：给事务添加传播属性，让其形成新事务。 @Transactional(添加传播属性) 传播属性 事务管理员 事务协调员 REQUIRED（默认） 开启T 加入T 无 新建T2 REQUIRES_NEW 开启T 新建T2 无 新建T2 SUPPORTS 开启T 加入T 无 无 NOT_SUPPORTED 开启T 无 无 无 MANDATORY 开启T 加入T 无 ERROR NEVER 开启T ERROR 无 无 NESTED 设置 savePoint， 由客户响应提交 / 回滚 ","date":"2022-08-05","objectID":"/20220805_spring-intro/:7:2","tags":["Java","Spring"],"title":"Spring Framework Introduction","uri":"/20220805_spring-intro/"},{"categories":["Technology"],"content":"Spark SQL is the top active component in spark 3.0 release. Most of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in latest release. ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:0:0","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"Explain 查看执行计划 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:1:0","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"语法 .explain(mode=\"xxx\") explain(mode=\"simple\"): 只展示物理执行计划 explain(mode=\"extended\"): 展示物理计划和逻辑执行计划 \"codegen\": 展示 codegen 生成的可执行 Java 代码 \"cost\": 展示优化后的逻辑执行计划以及相关的统计 \"formatted\": 分隔输出，输出更易读的物理执行计划并展示每个节点的详细信息 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:1:1","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"执行计划处理流程 处理流程 核心过程 🔴Unresolved 逻辑执行计划：== Parsed Logical Plan == Parser 组件检查 SQL 语法是否有问题，然后生成 Unresolved 的逻辑计划，不检查表名、不检查列明 🟠Resolved 逻辑执行计划：==Analyzed Logical Plan == Spark 中的 Catalog 存储库来解析验证语义、列名、类型、表名等 🟡优化后的逻辑执行计划：== Optimized Logical Plan == Catalyst 优化器根据各种规则进行优化 🟢物理执行计划：== Physical Plan == HashAggregate 运算符表示数据聚合，一般 HashAggregate 是成对出现，第一个 HashAggregate 是将执行节点本地的数据进行局部聚合，另一个 HashAggregate 是将各个分区的数据进行聚合计算 Exchange 运算符其实就是 shuffle，表示需要在集群上移动数据。很多时候 HashAggregate 会以 Exchange 分隔开 Project运算符是 SQL 中的选择列，select name, age BroadcastHashJoin表示通过基于广播方式进行 HashJoin LocalTableScan 表示全表扫描本地的表 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:1:2","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"资源调优 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:2:0","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"资源规划 资源设定考虑 总体原则 单台服务器 128G 内存，32线程。 先设定单个 Executor 核数，根据 Yarn 配置得出每个节点最多的 Executor 数量，(Yarn 总核数 / 每个executor核数(通常为4)) = 单个节点的executor数量； 28 / 4 = 7 单个节点的executor数量； 总的 executor 数 = 单节点executor数量 * nm节点数。 具体提交参数 executor-cores 每个 executor 的最大核数。3 ~ 6 之间比较合理，通常为4 num-executors num-executors = 每个节点的 executor 数 * work 节点数； 每个 node 的 executor 数 = 单节点 yarn 总核数 / 每个 executor 的最大 cpu 核数； 32线程有28线程用在 Yarn 上； 那么每个 node 的 executor 数 = 28 / 4 = 7； 假设集群节点为10； 那么 num-executors = 7 * 10 = 70。 executor-memory⭐ executor-memory = Yarn 内存 / 单个节点的executor数量； 100G(总128G, 100G 给 Yarn) / 7 = 14G; 内存设置 一个 executor 内部 堆内存模型 🟢估算 Other 内存 = 自定义数据结构 * 每个 Executor 核数 🔵估算 Storage 内存 = 广播变量 + cache/Executor 数量 🟣估算 Executor 内存 = 每个 Executor 核数 * (数据集大小/并行度) Sparksql 并行度默认为 200，并行度即为 Task 数量 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:2:1","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"持久化和序列化 RDD Kryo 序列化缓存 使用： sparkConf 指定 kryo 序列化器 sparkConf 注册样例类 new SparkConf() ... .set(\"spark.serializer\",\"...kryoSerializer\") .registerKryoClasses(Array(classOf[...])) result.persist(StorageLevel.MEMORY_ONLY_SER) 测试： 2G 的 HIVE 元数据，使用 RDD 缓存，完成 100% Fraction Cached 需要 7 G左右内存，使 partition 很容易挂掉。使用 Kryo 序列器完成 Cached 需要 1 G内存。 DF、DS cache 默认使用 MEMORY_AND_DISK缓存 Note 序列化器(Java, Kryo)是针对 RDD 而言的；而 DF、DS 是由 encoder 选择的。 encoder 由 SparkSql 自己实现的，也有可能使用 kryo 的方式。 对 DF、DS使用序列化差别不大。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:2:2","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"CPU优化 CPU 低效原因 并行度 并行度就是 Task 数量。 🟠RDD 并行度参数： spark.default.parallelism 不设置时，默认由 join、reduceByKey 和 parallelize 等转换决定。 🟡SparkSQL 并行度参数：与 RDD 并行度互不影响 spark.sql.shuffle.partitions 默认是 200，只能控制 SparkSQL、DataFrame、Dataset 分区个数。 并发度 并发度：同时执行的 Task 数量。 CPU 低效原因 并行度较低、数据分片较大容易导致 CPU 线程挂起； 并行度过高、数据过于分散会让调度开销更多； CPU 资源调整 spark-submit --master yarn --deploy-mode client --driver-memory 1g --num-executors 3 --executor-cores 4 --executor-memory 6g --class com.jar 🟣官方推荐并行度（Task 数）设置成并发度（vcore 数）的 2 倍到 3 倍。 例：如果以目前的资源（3 个 executor）去提交，每个 executor 有两个核，总共 6 个核，则并行度设置为 12 ~ 18。 SparkConf() ... .set(\"spark.sql.shuffle.partitions\", \"18\") ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:2:3","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"SparkSQL 语法优化 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:3:0","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"基于 RBO 优化 谓词下推 //=============Inner on 左表============= spark.sqlContext.sql( \"\"\" |select | l.id, | l.name, | r.id, | r.name |from course l join student r | on l.id=r.id and l.dt=r.dt and l.dn=r.dn |on l.id\u003c2 |\"\"\".stripMargin) //=============Inner where 左表============= spark.sqlContext.sql( \"\"\" |select | l.id, | l.name, | r.id, | r.name |from course l join student r | on l.id=r.id and l.dt=r.dt and l.dn=r.dn |where l.id\u003c2 |\"\"\".stripMargin) inner join 无论是 ON 还是 WHERE，无论条件是右表还是左表。从 logic plan -\u003e Analyzed logical plan 到 optimized logical plan，sparkSQL 都会优化先过滤数据再进行 join 连接，而且其中一表过滤，另一表也优化提前过滤（最终要过滤数据，另一表也没有存在的必要） left join 条件在 左表 条件在 右表 条件在 on 后 只下推右表 只下推右表 条件在 where 后 两表都下推 两表都下推 注意：外关联时，过滤条件在 on 与 where，语义是不同的，结果也是不同的。 列裁剪 扫描数据源的时候，只读取那些与查询相关的字段。 常量替换 Catalyst 会使用 constantFolding 规则，自动用表达式的结果进行替换。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:3:1","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"基于 CBO 优化 Statistics 收集 需要先执行特定的 SQL 语句来收集所需的表和列的统计信息。 🔵 生成表级别统计信息（扫表）： ANALYZE TABLE 表明 COMPUTE STATISTICS 使用 CBO 通过 spark.sql.cbo.enabled 来开启，默认是 false。CBO 优化器可以基于表和列的统计信息，选择出最优的查询计划。比如：Build 侧选择、优化 Join 类型、优化多表 Join 顺序。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:3:2","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"广播 Join 通过参数指定自动广播 广播 join 默认值为 10MB，由 spark.sql.autoBroadcastJoinThreshold参数控制。 指定广播 sparkSQL 加 HINT 方式 使用 function._ broadcast API ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:3:3","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"SMB Join 大表 JOIN 大表，进行 SMB（sort merge bucket）操作： 需要进行分桶，首先会进行排序，然后根据 key 值合并，把相同 key 的数据放到同一个 bucket 中（按照 key 进行 hash）。分桶的目的就是把大表化成小表。相同的 key 的数据都在同一个桶中，再进行 join 操作，那么在联合的时候就会大幅度的减小无关项的扫描。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:3:4","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"数据倾斜 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:4:0","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"现象 绝大多数 Task 任务运行速度很快，但几个 Task 任务运行速度极其缓慢，慢慢的可能接着报内存溢出的问题。 Task数据倾斜 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:4:1","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"原因 数据倾斜发生在 shuffle 类的算子，比如 distinct、groupByKey、reduceByKey、aggregateByKey、join、cogroup 等，涉及到数据重分区，如果其中某一个 key 数量特别大，就发生了数据倾斜。需要先对大 Key 进行定位。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:4:2","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"数据倾斜大 key 定位 使用抽取采样方法 val top10key = df .select(keyColumn).sample(false, 0.1).rdd //抽取 10% .map(k =\u003e (k, 1)).reduceByKey(_+_) .map(k =\u003e (k._2, k._1)).sortByKey(false) //按统计的key进行排序 .take(10) ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:4:3","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"单表数据倾斜优化 单表优化 为了减少 shuffle 以及 reduce 端的压力，SparkSQL 会在 map 端会做一个 partial aggregate（预聚合或者偏聚合），即在 shuffle 前将同一分区内所属同 key 的记录先进行一个预结算，再将结果进行 shuffle，发送到 reduce 端做一个汇总，类似 MR 的提前 Combiner，所以执行计划中 Hashaggregate 通常成对出现。 二次聚合 select id, sum(course) total from ( select remove_random_prefix(random_courseid) courseid, course from ( select random_id, sum(sellmoney) course from ( select random_prefix(id, 6) random_id, sellmoney from doubleAggre )t1 group by random_id )t2 )t3 group by id def randomPrefixUDF(value, num):String = { new Random().nextInt(num).toString + \"_\" + value } def removeRandomPrefixUDF(value):String = { value.toString.split(\"_\")(1) } ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:4:4","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"Join数据倾斜优化 广播Join 通过参数指定自动广播 广播 join 默认值为 10MB，由 spark.sql.autoBroadcastJoinThreshold参数控制。 指定广播 sparkSQL 加 HINT 方式 使用 function._ broadcast API 拆分大 key 打散大表 扩容小表 与单表数据倾斜优化的二次聚合不同，join 数据倾斜调优要对两表都进行调整。 因为大表为了分区加入了前缀，为了和小表匹配上，小表也应建立对应的前缀与之匹配。如：（假设有3个 task，把大key重新打散到所有task上） 1 1 1 1 1 1 1 1 2 3 Join 1 2 3 拆分大 key 打散大表 扩容小表 —\u003e 0_1 1_1 2_1 0_1 1_1 2_1 0_1 1_1 Join 0_1 1_1 2_1 2 3 Union 2 3 Join 1 2 3 拆分倾斜的 key：根据 key 过滤出倾斜的数据和除倾斜外的其他数据； 将切斜的 key 打散：打散成 task 数量的份数(比如有36个task)，key 值前加(0 ~ 36)随机数； 小表进行扩容：扩大成 task 数量的份数，key 值用 flatmap 生成 36 份，i + \"_\" + key 倾斜的大 key 与扩容后的表进行join； 没有倾斜的 key与原来的表进行join； 将倾斜 key join 后的结果与普通 key join 后的结果，union起来。 代价：shuffle 次数增多了，但是每次 shuffle 数据更均匀了。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:4:5","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"Job 优化 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:5:0","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"Map 端优化 Map 端聚合 parkSQL 会在 map 端会做一个 partial aggregate（预聚合或者偏聚合），即在 shuffle 前将同一分区内所属同 key 的记录先进行一个预结算，再将结果进行 shuffle，发送到 reduce 端做一个汇总，类似 MR 的提前 Combiner，所以执行计划中 Hashaggregate 通常成对出现。 SparkSQL 本身的 Hashaggregate 就会实现本地预聚合 + 全局聚合。 读取小文件的优化 HIVE 的 CombineHiveInputformat 输入格式会将小文件读到同一个 Map 端里面去。 SparkSQL 中也会自动合并，参数如下： spark.sql.files.maxPartitionBytes=128MB #默认 128m spark.files.openCostInBytes=4194304 #默认 4m 切片大小 = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore)) bytesPerCore = totalBytes / defaultParallelism 计算 totalBytes 时，每个文件都要加上一个 open 开销： 当 (文件1大小 + openCostInBytes) + (文件2大小 + openCostInBytes)+… \u003c= maxPartitionBytes 时，n个文件可以读入同一分区。 增大 map 溢写时输出流 buffer SortShuffle 源码理解： /** * Spills the current in-memory collection to disk if needed. Attempts to acquire more * memory before spilling. * * @param collection collection to spill to disk * @param currentMemory estimated size of the collection in bytes * @return true if `collection` was spilled to disk; false otherwise */ protected def maybeSpill(collection: C, currentMemory: Long): Boolean = { var shouldSpill = false if (elementsRead % 32 == 0 \u0026\u0026 currentMemory \u003e= myMemoryThreshold) { // Claim up to double our current memory from the shuffle memory pool val amountToRequest = 2 * currentMemory - myMemoryThreshold val granted = acquireMemory(amountToRequest) myMemoryThreshold += granted // If we were granted too little memory to grow further (either tryToAcquire returned 0, // or we already had more memory than myMemoryThreshold), spill the current collection shouldSpill = currentMemory \u003e= myMemoryThreshold } shouldSpill = shouldSpill || _elementsRead \u003e numElementsForceSpillThreshold // Actually spill if (shouldSpill) { _spillCount += 1 logSpillage(currentMemory) spill(collection) _elementsRead = 0 _memoryBytesSpilled += currentMemory releaseMemory() } shouldSpill } map 端 Shuffle Write 有一个缓冲区，初始阈值 5m，超过阈值尝试增加到 2*当前使用内存，自动扩容。如果申请不到内存，则进行溢写。这个参数是 internal，指定无效。 溢写时使用输出流缓冲区默认 32k，这些缓冲区减少了磁盘搜索和系统调用次数，适当提高可以提升溢写效率。 Property Name Default Meaning spark.shuffle.file.buffer 32k Size of the in-memory buffer for each shuffle file output stream, in KiB unless otherwise specified. These buffers reduce the number of disk seeks and system calls made in creating intermediate shuffle files. Shuffle 文件涉及到序列化，是采用批的方式读写，默认没批次 1 万条去读写，设置得太低会导致在序列化时过度复制。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:5:1","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"Reduce 端优化 合理设置 Reduce 数量 Reduce 的数量 = shuffle 后的分区数 = 也就是 Task 数量 = 也就是并行度 = spark.sql.shuffle.partitions 默认的200。 并发度是整个集群 spark 的核数，如并发度是12，并行度为并发度的 3~6 倍，设置为 36 过多的 CPU 资源出现空转浪费，过少影响任务性能。 输出产生小文件优化 Join后的结果插入新表 join 结果插入新表，生成的文件数等于 shuffle 并行度，默认就是 200 份插入到 hdfs 上。 解决方式一：在插入表数据前进行缩小分区操作来解决小文件过多问题，如 coalesce、repartition 算子。 解决方式二：调整并行度。 动态分区插入数据 没有 shuffle 的情况下。最差的情况。每个 Task 中都有各个分区的记录，那最终文件数达到 Task 数量 * 表分区数。这种情况极易产生小文件。 INSERT overwrite table A partition (aa) SELECT * FROM B; 动态分区插入-没有shuffle 有 shuffle 的情况下。上面的 Task 数量就变成了 200。那么最差情况就会有 200 * 表分区数。 当 shuffle.partitions 设置大了小文件问题就产生了，设置小了，任务的并行度就下降了，性能随之受到影响。 最理想的情况是根据分区字段进行 shuffle，在上面的 SQL 中加入 distribute by aa。把同一分区的记录都哈希到同一分区中去，由一个 Spark 的 Task 进行写入，这样只会产生 N 个文件，但这种情况也容易出现数据倾斜的问题。 动态分区插入-shuffle 数据倾斜解决思路： 结合之前解决数据倾斜的思路，在确定哪个分区键倾斜的情况下，将倾斜的分区键单独拿出： 将入库的 SQL 拆成（where 分区 != 倾斜分区键）和（where 分区 = 倾斜分区键）两个部分，非倾斜分区键的部分正常 distribute by 分区字段，倾斜分区键的部分 distribute by 随机数： #1.非倾斜部分 INSERT overwrite table A partition (aa) SELECT * FROM B where aa != 大key distribute by aa; #主动产生 shuffle，将同一个分区的数据放到一个 task 中，再执行写入 #2.倾斜键部分 INSERT overwrite table A partition (aa) SELECT * FROM B where aa = 大key distribute by cast(rand() * 5 as int); #打散成5份，5个task，写入5个文件 增大 reduce 缓冲区，减少拉去次数 一般不会调整，ShuffleReader.scala，默认值 reduce 一次读取 48M。 调节 reduce 端拉取数据重试次数 一般不会调整，默认为 3 次。 调节 reduce 端拉取数据等待间隔 一般不会调整，默认为 5 秒。 合理利用 bypass Property Name Default Meaning spark.shuffle.sort.bypassMergeThreshold 200 (Advanced) In the sort-based shuffle manager, avoid merge-sorting data if there is no map-side aggregation and there are at most this many reduce partitions. 当 shuffleManager 为 SortShuffleManager 时，如果 shuffle read task 的数量小于这个阈值（默认200）且不需要 map 端进行合并操作（使用 groupby、sum 聚合算子会预聚合，从执行计划可以得知有一个 hashaggregate -\u003e exchange -\u003e hashaggregate），则shuffle write 过程不会进行排序，使用 BypassMergeSortShuffleWriter 去写数据，但最后会将每个 task 产生的所有临时磁盘文件都合并成一个文件，并会创建单独的索引文件。 当使用 shuffleManager 时，如果确实不需要排序操作，那么建议将这个参数调大一些，大于 shuffle read task 的数量。那么此时就会自动启动 bypass 机制，map-side 就不会进行排序了，减少了排序的性能开销。但这种方式下，依然会产生大量的磁盘文件，因此 shuffle write 性能有待提高。 /** * Register a shuffle with the manager and obtain a handle for it to pass to tasks. */ override def registerShuffle[K, V, C]( shuffleId: Int, numMaps: Int, dependency: ShuffleDependency[K, V, C]): ShuffleHandle = { if (SortShuffleWriter.shouldBypassMergeSort(SparkEnv.get.conf, dependency)) { // If there are fewer than spark.shuffle.sort.bypassMergeThreshold partitions and we don't // need map-side aggregation, then write numPartitions files directly and just concatenate // them at the end. This avoids doing serialization and deserialization twice to merge // together the spilled files, which would happen with the normal code path. The downside is // having multiple files open at a time and thus more memory allocated to buffers. new BypassMergeSortShuffleHandle[K, V]( shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]]) } else if (SortShuffleManager.canUseSerializedShuffle(dependency)) { // Otherwise, try to buffer map outputs in a serialized form, since this is more efficient: new SerializedShuffleHandle[K, V]( shuffleId, numMaps, dependency.asInstanceOf[ShuffleDependency[K, V, V]]) } else { // Otherwise, buffer map outputs in a deserialized form: new BaseShuffleHandle(shuffleId, numMaps, dependency) } } ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:5:2","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"整体优化 调节数据本地化等待时长 在 Spark 项目开发阶段，可以使用 client 模式对程序进行测试，此时可以看到比较全的日志信息，日志信息中有明确的 task 数据本地化的级别，如果大部分都是 Process_LOCAL(进程本地化: 数据和计算是在同一个JVM进程里面)、NODE_LOCAL(节点本地化: 数据和计算是在同一个服务器上)，那么就无需进行调节，但如果很多是 RACK_LOCAL(机架本地化: 数据和计算是在同一个机架上)、ANY，那么需要对本地化的等待时长进行调节，慢慢调整大一些，应该是反复调节，每次调节后观察运行日志，看看大部分的 task 的本地化级别有没有提升，观察整个 spark 作业的运行时间有没有缩短。 使用堆外内存 堆外内存可以减轻垃圾回收的工作，也加快了复制的速度。 当需要缓存非常大的数据量时，虚拟机将承受非常大的 GC 压力，因为虚拟接必须检查每个对象是否可以手机并必须访问所有内存也，本地缓存是最快的，但会给虚拟机带来 GC 压力，所以当需要处理非常多的数据量时可以考虑使用堆外内存来进行优化，因为这不会给 Java GC 带来任何压力，让 Java GC 为引用程序完成工作，缓存操作交给堆外。 result.persist(StorageLevel.OFF_HEAP) 调节连接等待时长 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:5:3","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"故障排除 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:6:0","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"控制 reduce 端缓冲大小以避免 OOM reduce 缓冲区默认 48 m，调大是以性能换执行。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:6:1","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"JVM GC 导致的 shuffle 文件拉取失败 GC 导致连接停滞，连接停滞导致 timeout。提高重试次数和等待时长。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:6:2","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Technology"],"content":"各种序列化导致的报错 不可以在 RDD 的元素类型、算子函数里使用第三方不支持序列化的类型，例如 connection。 ","date":"2022-07-19","objectID":"/20220719_spark-optimization/:6:3","tags":["Tuning","Spark","SQL","Hive"],"title":"Spark Performance Tuning","uri":"/20220719_spark-optimization/"},{"categories":["Life"],"content":"Nonviolent Communication is about connecting with ourselves and others from the heart. It’s about seeing the humanity in all of us. It’s about recognizing our commonalities and differences and finding ways to make life wonderful for all of us. ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:0:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"非暴力沟通 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"非暴力沟通四要素 要素一：客观描述事物，不夹杂任何评判 Tip 我们的挑战在于不夹杂任何评判，无论喜欢与否，我们只是说出人们做了什么。 要素二：表达出我们看到这些行为时的感受 Question 是感到伤心、害怕、喜悦、有趣还是心烦呢? 要素三：表达出我们感受的原因 要素四：提出一个具体的请求 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:1","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"四要素的使用举例 Quote 如： “菲利克斯，看到咖啡桌下的两团脏袜子和电视机旁边的三团脏袜子， 我很生气， 因为我希望在我们共用的空间里能多些整洁。 你愿意把你的袜子放在你房间或放进洗衣机里吗？” ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:2","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"四要素的重要性 对这四个要素的清晰表达，表达的形式也不拘泥于语言。而另一部分则是从对方的表达中了解这四个要素。我们通过感知对方此刻的观察、感受和需要，与他们建立连结，进而聆听他们的请求，来找到通过什么方式让他们的生命变得更丰富。 当我们将注意力持续聚焦在以上几个方面，并协助对方也这样做，我们便在彼此的沟通中创造了一种流动，如此你来我往，最终双方都能自然而然地展现善意： 我此刻的观察、感受和需要是什么； 为了让我的生命更美好，我的请求是什么； 你此刻的观察、感受和需要是什么； 为了让你的生命更美好，你的请求是什么。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:1:3","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"疏离生命的言语 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:2:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"道德评判 当他人的行为与我们的价值观不符，我们便认为这个人是错的或是恶的。例如： “你的问题就是太自私了。” “他们有偏见。” “这样做不恰当。” 当我们在分析和评判时，其实都是在表达自身的价值观和需要，但这样的表达方式却是悲剧性的，引发的是对方的防卫与抗拒。就算他人遵从了我们，很有可能是出于恐惧、内疚或羞愧，而非发自内心。而同时，人们这样做其实意味着他们接受了我们的评判，真是两败俱伤。迟早有一天，我们会发现对方不再那么友好，因为由于内部或外部压力而屈服的人们一定会心怀怨恨，他们由此失去尊严，在情绪上付出代价，更不可能怀着善意回应我们的需要和价值观。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:2:1","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"推卸责任 每个人都对自己的思想、情感与行为负责，若无法意识到这点，沟通也会疏离与生命的连结。 Quote 一位老师表示：“我厌恶打分。我认为这对学生没什么好处，反而为他们带来焦虑。但我不得不打分，这是学区的规定。”于是，我建议那位老师练习将“我不得不打分，因为这是学区的规定”转换为“我选择打分，因为我想要……”她脱口而出回答：“我选择打分，因为我想保住这份工作。” ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:2:2","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"不带评论的观察 非暴力沟通的第一个要素是区分观察与评论。 当我们想要清晰且诚恳地向他人表达我们的状态时，“观察”是一个重要的要素。如果我们在观察中夹杂着评论，人们便不那么容易真正听见我们想要表达的内容，反而会听到批评，甚至产生抗拒心理。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:3:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"区分观察和评论 表达方式 混淆观察和评论 区分观察和评论 1.未对自己的评论负责 “你太大方了” “当我看到你将午餐钱都给了别人，我认为你这样做太大方了” 2.使用的动词暗含评论 “你爱拖延” “你只在考试前一碗学习” 3.推断他人的想法、感受或愿望时，暗示这是唯一的可能 “她无法完成工作” “我不认为她能完成工作” 4.把预测当做确定的事实 “如果你的饮食不均衡，健康就会出问题” “如果你的饮食不均衡，我担心你的健康会出问题” 5.指代不具体 “这户移民家庭照顾不好自己的后院” “我没有看到住在罗斯路1679号的那户移民家庭铲路边的雪” 6.在描述他人能力时，并未表明那只是自己所做的评论 “史密斯是个糟糕的足球运动员” “史密斯在20场球赛中未能进一个球” 7.使用含有评论意味的形容词和副词 “库克长得难看” “库克的长相对我没有吸引力” 注意：总是、永远、从来、每次 之类的词语。这些词在负面表达中经常会引发他人的逆反心理，而非慈悲之情。“经常”、“很少”这样的词也可能混淆观察和评论。 评论 观察 你很少配合我 上一次当我提议一项活动时，你说你不想做 他时常旷课 他上个月每周至少三次旷课 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:3:1","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"体会与表达感受 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:4:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"压抑感受的代价 Quote 类似“我觉得我就像嫁给了一堵墙”这样的话，不太可能让她的先生明白她的感受与渴望，反而更容易认为她在批评他。并且，这类话语最后往往会真的应验，也就是说，当先生听到太太批评他“像一堵墙”时，会感到委屈和气馁，因而不愿意做出回应。这样一来，先生像是一堵墙的印象便会在太太心中进一步得到强化。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:4:1","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"区分感受与想法 在非暴力沟通中，我们需要区分：我们是在表达自己的感受，还是在表达对自己的想法或诉求。 Tip 我们使用语言的习惯时常会引发这样一个混淆，在使用“觉得／感到／感觉”这类词时，我们实际上并没有在表达感受。例如，在“我觉得没有得到公平对待”这句话中，将“我觉得”换成“我认为”也许更恰当。在以下例子中，说话者在使用“觉得”时都并未准确地表达自己的感受。 以下例子都并未准确地表达自己的感受： 在“我感到/觉得”后加上一句话或类似“就像”“好似”这样的词： “我觉得你应该更懂事。” “我觉得自己就像个失败者。” “我觉得好像在和一堵墙生活。” 在“我感到/觉得”后带上“我、你、他、她或者名词”这些人称。 “我觉得自己一直在被使唤。” “我觉得这样做是没有用的。” “我觉得艾米挺负责的。” “我觉得我的上司很爱控制人。” “我感觉你不爱我了。” “你不爱我”是对他人如何感受的判断，而不是感受。在“我感到”之后如果跟随的是我、你、他、她、他们、它、像、好似……通常都不是在表达感受。 相反，实际在表达感受时，我们甚至可以完全不使用“感到/觉得”这个词。我们可以说“我觉得很恼火”，也可以简单地说“我很恼火”。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:4:2","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"建立感受词汇表 使用具体而非模糊或笼统的情绪词汇，有助于我们表达感受。 通过建立表达感受的词汇表，我们可以更清晰明确地体会和表达感受，从而更好地与他人建立连结。允许自己表达感受、袒露脆弱，也会有助于化解冲突。 当需要得到满足时，我们的感受可能是： 感受 Feel 感受 Feel 感受 Feel 全神贯注 absorbed 聚精会神 engrossed 感动 moved 新奇 adventurous 活跃 enlivened 乐观 optimistic 深情 affectionate 热情 enthusiastic 大喜 over-joyed 机敏 alert 激动 excited 受宠若惊 overwhelmed 有活力 alive 兴奋 exhilarated 宁静 peaceful 惊奇 amazed 开朗 expansive 活泼 perky 开怀 amused 期盼 expectant 愉快 pleasant 生机勃勃 animated 兴高采烈 exultant 开心 pleased 感激 appreciative 着迷 fascinated 自豪 proud 热心 ardent 自在 free 安静 quiet 振作 aroused 友善 friendly 容光焕发 radiant 惊讶 astonished 满足 fulfilled 痴迷 rapturous 极为幸福 blissful 高兴 glad 神清气爽 refreshed 屏气敛息 breathless 欢欣雀跃 gleeful 放松 relaxed 快活 buoyant 明媚 glorious 如释重负 relieved 平静 calm 热情洋溢 glowing 满意 satisfied 无忧无虑 carefree 快乐 good-humored 安全 secure 开心 cheerful 感谢 grateful 敏感 sensitive 舒适 comfortable 心满意足 gratified 安详 serene 得意 complacent 幸福 happy 入迷 spellbound 镇静 composed 热心 helpful 心花怒放 splendid 关切 concerned 满怀希望 hopeful 激励 stimulated 自信 confident 感兴趣 inquisitive 惊喜 surprised 惬意 contented 受启迪 inspired 柔软 tender 冷静 cool 热切 intense 欣慰 thankful 好奇 curious 趣味 interested 乐不可支 thrilled 倾倒 dazzled 迷住 intrigued 触动 touched 愉悦 delighted 精神焕发 invigorated 恬静 tranquil 热忱 eager 热衷 involved 满怀信任 trusting 奔放 ebullient 欢乐 joyous, joyful 乐观开朗 upbeat 欣喜若狂 ecstatic 欢腾 jubilant 温暖 warm 兴高采烈 effervescent 亢奋 keyed-up 清醒 wide-awake 欢欣鼓舞 elated 有爱心 loving 美妙 wonderful 陶醉 enchanted 甜美 mellow 热情高涨 zestful 受鼓舞 encouraged 轻快 merry 精力充沛 energetic 欢喜 mirthful 当需要没有得到满足时，我们的感受可能是： 感受 Feel 感受 Feel 感受 Feel 畏惧 afraid 烦躁 edgy 紧绷 overwhelmed 气恼 aggravated 惭愧 embarrassed 惊慌失措 panicky 焦躁 agitated 愤恨 embittered 被动 passive 惊慌 alarmed 恼火 exasperated 迷茫 perplexed 冷漠 aloof 耗尽 exhausted 悲观 pessimistic 愤怒 angry 精疲力竭 fatigued 苦思冥想 puzzled 痛苦 anguished 担忧 fearful 充满敌意 rancorous 烦扰 annoyed 坐立不安 fidgety 迟疑 reluctant 焦虑 anxious 愁苦 forlorn 排斥 repelled 冷淡 apathetic 惊吓 frightened 仇恨 resentful 担心 apprehensive 懊恼 frustrated 躁动 restless 刺激 aroused 大怒 furious 难过 sad 羞愧 ashamed 低落 gloomy 害怕 scared 丧气 beat 内疚 guilty 敏感 sensitive 迷惑 bewildered 逼迫 harried 颤抖 shaky 苦涩 bitter 沉重 heavy 震惊 shocked 乏味 blah 无助 helpless 批判 skeptical 忧郁 blue 犹豫 hesitant 困乏 sleepy 无聊 bored 恐惧 horrified 哀痛 sorrowful 心碎 brokenhearted 敌对 hostile 歉意 sorry 屈辱 chagrined 灼热 hot 没有斗志 spiritless 寒冷 cold 单调 humdrum 失魂落魄 startled 忧虑 concerned 受伤 hurt 惊讶 surprised 困惑 confused 不耐烦 impatient 怀疑 suspicious 冷酷 cool 不在乎 indifferent 不冷不热 tepid 生气 cross 激烈 intense 惊恐 terrified 低落 dejected 怒气 irate 劳累 tired 抑郁 depressed 恼怒 irked 烦恼 troubled 绝望 despairing 激怒 irritated 不适 uncomfortable 沮丧 despondent 嫉妒 jealous 不感兴趣 unconcerned 抽离 detached 紧张不安 jittery 心神不宁 uneasy 不满 disaffected 激动不安 keyed-up 心烦意乱 unglued 失望 disappointed 懒散 lazy 不开心 unhappy 挫败 discouraged 疑虑 leery 气馁 unnerved 幻灭 disenchanted 倦怠 lethargic 不稳定 unsteady 不高兴 disgruntled 无精打采 listless 烦乱 upset 厌恶 disgusted 孤独 lonely 拘谨 uptight 灰心 disheartened 气愤 mad 棘手 vexed 惊愕 dismay 汗颜 mean 厌倦 weary 不快 displeased 郁闷 miserable 伤感 wistful 不安 disquieted 昏沉 mopey 退缩 withdrawn 苦恼 distressed 阴郁 morose 悲伤 woeful 麻烦 disturbed 哀悼 mournful 着急 worried 垂头丧气 downcast 紧张 nervous 糟糕 wretched 消沉 downhearted 心乱如麻 nettled 沉闷 dull 麻木 numb ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:4:3","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"为自己的感受负责 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:5:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"听到不中听的话有四种选择 第一种选择是指责自己，将错误归咎到自己身上。例如，有人气愤地说：“我从没见过像你这么自私的人！”如果选择认同对方的评判并且自我指责，我们也许就会表示：“哦，我应该更体贴一点。”如此这般带给我们的感受是内疚、羞愧和压抑，我们的自尊心也会受到伤害。 第二种选择是指责对方，将错误归咎到对方身上。这时，我们也许会反驳对方说：“你没有资格这么说！我总是考虑你的需要，你才自私呢！”此时我们通常会感到愤怒。 第三种选择是关注我们自己的感受和需要。我们会感到伤心，并发现伤心的感受来自内在的需要——渴望自己的付出得到肯定。这时，我们也许会回应：“听到你说‘我从没见过像你这么自私的人’，我感到伤心，因为我渴望得到认可。我也在努力体贴你，希望这一点能得到你的认可。” 第四种选择是关注对方想要表达的感受和需要。这时，我们可能会问对方：“你感到伤心，希望你的喜好能得到照顾，是吗？” ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:5:1","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"需要：感受的根源 当我们评判、批评、分析或判断他人的言行时，实际上是在用疏离生命的方式表达自己的需要。如果有人说：“你从不懂我。”他真正的心声是渴望得到理解。如果太太说：“这星期你每天都工作到很晚，相比我，你更爱工作！”那她在诉说的是，亲密的需要未能得到满足。 Success 当我们通过评判、判断和想象等方式间接地表达自己的需要时，他人很容易认为我们在批评他们，并随之启动自我辩护和反抗。我们越能直接说出感受以及相关联的需要，他人也越有可能对我们做出善意的回应。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:5:2","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"正确的提出请求 使用正向、具体的语言 先表达感受与需要，再提出请求 请求对方重述你的话 请求诚实表达 区分请求与要求 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:6:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"爱自己 每个人都是独一无二的 面对自己的不完美 转化自我评判和自我要求 不做任何没有乐趣的事！ “不得不”转化为“我选择” ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:7:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"充分表达愤怒 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:8:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"表达愤怒的四个步骤 停下来，深呼吸。 看看我们有哪些评判性的想法。 与我们的需要连结。 表达我们的感受和未被满足的需要。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:8:1","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Life"],"content":"化解冲突 首先表达自己的需要。 其次无论对方说什么，设法找出他们真正的需要。如果他们表达的是想法、评判或分析而不是需要，可以继续寻找背后的需要。 确认我们都准确理解彼此的需要，否则继续寻找背后的需要。 尽可能地同理对方，以便让双方都能确切了解彼此的需要。 当双方都澄清了事件中的需要后，我们就可以用正向的描述提出化解冲突的策略。 ","date":"2022-07-06","objectID":"/20220706_nonviolent-communication/:9:0","tags":["Life","Book"],"title":"Book Summary: Nonviolent Communication","uri":"/20220706_nonviolent-communication/"},{"categories":["Technology"],"content":"There is a problem about word segmentation when I use Chinese in ES. Elasticsearch is the distributed, restful search and analytics engine. You can use Elasticsearch to store, search, and manage data for Logs，Metrics，A search backend，Application monitoring，Endpoint security. ","date":"2022-05-12","objectID":"/20220512_es-wildcard-search/:0:0","tags":["Java","ElasticSearch"],"title":"Elasticsearch Wildcard Search","uri":"/20220512_es-wildcard-search/"},{"categories":["Technology"],"content":"问题描述 Question ES 使用 wildcard 进行模糊查询，有些情况模糊查询失败，如：\"*日本*\"，但测试别的数据，如 “*192.168*” 可以模糊匹配。这是因为 ES 对查询文本分词造成的结果。 ","date":"2022-05-12","objectID":"/20220512_es-wildcard-search/:1:0","tags":["Java","ElasticSearch"],"title":"Elasticsearch Wildcard Search","uri":"/20220512_es-wildcard-search/"},{"categories":["Technology"],"content":"match：分词模糊查询 比如“Everything will be OK, All is well”，会被分词一个一个单词（不是单个字母） { \"from\": 0, \"size\": 20, \"query\": { \"bool\": { \"should\": [{ \"term\": { \"form_name\": \"will\" } } ] } } } ","date":"2022-05-12","objectID":"/20220512_es-wildcard-search/:2:0","tags":["Java","ElasticSearch"],"title":"Elasticsearch Wildcard Search","uri":"/20220512_es-wildcard-search/"},{"categories":["Technology"],"content":"match_phrase ：短语模糊查询 match_phrase是短语搜索，即它会将给定的短语（phrase）当成一个完整的查询条件。 比如查询 “Everything will”，会当成一个完整的短语进行查询， 会查出含有该查询条件的内容。 GET /basic_index*/_search { \"from\": 0, \"size\": 20, \"query\": { \"bool\": { \"should\": [{ \"match\": { \"form_name\": \"Everything will\" } } ] } } } 如果是查询单个字母，match就不管用了。 ","date":"2022-05-12","objectID":"/20220512_es-wildcard-search/:3:0","tags":["Java","ElasticSearch"],"title":"Elasticsearch Wildcard Search","uri":"/20220512_es-wildcard-search/"},{"categories":["Technology"],"content":"wildcard：通配符模糊查询 ? 匹配任意字符 * 匹配0个或多个字符 GET /basic_index*/_search { \"size\": 20, \"from\": 0, \"query\": { \"bool\": { \"should\": [{ \"wildcard\": { \"form_name\": \"*very* } }] } } } 记录是存在的，但是没有查出来？ 因为分词的影响，添加keyword 进行处理 { \"wildcard\": { \"form_name.keyword\": \"*very*\" } } Wildcard 性能会比较慢。如果非必要，尽量避免在开头加通配符 ? 或者 *，这样会明显降低查询性能 如果查询的内容非空，怎么处理？ 直接用** { \"wildcard\": { \"form_name\": \"*\" } } ","date":"2022-05-12","objectID":"/20220512_es-wildcard-search/:4:0","tags":["Java","ElasticSearch"],"title":"Elasticsearch Wildcard Search","uri":"/20220512_es-wildcard-search/"},{"categories":["Technology"],"content":"总结 Es 模糊查询， 分词的用match； 短语的用match_phrase；查询任意的，用wildcard通配符，注意查询的内容是否分词，分词的添加keyword，查询非空的情况，用\"**\"。 ","date":"2022-05-12","objectID":"/20220512_es-wildcard-search/:5:0","tags":["Java","ElasticSearch"],"title":"Elasticsearch Wildcard Search","uri":"/20220512_es-wildcard-search/"},{"categories":["Technology"],"content":"Article description.","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"CAPTCHA stands for ‘Completely Automated Public Turing test to tell Computers and Humans Apart’. It’s already possible to solve it with the rise of deep learning and computer vision. ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:0:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"总体流程 抓取验证码 给验证码打标签 图片预处理 保存数据集 构建模型训练 提取模型使用 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:1:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"抓取验证码 下载数据集，下载了 750 张验证码，用 500 张做训练，剩下 250 张验证模型效果。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:2:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"给验证码打标签 这里的验证码有750张，要给每张验证码标签，自己手工打、买人工识别的服务或直接准备带有标签的验证码。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:3:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"图片预处理 图片信息： 此验证码是 68x23，JPG格式 二值化： 我确信这个验证码足够简单，在丢失图片的颜色信息后仍然能被很好的识别。并且可以降低模型复杂度，因此可以将图片二值化。即只有两个颜色，全黑或者全白。 切割验证码： 观察验证码，没有特别扭曲或者粘连，所以可以把验证码平均切割成4块，分别识别，这样图片识别模型就只需要处理10个分类（如果有字母那将是36个分类而已）由于验证码外面有一圈边框，所以顺带把边框也去掉了。 处理结果： 16x21，黑白2位 代码： img = Image.open(file).convert('L') # 读取图片并灰度化 img = img.crop((2, 1, 66, 22)) # 裁掉边变成 64x21 # 分离数字 img1 = img.crop((0, 0, 16, 21)) img2 = img.crop((16, 0, 32, 21)) img3 = img.crop((32, 0, 48, 21)) img4 = img.crop((48, 0, 64, 21)) img1 = np.array(img1).flatten() # 扁平化，把二维弄成一维 img1 = list(map(lambda x: 1 if x \u003c= 180 else 0, img1)) # 二值化 img2 = np.array(img2).flatten() img2 = list(map(lambda x: 1 if x \u003c= 180 else 0, img2)) img3 = np.array(img3).flatten() img3 = list(map(lambda x: 1 if x \u003c= 180 else 0, img3)) img4 = np.array(img4).flatten() img4 = list(map(lambda x: 1 if x \u003c= 180 else 0, img4)) ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:4:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"保存数据集 数据集有输入输入数据和标签数据，训练数据和测试数据。 因为数据量不大，简便起见，直接把数据存成python文件，供模型调用。就不保存为其他文件，然后用 pandas 什么的来读取了。 最终输入模型的数据形状为 [[0,1,0,1,0,1,0,1…],[0,1,0,1,0,1,0,1…],…] 标签数据很特殊，本质上是对输入的数据进行分类，所以虽然标签应该是0到9的数字，但是这里使标签数据格式是 one-hot vectors [[1,0,0,0,0,0,0,0,0,0,0],…] 一个one-hot向量除了某一位的数字是1以外其余各维度数字都是0**，比如[1,0,0,0,0,0,0,0,0,0] 代表1，[0,1,0,0,0,0,0,0,0,0]代表2. 更进一步，这里的 one-hot 向量其实代表着对应的数据分成这十类的概率。概率为1就是正确的分类。 代码： # 保存输入数据 def px(prefix, img1, img2, img3, img4): with open('./data/' + prefix + '_images.py', 'a+') as f: print(img1, file=f, end=\",\\n\") print(img2, file=f, end=\",\\n\") print(img3, file=f, end=\",\\n\") print(img4, file=f, end=\",\\n\") # 保存标签数据 def py(prefix, code): with open('./data/' + prefix + '_labels.py', 'a+') as f: for x in range(4): tmp = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] tmp[int(code[x])] = 1 print(tmp, file=f, end=\",\\n\") 经过上面两步，在就获得了训练和测试用的数据和标签数据 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:5:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"构建模型训练 数据准备好啦，到了要搭建“管道”的时候了。 也就是你需要告诉 TensorFlow： ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"1. 输入数据的形状是怎样的？ x = tf.placeholder(tf.float32, [None, DLEN]) None 表示不定义有多少训练数据，DLEN是 16*21，即一维化的图片的大小。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:1","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"2. 输出数据的形状是怎样的？ y = tf.placeholder(\"float\", [None, 10]) 同样None 表示不定义有多少训练数据，10 就是标签数据的维度，即图片有 10 个分类。每个分类对应着一个概率，所以是浮点类型。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:2","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"3. 输入数据，模型，标签数据怎样拟合？ W = tf.Variable(tf.zeros([DLEN, 10])) # 权重 b = tf.Variable(tf.zeros([10])) # 偏置 y_ = tf.nn.softmax(tf.matmul(x, W) + b) 一个很简单的模型，大体就是 y = softmax(Wx+b) 其中 W 和 b 是 TensorFlow 中的变量，他们保存着模型在训练过程中的数据，需要定义出来。而模型训练的目的，也就是把 W 和 b 的值确定，使得这个式子可以更好的拟合数据。 softmax 是所谓的激活函数，把线性的结果转换成需要的样式，也就是分类概率的分布。 关于 softmax 之类更多解释请查看参考链接。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:3","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"4. 怎样评估模型的好坏？ 模型训练就是为了使模型输出结果和实际情况相差尽可能小。所以要定义评估方式。 这里用所谓的交叉熵来评估。 cross_entropy = -tf.reduce_sum(y_*tf.log(y)) ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:4","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"5. 怎样最小化误差？ 现在 TensorFlow 已经知道了足够的信息，它要做的工作就是让模型的误差足够小，它会使出各种方法使上面定义的交叉熵 cross_entropy 变得尽可能小。 TensorFlow 内置了不少方式可以达到这个目的，不同方式有不同的特点和适用条件。在这里使用梯度下降法来实现这个目的。 train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy) ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:5","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"训练准备 Python 作为解释型语言，运行效率不能算是太好，而这种机器学习基本是需要大量计算力的场合。TensorFlow 在底层是用 C++ 写就，在 Python 端只是一个操作端口，所有的计算都要交给底层处理。这自然就引出了会话的概念，底层和调用层需要通信。也正是这个特点，TensorFlow 支持很多其他语言接入，如 Java, C，而不仅仅是 Python。 和底层通信是通过会话完成的可以通过一行代码来启动会话： sess = tf.Session() # 训练 ... sess.close() 别忘了在使用完后关闭会话。当然你也可以使用 Python 的 with 语句来自动管理。 在 TensorFlow 中，变量都是需要在会话启动之后初始化才能使用。 sess.run(tf.global_variables_initializer()) ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:6","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"开始训练 for i in range(DNUM): batch_xs = [train_images.data[i]] batch_ys = [train_labels.data[i]] sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys}) 把模型和训练数据交给会话，底层就自动处理。可以一次传入任意数量数据给模型（上面设置None的作用），为了训练效果，可以适当调节每一批次训练的数据。甚至于有时候还要随机选择数据以获得更好的训练效果。在这里使用的是一条一条训练，反正最后效果还可以。要了解更多可以查看参考链接。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:7","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"检验训练结果 这里使用测试数据检验结果 correct_prediction = tf.equal(tf.argmax(y,1), tf.argmax(y_,1)) accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) print(sess.run(accuracy, feed_dict={x: test_images.data, y_: test_labels.data})) 模型输出是一个数组，里面存着每个分类的概率，所以要拿出概率最大的分类和测试标签比较。看在这 250 条测试数据里面，正确率是多少。当然这些也是定义完操作步骤，交给会话来运行处理的。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:6:8","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"提取模型使用 在上面已经把模型训练好了，而且效果还不错哦，近 99% 的正确率，或许比人工打码还高一些呢（获取测试数据时候常常返回有错误的值）。但是问题来了，我现在要把这个模型用于生产怎么办，总不可能每次都训练一次。在这里就要使用到 TensorFlow 的模型保存和载入功能了。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:7:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"保存模型 先在模型训练的时候保存模型，定义一个 saver，然后直接把会话保存到一个目录就好了。 saver = tf.train.Saver() # 训练代码 # ... saver.save(sess, 'model/model') sess.close() 当然这里的 saver 也有不少配置，比如保存最近多少批次的训练结果之类，可以自行查资料。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:7:1","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Technology"],"content":"恢复模型 同样恢复模型也很简单 saver.restore(sess, \"model/model\") 当然你还是需要定义好模型，才能恢复。我的理解是这里模型保存的是训练过程中各个变量的值，权重偏置什么的，所以结构架子还是要事先搭好才行。 ","date":"2022-03-31","objectID":"/20220331_tensorflow-captcha/:8:0","tags":["TensorFlow","MachineLearning"],"title":"Tensorflow Identify Simple Captcha","uri":"/20220331_tensorflow-captcha/"},{"categories":["Life"],"content":"Article description.","date":"2021-12-18","objectID":"/20211218_2021-summary-video/","tags":["Life","Video","Shang hai","Bei jing"],"title":"Memories of 2021","uri":"/20211218_2021-summary-video/"},{"categories":["Life"],"content":"Photos and videos recorded in 2021. ","date":"2021-12-18","objectID":"/20211218_2021-summary-video/:0:0","tags":["Life","Video","Shang hai","Bei jing"],"title":"Memories of 2021","uri":"/20211218_2021-summary-video/"},{"categories":["Life"],"content":"Piano Show: The Wind That Goes Away No Longer Sings ","date":"2021-07-01","objectID":"/20210701_2021-piano/:0:0","tags":["Piano","Music"],"title":"Piano Show: The wind no longer sings","uri":"/20210701_2021-piano/"},{"categories":["Technology"],"content":"Flink optimization includes resource configuration optimization, back pressure processing, data skew, KafkaSource optimization and FlinkSQL optimization. ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:0:0","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"资源配置调优 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:1:0","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"内存设置 给任务分配资源，在一定范围内，增加资源的分配与性能的提升是正比的（部分情况如资源过大，申请资源时间很长）。 提交方式主要是 yarn-per-job，资源的分配在使用脚本提交 Flink 任务时进行指定。 目前通常使用客户端模式，参数使用 -D\u003cproperty=value\u003e 指定 bin/fink run \\ -t yarn-per-job \\ -d \\ -p 5 \\ 指定并行度 -Dyarn.application.queue=test \\ 指定 yarn 队列 -Djobmanager.memory.process.size=2048mb \\ jm 2-4G足够 -Dtaskmanager.memory.process.size=6144mb \\ 单个 tm 2-8G 足够 -Dtaskmanager.numberOfTaskSlots=2 \\ 与容器核数 1core:1slot 或 1core:2slot ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:1:1","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"并行度设置 最优并行度计算 完成开发后，压测，任务并行度给 10 以下，测试单个并行度的处理上限。然后 总 QPS / 单并行度的处理能力 = 并行度 不能只根据 QPS 得出并行度，因为有些字段少，逻辑简单任务。最好根据高峰期的 QPS 压测，并行度 * 1.2，富余一些资源。 Source 端并行度的配置 数据源端是 Kafka，Source 的并行度设置为 Kafka 对应 Topic 的分区数。 如果等于 Kafka 的分区数，消费速度仍跟不上数据生产速度，需考虑 Kafka 要扩大分区，同时调大并行度等于分区数。 Tip 优先加资源； Kafka 的分区数增加不可逆操作，增加分区放在最后使用。 Transform 端并行度的配置 keyby 之前的算子 一般不会做太重的操作，比如是 map、filter、flatmap 等处理较快的算子，并行度可以和 source 保持一致。 keyby 之后的算子 如果并行比较大，可以设置并行度为 2 的整次幂； 小并发任务的并行度不一定需要设置成 2 的整数次幂； 大并发如果没有 keyby，并行度也无需设置为 2 的整次幂。 Sink 端并行度的配置 可以根据 Sink 端的数据量及下游服务抗压能力进行评估。 如果 Sink 端是 Kafka，可以设为 Kafka 对应 Topic 的分区数。 另外 Sink 端要与下游的服务进行交互，要考虑延迟和压测，可以采用： 控制并行度； 采用批量提交的方式。公司采用 CH 采用每5条一提交，加上时间控制 (时间到了，但不足5条，也会提交) 提交的方式。 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:1:2","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"RocksDB 大状态调优 状态客户端，Flink 提供的用于管理状态的组件，RocksDB 基于 LSM Tree 实现 (类似于 HBASE)，写数据都是先缓存到内存中，所以 RocksDB 的写请求效率比较高。 Memory 本地：taskmanager内存 远程：jm内存 fs 本地：tm内存 远程：文件系统（一般是HDFS） rocksDB 本地：当前机器的内存 + 磁盘 远程：文件系统（HDFS） 调优： 增量检查点，（独有的） 设置为 机械 + 内存模式，有条件上 SSD 缓存命中率，和 HBASE 一样，cache 越大，缓存命中率越高。一般设置为 64 ~ 256 MB。 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:1:3","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"CheckPoint 设置 cp 防止数据挂掉，从 cp 恢复数据； 一般在生产环境，时间间隔设置为分钟级别； 如果访问 HDFS 比较耗时，时间间隔可以设置为 5 ~ 10 分钟； ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:1:4","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"Flink parameterTool 读取配置 读取运行参数 使用 parameterTool.get(\"host\") 替换 args[0] 传参时使用 --host hadoop102 --poart 9999 读取系统属性 读取配置文件 parameterTool.fromPropertiesFile(\"/application.properties\") 配置全局参数 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:1:5","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"压测方式 先在 kafka 中积压数据，之后开启 Flink 任务，出现反压，就是处理瓶颈。 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:1:6","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"反压处理 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:2:0","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"反压现象及定位 反压现象 kafka 反压： 逐级传递； transform 的 compute 端有读缓存和写缓存，缓存满了向上级拉取数据的延迟增大；比如上级是 source 端； source 端同理，向 kafka 客户端服务端拉取数据延迟增大； 会造成 kafka 数据延迟，有可能造成机器内存溢出。 spark 反压 spark 开启反压一定是基于 receiver 模式。 executor -\u003e executor -\u003e executor… 与 kafka 不同 executor 出现数据反压不会逐级向上游传递，会直接通信 Driver，Driver 会直接通信最上级 executor 放慢速度。 Tip 逐级更好： 设置了缓存就是为了处理这种情况 直接通信 Driver，会影响整体的进程，不方便确定问题。 利用 Flink Web UI 定位 利用 Metrics 监控工具定位反压位置 反压时，观察到 遇到瓶颈的该 Task 的 inPoolUage 为 1 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:2:1","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"反压的原因及处理 系统资源 内存和CPU资源。一般情况 本都磁盘及网卡资源不会是瓶颈。如果某些资源被充分利用或大量使用，可以借助分析工具分析性能瓶颈（JVM Profiler + FlameGraph 生成火焰图） 火焰图表示了 CPU 执行各个功能的时间，从下自上展示了方法的调用关系，尖顶通常表示执行正常，平顶表示运行时间过长，通常为瓶颈。 针对特定的资源调优 Flink 减少瓶颈算子上游的并行度，从而减少瓶颈算子接受的数据量（可能会造成 Job 数据延迟增大） 垃圾回收GC 运行时打开参数 printGCDetails 下载 GC 日志：因为是 on yarn 模式，一个一个节点看日志比较麻烦，可以打开 WebUI，在 JobManager 或 TaskManager，下载 Stdout 使用 GCviewer 分析数据：最重要的指标是 Funll GC 后，老年代剩余大小 这个指标，按照 Java 性能优化权威指南 这本书 Java 堆大小计算法则，设 Full GC 后老年代剩余大小空间为 M，那么堆的大小建议设置为 3 ~ 4倍 M，新生代为 1 ~ 1.5 倍 M，老年代为 2 ~ 3倍 M。 CPU/线程瓶颈 \u0026 线程竞争 （还是资源问题，使用压测保证资源合理）。 subtask 可能会因为共享资源上高负载线程的竞争成为瓶颈。同样可以考虑上述分析工具。考虑在代码中查找同步开销、锁竞争、尽管避免在代码中添加同步。 负载不平衡 属于数据倾斜问题 外部依赖 source 端读取性能比较低或者 Sink 端性能较差，需要检查第三方组件是否遇到瓶颈。如： kafka集群是否需要扩容，kafka 连接器是否并行度较低； HBASE 的 rowkey 是否遇到热点问题。 需要结合具体组件来分析 解决方案：旁路缓存 + 异步IO ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:2:2","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"数据倾斜 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:3:0","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"判断是否存在数据倾斜 通过 Flink Web UI 可以精确的看到每个 subtask 处理了多少数据，即可以判断出 Flink 任务是否存在数据倾斜。通常数据倾斜也会引起反压。 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:3:1","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"数据倾斜的解决 keyBy 前发生数据倾斜 上游算子的某些实例可能处理的数据较多，某些实例可能处理的数据较少，产生改情况可能是因为数据源本身就不均匀，例如由于某些原因 Kafka 的 topic 中某些 partition 的数据量较大，对于不存在 keyby 的 Flink 任务也会出现该情况。 解决方案：需要让 Flink 任务强制进行 shuffle。使用 shuffle、rebalance（reb的轮询比shuffle的随机更好，rescale是范围轮询） 或 rescale 算子即可将数据均匀分配，从而解决数据倾斜的问题。 keyBy 后发生数据倾斜 扩展 Hive、mr、spark 都有可能 keyby 数据倾斜的通用方法： 双重聚合，如 a 开头的数据有10万条，把 a 数据用 hash 打散，a_0 ~ a_9，每个数据 1 万条；第二次把后缀去掉，把 a 的十条数据再做一次累加得到结果。 Flink 直接聚合，不用开窗用不了。因为流处理还是一条条写数据，没有解决数据数量问题，下游去掉随机数重新 keyby 还是 a 数据还是到一起了，仍然有可能数据倾斜 + 重复数据问题导致结果不对，由于流处理，keyby 本身是 1+1+1+1，流处理变成了1+2+3+4。当然使用开窗可以规避，但开窗还会数据倾斜。 使用 LocalKeyBy 的思想：定义状态，积攒批次的处理。类似 MapReduce 中 Combiner 的思想。 在本地定义状态，使用定时器或者设置数量到达多少的时候，map 端的数据进行一个累加经过 keyby 再到 reduce 端。 keyBy 后的窗口聚合操作存在数据倾斜 窗口就是批处理，窗口只输出一次结果。 但是两个窗口也会出现 4+4 变成 4+8 的现象。（将多个窗口的数据放在一起）（第一个窗口 4 个 a，最终窗口keyby后输出 [a, 4]，第二个窗口又来 4 个 a，去掉随机数后由于都是 a，第二窗口的 [a, 4] 就会和第一个窗口的 [a, 4] sum 操作 变成 [a, 8]，第一个窗口和第二个窗口累加就会出现 [a, 12] 这个错误结果） 解决方法：带上窗口信息进行聚合。 实现思路： key 拼接随机数前缀或后缀，进行 keyby 、开窗、聚合； 注意： 聚合完不再是 windowedStream，要获取 windowEnd 作为窗口标记作为第二阶段分组依据，避免不同窗口的结果聚合到一起。 去掉随机数前缀或后缀，按照原来的 key 及 windowEnd 作为 keyby、聚合。 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:3:2","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"KafkaSource 调优 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:4:0","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"动态发现分区 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:4:1","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"从 kafka 数据源生成 watermark kafka 单分区内有序，多分区无序，这种情况下，由于网络延迟等因素造成数据无序、错乱等隐患。 使用 flink 中可识别 kafka分区的 watermark 生成机制。将在 kafka 消费端内部针对每个 kafka 分区生成 watermark，且不同分区 watermark 的合并方式与在数据流 shuffle 时的合并方式相同。 ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:4:2","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"设置空闲等待 如果数据源中的某一分区/分片在一段时间内未发送事件数据，则以为着 watermarkGenerator 也不会过得任何新数据去生成 watermark。该分区watermark 一直为 long 的最小值 minValue。比如在 kafka 的 topic 中，个别 partition 一直没有新的数据。 由于下游算子 watermark 的计算方式是取所有不同的上游并行数据源 watermark 的最小值，则导致窗口、定时器不会被触发。 可以使用 watermarkStrategy 来检测空闲输入并将其标记为空闲状态。 .withIdleness(Duration.ofMinutes(5)) ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:4:3","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"kafka 的 offset 消费策略 默认消费策略：读取上次的 offset 信息，第一次读会根据 auto.offset.reset 的值来进行消费数据 从最早的数据开始进行消费，忽略存储 offset 信息； 从最新的数据信息消费，忽略存储 offset 信息； 从指定位置开始消费； ","date":"2021-06-23","objectID":"/20210623_flink-optimize/:4:4","tags":["DataWarehouse","Tuning","Flink"],"title":"Flink Performance Tuning","uri":"/20210623_flink-optimize/"},{"categories":["Technology"],"content":"There are systems that can store values of different columns separately, but that can’t effectively process analytical queries due to their optimization for other scenarios. Examples are HBase and BigTable. You would get throughput around a hundred thousand rows per second in these systems, but not hundreds of millions of rows per second. ","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:0:0","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"MergeTree 选型 ","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:1:0","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"ReplacingMergeTree 在 MergeTree 的基础上，添加了 “处理重复数据” 的功能，该引擎和 MergeTree 的不同之处在于它会删除具有相同主键的重复项。使用 order by 使用的字段来规定去重字段。 ","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:1:1","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"SummingMergeTree 在 MergeTree 的基础上，添加了 “合并重复数据” 的功能，会把具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。 ","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:1:2","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"选型 ReplacingMergeTree 使用 SummingMergeTree 不能保证数据准确性，如中断重启数据会重新进来合并。 解决数据合并的方式有很多。 ","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:1:3","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"Flink JDBC CH 业务： result.addSink(JdbcSink.sink()); //question: Phoenix 也是 JDBC，为什么没有用 JdbcSink? // 因为之前访问 Hbase 的表都不一样，表字段也不一样。( phoenix 可以用 自定义的 MySQLSink 或 JDBC) // CH 数据来源唯一 //使用工具类 result.addSink(ClickHouseUtil.getSink(\"insert into line(?, ?, ?)\")) 工具类： public class ClickHouseUtil{ public static \u003cT\u003e SinkFunction\u003cT\u003e getSink()String sql{ return JdbcSink.\u003cT\u003esink(sql, new JdbcStatementBuilder\u003cT\u003e(){ @Override public void accept(PreparedStatement preparedStatement, T t) throws SQLException{ //获取所有属性信息 Field[] fields = t.getClass().getDeclaredFields(); for (int i = 0; i \u003c fields.length; i++){ Field field = fields[i]; //反射 获取值 Object value = field.get(t); //给预编译 SQL 对象赋值 preparedStatement.setObject(i + 1, value); } } }, new JdbcExecutionOptions.Builder().withBatchSize(5).build(), new JdbcConnectionOptions.JdbcConnectionOptionsBuilder() .withDriverName(\"\") .withUrl(\"\") .build()); } } ","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:2:0","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"Kafka2CH Goods topic from Kafka2CH //1.get exeEnv //2.read kafka all topic, create stream //3.uniform all stream format //4.union all stream //5.get tm from data and create WaterMark //6.group by, window, reduce. (按 id 分组，10秒滚动窗口，增量聚合(累加值)和全量聚合(提取窗口信息)) //7.join dimension info //8.wite stream to CH ","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:3:0","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"Sugar 大屏展示","date":"2021-04-13","objectID":"/20210413_clickhouse-flink/:4:0","tags":["DataWarehouse","Flink","ClickHouse"],"title":"Data Warehouse: ClickHouse With Flink","uri":"/20210413_clickhouse-flink/"},{"categories":["Technology"],"content":"Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information. ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:0:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"DWM 数据中间层 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"业务数据写入DWM 数据流和程序流程 数据流：：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) -\u003e [FlinkApp -\u003e Kafka(dwm)] 程序：mocklog -\u003e Mysql -\u003e FlinkCDC -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka/Phoneix(zk/hdfs/hbase) -\u003e [UniqueVisitApp -\u003e Kafka] 代码实现 - 用户第一次访问 //UniqueVisitApp // 1.获取执行环境 StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); env.setParallelism(1); // 2.读取 kafka dwd_page_log 主题的数据 env.addSource(MyKafkaUtil.getKafkaConsumer(sourceTopic, groupId)); // 3.将每行数据转换为 JSON 对象 jsonObjDS = KafkaDS.map(JSON::parseObject); // 4.过滤数据、状态编程、只保留每天第一个数据 keyedStream = jsonObjDS.keyBy(jsonObj -\u003e jsonObj.getJSONObject(\"common\")).getString(\"mid\"); uvDS = keyedStream.filter(new RichFilterFunction\u003cJSONObject\u003e(){ private ValueState\u003cString\u003e dataState; private SimpleDateFormat simpleDateFormat; @Override public void open(Configuration parameters) throws Exception { valueStateDescriptor = new ValueStateDescriptor\u003c\u003e(\"data-state\", String.class); dataState = getRuntimeContext().getState(valueStateDescriptor); simpleDateFormat = new SimpleDateFormat(\"yyyy-MM-dd\"); } @Override public boolean filter(JSONObject value) throws Exception{ return {...}; } }); // 5.将数据写入 Kafka uvDS.map(JSONAware::toJSONString).addSink(MyKafkaUtil.getKafkaProducer(sinkTopic)); // 6.启动 env.execute(\"UniqueVisitApp\"); 代码实现 - 用户跳出页面 //UserJumpDetailApp // 1.获取执行环境 // 2.读取 kafka 主题数据创建流 // 3.将每行数据转换为 JSON 对象，提取时间戳 // 4.定义模式序列 // 5.将模式序列作用到流上 // 6.提取匹配上的超时事件, UNION 两种事件 // 7.将数据写入 Kafka // 6.启动 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"Flink Stream Join Window Join stream.join(otherStream) .where(\u003cKeySelector\u003e) .equalTo(\u003cKeySelector\u003e) .window(\u003cWindowAssigner\u003e) .apply(\u003cJoinFunction\u003e) Tumbing Window Join 滚动窗口：和 SparkStreaming 直接 Join 一样 Sliding Window Join 滑动窗口：有可能数据重复 Join Session Window join 会话窗口：两个流的数据一段时间都没有数据，两个流开始 Join Interval Join 对被Join的流有 lower bound 和 upper bound 范围选择（范围选择则要用到状态编程、当前仅支持状态时间，需提取出状态时间） 代码实现 - 订单明细表双流 Join //OrderWideApp //1.获取执行环境 //2.读取 Kafka 主题的数据，转化为 JavaBean 对象并且提取时间戳生成 WaterMark //3.双流 Join //4.关联维度信息 //5.写入Kafka //6.启动任务 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:2","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"维度表关联 优化1：加入旁路缓存 维度关联时查询维度表，先查 Redis，再查 HBASE。 Redis 作为旁路缓存，订单宽表查询维度数据时先查旁路缓存。没有再查询维度数据，同时同步到旁路缓存。 注意问题 1.缓存要设过期时间，不然冷数据会常驻缓存浪费资源。 2.要考虑维度数据是否会发生变化，如果发生变化要主动清楚缓存。 缓存选型 两种：堆缓存 或者 独立缓存服务(Redis, memcache) 堆缓存，性能好，管理性差。 独立缓存服务，有创建连接、网络 IO 等消耗。管理性更强，更容易扩展。 联合使用(LRU Cache，最近最少使用) 代码实现 public class RedisUtil{...} public class DimUtil{//维度表关联 //查询 HBASE 表之前先查询 Redis Jedis jedis = RedisUtil.getJedis(); String dimInfoJsonStr = jedis.get(redisKey); // RedisKey 不用 hash 的原因： // 1.用户数据量大，使用大量数据可能到一条 hash 上，造成热点问题。 // 2.需要设置过期时间。 // RedisKey 不用 Set 的原因： // 1.查询不方便 // 2.需要设置过期时间。 if(dimInfoJsonStr != null){ //归还连接 jedis.close(); //重置过期时间 jedis.expire(redisKey, 24 * 60 * 60); //返回结果 return JsonObject.parseObject(dimInfoJsonStr); } //从 HBASE 拿数据 .... //将数据写入 Redis jedis.set(); jedis.expire(); jedis.close(); return dimInfoJson; } 优化2：异步查询 异步查询把查询操作托管给单独的线程池完成，这样不会因为某个查询造成阻塞，单个并行可以连续发送多个请求，提高并发效率。 代价：消耗更多的 Tasks，threads、Flink-internal network connections 数据流和程序流程 数据流：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) -\u003e [FlinkApp(redis)] -\u003e Kafka(dwm) 程序：mocklog -\u003e Mysql -\u003e FlinkCDC -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka/Phoneix(zk/hdfs/hbase) -\u003e [OrderWideApp(Redis) -\u003e Kafka] 代码实现 public class DimAsyncFunction\u003cT\u003e extends RichAsyncFunciton\u003cT, T\u003e{ @Override public void open(){} @Override public void asyncInvoke(){} @Override public void timeout(){} } public class ThreadPoolUtil{ private static ThreadPoolExecutor threadPoolExecutor = null; private ThreadPoolUtil(){ } public static ThreadPoolExecutor getThreadPool{ //懒汉式-单例模式 if(threadPoolExecutor == null){ synchronized(ThreadPoolUtil.class){ if(threadPoolExecutor == null){ threadPoolExecutor = new ThreadPoolExecutor(corePoolSize = 8,maximumPoolSize = 16,keepAliveTime = 1L,TimUnit.MiNUTES,new LinkedBlockingDeque\u003c\u003e()); } } } return threadPoolExecutor; } } ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:3","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"主流数据丢失 使用 intervalJoin 来管理流的状态时间，保证当支流数据到达时主流数据还保存在状态中。 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:4","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"业务数据写入DWM 代码实现 - 支付宽表 PaymentWideApp; //1.获取执行环境 //2.读取 kafka 主题数据创建流 //3.双流 Join //4.写入Kafka //5.启动任务 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:1:5","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"DWS 数仓汇总层 轻度聚合，因为 DWS 层要应对很多实时查询，如果完全的明细那么查询的压力是非常大的。 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:2:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"主题宽表写入DWS 数据流和程序流程 数据流：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) -\u003e FlinkApp(redis) -\u003e Kafka(dwm) -\u003e [FlinkApp -\u003e ClickHouse] 程序：mocklog -\u003e Mysql -\u003e FlinkCDC -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka/Phoneix(zk/hdfs/hbase) -\u003e OrderWideApp(Redis) -\u003e Kafka -\u003e [uv/uj -\u003e kafka -\u003e VisitorStatsApp -\u003e ClickHouse] 代码实现 - 访客主题宽表 VisitorStatsApp; //1.获取执行环境 //2.读取 kafka 数据创建流 //3.将每个流处理成相同的数据类型 //4.Union 所有流 //5.提取时间戳生成 WaterMark //6.按照维度信息分组 //7.开窗聚合 10s 的滚动窗口 WindowedStream\u003cVisitorStats, Tuple4, TimeWindow\u003e windowedStream = keyedStream.window(TumblingEventTimeWindows.of(Time.seconds(10))); // reducefunction 增量聚合，效率高 // windowsfunction 全量聚合，包含窗口信息 // reduce + window windowedStream.reduce(new ReduceFunction\u003cVisitorStats\u003e(){}, new WindowFunction\u003cVisitorStats, VisitorStats, Tuple4, TimeWindow(){}) //8.数据写入 ClickHouse //9.启动任务 Info 维度聚合采用方式：uv, 0, 0, 0 ,0 union 0, pv, 0, 0, 0 union 0, 0, sv, 0, 0 … 先union再根据主键聚合 Question uv pv sv uj dur 不同流的数据进行维度聚合时，出现有流数据( uj )会丢失一直显示是 0 的情况，为什么？ 计算完 uj 表输入流，滚动窗口设置的 10 秒已经关闭。 dwd-page-log 第一条数据到达，visitors-topic 开窗[ts + 0, ts + 10)秒的窗口来接收dwd-page-log这10秒的全部数据 但 visitors-topic 同时使用 dwd-page-log 和 uj 两个流的数据，uj 需要10秒后才输出流，visitors-topic 开窗已经关闭 解决方法： 方法一：数据本身时间切换成处理的时间。不建议，数据时间不统一，不具有幂等性 方法二：增加延迟时间 ","date":"2021-03-21","objectID":"/20210321_dw-flink-part2/:2:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅱ","uri":"/20210321_dw-flink-part2/"},{"categories":["Technology"],"content":"Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information. real time process ","date":"2021-03-08","objectID":"/20210308_dw-flink/:0:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"ODS 数据采集层 原始数据，日志和业务数据 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"日志数据采集 springboot springboot 好处： 不需要那些繁琐重复的 xml 文件 内嵌 Tomcat，不需要外部 Tomcat 更方便的和各个第三方工具整合，只要维护一个配置文件即可（mysql、redis、es、dubbo、kafka） springboot 和 ssm 的关系 springboot 整合了 springmvc，spring 等核心功能，也就是说本质还是原有的 spring，springmvc 的包，但是 springboot 单独包装了一层，用户不必直接对 springmvc，spring 等在 xml 中配置。 springboot分层 掌握写数据接口 controller层：拦截用户请求，调用Service，响应请求 service层：调用 DAO，处理数据 DAO（使用 MyBatis 时称为 Mapper 层）：获取数据 持久化层：存储数据 SpringBoot 整合Kafka import org.springframework.stereotype.Controller; import org.springframework.web.bind.annotation.RequestMapping; //@Controller //注解标明 Controller @RestController //@Controller + @ResponseBody @Slf4j //lombok中的包 public class LoggerController{ @RequestMapping(\"test2\") public String test2(@RequestParam(\"name\") String nn, @RequestParam(\"age\", defaultValue = \"18\") int age){ return \"success\" + name + age; } @Autowired //自动注入 private KafkaTemplate\u003cString, String\u003e kafkaTemplate; @RequestMapping(\"applog\") public String test2(@RequestParam(\"param\") String jsonStr){ //sparkmall-mock：模拟生成数据模块 //mock服务器不停的访问 http://pc_IP:8080/applog?jsonStr=* 产生原始数据 /*1.修改SpringBoot核心配置： server.port=8081 指定kafka代理地址，可以多个：spring.kafka.bootstrap-servers=hadoop102.9092 指定消息key和消息体的编解码方式： spring.kafka.producer.key-serializer=org.apache.kafka.common.serialization.StringSerilizer spring.kafka.producer.value-serializer=org.apache.kafka.common.serialization.StringSerilizer*/ /*2.Resources中添加 logback.xml 配置文件：决定如何打印、落盘、打印哪些日志 ，类似log4j 在注释@Slf4j后，使用log.info(jsonStr);log.warn(jsonStr);log.debug();log.error();log.trace()*/ /*3.自动注入kafkaTemplate，producer send 数据，打开zk、kk服务 运行kafka消费者：bin/kafka-console-consumer.sh --bootstrap-server hadoop102:9092 --topic ods_base_log*/ kafkaTemplate.send(\"ods_base_log\", jsonStr); return \"success\"; } } 打包集群部署，Nginx 反向代理 简介： 高性能的 HTTP 和 反向代理服务器，特点是占有内存少，并发能力强，在同类型的网页服务器中表现较好。 反向代理和正向代理： 正向代理类似一个跳板机，代理访问外部资源，如 VPN。 反向代理是指以代理服务器来接受 internet 上的连接请求，然后将请求转发给内部网络上的服务器。 Nginx 主要应用 静态网站部署：Nginx 是一个 HTTP 的 web 服务器，可以将服务器上的静态文件（如 HTML、图片等）通过 HTTP 协议返回给浏览器客户端。 负载均衡：如数据请求发到集群中不同的机器上，常用策略：轮询、权重、备机 静态代理：把所有静态资源的访问改为访问 Nginx 而不是 Tomcat，Nginx 更擅长静态资源的处理。 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"业务数据采集 Flink-CDC CDC Change Data Capture(变更数据获取)的简称，捕获增删改。 CDC的种类 CDC主要分为基于查询和基于Binlog两种方式 基于查询 基于Binlog 开源产品 Sqoop、Kafka JDBC Source Canal、Maxwell、Debezium 执行模式 Batch Streaming 是否可以捕获所有数据变化 否 是 延迟性 高延迟 低延迟 是否增加数据库压力 是 否 Flink-cdc-connectors 可以直接从 MySQL、PostgreSQL等数据库直接 读取全量数据和增量变更数据的 source 组件 组件 业务流程 Flink： MySQL -\u003e Canal -\u003e kafka -\u003e Flink -\u003e 业务处理 Flink-CDC: MySQL -\u003e Fink-CDC -\u003e 业务处理 Flink-cdc code public class FlinkCDC{ public static void main(String[] args){ //1.获取执行环境 //1.1开启 CD 并指定后端为FS memory fs rocksdb //1.2开启 checkpoint 实现断点续传 //2.通过 flinkcsc 构建 sourcefunction 并读取数据 //3.打印数据 //4.启动任务 } } FlinkSQL FlinkSQL只能做单表的监控 Debezium可以做多表的监控，需要自定义反序列化器 public class FlihnkCDCWithSQL{ public staic void main(String[] args){ //1.获取执行环境 //2.DDL方式建表 //3.查询数据 //4.将动态表转化为流 //5.启动任务 } } Flink-cdc DataStream CustomerDeserialization @Override public void deserialize(SourceRecord sourceRecord, Collector...){ //1.创建Json对象用于存储最终数据 //2.获取库名、表明 //3.获取\"before\"数据 //4.获取\"after\"数据 //5.获取操作类型 //6.写入JSON对象 //7.输出数据 } CDC Maxwell Canal 对比 FlinkCDC Maxwell Canal SQL和数据关系 无 无 一对一 初始化功能 有(多库多表) 有(单表) 无 断点续传 ck MySQL 本地磁盘 封装格式 自定义 JSON JSON(c/s自定义) 高可用 运行集群高可用 无 集群(ZK) 环境搭建 IDEA目录 功能 app 产生各层数据的 flink 任务 bean 数据对象 common 公共常量 utils 工具类 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:2","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"ODS 总结 保持数据原貌，不做任何修改 FlinkCDC: DataStream / FlinkSQL 区别 FlinkCDC / Maxwell / Canal 区别 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:1:3","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"DWD 明细数据层 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:0","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"日志数据写入DWD层 数据流和程序流程 数据流：：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) 程序：mocklog -\u003e Nginx -\u003e Logger.sh -\u003e KafkaZ(zk) -\u003e BaseLogApp -\u003e Kafka 代码实现 public class BaseLogApp{ public static void main(String[] args){ //1.获取执行环境 //2.消费 ods_base_log 主题数据创建流 //3.将每行数据转换为JSON对象 //3.1发生异常，如不满足JSON格式，捕获写入侧输出流 //4.新老用户校验，状态编辑 //4.1 转态编程 jsonObjDS.keyBy(...).map(new RichMapFunction\u003cJSONObject, JSONObject\u003e()){ private ValueState\u003cString\u003e valueState; @Override public void open(Configuration parameters) throws Exception { valueState = getRuntimeContext().getState(new ValueStateDescriptor\u003cString\u003e(\"value-state\", String.class)); } @Override public JSONObject map(JSONObject value) throws Excepiton { //获取JSON中\"is_new\"标记 String isNew = value.getJSONObject(\"common\").getString(\"is_new\"); //isnew做处理，旧数据不处理 if(\"1\".equals(isNew)){ String state = valueState.value(); if(state != null){ //修改isnew标记 value.getJSONObject(\"common\").put(\"is_new\", \"0\"); }else{ valueState.update(\"1\"); } } return value; } }; //5.分流、侧输出流 页面：主流 启动：侧输出流 曝光：侧输出流 //6.提取侧输出流 //7.将三个流进行打印并输出到对应的kafka主题中 } } ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:1","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"业务数据写入DWD层 选型 在实时计算中一般把维度数据写入存储容器，如 HBASE、Redis、MySQL 等。 一般把实时数据写入流中，进入dwd层，最终形成宽表。 HBASE：HBase是建立在HDFS之上,提供高可靠性的列存储，实时读写的数据库系统。它介于Nosql和关系型数据库之间，仅通过主键和主键的range来检索数据，仅支持单行事务。主要用来存储非结构化和半结构化的松散数据。 Redis：分布式缓存，基于内存，强调缓存，支持数据持久化，支持事务操作，NoSQL 类型的Key/vale数据库，同时支持List、Set等更丰富的类型。适合放一些频繁使用，比较热的数据，因为是放在内存中，读写速度都非常快。 MySQL：并发压力大，读取速度慢。 Phoenix \u0026 HBASE 功能： Phoenix是一种专门针对于Hbase 所设计的SQL on Hbase 的一个工具 使用SQL对Hbase进行操作 使用phoenix自动构建二级索引来进行快速查询和维护 原理： 上层提供了SQL接口 底层全部通过Hbase Java API来实现，通过构建一系列的Scan和Put来实现数据的读写 功能非常丰富 底层封装了大量的内置的协处理器，可以实现各种复杂的处理需求，例如二级索引等 优点 支持SQL接口 支持自动维护二级索引 缺点 SQL支持的语法不全面 Bug比较多 实现动态分流方案 ZK存储，通过 watch 感知数据变化 MySQL存储，周期性的同步 *MySQL存储，使用广播流 数据流和程序流程 数据流：：web/app -\u003e Nginx -\u003e SpringBoot -\u003e Mysql -\u003e FlinkApp -\u003e Kafka(ods) -\u003e FlinkApp -\u003e Kafka/Hbase(dwd-dim) 程序： mockDB -\u003e Mysql -\u003e FlinkCDC -\u003e Kafka(ZK) -\u003e BaseDBApp -\u003e Kafka/Hbase(Phoenix, zk, hdfs) 代码实现 BaseDBApp; //1.获取执行环境 //2.消费 kafka ods_base_db 主题数据创建流 //3.将每行数据转换成JSON对象并过滤 主流 //4.使用FlinkCDC消费配置表并处理成 广播流 //5.连接主流和广播流 //6.分流 处理数据 广播流数据和主流数据(根据广播流数据进行处理) //7.提取kafka流数据和HBASE流数据 //8.将kafka数据写入Kafka主题，将HBASE数据写入Phoenix表 //9.启动 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:2","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"维表层DIM层 层 业务 ODS / DWD / DIM 一般与业务需求无关 (离线数仓是 DWS 和 DWT) / (实时数仓是 DWM / DWS) 与业务相关性非常高 ADS 等于业务需求 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:3","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"DWD-DIM 总结 行为数据：DWD（Kafka） 过滤脏数据 –\u003e 侧输出流 脏数据率 新老用户校验 –\u003e 前台校验不准 分流 –\u003e 侧输出流 页面、启动、曝光、动作、错误 写入 kafka 业务数据：DWD（Kafka）- DIM（HBASE） 过滤数据 –\u003e 删除数据 读取配置表创建广播流 连接主流和广播流并处理 广播流数据： 解析数据，Phoenix建表 写入状态广播 主流数据 读取状态 过滤字段 分流（添加SinkTable字段） 提取Kafka和HBASE流分别对应的位置 HBASE流：自定义Sink Kafka流：自定义序列化方式 ","date":"2021-03-08","objectID":"/20210308_dw-flink/:2:4","tags":["DataWarehouse","Flink"],"title":"Data Warehouse: Real-Time, part Ⅰ","uri":"/20210308_dw-flink/"},{"categories":["Technology"],"content":"Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information. ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:0:0","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据仓库 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:0","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"概念 offline DW process 数仓是对数据进行 备份、清洗、聚合、统计 等操作。 ODS原始数据层；DWD明细数据层；DWS服务数据层；DWT数据主题层；ADS数据应用层 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:1","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"技术组件 数据采集传输：Flume，Kafka，DataX，Sqoop，Logstash 数据存储：MySQL，HDFS，HBASE，Redis，MongoDB 数据计算：Hive，Spark，Flink，Tez，Storm 数据查询：Presto，Kylin，Impala，Druid 数据可视化：Echarts、Superset、QuickBI、DataV 任务调度：Azkaban、Oozie 集群监控：Zabbix 元数据管理：Atlas 权限管理：Ranger ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:2","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"系统流程 业务数据 / 前端 js 数据：持久化或不持久化写入数据库。 Nginx：负载均衡，使每个节点数据量保持合理。 Flume：采集日志。可以直接采集到 Hadoop，但 hadoop 可能处理很慢，如双11。可以先写到 Kafka 里。 Flume 组成，Put 事务，Take 事务 Flume 三个器：拦截器，选择器，监控器 Flume 优化：内存，台数 Kafka：23 件事 1.kafka基本信息 2.挂了 3.丢失 4.重复 5.积压 6.优化 7.高效读写的原因 Zookeeper：分布式协调 1.部署多少台 2.选举机制，Paxos算法 Flume： 消费传到 Hadoop Hive： 1.Hive内部表、外部表区别 2.4个by 3.系统函数 4.UDF、UDTF函数 5.窗口函数 6.Hive优化 7.数据倾斜 8.Hive引擎 9.元数据备份 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:3","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"集群配置 配置原则: 消耗内存的组件要分开：HDFS 的 NameNode、Yarn 的 ResourceManager 分开配置 kafka、zk、flume 传输数据比较紧密的放在一起 客户端放在一到两台服务器上，方便外部访问 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:1:4","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据采集模块 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:0","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Hadoop 优化： 数据均衡：节点与节点间，磁盘与磁盘间，都可以使用命令使数据均衡，threshold 设置差值。 LZO 压缩：hadoop 额外支持 gzip、Snappy、Lzo、Lzop(分片) 压缩方式。LZO创建索引后可以实现分片。 需要压缩的三个地方： Map Reduce 输入端 快 输出端 先考虑数据量 1.小 =\u003e 快 snappy 看需求 1.永久保存：压缩越小越好 2.数据量大 =\u003e 切片 bzip2 lzo =\u003e 需要创建索引 lzo 2.下一个mapreduce输入端 数据量小 =\u003e 快 数据量大 =\u003e 切片 压缩 HDFS参数调优： 对于大集群或者大量客户端的集群来说，通常需要调大 dfs.namenode.handler.count 的个数，默认值为10个。建议调整为 20 * logecluster size Yarn参数调优 yarn-site.xml： 情景描述：总共7台机器，每天几亿条数据，数据源 -\u003e Flume -\u003e Kafka -\u003e HDFS -\u003e Hive 问题：数据统计主要用 HiveSQL，没有数据倾斜，小文件已经做了合并处理，开启了 JVM 重用，而且IO 没有阻塞，内存用了不到 50%，但还是跑的非常慢，而且数据量洪峰时，整个集群都会宕掉，基于这种情况的优化方案是什么。 解决办法：典型的内存利用率不够，一般是 Yarn 的2个配置造成的，单个任务可以申请的最大内存大小，和 Hadoop 单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。 （NodeManager 默认内存大小 8g，单个任务默认内存大小 8g，maptask默认内存大小 1g，reducetask默认内存大小 1g。）NodeManager内存改为节点 80% 左右的内存大小。单个任务内存大小根据每 128M 数据分配1g内存。maptask 和 reducetask 内存若不支持压缩也需要调大。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:1","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Zookeeper ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:2","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Kafka kafka 机器数量计算： 机器数量（经验公式）= 2 *（峰值生产速度 * 副本数 / 100) + 1 先通过压测拿到峰值生产速度，比如峰值生产速度是 50M/s。副本数默认是1个，生产环境可以设置为2。Kafka 机器数量 = 2 * （50 * 2 / 100）+ 1 = 3 台。 kafka producer 生产者压力测试： bin/kafka-producer-perf-test.sh --topic test --record-size 100 --num-records 100000 --throughput -1 --producer-props bootstrap.servers=hadoop102:9092,hadoop103:9092 参数说明： –record-size：是一条信息有多大，单位是字节。 –num-records：是总共发送多少条信息。 –throughput：是每秒多少条信息，设成-1表示不限流，可测出生产者最大吞吐量。 kafka consumer 消费者压力测试： 如果 IO、CPU、内存、网络 这四个指标都不能改变，考虑增加分区数来提升性能。 参数说明： –broker-list：指定 kafka 集群地址 –topic：指定 topic 的名称 –fetch-size：指定每次 fetch 的数据大小 –messages：总共要消费的消息个数 kafka 分区数计算： 创建一个只有一个分区的topic 测试这个 topic 的 producer 吞吐量和 consumer 吞吐量。 假设他们的值是 Tp 和 Tc，单位是 MB/s。 然后假设总的目标吞吐量是 Tt，那么分区数 = Tt/min( Tp，Tc) 分区数一般设置为：3 - 10个 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:3","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Flume Flume 组件选型 Source： Taildir Source：支持断点续传、多目录。但可能有数据重复（重复数据在下一级解决：在 hive 的 dwd 层、spark streaming 使用 group by、开窗取窗口第一条数据） Channel： kafka channel：数据存放在 kafka 里面。kafka 数据存在磁盘里。 基于磁盘，可靠性高 kafka channel \u003e memory channel + kafka sink 如果下一级是 kafka，优先选择 kafka channel 如果下一级不是 kafka，对可靠性要求比较高，金融领域，优先选择 file channel 如果下一级不是 kafka，对可靠性要求不高，选择 memory channel，追求效率 kafka channel 1.6版本产生，但有 bug 产生的数据是 header + 内容，1.7 版本解决 Flume 拦截器 Flume 消费 Kafka 里面的数据时，可能已经是第二天，数据有可能发往第二天的 HDFS 路径。 解决思路：拦截 json 日志，通过 fastjson 解析 json，获取实际时间 ts。将获取的 ts 时间写入拦截器 header 头，header 的 key 必须是 timestamp，因为 Flume 框架会根据这个 key 的值识别为时间，写入到 HDFS。 创建 Maven 工程 flume-interceptor 创建包名：com.whatever.flume.interceptor 在 pom.xml 文件中添加配置 在 com.whatever.flume.interceptor 包下创建 JSONUtils 类 在 com.whatever.flume.interceptor 包下创建 ETLInterceptor 类，重写方法，判断是否符合 JSON 格式 打包，分发到 Flume 节点 flume_scs Flume优化： 内存参数设置优化： JVM heap 一般设置为 4G 或更高 -Xmx（启动内存） 与 -Xms（运行内存） 最好设置一致，减少内存抖动带来的性能影响，如果不一致容易倒置 fullgc ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:4","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"DataX 同步策略： 同步策略 优点 缺点 全量同步 逻辑简单 效率低 增量同步 效率高，无需同步和存储重复数据 逻辑复杂 DadaX 相比 Sqoop 功能更强大： 功能 DataX Sqoop 运行模式 单进程多线程 MR 分布式 不支持 可以通过调度系统规避 支持 流控 有 需定制 统计信息 有 分布式数据收集不方便 数据校验 有 需定制 监控 需定制 需定制 DataX 调度决策思路： 举例，用户提交了一个 DataX 作业，并且配置了总的并发数为 20，目的是对一个有 100 张分表的 MySQL 进行同步。调度决策思路是： dataX job 根据分库分表切分策略，将同步工作分成 100 个 Task。 根据配置的总的并发度 20，以及每个 TaskGroup 的并发度 5，dataX共需要 4 个 TaskGroup。 4 个 TaskGroup 分 100 个 Task，每个 TaskGroup 负责 25个 Task。 同步 HDFS 数据到 MySQL 案例： 选用 HDFSReader 和 MySQLWriter，分别设置 json 配置文件。 DataX优化： 提升 DataX job 内 channel 并发数，内存的占用显著增加，每个 channel 会有一个 Buffer，作为临时数据交换的缓冲区，而在 reader 和 writer 中，也会有 buffer，为了防止 OOM，需要调大 JVM 的堆内存。 调整 JVM xms xmx 参数的两种方式：一种是直接更改 datax.py 脚本；另一种是在启动的时候，加上参数： python datax/bin/datax.py --jvm=\"-Xms8G -Xmx8G\" /path/to/your/job.json ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:5","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Maxwell MySQL 变更数据抓取软件，实时监控 MySQL 的 insert、update、delete。并将变更数据发送给 Kafka 等流数据处理平台。 原理： 读取 MySQL 的二进制日志（Binlog），从中获取变更数据。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:2:6","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据采集模块总结 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:0","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Linux # 命令 top ps -ef df -h #磁盘空间 netstat #查看端口号 # 4个工具 awk sed sort cut # 写过什么脚本 1. 同步分发 2. 启动停止 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:1","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Sqoop 导入数据的方式： 全量：where 1=1 增量：where 创建时间 = 当天 新增及变化：where 创建时间 = 当天 or 操作时间 = 当天 特殊：只导入一次 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:2","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Hadoop 常用端口号： 2.x：50070/8020：HDFS 8088：任务情况 19888：Job history 3.x：9870/8020：HDFS 8088：任务情况 19888：Job history 配置文件： /etc/profile 2.x：core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml slaves 3.x：core-site.xml hdfs-site.xml yarn-site.xml mapred-site.xml workers ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:3","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"HDFS HDFS 的读写流程，笔试： 写详细步骤： 客户端向NameNode发出写文件请求。 检查是否已存在文件、检查权限。若通过检查，直接先将操作写入EditLog，并返回输出流对象。 （注：WAL，write ahead log，先写Log，再写内存，因为EditLog记录的是最新的HDFS客户端执行所有的写操作。如果后续真实写操作失败了，由于在真实写操作之前，操作就被写入EditLog中了，故EditLog中仍会有记录，我们不用担心后续client读不到相应的数据块，因为在第5步中DataNode收到块后会有一返回确认信息，若没写成功，发送端没收到确认信息，会一直重试，直到成功） client端按128MB的块切分文件。 client将NameNode返回的分配的可写的DataNode列表和Data数据一同发送给最近的第一个DataNode节点，此后client端和NameNode分配的多个DataNode构成pipeline管道，client端向输出流对象中写数据。client每向第一个DataNode写入一个packet，这个packet便会直接在pipeline里传给第二个、第三个…DataNode。 （注：并不是写好一个块或一整个文件后才向后分发） 每个DataNode写完一个块后，会返回确认信息。 （注：并不是每写完一个packet后就返回确认信息，因为packet中的每个chunk都携带校验信息，没必要每写一个就汇报一下，这样效率太慢。正确的做法是写完一个block块后，对校验信息进行汇总分析，就能得出是否有块写错的情况发生） 写完数据，关闭输输出流。 发送完成信号给NameNode。 （注：发送完成信号的时机取决于集群是强一致性还是最终一致性，强一致性则需要所有DataNode写完后才向NameNode汇报。最终一致性则其中任意一个DataNode写完后就能单独向NameNode汇报，HDFS一般情况下都是强调强一致性） 读详细步骤： client访问NameNode，查询元数据信息，获得这个文件的数据块位置列表，返回输入流对象。 就近挑选一台datanode服务器，请求建立输入流 。 DataNode向输入流中中写数据，以packet为单位来校验。 关闭输入流 小文件危害： 存储时间：小文件会占用 namenode 一整个区块，一个文件块150字节。128g内存的 namenode 能存储 9亿个文件块。 计算时间：1个文件块开启1个maptask，一个maptask就是1g内存。 如何解决： 合并：CombineTextinputformat =\u003e 减少 maptask har归档 JVM重用 HDFS 有几个副本： 3 HDFS 块大小： 2.x 3.x：128M ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:4","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Mapreduce shuffle 及其优化： shuffle 定义：map 方法之后，reduce方法之前混洗的过程。 map 阶段之后： 先调用 getpartition 方法，标记数据是哪一个分区。 进入环形缓冲区（缓冲区大小100M ），到达 80% 进行溢写（排序手段：对 key 的索引按照 字典序 进行 快排） 对溢写文件进行归并，写到指定磁盘，持久化到磁盘。 reduce 阶段之前： 拉取磁盘上指定的数据，放到内存中（内存不够在磁盘中） reduce 操作 优化： getpartition 方法时：自定义分区：解决数据倾斜问题。 调整缓冲区大小和溢写阈值，缓冲区大小由100M调整到200M，溢写阈值由80%调整到90%。产生大量的溢写文件，使用combiner聚类文件（前提是不影响最终业务逻辑）。这样减少了写文件的个数，合并会快一些。 溢写文件归并到磁盘：减少磁盘IO压缩，使用快的压缩算法：snappy/lzo Map Reduce 输入端 快 输出端 先考虑数据量 1.小 =\u003e 快 snappy 看需求 1.永久保存：压缩越小越好 2.数据量大 =\u003e 切片 bzip2 lzo =\u003e 需要创建索引 lzo 2.下一个mapreduce输入端 数据量小 =\u003e 快 数据量大 =\u003e 切片 压缩 reduce 拉取磁盘指定的数据，默认拉取5个，可以改成10 - 20个 yarn 和 hadoop内存大小调整：单个任务可以申请的最大内存大小，和 Hadoop 单个节点可用内存大小。调节这两个参数能提高系统内存的利用率。 （NodeManager 默认内存大小 8g，单个任务默认内存大小 8g，maptask默认内存大小 1g，reducetask默认内存大小 1g。）NodeManager内存改为节点 80% 左右的内存大小。单个任务内存大小根据每 128M 数据分配1g内存。maptask 和 reducetask 内存若不支持压缩也需要调大。 进行了几次排序： map后进行一次快排，一次归并。 reduce时进行一次归并，一次分组。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:5","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Yarn yarn 的工作机制： 用户使用客户端向 RM 提交一个任务，同时指定提交到哪个队列和需要多少资源。用户可以通过每个计算引擎的对应参数设置，如果没有特别指定，则使用默认设置。 RM 在收到任务提交的请求后，先根据资源和队列是否满足要求选择一个 NM，通知它启动一个特殊的 container，称为 ApplicationMaster（AM），后续流程由它发起。 AM 向 RM 注册后根据自己任务的需要，向 RM 申请 container，包括数量、所需资源量、所在位置等因素。 如果队列有足够资源，RM 会将 container 分配给有足够剩余资源的 NM，由 AM 通知 NM 启动 container。 container 启动后执行具体的任务，处理分给自己的数据。NM 除了负责启动 container，还负责监控它的资源使用状况以及是否失败退出等工作，如果 container 实际使用的内存超过申请时指定的内存，会将其杀死，保证其他 container 能正常运行。 各个 container 向 AM 汇报自己的进度，都完成后，AM 向 RM 注销任务并退出，RM 通知 NM 杀死对应的 container，任务结束。 hadoop 调度器： 三种调度器：FIFO、容量调度器、公平调度器 Apache默认调度器：容量调度器（apache默认资源很小） CDH默认调度器：公平调度器 特点： FIFO调度器特点：支持单队列，任务先进先出，企业中几乎不用。 容量调度器：底层是FIFO调度器。支持多队列，可以借用其他队列资源。先进入的任务优先执行。 公平调度器：支持多队列(并发度比容量调度高)，可以借用其他队列资源。队列中所有任务公平享有队列资源。 容器调度器配置多队列： 按照分析引擎创建队列：mr、spark、flink、hive 按照业务：登录、注册、购物车、下单、支付 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:6","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Zookeeper 选举机制： 半数机制：3台服务器，2台投谁谁就是老大 常用命令： delete create 安装台数： 根据集群机器数量，3-11台。zk台数多了，增加可靠性，但降低选举效率。 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:7","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Flume 组成：（source、channel、sink、put事务、take事务） source：（Taildirsource） 优点：断点续传、多目录，实时监控。缺点：文件名更名(如log4j就是更名的日志框架)之后重新读取该文件造成重复。注意：让Taildirsource判断文件时只看iNode值 Apache 1.7、CDH1.6版本产生的 没有断点续传怎么办：自定义 source，读取的位置始终持久到磁盘上 挂了：不会丢数据，但有可能重复数据 如何处理重复数据：不处理或下一级处理，采用事务方式效率太低 不支持递归遍历文件夹：自定义递归 channel：（KafkaChannel） 优点：将数据写入Kafka，省了一层sink file channel 磁盘|可靠性高|性能低 memory channel 内存|可靠性低|性能高 kafka channel 存kafka里，kafka存在磁盘|可靠性高|性能优于 memory channel + kafka sink sink：（HDFS sink） 用来解决小文件问题 设置大小128M、时间（1-2小时）、event个数禁止0 KafkaChannel：在kafka中既可以生产者也可以作为消费者 用法： source - kafkachannel - sink source - kafkachannel（作为消费者） kafkachannel - sink（作为生产者） 三个器：（拦截器、选择器、监控器） 拦截器：时间戳拦截器 或者 自定义拦截器 选择器：默认是replicating，把数据发往下一级所有通道 监控器：g, 监控 put、take 事务尝试提交的次数远远大于最终提交成功的次数，说明 flume 异常。自身：增加内存，其他：增加 flume 台数 优化： file channel 能配置多目录就配置多目录（不同磁盘） 使用 HDFS sink 用来解决小文件问题 通过监控器找寻原因，增加内存和台数 挂了恢复： channel 挂了：memory channel 100个event 数据丢失。（file channel 100万个event，没有丢失风险） taildir source 挂了：不会丢数，可能重复数据 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:8","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Kafka 基本信息： 组成： producer： ACK：0 1 -1 拦截器，序列化器，分区器 发送流程 幂等性，事务 分区规则： 有指定分区则发往指定分区 ；没有指定分区，则根据key值Hash；没有指定分区也没有key的时候，轮询（粘性） broker： Topic：副本（高可靠） ISR：LEO、HW 分区（高并发，负载均衡，防止热点） consumer 分区分配规则 offset保存： 默认：__consumer_offsets主题。 其他：手动维护Offset（MySQL） 保存数据\u0026保存Offset写到一个事物 精准一次消费 先保存数据后保存Offset 重复数据+幂等性（精准一次消费） 先保存Offset后保存数据 丢失数据 zk：存放 broker（id, topic），consumer（相应的 offset），没有生产者信息 安装台数：2 * (生产者峰值生产速率(压测) * 副本 / 100) + 1, 先使用3台机器压测 副本数：通常2个副本，默认1个副本。多：可靠性高，传输性能差 数据量：100万日活 一人100条 一条1k。 1M/s 数据保存多久：3天 磁盘预留：100g(一天数据) * 2个副本 * 3天 / 0.7 监控：kafka egale：开源框架 分区：提高并发度：先设置1个，通过压测生产者峰值生产速率tp、消费者峰值消费速率tc 期望吞吐量t 100m/s 分区数 = t / min (tp, tc) 分区分配策略： range 默认 roundrobin 采用 roundrobin 所有数据采用 hash 方式大散，再采用轮询的方式执行 isr：主要解决leader挂了谁当老大，在 isr 队列的都有机会当老大。 多少个 topic：满足下一级消费者即可。 挂了: 短期：flume channel 缓冲数据 长期：日志服务器保留30天日志 丢：ack 配置参数 0：发送数据，不需要应答。可靠性最差，传输性能最好。 1：发送数据，leader 应答。可靠性一般，传输性能一般。 -1：发送数据，leader 和 follower 共同应答。可靠性最高，传输性能最差。 企业中：0几乎不用，不太重要1，重要-1 重复： 采用 事务、幂等性、ack = -1 幂等性：单分区单会话内数据不重复 事务：每条数据比较，可以比较多个分区 事务： 在下一级处理： 积压：kafka 保存数据的时间是有限的，没有及时消费完 增加分区，同时增加消费者对应的CPU核数 增加消费者 batchsize 优化： 日志保存3天 增加网络通信延迟时间 高效读写原因： 集群，分区 顺序读写600m/s(vip)，随机读写100m/s 零拷贝 传输一条2m日志文件，现象：卡住。调整两个参数大小 数据过期：删除 或者 压缩，通常删除，副本备份30天数据 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:9","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Hive 组成： hive_structure 内部表和外部表区别： 内部表删除数据：元数据、原始数据 删掉 外表删数据：元数据删掉，原始数据保留 只有自己临时使用表，创建内部表。绝大部分场景都是外部表 4个by： order by：全局排序 （只有一个reduce task，数据倾斜，资源分配不足）不使用 sort by：局部排序，只保证每个 reducer task 输出有序 distribute by：分桶，取余分组，没有排序 cluster by： distribute by + sort by 自定义函数： 自定义udf函数步骤： 1进1出，一行，定义类 继承udf，重写 evaluate 方法 自定义udtf函数步骤：多进多出，继承 udtf，重写三个方法（初始化（定义名称、校验返回值类型）、close、process）。打包+上传hdfs+在hive客户端创建 窗口函数：rank、over、topn 优化： 行列过滤：join -\u003e where =\u003e where -\u003e join 分区 小文件：combineHiveInputformat 或者 JVM重用 或者 merge 压缩 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:3:10","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据仓库 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:0","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据分层 ODS（Operation Data Store）原始数据层：存放原始数据，起到备份的作用 DWD（Data Warehouse Detail）明细数据层：清洗，去空，去脏数据，维度退化 DWS（DW service）按天进行轻度汇总 DWT（DW Topic）按主题进行汇总 ADS（Application Data Store）为各种统计报表提供数据 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:1","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数仓建模 ODS层： 创建支持LZO压缩的表：减少存储空间，100g -\u003e 10g - 5g(取决于文件格式，比如JSON的嵌套方式，CSV的列数量) 创建分区表：防止全表扫描（导数据：全量、增量、新增和变化） 保持数据原貌不做任何修改，起到备份数据的作用 DWD层：🔶 选择业务过程 -\u003e 声明粒度 -\u003e 确认维度 -\u003e 确认事实 业务过程：哪些协议需要做处理 声明粒度：粒度可以是天、一人、区域 确认维度：时间维度、地区维度、用户维度 确认事实：业务中的度量值（次数、个数、件数、金额、可以进行累加） DWS层： 对各个主题对象进行统计：天、月、年、地区、用户 DWT层： 统计累计行为：从开始到结束，总结表 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:2","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"数据处理 数据处理大致分为两大类： OLTP（on-line transaction processing）联机事务处理 OLAP（on-line Analytical Processing）联机分析处理 ","date":"2021-01-08","objectID":"/20210108_dw-behavior-collection/:4:3","tags":["DataWarehouse","Tuning"],"title":"Data Warehouse: Offline Tuning","uri":"/20210108_dw-behavior-collection/"},{"categories":["Technology"],"content":"Article description.","date":"2020-12-15","objectID":"/20201215_parquet-format/","tags":["Spark","Hdfs","Hive"],"title":"HDFS: Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types. This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO. HDFS 默认存的是文本格式，所以 hive, presto 都是在文本格式上做计算，hadoop本身是全表扫，只是分布式而以，所以之前使用的就是分布式的全表扫，没有发挥出数据仓库该有的功能,列式存储，天然擅长分析，千万级别的表，count，sum，group by ，秒出结果。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:0:0","tags":["Spark","Hdfs","Hive"],"title":"HDFS: Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"1、场景描述 对客户日志做了数据仓库，但实际业务使用中有一些个共同点， A 需要关联维度表 B 最终仅取某个产品一段时间内的数据 C 只关注其中极少的字段 基于以上业务，我们决定每天定时统一关联维度表，对关联后的数据进行另外存储。各个业务直接使用关联后的数据进行离线计算。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:1:0","tags":["Spark","Hdfs","Hive"],"title":"HDFS: Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"2、选择 parquet 的外部因素 在各种列存储中，我们最终选择 parquet 的原因有许多。除了 parquet 自身的优点，还有以下因素： A、公司当时已经上线spark 集群，而spark天然支持parquet，并为其推荐的存储格式(默认存储为parquet)。 B、hive 支持 parquet 格式存储，如果以后使用hivesql 进行查询，也完全兼容。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:2:0","tags":["Spark","Hdfs","Hive"],"title":"HDFS: Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"3、选择 parquet 的内在原因 下面通过对比 parquet 和 csv 说说 parquet 自身都有哪些优势 csv在hdfs上存储的大小与实际文件大小一样。若考虑副本，则为实际文件大小*副本数目。（若没有压缩） 3.1 parquet采用不同压缩方式的压缩比 说明：原始日志大小为214G左右，120多的字段 采用csv（非压缩模式）几乎没有压缩。 采用 parquet 非压缩模式、gzip、snappy格式压缩后分别为17.4G、8.0G、11G，达到的压缩比分别是：12、27、19。 csv parquet 非压缩 parquet gzip parquet snappy 存储大小 214G 17.4G 8.0G 11G 压缩比例 12 27 19 若我们在 hdfs 上存储3份，压缩比仍达到4、9、6倍 3.2 分区过滤与列修剪 3.2.1分区过滤 parquet结合spark，可以完美的实现支持分区过滤。如，需要某个产品某段时间的数据，则hdfs只取这个文件夹。 spark sql、rdd 等的filter、where关键字均能达到分区过滤的效果。 使用spark的partitionBy 可以实现分区，若传入多个参数，则创建多级分区。第一个字段作为一级分区，第二个字段作为2级分区。。。。。 3.2.2 列修剪 列修剪：其实说简单点就是我们要取回的那些列的数据。 当取得列越少，速度越快。当取所有列的数据时，比如我们的120列数据，这时效率将极低。同时，也就失去了使用parquet的意义。 3.2.3 分区过滤与列修剪测试如下： 说明： A、task数、input值、耗时均为spark web ui上的真实数据。 B、之所以没有验证csv进行对比，是因为当200多G，每条记录为120字段时，csv读取一个字段算个count就直接lost excuter了。 C、注意：为避免自动优化，我们直接打印了每条记录每个字段的值。（以上耗时估计有多部分是耗在这里了） D、通过上图对比可以发现： 当我们取出所有记录时，三种压缩方式耗时差别不大。耗时大概7分钟。 当我们仅取出某一天时，parquet的分区过滤优势便显示出来。仅为6分之一左右。貌似当时全量为七八天左右吧。 当我们仅取某一天的一个字段时，时间将再次缩短。这时，硬盘将只扫描该列所在rowgroup的柱面。大大节省IO。如有兴趣，可以参考 深入分析Parquet列式存储格式 E、测试时请开启filterpushdown功能 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:3:0","tags":["Spark","Hdfs","Hive"],"title":"HDFS: Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"4、结论 parquet的gzip的压缩比率最高，若不考虑备份可以达到27倍。可能这也是spar parquet默认采用gzip压缩的原因吧。 分区过滤和列修剪可以帮助我们大幅节省磁盘IO。以减轻对服务器的压力。 如果你的数据字段非常多，但实际应用中，每个业务仅读取其中少量字段，parquet将是一个非常好的选择。 ","date":"2020-12-15","objectID":"/20201215_parquet-format/:4:0","tags":["Spark","Hdfs","Hive"],"title":"HDFS: Parquet Format","uri":"/20201215_parquet-format/"},{"categories":["Technology"],"content":"Article description.","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"A high performance columnar OLAP database management system for real-time analytics using SQL. ClickHouse can be customized with a new set of efficient columnar storage engines, and has realized rich functions such as data ordered storage, primary key indexing, sparse indexing, data sharding, data partitioning, TTL, and primary and backup replication. ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:0:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Feature Yandex，列式存储数据库，在线分析处理查询（OLAP），SQL查询实时生成分析数据报告。 列式存储 id1 id2 id3 name1 name2 name3 value1 value2 value3 更擅长做 count、sum 、聚合等操作，优于行式存储。 压缩效率高。由于某一列的数据类型都是相同的，针对于数据存储更容易进行数据压缩，每一列选择更优的数据压缩算法，大大提高了数据压缩的比重。 OLAP OLAP：更擅长一次写入，多次读取。更偏向于查数据 OLTP：更偏向于增删改查数据 高吞吐的写入能力 与 HBASE 的存储结构相似，ClickHouse 采用类 LSM Tree 的结构，数据写入后定期在后台 compaction 操作。通过类 LSM tree的结构，ClickHouse 在数据导入时全部是顺序 append 写，写入后数据段不可更改，在后台 compaction 时也是多个段 merge sort 后顺序写回磁盘。顺序写的特性，充分利用了磁盘的吞吐能力，磁盘的吞吐能力，即便在 HDD 上也有着不错的写入性能。 数据分区与线程级并行 分区和切片不同，分区是将数据分为多个 partition，每个 partition 再进一步划分为多个 index granularity (索引粒度)，然后通过 CPU 的多核分别处理其中的一部分来实现并行数据处理。单条 query 查询就能利用整机所有 CPU。 极致的并行处理能力，极大的降低了查询延时。 所以也不利于多条并发查询，对于高 qps 的查询业务不是 CH 的强项。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:1:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Data Type MySQL Hive ClickHouse(区分大小写) byte tinyint Int8 short smallint Int16 int bigint Int64 timestamp timestamp DataTime … … … 枚举 数据类型中 没有布尔值，可以通过枚举代替。 创建一个带有一个枚举 Enum8(’true’ = 1, ‘false’ = 2) 类型的列： CREATE TABLE t_enum ( x Enum8('true' = 1, 'false' = 2) ) ENGINE = TineLog # t_enum 表只可以放 true、false、1、2 内容 数组 Array(T)，不推荐使用多维数组，对多维数组的支持有限。例如，不能在 MergeTree 表中存储多维数组。 可以使用 array(T) 或 [] 创建数组 SELECT array(1, 2) AS arr, toTypeName(arr) 元组 Tuple(T1, T2, …)，每个元素都有单独的类型 SELECT array(1, 'a') AS arr, toTypeName(arr) ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:2:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Table Engine⭐ MySQL 默认的引擎：InnoDB 是事务型数据库的首选引擎，支持事务安全表（ACID）。 使用二十种表引擎决定了： 数据存储方式和位置，写到内存还是磁盘 支持哪些查询以及如何支持 并发访问数据 索引的使用 是否多线程请求 数据复制参数 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"1. TinyLog 最简单的表引擎，用于将数据存储在磁盘上。每列都存储在单独的压缩文件中，写入时，数据将附加到文件末尾。 磁盘 不支持索引 不支持并发写，不支持一边读一边写 create tabele t (a UInt16, b String) engine = TinyLog ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:1","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"2. Memory 内存引擎，重启数据就会消失，读写不互相阻塞，不支持索引。简单查询性能表现超过 10 G/s。测试场景或数据量又不太大的场景（上限大约 1 亿行）。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:2","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"3. Merge （不要和 MergeTree 引擎混淆）本身不存储数据，但可用于同时从任意多个其他的表中读取数据，读是自动并行的，不支持写入。读数据时，那些被真正读取到数据的表的索引会被使用。 create table t(id UInt16, name String) engine = Merge(currentDatabase(), '^t'); ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:3","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"4. MergeTree (重点) clickhouse 中最强大的表引擎，当巨量数据要插入到表中，需要高效地一批批写入数据片段，并希望这些数据片段在后台按照一定规则合并。相比插入时不断修改（重写）数据进行存储，这种策略会高效很多。 数据按照主键排序 可以使用分区（如果指定了主键） 支持数据副本 支持数据采样 建表参数 ENGINE = MergeTree() -[PARTITION BY]: 分区键。 作用：与 hive 的分区相同，目的主要是降低扫描的范围，优化查询速度。 默认：为一个分区。 分区目录：MergeTree 是以列文件 + 索引文件 + 表定义文件组成的，如果设定了分区那么这些文件就会保存到不同的分区目录中。(tip: 要按月分区，可以使用表达式 toYYYYMM(data_column) ) 并行：跨分区的查询统计，CH 会以分区为单位并行处理。 写入和分区合并：任何一个批次的数据写入都会产生一个临时的分区，不会纳入任何已有的分区。十几分钟后 CH 会自动执行合并操作。或手动合并 optimize table x final; 通常按天分区比较合适。 -[PRIMARY KEY]: 主键。 与其他数据库不同，它只提供了数据的一级索引，但却不是唯一约束。可以有相同的 key 数据。 主键的设定依据查询语句，查询最频繁的字段作为 主键。根据条件通过对主键进行某种形式的二分查找，能够定位到对应的 index granularity，避免了全表扫描。 index_granularity: 稀疏索引，索引粒度，即索引中相邻【标记】间的数据行数，默认 8192 行一个索引。这就要求字段有序 ORDER BY: 表的排序键，可以是一组列的元组或任意的表达式。 设定 分区内 的数据按照哪些字段顺序进行有序保存。 ORDER BY 是 MergeTree 中唯一一个必须项，比 primary key 更重要，没有主键依照 order by 的字段进行处理（比如去重汇总） 主键必须是 order by 字段的前缀字段 ，如 order by 字段是 (id, sku_id, line)，那么主键必须是 id、 (id, sku_id) 或者 (id, sku_id, line) -[SAMPLE BY]: 用于抽样的表达式，如果要用抽样表达式，主键中必须包含这个表达式 SETTINGS: 影响 MergeTree 性能的额外参数： use_minimalistic_part_header_in_zookeeper: 数据片段头在 Zookeeper 中的存储方式 min_merge_bytes_to_use_direct_io: 使用直接 I/O （不经过缓存 I/O）来操作磁盘的合并操作时要求的最小数据量。当数据量特别大时，没必要经过缓存 I/O，默认数据小于 10G 会开启缓存 I/O 数据TTL Time To Live, MergeTree 提供了可以管理数据或列的 生命周期 的功能。给字段设置时间，到期后字段数据归 0 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:4","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"5. ReplacingMergeTree 在 MergeTree 的基础上，添加了 “处理重复数据” 的功能，该引擎和 MergeTree 的不同之处在于它会删除具有相同主键的重复项。使用 order by 使用的字段来规定去重字段。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:5","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"6. SummingMergeTree 在 MergeTree 的基础上，添加了 “合并重复数据” 的功能，会把具有相同主键的行合并为一行，该行包含了被合并的行中具有数值数据类型的列的汇总值。 create table smt_table (date Date, name String, sum Uint16, not_sum UInt16) engine = SummingMergeTree(sum) partition by date order by (date, name) ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:6","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"7. Distributed (重点) 分布式引擎，本身不存储数据，但可以在多个服务器上进行分布式查询。读是自动并行的。读取时，远程服务器表的索引会被使用。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:3:7","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"SQL Operation insert update / delete select alter ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:4:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Im/Export HDFS Clickhouse 从 18.16.0 版本开始支持从 HDFS 读取文件，在 19.1.6 版本支持读和写，在 19.4 版本开始支持 Parquet 格式。 案例一：client 通过 clickhouse 查询引擎访问 HDFS 上的文件 # 上传 csv 到 hdfs 根目录 hadoop fs -put module.csv / # 进入 clickhouse 命令 clickhouse-client -h hadoop2 -m # 建表 create table hdfs_module_csv ( id Int8, name String ) Engine = HDFS('hdfs://hadoop2:9000/module.csv','CSV'); 验证： # 删除 HDFS 上的 CSV，验证是否在 clickhouse 上占用空间 hadoop fs -rm -r /module.csv # sql SELECT * from hdfs_module_csv; # error 案例二：HDFS 插入数据到本地存储引擎，client 通过 clickhouse 查询引擎查询 clickhouse 本地数据 # 通过 sql 插入到本地 insert into student_local select * from hdfs_module_csv ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:5:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Back Up 基于 ZK，需在配置 ZK ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:6:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Shard ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:7:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"集群写入流程 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:7:1","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"集群读取流程 优先选择 errors_count 小的副本 errors_count 相同的有随机、顺序、随机、host 名称相似等四种选择方式。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:7:2","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"配置三节点集群及副本 hadoop102 hadoop103 hadoop104 shard1, shard1副本 shard1, shard1副本 (103数据和102数据相同) shard2, shard2副本 102_shard1_replica1 + 104_shard2_replica1 组成完整数据 103_shard1_replica2 + 104_shard2_replica1 也可以组成完整数据 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:7:3","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Optimize max_memory_usage 此参数在 /etc/clickhouse-server/user.xml 中，表示单词 Query 占用内存最大值，超过 Query 失败，尽量调大。 删除多个节点上的同一张表 使用 on cluster 关键字。 drop table * on cluster table_name 自动数据备份 只用 MergeTree 引擎支持副本。 设置分片和分片副本节点。 ","date":"2020-11-07","objectID":"/20201107_clickhouse-intro/:8:0","tags":["ClickHouse","SQL"],"title":"Clickhouse Introduction","uri":"/20201107_clickhouse-intro/"},{"categories":["Technology"],"content":"Article description.","date":"2020-10-26","objectID":"/20201026_hbase-optimize/","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"HBase is a high reliability, high performance, column-oriented, and scalable distributed database. However, the READ/write performance deteriorates when a large amount of concurrent data or existing data is generated. You can use the following methods to improve the HBase search speed. ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:0:0","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"HBase 数据表优化 HBase 是一个高可靠性、高性能、面向列、可伸缩的分布式数据库，但是当并发量过高或者已有数据量很大时，读写性能会下降。我们可以采用如下方式逐步提升 HBase 的检索速度。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:0","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"预先分区 默认情况下，在创建 HBase 表的时候会自动创建一个 Region 分区，当导入数据的时候，所有的 HBase 客户端都向这一个 Region 写数据，直到这个 Region 足够大了才进行切分。一种可以加快批量写入速度的方法是通过预先创建一些空的 Regions，这样当数据写入 HBase 时，会按照 Region 分区情况，在集群内做数据的负载均衡。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:1","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"Rowkey 优化 HBase 中 Rowkey 是按照字典序存储，因此，设计 Rowkey 时，要充分利用排序特点，将经常一起读取的数据存储到一块，将最近可能会被访问的数据放在一块。 此外，Rowkey 若是递增的生成，建议不要使用正序直接写入 Rowkey，而是采用 reverse 的方式反转 Rowkey，使得 Rowkey 大致均衡分布，这样设计有个好处是能将 RegionServer 的负载均衡，否则容易产生所有新数据都在一个 RegionServer 上堆积的现象，这一点还可以结合 table 的预切分一起设计。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:2","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"减少 ColumnFamily 数量 不要在一张表里定义太多的 ColumnFamily。目前 Hbase 并不能很好的处理超过 2~3 个 ColumnFamily 的表。因为某个 ColumnFamily 在 flush 的时候，它邻近的 ColumnFamily 也会因关联效应被触发 flush，最终导致系统产生更多的 I/O。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:3","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"缓存策略 (setCaching） 创建表的时候，可以通过 HColumnDescriptor.setInMemory(true) 将表放到 RegionServer 的缓存中，保证在读取的时候被 cache 命中。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:4","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"设置存储生命期 创建表的时候，可以通过 HColumnDescriptor.setTimeToLive(int timeToLive) 设置表中数据的存储生命期，过期数据将自动被删除。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:5","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"硬盘配置 每台 RegionServer 管理 10~1000 个 Regions，每个 Region 在 1~2G，则每台 Server 最少要 10G，最大要 1000*2G=2TB，考虑 3 备份，则要 6TB。方案一是用 3 块 2TB 硬盘，二是用 12 块 500G 硬盘，带宽足够时，后者能提供更大的吞吐率，更细粒度的冗余备份，更快速的单盘故障恢复。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:6","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"分配合适的内存给 RegionServer 服务 在不影响其他服务的情况下，越大越好。例如在 HBase 的 conf 目录下的 hbase-env.sh 的最后添加 export HBASE_REGIONSERVER_OPTS=”-Xmx16000m $HBASE_REGIONSERVER_OPTS” 其中 16000m 为分配给 RegionServer 的内存大小。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:7","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"写数据的备份数 备份数与读性能成正比，与写性能成反比，且备份数影响高可用性。有两种配置方式，一种是将 hdfs-site.xml 拷贝到 hbase 的 conf 目录下，然后在其中添加或修改配置项 dfs.replication 的值为要设置的备份数，这种修改对所有的 HBase 用户表都生效，另外一种方式，是改写 HBase 代码，让 HBase 支持针对列族设置备份数，在创建表时，设置列族备份数，默认为 3，此种备份数只对设置的列族生效。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:8","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"WAL（预写日志） 可设置开关，表示 HBase 在写数据前用不用先写日志，默认是打开，关掉会提高性能，但是如果系统出现故障 (负责插入的 RegionServer 挂掉)，数据可能会丢失。配置 WAL 在调用 Java API 写入时，设置 Put 实例的 WAL，调用 Put.setWriteToWAL(boolean)。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:9","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"批量写 HBase 的 Put 支持单条插入，也支持批量插入，一般来说批量写更快，节省来回的网络开销。在客户端调用 Java API 时，先将批量的 Put 放入一个 Put 列表，然后调用 HTable 的 Put(Put 列表) 函数来批量写。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:10","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"客户端一次从服务器拉取的数量 通过配置一次拉去的较大的数据量可以减少客户端获取数据的时间，但是它会占用客户端内存。有三个地方可进行配置： 1）在 HBase 的 conf 配置文件中进行配置 hbase.client.scanner.caching； 2）通过调用 HTable.setScannerCaching(int scannerCaching) 进行配置； 3）通过调用 Scan.setCaching(int caching) 进行配置。三者的优先级越来越高。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:11","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"RegionServer 的请求处理 IO 线程数 较少的 IO 线程适用于处理单次请求内存消耗较高的 Big Put 场景 (大容量单次 Put 或设置了较大 cache 的 Scan，均属于 Big Put) 或 ReigonServer 的内存比较紧张的场景。 较多的 IO 线程，适用于单次请求内存消耗低，TPS 要求 (每秒事务处理量 (TransactionPerSecond)) 非常高的场景。设置该值的时候，以监控内存为主要参考。 在 hbase-site.xml 配置文件中配置项为 hbase.regionserver.handler.count。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:12","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"Region 大小设置 配置项为 hbase.hregion.max.filesize，所属配置文件为 hbase-site.xml.，默认大小 256M。 在当前 ReigonServer 上单个 Reigon 的最大存储空间，单个 Region 超过该值时，这个 Region 会被自动 split 成更小的 Region。小 Region 对 split 和 compaction 友好，因为拆分 Region 或 compact 小 Region 里的 StoreFile 速度很快，内存占用低。缺点是 split 和 compaction 会很频繁，特别是数量较多的小 Region 不停地 split, compaction，会导致集群响应时间波动很大，Region 数量太多不仅给管理上带来麻烦，甚至会引发一些 Hbase 的 bug。一般 512M 以下的都算小 Region。大 Region 则不太适合经常 split 和 compaction，因为做一次 compact 和 split 会产生较长时间的停顿，对应用的读写性能冲击非常大。 此外，大 Region 意味着较大的 StoreFile，compaction 时对内存也是一个挑战。如果你的应用场景中，某个时间点的访问量较低，那么在此时做 compact 和 split，既能顺利完成 split 和 compaction，又能保证绝大多数时间平稳的读写性能。compaction 是无法避免的，split 可以从自动调整为手动。只要通过将这个参数值调大到某个很难达到的值，比如 100G，就可以间接禁用自动 split(RegionServer 不会对未到达 100G 的 Region 做 split)。再配合 RegionSplitter 这个工具，在需要 split 时，手动 split。手动 split 在灵活性和稳定性上比起自动 split 要高很多，而且管理成本增加不多，比较推荐 online 实时系统使用。内存方面，小 Region 在设置 memstore 的大小值上比较灵活，大 Region 则过大过小都不行，过大会导致 flush 时 app 的 IO wait 增高，过小则因 StoreFile 过多影响读性能。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:13","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"HBase 配置 建议 HBase 的服务器内存至少 32G，表 1 是通过实践检验得到的分配给各角色的内存建议值。 表 1. HBase 相关服务配置信息 模块 服务种类 内存需求 HDFS HDFS NameNode 16GB HDFS DataNode 2GB HBase HMaster 2GB HRegionServer 16GB ZooKeeper ZooKeeper 4GB HBase 的单个 Region 大小建议设置大一些，推荐 2G，RegionServer 处理少量的大 Region 比大量的小 Region 更快。对于不重要的数据，在创建表时将其放在单独的列族内，并且设置其列族备份数为 2（默认是这样既保证了双备份，又可以节约空间，提高写性能，代价是高可用性比备份数为 3 的稍差，且读性能不如默认备份数的时候。 ","date":"2020-10-26","objectID":"/20201026_hbase-optimize/:1:14","tags":["Tuning","Hbase","Hdfs"],"title":"Hbase Performance Tuning","uri":"/20201026_hbase-optimize/"},{"categories":["Technology"],"content":"Article description.","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"Rows in HBase are sorted lexicographically by row key. This design optimizes for scans, allowing you to store related rows, or rows that will be read together, near each other. However, poorly designed row keys are a common source of hotspotting. Hotspotting occurs when a large amount of client traffic is directed at one node, or only a few nodes, of a cluster. This traffic may represent reads, writes, or other operations. The traffic overwhelms the single machine responsible for hosting that region, causing performance degradation and potentially leading to region unavailability. This can also have adverse effects on other regions hosted by the same region server as that host is unable to service the requested load. It is important to design data access patterns such that the cluster is fully and evenly utilized. ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:0:0","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey的作用 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:1:0","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey在查询中的作用 HBase中RowKey可以唯一标识一行记录，在HBase中检索数据有以下三种方式： 通过 get 方式，指定 RowKey 获取唯一一条记录 通过 scan 方式，设置 startRow 和 stopRow 参数进行范围匹配 全表扫描，即直接扫描整张表中所有行记录 当大量请求访问HBase集群的一个或少数几个节点，造成少数RegionServer的读写请求过多、负载过大，而其他RegionServer负载却很小，这样就造成热点现象。大量访问会使热点Region所在的主机负载过大，引起性能下降，甚至导致Region不可用。所以我们在向HBase中插入数据的时候，应尽量均衡地把记录分散到不同的Region里去，平衡每个Region的压力。 下面根据一个例子分别介绍下根据RowKey进行查询的时候支持的情况。 如果我们RowKey设计为uid+phone+name，那么这种设计可以很好的支持一下的场景: uid=873969725 AND phone=18900000000 AND name=zhangsanuid= 873969725 AND phone=18900000000uid= 873969725 AND phone=189?uid= 873969725 难以支持的场景： phone=18900000000 AND name = zhangsanphone=18900000000 name=zhangsan 从上面的例子中可以看出，在进行查询的时候，根据RowKey从前向后匹配，所以我们在设计RowKey的时候选择好字段之后，还应该结合我们的实际的高频的查询场景来组合选择的字段，越高频的查询字段排列越靠左。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:1:1","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey在Region中的作用 在 HBase 中，Region 相当于一个数据的分片，每个 Region 都有StartRowKey和StopRowKey，这是表示 Region 存储的 RowKey 的范围，HBase 表的数据时按照 RowKey 来分散到不同的 Region，要想将数据记录均衡的分散到不同的Region中去，因此需要 RowKey 满足这种散列的特点。此外，在数据读写过程中也是与RowKey 密切相关，RowKey在读写过程中的作用： 读写数据时通过 RowKey 找到对应的 Region； MemStore 中的数据是按照 RowKey 的字典序排序； HFile 中的数据是按照 RowKey 的字典序排序。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:1:2","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey的设计 在HBase中RowKey在数据检索和数据存储方面都有重要的作用，一个好的RowKey设计会影响到数据在HBase中的分布，还会影响我们查询效率，所以一个好的RowKey的设计方案是多么重要。首先我们先来了解下RowKey的设计原则。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:0","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey设计原则 长度原则 RowKey是一个二进制码流，可以是任意字符串，最大长度为64kb，实际应用中一般为10-100byte，以byte[]形式保存，一般设计成定长。建议越短越好，不要超过16个字节，原因如下： 数据的持久化文件HFile中时按照Key-Value存储的，如果RowKey过长，例如超过100byte，那么1000w行的记录，仅RowKey就需占用近1GB的空间。这样会极大影响HFile的存储效率。 MemStore会缓存部分数据到内存中，若RowKey字段过长，内存的有效利用率就会降低，就不能缓存更多的数据，从而降低检索效率。 目前操作系统都是64位系统，内存8字节对齐，控制在16字节，8字节的整数倍利用了操作系统的最佳特性。 唯一原则 必须在设计上保证RowKey的唯一性。由于在HBase中数据存储是Key-Value形式，若向HBase中同一张表插入相同RowKey的数据，则原先存在的数据会被新的数据覆盖。 排序原则 HBase的RowKey是按照ASCII有序排序的，因此我们在设计RowKey的时候要充分利用这点。 散列原则 设计的RowKey应均匀的分布在各个HBase节点上。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:1","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey字段选择 RowKey字段的选择，遵循的最基本原则是唯一性，RowKey必须能够唯一的识别一行数据。无论应用的负载特点是什么样，RowKey字段都应该参考最高频的查询场景。数据库通常都是以如何高效的读取和消费数据为目的，而不是数据存储本身。然后，结合具体的负载特点，再对选取的RowKey字段值进行改造，组合字段场景下需要重点考虑字段的顺序。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:2","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"避免数据热点的方法 在对HBase的读写过程中，如何避免热点现象呢？主要有以下几种方法： Reversing 如果经初步设计出的RowKey在数据分布上不均匀，但RowKey尾部的数据却呈现出了良好的随机性，此时，可以考虑将RowKey的信息翻转，或者直接将尾部的bytes提前到RowKey的开头。Reversing可以有效的使RowKey随机分布，但是牺牲了RowKey的有序性。 缺点： 利于Get操作，但不利于Scan操作，因为数据在原RowKey上的自然顺序已经被打乱。 Salting Salting（加盐）的原理是在原RowKey的前面添加固定长度的随机数，也就是给RowKey分配一个随机前缀使它和之间的RowKey的开头不同。随机数能保障数据在所有Regions间的负载均衡。 缺点： 因为添加的是随机数，基于原RowKey查询时无法知道随机数是什么，那样在查询的时候就需要去各个可能的Regions中查找，Salting对于读取是利空的。并且加盐这种方式增加了读写时的吞吐量。 Hashing 基于 RowKey 的完整或部分数据进行 Hash，而后将Hashing后的值完整替换或部分替换原RowKey的前缀部分。这里说的 hash 包含 MD5、sha1、sha256 或 sha512 等算法。 缺点： 与 Reversing 类似，Hashing 也不利于 Scan，因为打乱了原RowKey的自然顺序。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:2:3","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey设计案例剖析 1. 查询某用户在某应用中的操作记录 reverse(userid) + appid + timestamp 2. 查询某用户在某应用中的操作记录（优先展现最近的数据） reverse(userid) + appid + (Long.Max_Value - timestamp) 3. 查询某用户在某段时间内所有应用的操作记录 reverse(userid) + timestamp + appid 4. 查询某用户的基本信息 reverse(userid) 5. 查询某eventid记录信息 salt + eventid + timestamp 如果 userid是按数字递增的，并且长度不一，可以先预估 userid 最大长度，然后将userid进行翻转，再在翻转之后的字符串后面补0（至最大长度）；如果长度固定，直接进行翻转即可（如手机号码）。 在第5个例子中，加盐的目的是为了增加查询的并发性，加入Slat的范围是0~n，可以将数据分为n个split同时做scan操作，有利于提高查询效率。 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:3:0","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"RowKey总结 在HBase的使用过程，设计RowKey是一个很重要的一个环节。我们在进行RowKey设计的时候可参照如下步骤： 结合业务场景特点，选择合适的字段来做为RowKey，并且按照查询频次来放置字段顺序 通过设计的RowKey能尽可能的将数据打散到整个集群中，均衡负载，避免热点问题 设计的RowKey应尽量简短 ","date":"2020-10-16","objectID":"/20201016_hbase-rowkey-design/:4:0","tags":["Hbase","Hdfs","SQL"],"title":"Hbase Rowkey Design","uri":"/20201016_hbase-rowkey-design/"},{"categories":["Technology"],"content":"Article description.","date":"2020-08-11","objectID":"/20200811_yarn-clusteryarn-client/","tags":["Spark","Yarn"],"title":"Spark On Yarn: yarn-cluster, yarn-client","uri":"/20200811_yarn-clusteryarn-client/"},{"categories":["Technology"],"content":"YARN is a generic resource-management framework for distributed workloads; in other words, a cluster-level operating system. Although part of the Hadoop ecosystem, YARN can support a lot of varied compute-frameworks (such as Tez, and Spark) in addition to MapReduce. Spark支持可插拔的集群管理模式(Standalone、Mesos以及YARN )，集群管理负责启动executor进程，编写Spark application 的人根本不需要知道Spark用的是什么集群管理。Spark支持的三种集群模式，这三种集群模式都由两个组件组成:master和slave。Master服务(YARN ResourceManager,Mesos master和Spark standalone master)决定哪些application可以运行，什么时候运行以及哪里去运行。而slave服务( YARN NodeManager, Mesos slave和Spark standalone slave)实际上运行executor进程。 当在YARN上运行Spark作业，每个Spark executor作为一个YARN容器(container)运行。Spark可以使得多个Tasks在同一个容器(container)里面运行。这是个很大的优点。 注意这里和Hadoop的MapReduce作业不一样，MapReduce作业为每个Task开启不同的JVM来运行。虽然说MapReduce可以通过参数来配置。详见mapreduce.job.jvm.numtasks 从广义上讲，yarn-cluster适用于生产环境；而yarn-client适用于交互和调试，也就是希望快速地看到application的输出。 Application Master。在YARN中，每个Application实例都有一个Application Master进程，它是Application启动的第一个容器。它负责和ResourceManager打交道，并请求资源。获取资源之后告诉NodeManager为其启动container。 从深层次的含义讲，yarn-cluster和yarn-client模式的区别其实就是Application Master进程的区别，yarn-cluster模式下，driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行。然而yarn-cluster模式不适合运行交互类型的作业。而yarn-client模式下，Application Master仅仅向YARN请求executor，client会和请求的container通信来调度他们工作，也就是说Client不能离开。（上图是yarn-cluster模式，下图是yarn-client模式）： spark-yarn-f31 spark-yarn-f22 ","date":"2020-08-11","objectID":"/20200811_yarn-clusteryarn-client/:0:0","tags":["Spark","Yarn"],"title":"Spark On Yarn: yarn-cluster, yarn-client","uri":"/20200811_yarn-clusteryarn-client/"},{"categories":["Technology"],"content":"Article description.","date":"2020-08-03","objectID":"/20200803_spark-guide3/","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:0:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Project 业务场景 统计出租车利用率(有乘客乘坐的时间和无乘客空跑的时间比例) 技术要点 数据清洗 Json解析 地理位置信息处理 探索性数据分析 会话分析 数据读取 class TaxiAnalysisRunner{ def main(args: Array[String]): Unit = { //1创建SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"taxi\") .getOrCreate() //2导入隐式转换和函数 import spark.implicits._ import org.apache.spark.sql.functions._ //3数据读取 val taxiRaw: Dataset[Row] = spark.read .option(\"header\", value = true) .csv(\"dataset/half_trip.csv\") } } 抽象数据类 //dataframe是ROW类型的dataset //读取的dataframe是row类型的，如果是dataset[trip]把类型抽象，对数据方便处理 case class Trip( license: String, pickUpTime: Long, dropOffTime: Long, pickUpX: Double, pickUpy: Double, dropOffX: Double, dropOffY: Double ) 转换DF类型、清洗异常数据 //dataframe[Row] =\u003e dataset[] val taxiParsed:RDD[Either[Trip,(Row,Exception)]] = taxiRaw.rdd.map(safe(parse)) //异常数据 val exceptionResult = taxiParsed.filter(e =\u003e e.isRight) .map(e =\u003e e.right.get._1) val taxi Good: Dataset[Trip] = taxiParsed.map(either =\u003e either.left.get).toDS() def parse(row: Row): Trip = { val richRow = new richRow(row) val license = richRow.getAs[String](\"hack_license\").orNull val pickUpTime = parseTime(richRow, \"...\") val dropOffTime = parseTime(richRow, \"...\") val pickUpX = parseLocation(richRow, \"...\") val pickUpy = parseLocation(richRow, \"...\") val dropOffX = parseLocation(richRow, \"...\") val dropOffY = parseLocation(richRow, \"...\") Trip(license, pickUpTime, dropOffTime, pickUpX, pickUpy, dropOffX, dropOffY) } class RichRow(row: Row){ def getAs[T](field: String): Option[T] = { if(row.isNullAt(row.fieldIndex(field))){ None }else{ Some(row.getAs[T](field)) } } } def parseTime(row: RichRow, field: String): Long = { //规定格式 val pattern = \"yyyy-MM-dd HH:mm:ss\" val formatter = new SimpleDateFormat(pattern, locale.ENGLISH) //执行转换 val time = row.getAs[String](field) val timeOption = time.map(time =\u003e formatter.parse(time).getTime) //Option代表某个方法，结果可能为空，使得方法调用出必须处理为null的情况 //Option对象本身提供了一些对于null的支持 timeOption.getOrElse(0L) } def parseLocation(row: RickRow, field: String): Double = { val location = row.getAs[String](fiecld) val locationOption = location.map(loc =\u003e loc.toDouble) locationOption.getOrElse(0D) } //parse异常处理 //出现异常-\u003e返回异常信息，和当前调用 def safe[P, R](f: P =\u003e R): P =\u003e Either[R,(P,Exception)]={ new Function[P, Either[R,(P,Exception)]] with Serializable { override def apply(param: P): Either[R, (P, Exception)] = { try{ Left(param) }catch{ case e: Exception =\u003e Right((param, e)) } } } } 统计分布 //编写udf，将毫秒转为小时单位 val hours = (pickUpTime: Long, dropOffTime: Long) =\u003e { val duration = dropOffTime - pickUpTime val hours = TimeUnit.HOURS.convert(duration, TimeUnit.MILLISECONDS) hours } val hoursUDF = udf(hours) //统计 taxiGood.groupBy(hoursUDF($\"pickUpTime\",$\"dropOffTime\") as \"duration\") .count() .sort(\"duration\") .show() //直方图 spark.udf.register(\"hours\", hours) val taxiClean = taxiGood.where(\"hours(pickUpTime, dropOffTime) BETWEEN 0 AND 3\") JSON地理信息 case class FeatureCollection(features: List[Feature]) case class Feature(Properties: Map[String, String], geometry: JObject) { def getGeometry(): Geometry = { import org.json4s._ import org.json4s.jackson.JsonMethods._ val mapGeo = GeometryEngine.geoJsonToGeometry(compact(render(geometry)), 0, Geometry.Type.Unknown) mapGeo.getGeometry } } object FeatureExtraction{ //JSON解析 def parseJson(json: String): FeatureCollection = { //1导入一个formats隐式转换 implicit val formats = Serialization.formats(NoTypeHints) //2JSON -\u003e Obj import org.json4s.jackson.Serialization.read val featureCollection = read[FeatureCollection](json) featureCollection } } //链接行政区信息 //1读取数据 val geoJson = Source.fromFile(\"dataset/districts.geojson\").mkString val featureColleciton = FeatureExtraction.parseJson(geoJson) //2排序 //理论上大的区域数量多，把大的区域放在前面，减少搜索次数 val sortedFeatures = featureCollection.features.sortBy(feature =\u003e { (feature.properties(\"boroughCode\"), - feature.getGeometry().calculateArea2D()) }) //3广播 val featuresBC = spark.sparkContext.broadcast(sortedFeatures) //4UDF val boroughLookUp = (x: Double, y: Double) =\u003e { //1搜索经纬度所在的区域 val feature","date":"2020-08-03","objectID":"/20200803_spark-guide3/:1:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Spark Streaming Spark Streaming 的特点 Spark Streaming 并不是实时流，而是按时间切分小批量，一个一个的小批处理 Spark Streaming 对数据是按照时间切分为一个又一个的RDD，然后针对RDD进行处理 处理架构 批处理：HDFS 流处理：Kafka 混合处理：流式计算和批处理结合 Netcat Netcat以在两台设备上面相互交互，即侦听模式/传输模式 功能：Telnet功能、获取banner信息、传输文本信息、传输文件/目录、加密传输文件，默认不加密、远程控制、加密所有流量、流媒体服务器、远程克隆硬盘 object StreamingWordCount { def main(args: Array[String]): Unit = { val sparkConf = new SparkConff().setAppName(\"stream word count\").setMaster(\"local[6]\") val ssc = new StreamingContext(sparkConf, Seconds(1))//批次时间，每1秒收集一次数据 //在创建Streaming Context的时候也要用到conf，说明Spark Streaming是基于Spark Core的 //在执行master的时候，不能指定一个线程：因为在Streaming运行的时候，需要开一个新的线程去一直监听数据的获取 //socketTextStream方法会创建一个DStream，监听Socket输入，当做文本处理 //DStream可以理解是一个流式的RDD val lines: ReceiverInputDStream[String] = ssc.socketTextStream( hostnmae = \"192.168.169.101\", port = 9999, storageLevel = StoreageLevel.MEMORY_AND_DISK_SER ) //2数据处理 // 1拆分单词 val words = lines.flatMap(_.split(\" \")) // 2转换单词 val tuples = words.map((_, 1)) // 3词频reduce val counts = tuples.reduceByKey(_ + _) ssc.start() // main方法执行完毕后整个程序就会退出，所以需要阻塞主线程 ssc.awaitTermination() } } 容错 热备 当Receiver获取数据，交给BlockManager存储 如果设置了StorageLevel.MEMORY_AND_DISK_SER，则意味着BlockManager 不仅会在本机存储，也会发往其它的主机存储，本质就是冗余备份 如果某一个计算失败了，通过冗余的备份，再次进行计算即可 冷备 WAL 预写日志 当数据出错时，根据Redo log去重新处理数据 重放 有一些上游的外部系统是支持重放的，如 Kafka Kafka 可以根据Offset来获取数据 出错时，只需通过Kafka再次读取即可 ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:2:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Structured Streaming 编程模型演进 RDD： 针对自定义的数据对象进行处理，可以处理任意类型的对象，比较符合面向对象 RDD处理数据速较慢 RDD无法感知数据的结构，无法针对数据结构进行编程 DataFrame： 保留元信息，针对数据结构进行处理，例如根据某一列进行排序或者分组 DF在执行的时候会经过catalyst进行优化，并且序列化更加高效，性能会更好 DF无法处理非结构化数据，因为DF内部使用Row对象保存数据 DF的读写框架更加强大，支持多种数据源 DataSet： DS结合了RDD和DF的特点，可以处理结构化数据，也可以处理非结构化数据 序列化 将对象的内容变成二进制或存入文件中保存 数据场景： 持久化对象数据 网络中不能传输Java对象，只能将其序列化后传输二进制数据 序列化应用场景 Task分发：Master的driver往Worker的Executor任务分发 RDD缓存：序列化后分布式存储 广播变量：序列化后分布式存储 Shuffle过程 Spark Streaming 的 Receiver：kafka传入的数据是序列化的数据 RDD的序列化 Kryo是Spark引入的一个外部的序列化工具，可以增快RDD的运行速度 因为Kryo序列化后的对象更小，序列化和反序列化速度非常快 val conf = new SparkConf() .setMaster(\"local[2]\") .setAppName(\"KyroTest\") conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") conf.registerKryoClasses(Array(classOf[Person])) val sc = new SparkContext(conf) rdd.map(arr =\u003e Person(arr(0), arr(1), arr(2))) StructuredStreaming区别 StructuredStreaming相比于SparkStreaming的进步类似于RDD到Dataset的进步 StructuredStreaming支持连续流模型，类似于Flink那样的实时流 StructuredStreaming Project 需求 对流式数据进行累加词频统计 整体结构 Socket Server 发送数据， Structured Streaming 程序接受数据 Socket Server 使用 Netcat nc 来实现 //1.创建sparkSession //2.数据读取 val source: DataFrame = spark.readStream .format(\"socket\") .option(\"host\", \"192.168.168.101\") .option(\"port\", 9999) .load() val sourceDS: Dataset[String] = source.as[String] //3.数据处理 val words = sourceDS.flatMap(_.split(\" \")) .map((_, 1)) .groupByKey(_._1) .count() //4.结果生成 words.writeStream .outputMode(OutputMode.Complete()) .format(\"console\") .start() .awaitTermination() # 开启Netcat nc -lk 9999 StreamExecution 分为三个重要的部分 Source 从外部数据源读取数据，例如kafka LogicalPlan 逻辑计划，在流上查询计划，根据源头DF处理生成逻辑计划 Sink 写入结果 StateStore Structured Streaming 虽然从API角度上模拟出来的是一个无线扩展的表，但其内部还是增量处理。 每一批次处理完成，会将结果写入状态。每一批次处理之前，拉出来最新的状态，合并到处理过程中 ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:3:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Structured Streaming HDFS 场景 Sqoop MySQL -\u003e Sqoop -\u003e HDFS[增量数据1，增量数据2，… ] -\u003e Structured Streaming -\u003e Hbase Ngix Ngix[log1, log2，… ] -\u003e Flume -\u003e HDFS[增量数据1，增量数据2，… ] -\u003e Structured Streaming -\u003e Hbase 特点 会产生大量小文件在HDFS上 Project Python程序生成数据到HDFS Structured Streaming 从HDFS中获取数据 Structured Streaming 处理数据 # Python程序生成数据到HDFS import os for index in range(100): #1.文件内容 content = \"\"\" {\"name\": \"Michael\"} {\"name\": \"Andy\", \"age\": 30} {\"name\": \"Justin\", \"age\": 19} \"\"\" #2.文件路径 file_name = \"/export/dataset/text{0}.json\".format(index) #3.打开文件，写入内容 with open(file_name, \"w\") as file: file.write(content) #4.执行HDFS命令，创建HDFS目录，上传文件到HDFS中 os.system(\"/export/servers/haddop/bin/hdfs dfs -mkdir -p /dataset/dataset/\") os.system(\"/export/servers/haddop/bin/hdfs dfs -put {0} /dataset/dataset\".format(file_name)) //Structured Streaming 从HDFS中获取数据 object HDFSSource{ def main(args: Array[String]): Unit = { System.setProperty(\"hadoop.home.dir\", \"C:\\\\winutil\") //1.创建SparkSession val spark = SparkSession.builder() .appName(\"hdfs_souce\") .master(\"local[6]\") .getOrCreate() //2.数据读取 val schema = new StructType() .add(\"name\", \"string\") .add(\"age\", \"integer\") val souce = spark.readStream .scheme(schema) .json(\"hdfs://node01:8020/dataset/dataset\") //3.输出结果 source.writeStream .outputMode(OutputMode.Append()) .format(\"console\") .start() .awaitTermination() } } ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:4:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Structured Streaming Kafka Kafka是一个 Pub/Sub 系统 Publisher / Subscriber 发布订阅系统 发布者 --\u003e kafka --\u003e 订阅者 发布订阅系统可以有多个Publisher对应一个Subscriber，例如多个系统都会产生日志，一个日志处理器可以简单的获取所有系统产生的日志 用户系统 --\u003e 订单系统 --\u003e kafka --\u003e 日志处理器 内容系统 --\u003e 发布订阅系统也可以一个Publisher对应多个Subscriber， 这样就类似于广播了，例如通过这样的方式可以非常轻易的将一个订单的请求分发给所有感兴趣的系统，减少耦合性 --\u003e 日志处理器 用户系统 --\u003e kafka --\u003e 日志处理器 --\u003e 日志处理器 大数据系统中，消息系统往往可以作为整个数据平台的入口，左边对接业务系统各个模块，右边对接数据系统各个计算工具 业务系统 数据系统 [用户系统] --\u003e --\u003e [HDFS] [订单系统] --\u003e kafka --\u003e [Structured Streaming] [服务系统] --\u003e --\u003e [MapReduce] Kafka 的特点 Kafka 非常重要的应用场景就是对接业务系统和数据系统，作为一个数据管道，其需要流通的数据量惊人，所以 Kafka 一定有： 高吞吐量 高可靠性 Topic 和 Partitions 消息和事件经常是不同类型的，例如用户注册是一种消息，订单创建也是一种消息 创建订单事件 --\u003e kafka --\u003e structured Streaming 用户注册事件 --\u003e Kafka 中使用 Topic 来组织不同类型的消息 创建订单事件 --\u003e Topic Order --\u003e structured Streaming 用户注册事件 --\u003e Topic Order Kafka 中的 Topic 要承受非常大的吞吐量，所以 Topic 应该是可以分片的，应该是分布式的 Anatomy of a Topic Partition 0 [0][1][2][3] Partition 1 [0][1] Partition 3 [0][1][2] Old --\u003e New Kafka 和 Structured Streaming 整合的结构 Structured Streaming 中使用 Source 对接外部系统，对接 Kafka 的 Source 叫做 KafkaSource KafkaSource 中会使用 KafkaSourceRDD 来映射外部 Kafka 的 Topic，两者的 Partition 一一对应 Structured Streaming 会并行的从 Kafka 中获取数据 Structured Streaming 读取 Kafka 消息的三种方式 Earlist 从每个 Kafka 分区最开始处开始获取 Assign 手动指定每个 Kafka 分区中的 Offset Latest 不再处理之前的消息，只获取流计算启动后新产生的数据 PROJECT 需求 模拟物联网系统的数据统计 使用生产者在 Kafka 的 Topic：Streaming-test 中输入 JSON 数据 使用 Structured Streaming 过滤出来家里有人的数据 创建 Topic 并输入数据到 Topic 使用命令创建 Topic bin/kafka-topics.sh --create streaming-test --replication-factor 1 --partitions 3 --zookeeper node01:2181 开启 Producer bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 -topic streaming-test 把 Json 转为单行输入 Spark 读取 kafka 的 Topic object KafkaSource{ def main(args: Array[String]): Unit = { //1.创建 SparkSession //2.读取 Kafka 数据 val source: Dadaset[String] = spark.readSteam .format(\"kafka\") .option(\"kafka.bootstrap.servers\", \"node01:9092,node02:9092,node03:9092\") .option(\"subscribe\", \"streaming_test_1\") .option(\"startingOffsets\", \"earliest\") .load() .selectExpr(\"CAST(value AS STRING) as value\") .as[String] //3.处理数据，Dataset(String) -\u003e Dataset(id, name, category) //1::Toy Story (1995)::Animation|Children's|Comedy source.map(item =\u003e { val arr = item.split(\"::\") (arr(0).toInt, arr(1).toString, arr(2).toString) }).as[(Int, String, String)].toDF(\"id\", \"name\", \"category\") //4.Sink to HDFS result.writeStream .format(\"parquet\") .option(\"path\", \"/dataset/streaming/movies/\") .option(\"checkpointLocation\", \"checkpoint\") .start() .awaitTermination() //4.Sink to Kafka result.writeStream .format(\"kafka\") .outputMode(OutputMode.Append()) .option(\"checkpointLocation\", \"checkpoint\") .option(\"kafka.bootstrap.servers\", \"node01:9092, node2:9092\") .option(\"topic\", \"streaming_test_3\") .start() .awaitTermination() //4.Sink to Mysql //使用foreachWriter } } Sink Trigger 微批次 默认一秒间隔 连续流 Trigger.Continuous(“1 second”)，只支持Map类的类型操作，不支持聚合，Source和Sink只支持Kafka ","date":"2020-08-03","objectID":"/20200803_spark-guide3/:5:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅲ","uri":"/20200803_spark-guide3/"},{"categories":["Technology"],"content":"Article description.","date":"2020-07-07","objectID":"/20200707_spark-guide2/","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:0:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Advanced Operation closure def test(): Unit = { val f = closure() f(5) } def closure(): Int =\u003e Double = { val factor = 3.14 val areaFunction = (r: int) =\u003e { math.pow(r,2) * factor } areaFunction } f就是闭包，闭包的本质就是一个函数 在scala中函数是一个特殊的类型，FunctionX 闭包也是一个FunctionX类型的对象 闭包是一个对象 class MyClass{ val field = \"Hello\" def doStuff(rdd: RDD[String]): RDD [String] = { rdd.map(x =\u003e field + x) //引用Myclass对象中的一个成员变量，说明其可以访问MyClass这个类的总用域，也是一个闭包。封闭的是MyClass这个作用域。 //在将其分发的不同的Executor中执行的时候，其依赖MyClass这个类当前的对象，因为其封闭了这个作用域。MyClass和函数都要一起被序列化。发到不同的结点中执行。 //1. 如果MyClass不能被序列化，将会报错 //2. 如果在这个闭包中，依赖了一个外部很大的集合，那么这个集合会随着每一个Task分发 } } Global accumulator 在任意地方创建long accumulator 累加 结果 val counter = sc.longAccumulator(\"counter\") val result = sc.parallelize(Seq(1,2,3,4,5)).foreach(counter.add(_)) counter.value Broadcast 广播变量允许将一个Read-Only的变量缓存到集群中的每个节点上，而不是传递给每一个Task一个副本 集群中的每个节点指的是一个机器 每一个Task，一个Task是一个Stage中的最小处理单元，一个Executor中可以有多个Stage，每个Stage有多个Task 所以在需要多个Stage的多个Task中使用相同数据的情况下，广播特别有用 val v = Map(\"Spark\" -\u003e \"http[123]\", \"scala\" -\u003e \"http[456]\") val config = new SparkConf().setMaster(\"local[6]\").setAppName(\"bc\") val sc = new SparkContext(config) //创建广播 val bc = sc.broadcast(v) val r = sc.parallelize(Seq(\"Spark\", \"Scala\")) //使用广播变量代替直接引用集合，只会复制和executor一样的数量 //在使用广播之前，复制map了task数量份 //在使用广播之后，复制次数和executor数量一致 val result = r.map(item =\u003e bc.value(item)).collect() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:1:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"SparkSQL Spark的RDD主要用于处理非结构化数据和半结构化数据 SparkSQL主要用于处理结构化数据 SparkSQL支持：命令式、SQL 优势： 虽然SparkSQL是基于RDD的，但是SparkSQL的速度比RDD要快很多 SparkSQL提供了更好的外部数据源读写支持 SparkSQL提供了直接访问列的能力 case class Person(name: String, age: Int) val spark: SparkSession = new sql.SparkSession.Builder() .appName(\"hello\") .master(\"local[6]\") .getOrCreate() impart spark.implicits._ val personRDD: RDD[people] = spark.sparkContext.parallelize(Seq(Person(\"zs\", 10),Person(\"ls\", 15))) val personDS: Dataset[Person] = PersonRDD.toDS() val teenagers: Dataset[String] = PersonDS.where('age \u003e 10) .where('age \u003c 20) .select('name) .as[String] RDD和SparkSQL运行时的区别 RDD的运行流程： RDD-\u003eDAGScheduler-\u003eTaskScheduleri-\u003eWorker 先将RDD解析为由Stage组成的DAG，后将Stage转为Task直接运行 SparkSQL的运行流程： 解析SQL，并且生成AST（抽象语法树） 在AST中加入元数据信息，做这一步主要是为了一些优化，例如 col = col 这样的条件 对已经加入元数据的AST，输入优化器，进行优化（例如：谓词下推，列值裁剪） 生成的AST其实最终还没办法直接运行，这个AST是逻辑计划，结束后，需要生成物理计划，从而生成RDD来运行。 Dataset \u0026 DataFrame RDD 优点： JVM对象组成的分布式数据集合 不可变并且有容错能力 可处理机构化和非结构化的数据 支持函数式转换 RDD缺点： 没有Schema 用户自己优化程序 从不同的数据源读取数据非常困难 合并多个数据源中的数据也非常困难 DataFrame: DataFrame类似一张关系型数据的表 在DataFrame上的操作，非常类似SQL语句 DataFrame中有行和列，Schema DataFrame的优点： Row对象组成的分布式数据集 不可变并且有容错能力 处理结构化数据 自带优化器Catalyset,可自动优化程序 Data source API DataFrame让Spark对结构化数据有了处理能力 DataFrame的缺点： 编译时不能类型转化安全检查，运行时才能确定是否有问题 对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象 Dataset的优点： DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据 和RDD一样，支持自定义对象存储 和DataFrame一样，支持结构化数据的sql查询 采用堆外内存存储，gc友好 类型转化安全，代码友好 def dataset1(): Unit = { //1.创建SparkSession val spark = new sql.SparkSession.Builder() .master(\"local[6]\") .appName(\"dateset1\") .getOrCreate() //2.导入隐式转化 import spark.implicits._ //3.demo val sourceRDD = spark.sparkContext.parallelize(Seq(Person(\"zs\", 10),Person(\"ls\", 15))) val dataset = sourceRDD.toDS() //Dataset支持强类型API dataset.filter(item =\u003e item.age \u003e 10).show() //Dataset支持弱类型API dataset.filter( 'age \u003e 10 ).show() dataset.filter( $\"age\" \u003e 10 ).show() //Dataset可以直接编写SQL表达式 dataset.filter( \"age \u003e 10\").show() DataFrame Practice: def dataframe1(): Unit = { //1. 创建SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"pm analysis\") .getOrCreate() //2.读取数据集 val souceDF = spark.read .option(\"header\", value = true) .csv(\"dataset/beijingPM.csv\") //3.处理数据集 sourceDF.select('year, 'month, 'PM_Dongsi) .where('PM_Dongsi =!= \"NA\") .groupBy('year, 'month) .count() .show() spark.stop() } DataFrame \u0026 Dataset 区别： DataFrame是Dataset的一种特殊情况，DataFrame是Dataset[Row]的别名 DataFrame表达的含义是一个支持函数式操作的表，而Dataset表达是一个类似RDD的东西，Dataset可以处理任何对象 DataFrame中存放的是Row对象，而Dataset中可以存放任何类型的对象 DataFrame是弱类型，Dataset是强类型。DataFrame的操作方式和Dataset是一样的，但是对于强类型的操作而言，他们处理的类型是不同的 DataFrame在进行强类型操作的时候，例如map算子，所处理的数据类型永远是Row 而Dataset，其中是什么类型，他就处理什么类型。 val df: DataFrame = personList.toDF() df.map( (row: Row) =\u003e Row(row.get(0), row,getAs[Int](1) * 2))(RowEncoder.apply(df.schema)) val ds: Dataset[person] = personList.toDS() ds.map((person: Person =\u003e Person(person.name, person.age * 2))) DataFrame只能做到运行时类型检查，Dataset能做到编译和运行都有类型检查 DataFrame弱类型是编译时不安全(df.groupBy(“name, school”)) Dataset所代表的操作，是类型安全的，编译时安全的(ds.filter(person =\u003e person.name)) Row DataFrame就是Row集合加上Schema信息 case class Person(name: String, age: Int) def row(): Unit = { //1.Row如何创建，是什么 //row对象必须配合Schema对象才会有列名 val person = Person(\"zs\", 15) val row = Row(\"zs\", 15) //2.如何从Row中获取数据 row.getString(0) row.getInt(1) //3.Row也是样例类 row match{ case Row(name, age) =\u003e println(name, age) } } Reader def reader1(): Unit = { //1.create SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"reader1\") .getOrCreate() //2.firstWay spark.read .format(\"csv\") .option(\"header\", value = true) .option(\"inferSchema\", value = true) .load(\"dataset/bjPM.csv\") .show(10) //3.sencendWay spark.read .option(\"header\", value = true) .option(\"inferSchema\", value = true) .csv(\"dataset/bjPM.csv\") .show(10) } Writer def writer1(): Unit = { System.setProperty(\"hodoop.home.dir\",\"c:\\\\winutils\") //1.create SparkSession val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"reader1\") .getOrCreate() //2.read data","date":"2020-07-07","objectID":"/20200707_spark-guide2/:2:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Data Type Transformation flatMap,map,mapPartitions,transform,as: class TypedTransformation{ //1.创建sparksession val spark = SparkSession.builder().master(\"local[6]\").appName(\"typed\").getOrCreate() import spark.implicits._ @Test def trans():Unit = { //flatmap val ds = Seq(\"hello spark\", \"hello hadoop\").toDS ds.flatMap(item =\u003e item.split(\" \")).show() //map val ds2 = Seq(Persion(\"zs\",15),Persion(\"lisi\",20)).toDS() ds2.map(person =\u003e Person(person.name, person.age*2)).show() //mappartitions ds2.mapPartitions{ //iter 不能大到每个Executor的内存放不下，不然就会OOM //对每个元素进行转换，后生成一个新的集合 iter =\u003e{ val result = iter.map(person =\u003e Person(person.name, person.age * 2)) result } } } } def trans1(): Unit = { val ds = spark.rage(10) //0-10 ds.transform(dataset =\u003e dataset.withColumn(\"doubled\", 'id * 2')) .show() } DF转成DS rdd.toDF -\u003e DataFrame //toDF把rdd转成DF dataFrame -\u003e Dataset //DataFrame就是Dataset[Row] case class Student(name:String, age:Int, gpa:Float) //读取 val schema = StructType( Seq( StructField(\"name\",StringType), StructField(\"age\",IntegerType), StructField(\"gpa\",FloatType) ) ) val df = spark.read .schema(schema) .option(\"delimiter\",\"\\t\") .csv(\"dataset/studenttab10k\") //转换 //本质上dataset[Row].as[Student] =\u003e Dataset[Student] val ds: Dataset[Student] = df.as[Student] //输出 ds.show() Filter def filter(): Unit = { val ds = Seq(Person(\"zs\",15),Person(\"ls\",20)).toDS() ds.filter(person =\u003e person.age \u003e 15).show() } Group groupByKey: val ds = Seq(Person(\"zs\",15),Person(\"ls\",20)).toDS() val grouped: KeyValueGroupedDataset[String, Person] = ds.groupByKey(person =\u003e person.name) val result: Dataset[(String, Long)] = grouped.count() result.show() Split val ds = spark.range(15) //randomSplit, the number of part, weight val datasets: Array[Dataset[lang.Long]] =ds.randomSplit(Array(5,2,3)) datasets.foeach(_.show()) //split ds.sample(withReplacement = false, fraction = 0.4).show() Sort val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() ds.orderBy('name.desc).show() ds.sort('name.asc).show() Distinct distinct,dropDuplicates: def dropDuplicates(): Unit = { val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() //重复列完全匹配 ds.distinct().show() //指定列去重 ds.dropDuplicates(\"age\").show() } Collection 差集、交集、并集、limit def collection(): Unit ={ val ds1 = spark.range(1,10) val ds2 = spark.range(5,15) ds1.except(ds2) ds1.intersect(ds2) ds1.union(ds2) ds1.limit(3) } ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:3:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Data Typeless Transformation select val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() ds.sort() .... .secect('name).show() ds.selectExpr(\"sum(age)\").show() import org.apache.spark.sql.funcitons._ ds.select(exper(\"sum(age)\")).show() Column val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() import org.apache.spark.sql.funcitons._ //如果想使用函数的功能 //1.使用functions.xx //2.使用表达式，可以使用expr(\"...\")随时编写表达式 ds.withColumn(\"random\",expr(\"rand()\")).show() ds.withColumn(\"name_new\",'name + ...).show() ds.withColumn(\"name_jok\",'name === \"\").show() ds.withColumnRenamed(\"name\",\"new_name\").show() Drop val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() ds.drop('age).show() GroupBy val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() //为什么groupByKey是有类型的，最主要的原因是因为groupByKey所生成的对象中的算子是有类型的 ds.groupByKey(item =\u003e item.name).mapValues() //为什么groupBy是无类型的，因为groupBy所生成的对象中的算子是无类型的，针对列进行处理 ds.groupBy('name).agg(mean(\"age\")).show() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:4:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Column Creation class Column{ val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"column\") .getOrCreate() def creation():Unit = { val ds = Seq(Person(\"zs\",15),Person(\"ls\",20),Person(\"zs\",8)).toDS() val df = Seq((\"zs\",15),(\"ls\",20),(\"zs\",8)).toDF(\"name\",\"age\") //1. ' 必须导入spark的隐式转化才能使用str.intern() val column: Symbol = 'name //2. $ 必须导入spark的隐式转化才能使用 val column1: ColumnName = $\"name\" //3. col 必须导入functions import org.apache.spark.sql.functions._ val column2:sql.Column = col(\"name\") //4. column 必须导入functions val column3:sql.Column = column(\"name\") //Dataset可以，DataFrame可以使用column对象 ds.select(column).show() df.select(column).show() //column有四种创建方式 //column对象可以用作于Dataset和DataFrame中 //column可以和命令式的弱类型的API配合使用:select where //5. dataset.col //使用dataset来获取column对象，会和某个dataset进行绑定，在逻辑计划中，就会有不同的表现 val column4 = ds.col(\"name\") val column5 = ds1.col(\"name\") ds.select(column5).show() //为什么要和dataset来绑定呢？ ds.join(ds1, ds.col(\"name\") === ds1.col(\"name\")) //6. dataset.apply val column6 = ds.apply(\"name\") val column7 = ds(\"name\") } } Type ds.select('name as \"new_name\").show() ds.select('age.as[Long]).show() API //添加新列 df.withColun(\"age\", 'age * 2).show() //模糊查询 ds.where('name like \"zhang%\").show() //排序 ds.sort('age asc).show() //枚举判断 ds.where('name isin (\"zs\",\"wu\",\"ls\")).show() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:5:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"N/A 缺失值的处理： 丢弃缺失值的行 替换初始值 DataFrameNaFunctions 创建 val naf: DataFrameNaFunctions = df.na 功能 naf.drop… naf.fill … df.na.drop.show() df.na.fill.show() class NullProcessor { @Test def nullAndNaN(): Unit = { //ss val spark = SparkSession.builder() .master(\"local[6]\") .appName(\"null processor\") .getOrCreate() //导入 //读取 // 1.通过spark-csv自动的推断类型来读取，推断数字的时候会将NaN推断为字符串 spark.read .option(\"header\", true) .option(\"inferSchema\",true) .csv(dataset/ds) // 2.直接读取字符串，在后续的操作中使用map算子转换类型 spark.read.csv().map(row =\u003e row...) // 3.指定Schema,不要自动推断 val schema = structType( list( StructField(\"id\",LongType), StructField(\"year\",IntegerType), StructField(\"day\",IntegerType), StructField(\"season\",IntegerType), StructField(\"pm\",DoubleType) ) ) val sourceDF = spark.read .option(\"header\", value = true) .schema(schema) .csv(\"dataset/data.csv\") .show() //丢弃 // 规则： // 1.any：只要有一个NaN就丢弃 sourceDF.na.drop(\"any\").show() sourceDF.na.drop().show() // 2.all: 所有数据NaN才丢弃 sourceDF.na.drop(\"all\").show() // 3.某些列 sourceDF.na.drop(\"any\",List(\"year\",\"month\",\"day\")).show() //填充 // 规则： // 1.针对所有列默认值填充 sourceDF.na.fill(0).show() // 2.针对特定列填充 sourceDF.na.fill(0,List(\"year\", \"month\")).show() } } SparkSQL处理异常字符串: def strProcessor(): Unit = { //1.丢弃 import spark.implicits._ sourceDF.where('PM_dongsi =!= \"NA\").show() //2.替换 import org.apache.spark.sql.functions._ sourceDF.select( 'No as \"id\", 'year, 'month, 'day, when('PM_Dongsi === \"NA\", Double.NaN) .otherwise('PM_Dongsi cast DoubleType) .as(\"pm\") ).show() sourceDF.na.replace(\"PM_Dongsi\", Map(\"NA\" -\u003e \"NaN\", \"NULL\" -\u003e \"null\")).show() } ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:6:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"groupBy groupBy //分组 val groupedDF = cleanDF.groupBy($\"year\",$\"month\") //使用functions函数来完成聚合 import org.apache.spark.sql.functions._ groupedDF.agg(avg($\"pm\") as \"pm_avg\") .orderBy($\"pm_avg\".desc) //分组第二种方式 groupedDF.avg(\"pm\") .select($\"avg(pm)\" as \"pm_avg\") .orderBy(\"pm_avg\") 多维聚合 //requirement 1:不同年，不同来源PM值的平均数 val postAndYearDF = pmFinal.groupBy('source,'year) .agg(avg($pm) as \"pm\") //requirement 2:按照不同的来源统计PM值的平均数 val postDF = pmFinal.groupBy($source) .agg(avg($pm) as \"pm\") .select($source, lit(null) as \"year\", $pm) //合并在同一个结果集中 postAndYearDF.union(postDF) .sort($source, $year asc_nulls_last, $pm) rollup 滚动分组：rollup(A, B)，生成三列：AB分组，A null分组，null(全局)的分组 //requirement 1: 每个城市，每年的销售额 //requirement 2: 每个城市，一共的销售额 //requirement 3: 总体销售额 val sales = Seq( (\"Bj\", 2016, 100), (\"Bj\", 2017, 200), (\"shanghai\", 2015, 50), (\"shanghai\", 2016, 150), (\"Guangzhou\", 2017, 50), ).toDF(\"city\", \"year\", \"amount\") sales.rollup($city, $year) .agg(sum($amount) as \"amount\") .sort($city asc asc_nulls_last, $year.asc_nulls_last) cube rollup对参数顺序有要求，cube是对rollup的弥补 rollup(A, B)，生成四列：AB分组，A null分组，null B分组，null(全局)的分组 import org.apache.spark.sql.functions._ pmFinal.cube($source, $year) .agg(avg($pm) as \"pm\") .sort($source.asc_nulls_last, $year.asc_nulls_last) RelationalGroupedDataset groupBy, rollup, cube后的数据类型都是RelationalGroupedDataset RelationalGroupedDataset并不是DataFrame，所以其中并没有DataFrame的方法，只有如下一些聚合相关的方法，下列方法调用后会生成DataFrame对象，然后就可以再次使用DataFrame的算子进行操作 操作符 解释 avg average count count max max min min mean average sum sum agg 聚合，可以使用sql.funcitons中的函数来配合进行操作 pmDf.groupBy($year).agg(avg($pm) as \"pm_avg\") ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:7:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Table Join Join class JoinProcessor{ //create Spark //import implicits._ @Test def introJoin(): Unit = { val person = Seq((0, \"Lu\", 0), (1, \"Li\", 0), (2,\"Tim\", 0)) .toDF(\"id\", \"name\", \"cityID\") val cities = Seq((0, \"BJ\"), (1, \"SH\"), (2,\"GZ\")) .toDF(\"id\", \"name\") val df = person.join(cities, person.col(\"cityID\") === cities.col(\"id\")) .select(person.col(\"id\"),person.col(\"name\"),cities.col(\"name\")) df.createOrReplaceTempView(\"user_city\") } } cross 交叉连接，笛卡尔积 def crossJoin(): Unit = { person.crossJoin(cities) .where(person.col(\"cityId\") === cities.col(\"id\")) spark.sql(\"select u.id, u.name, from person u cross join cities c\" + \"where u.cityId = c.id\") } inner 交集 select * from person inner join cities on person.cityId = cities.id person.join(right = cities, joinExprs = person(\"cityId\") === citeis(\"id\"), joinType = \"inner\") outer 全外连接 内连接的结果只有连接上的数据，而全外连接可以包含没有连接上的数据。 leftouter 左外连接 全外连接含没有连接上的数据，左外连接只包含左边没有连接上的数据。 semi\u0026anti Semi-join 通常出现在使用了exists或in的sql中，所谓semi-join即在两表关联时，当第二个表中存在一个或多个匹配记录时，返回第一个表的记录； 与普通join的区别在于semi-join时，第一个表里的记录最多只返回一次； Anti-join 而anti-join则与semi-join相反，即当在第二张表没有发现匹配记录时，才会返回第一张表里的记录； 当使用not exists/not in的时候会用到，两者在处理null值的时候会有所区别 使用not in且相应列有not null约束 not exists，不保证每次都用到anti-join代 ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:8:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"UDF 自定义列操作函数 ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:9:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Over Rank //1定义窗口 val window = Window.partitionBy($category) .orderBy($revenue.desc) //2处理数据 import org.apache.spark.sql.functions._ source.select($production, $category, dense_rank() over window as \"rank\") .where($rank \u003c= 2) .show() ","date":"2020-07-07","objectID":"/20200707_spark-guide2/:10:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅱ","uri":"/20200707_spark-guide2/"},{"categories":["Technology"],"content":"Article description.","date":"2020-06-15","objectID":"/20200615_hive-key-point/","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive is a Hadoop-based data warehouse tool that maps structured data files into a database table and provides complete SQL query functionality that converts SQL statements into MapReduce tasks for execution. It is very suitable for statistical analysis of data warehouse. ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:0:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive的两张表关联，使用MapReduce怎么实现？ 如果其中有一张表为小表，直接使用map端join的方式（map端加载小表）进行聚合。 如果两张都是大表，那么采用联合key，联合key的第一个组成部分是joinon中的公共字段，第二部分是一个flag，0代表表A，1代表表B，由此让Reduce区分客户信息和订单信息；在Mapper中同时处理两张表的信息，将joinon公共字段相同的数据划分到同一个分区中，进而传递到一个Reduce中，然后在Reduce中实现聚合。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:1:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive的特点，Hive和RDBMS有什么异同？ hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供完整的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。其优点是学习成本低，可以通过类SQL语句快速实现简单的MapReduce统计，不必开发专门的MapReduce应用，十分适合数据仓库的统计分析，但是Hive不支持实时查询。 Hive与关系型数据库的区别： hqlDifferents ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:2:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"hive中SortBy，OrderBy，ClusterBy，DistrbuteBy各代表什么意思？ Orderby：会对输入做全局排序，因此只有一个reducer（多个reducer无法保证全局有序）。只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。 Sortby：不是全局排序，其在数据进入reducer前完成排序。 Distributeby：按照指定的字段对数据进行划分输出到不同的reduce中。 Clusterby：除了具有distributeby的功能外还兼具sortby的功能。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:3:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive中split、coalesce及collect_list函数的用法（举例）？ split将字符串转化为数组，即：split(‘a,b,c,d’,’,’)==\u003e[“a”,“b”,“c”,“d”]。 coalesce(Tv1,Tv2,…)返回参数中的第一个非空值；如果所有值都为NULL，那么返回NULL。 collect_list列出该字段所有的值，不去重=\u003eselectcollect_list(id)fromtable。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:4:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive有哪些方式保存元数据，各有哪些特点？ Hive支持三种不同的元存储服务器，分别为：内嵌式元存储服务器、本地元存储服务器、远程元存储服务器，每种存储方式使用不同的配置参数。 内嵌式元存储主要用于单元测试，在该模式下每次只有一个进程可以连接到元存储，Derby是内嵌式元存储的默认数据库。 在本地模式下，每个Hive客户端都会打开到数据存储的连接并在该连接上请求SQL查询。 在远程模式下，所有的Hive客户端都将打开一个到元数据服务器的连接，该服务器依次查询元数据，元数据服务器和客户端之间使用Thrift协议通信。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:5:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive内部表和外部表的区别？ 创建表时：创建内部表时，会将数据移动到数据仓库指向的路径；若创建外部表，仅记录数据所在的路径，不对数据的位置做任何改变。 删除表时：在删除表的时候，内部表的元数据和数据会被一起删除，而外部表只删除元数据，不删除数据。这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:6:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive的函数：UDF、UDAF、UDTF的区别？ UDF：单行进入，单行输出 UDAF：多行进入，单行输出 UDTF：单行输入，多行输出 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:7:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"所有的Hive任务都会有MapReduce的执行吗？ 不是，从Hive0.10.0版本开始，对于简单的不需要聚合的类似SELECTfrom LIMITn语句，不需要起MapReducejob，直接通过Fetchtask获取数据。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:8:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive桶表的理解？ 桶表是对数据某个字段进行哈希取值，然后放到不同文件中存储。 数据加载到桶表时，会对字段取hash值，然后与桶的数量取模。把数据放到对应的文件中。物理上，每个桶就是表(或分区）目录里的一个文件，一个作业产生的桶(输出文件)和reduce任务个数相同。 桶表专门用于抽样查询，是很专业性的，不是日常用来存储数据的表，需要抽样查询时，才创建和使用桶表。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:9:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive底层与数据库交互原理？ Hive的查询功能是由HDFS和MapReduce结合起来实现的，对于大规模数据查询还是不建议在hive中，因为过大数据量会造成查询十分缓慢。Hive与MySQL的关系：只是借用MySQL来存储hive中的表的元数据信息，称为metastore（元数据信息）。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:10:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive本地模式 大多数的HadoopJob是需要Hadoop提供的完整的可扩展性来处理大数据集的。不过，有时Hive的输入数据量是非常小的。在这种情况下，为查询触发执行任务时消耗可能会比实际job的执行时间要多的多。对于大多数这种情况，Hive可以通过本地模式在单台机器上处理所有的任务。对于小数据集，执行时间可以明显被缩短。 用户可以通过设置hive.exec.mode.local.auto的值为true，来让Hive在适当的时候自动启动这个优化。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:11:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive中的压缩格式TextFile、SequenceFile、RCfile、ORCfile各有什么区别？ 1、TextFile 默认格式，存储方式为行存储，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，压缩后的文件不支持split，Hive不会对数据进行切分，从而无法对数据进行并行操作。并且在反序列化过程中，必须逐个字符判断是不是分隔符和行结束符，因此反序列化开销会比SequenceFile高几十倍。 2、SequenceFile SequenceFile是HadoopAPI提供的一种二进制文件支持，存储方式为行存储，其具有使用方便、可分割、可压缩的特点。 SequenceFile支持三种压缩选择：NONE，RECORD，BLOCK。Record压缩率低，一般建议使用BLOCK压缩。 优势是文件和hadoopapi中的MapFile是相互兼容的 3、RCFile 存储方式：数据按行分块，每块按列存储。结合了行存储和列存储的优点： 首先，RCFile保证同一行的数据位于同一节点，因此元组重构的开销很低； 其次，像列存储一样，RCFile能够利用列维度的数据压缩，并且能跳过不必要的列读取； 4、ORCFile 存储方式：数据按行分块每块按照列存储。 压缩快、快速列存取。 效率比rcfile高，是rcfile的改良版本。 小结： 相比TEXTFILE和SEQUENCEFILE，RCFILE由于列式存储方式，数据加载时性能消耗较大，但是具有较好的压缩比和查询响应。 数据仓库的特点是一次写入、多次读取，因此，整体来看，RCFILE相比其余两种格式具有较明显的优势。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:12:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Hive表关联查询，如何解决数据倾斜的问题？ 1）倾斜原因：map输出数据按keyHash的分配到reduce中，由于key分布不均匀、业务数据本身的特、建表时考虑不周、等原因造成的reduce上的数据量差异过大。 （1）key分布不均匀; （2）业务数据本身的特性; （3）建表时考虑不周; （4）某些SQL语句本身就有数据倾斜; 如何避免：对于key为空产生的数据倾斜，可以对其赋予一个随机值。 2）解决方案 （1）参数调节： hive.map.aggr=true hive.groupby.skewindata=true 有数据倾斜的时候进行负载均衡，当选项设定位true,生成的查询计划会有两个MRJob。第一个MRJob中，Map的输出结果集合会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupByKey有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupByKey分布到Reduce中（这个过程可以保证相同的GroupByKey被分布到同一个Reduce中），最后完成最终的聚合操作。 （2）SQL语句调节： ①选用joinkey分布最均匀的表作为驱动表。做好列裁剪和filter操作，以达到两表做join的时候，数据量相对变小的效果。 ②大小表Join： 使用mapjoin让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。 ③大表Join大表： 把空值的key变成一个字符串加上随机数，把倾斜的数据分到不同的reduce上，由于null值关联不上，处理后并不影响最终结果。 ④countdistinct大量相同特殊值: countdistinct时，将值为空的情况单独处理，如果是计算countdistinct，可以不用处理，直接过滤，在最后结果中加1。如果还有其他计算，需要进行groupby，可以先将值为空的记录单独处理，再和其他计算结果进行union。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:13:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Fetch抓取 Fetch抓取是指，Hive中对某些情况的查询可以不必使用MapReduce计算。例如：SELECT*FROMemployees;在这种情况下，Hive可以简单地读取employee对应的存储目录下的文件，然后输出查询结果到控制台。 在hive-default.xml.template文件中hive.fetch.task.conversion默认是more，老版本hive默认是minimal，该属性修改为more以后，在全局查找、字段查找、limit查找等都不走mapreduce。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:14:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"小表、大表Join 将key相对分散，并且数据量小的表放在join的左边，这样可以有效减少内存溢出错误发生的几率；再进一步，可以使用Group让小的维度表（1000条以下的记录条数）先进内存。在map端完成reduce。 实际测试发现：新版的hive已经对小表JOIN大表和大表JOIN小表进行了优化。小表放在左边和右边已经没有明显区别。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:15:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"大表Join大表 1）空KEY过滤 有时join超时是因为某些key对应的数据太多，而相同key对应的数据都会发送到相同的reducer上，从而导致内存不够。此时我们应该仔细分析这些异常的key，很多情况下，这些key对应的数据是异常数据，我们需要在SQL语句中进行过滤。例如key对应的字段为空。2）空key转换 有时虽然某个key为空对应的数据很多，但是相应的数据不是异常数据，必须要包含在join的结果中，此时我们可以表a中key为空的字段赋一个随机的值，使得数据随机均匀地分不到不同的reducer上。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:16:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"GroupBy 默认情况下，Map阶段同一Key数据分发给一个reduce，当一个key数据过大时就倾斜了。并不是所有的聚合操作都需要在Reduce端完成，很多聚合操作都可以先在Map端进行部分聚合，最后在Reduce端得出最终结果。 1）开启Map端聚合参数设置 （1）是否在Map端进行聚合，默认为True hive.map.aggr=true （2）在Map端进行聚合操作的条目数目 hive.groupby.mapaggr.checkinterval=100000 （3）有数据倾斜的时候进行负载均衡（默认是false） hive.groupby.skewindata=true当选项设定为true，生成的查询计划会有两个MRJob。第一个MRJob中，Map的输出结果会随机分布到Reduce中，每个Reduce做部分聚合操作，并输出结果，这样处理的结果是相同的GroupByKey有可能被分发到不同的Reduce中，从而达到负载均衡的目的；第二个MRJob再根据预处理的数据结果按照GroupByKey分布到Reduce中（这个过程可以保证相同的GroupByKey被分布到同一个Reduce中），最后完成最终的聚合操作。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:17:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Count(Distinct)去重统计 数据量小的时候无所谓，数据量大的情况下，由于COUNTDISTINCT操作需要用一个ReduceTask来完成，这一个Reduce需要处理的数据量太大，就会导致整个Job很难完成，一般COUNTDISTINCT使用先GROUPBY再COUNT的方式替换 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:18:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"笛卡尔积 尽量避免笛卡尔积，join的时候不加on条件，或者无效的on条件，Hive只能使用1个reducer来完成笛卡尔积 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:19:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"行列过滤 列处理：在SELECT中，只拿需要的列，如果有，尽量使用分区过滤，少用SELECT*。 行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在Where后面，那么就会先全表关联，之后再过滤。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:20:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"并行执行 Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。不过，如果有更多的阶段可以并行执行，那么job可能就越快完成。 通过设置参数hive.exec.parallel值为true，就可以开启并发执行。不过，在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。 ","date":"2020-06-15","objectID":"/20200615_hive-key-point/:21:0","tags":["Hdfs","Hive"],"title":"Hive Key Points","uri":"/20200615_hive-key-point/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-27","objectID":"/20200527_spark-guide1/","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API. ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:0:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Spark Introduction 1 Spark Component Spark提供了批处理（RDDs），结构化查询（DataFrame），流计算（SparkStreaming），机器学习（MLib），图计算（GraphX）等组件 这些组件均是依托于通用的计算引擎RDDs而构建出，所以spark-core的RDDs是整个Spark的基础 sparkStructure 2 Spark \u0026 Hadoop Hadoop Spark 类型 基础平台，包含计算，存储，调度 分布式计算工具（主要代替Hadoop的计算功能） 场景 大规模数据集上的批处理 迭代计算，交互式计算，流计算 延迟 大 小 易用性 API较为底层，算法适应性差 API较为顶层，方便使用 价格 性能要求低，便宜 对内存要求高，相对较贵 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:1:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Spark Cluster 1 Cluster relation clusterManager Driver：该进程调用Spark程序的main方法，并且启动SparkContext Cluster Manager：该进程负责和外部集群工具打交道，申请或释放集群资源 Worker：该进程是一个守护进程，负责启动和管理Executor Executor：该进程是一个JVM虚拟机，负责运行Spark Task 运行一个Spark程序大致经历如下几个步骤： 启动Driver，创建SparkContext Client提交程序给Drive，Drive向Cluster Manager申请集群资源 资源申请完毕，在Worker中启动Executor Driver将程序转化为Tasks，分发给Executor执行 2 Build Cluster Download Spark Upload Config HistoryServer Distribute: scp -r spark node02: $PWD Start 3 High Availability 对于 Spark Standalone 集群来说，当Worker调度出现问题时，会自动的弹性容错，将出错的Task调度到其他Worker执行。 但对于Master来说，是会出现单点失败的，为了避免可能出现的单点失败问题，Spark提供了两种方式满足高可用 使用Zookeeper实现Master的主备切换(Zookeeper是一个分布式强一致性的协调服务，Zookeeper最基本的一个保证是：如果多个节点同时创建一个ZNode)只有一个能够成功创建，这个做法的本质使用的是Zookeeper的ZAB协议，能够在分布式环境下达成一致。 使用文件系统做主备切换 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:2:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Running Process 1 Spark-Shell Run val rdd1 = sc.textFile(\"/data/wordcount.txt\") //Hadoop默认读取hdfs路径：hdfs:///data/wordcount.txt val rdd2 = rddflatMap(item =\u003e item.split(\" \")) val rdd3 = rdd2.map(item =\u003e (item,1)) val rdd4 = rdd3.reduceByKey((curr,agg) =\u003e curr + agg) rdd4.collect() sparkRunProcess 2 Local IDEA Run def main(args:Arrary[String]): Unit = { // 创建SparkContext val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"word_count\") val sc = new SparkContext(conf) //2. 加载文件 // 准备文件 // 2.读取文件 val rdd1 = sc.testFile(path = \"dataset/wordcount.txt\") //3. 处理 // 拆分为多个单词 val rdd2 = rddflatMap(item =\u003e item.split(\" \")) // 2.把每个单词指定一个词频 val rdd3 = rdd2.map(item =\u003e (item,1)) // 3.聚合 val rdd4 = rdd3.reduceByKey((curr,agg) =\u003e curr + agg) //4.得到结果 val result = rdd4.collect() result.foreach(item =\u003e println(item)) } 3 Submit Run 修改代码 去掉master设置，并修改文件路径 Maven打包上传 在集群中运行 bin/spark -submit --class cn.demo.spark.rdd.WordCount --master spark://node01:7077 ~/original -spark-0.0.jar ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:3:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"RDD 1 Cause of creation 在RDD出现之前，MapReduce是比较主流的 但多个MapReduce任务之间没有基于内存的数据共享方式，只能通过磁盘来进行共享，这种方式明显比较低效。 RDD如何解决迭代计算非常低效的问题 在Spark中，最终Job3从逻辑上的计算过程是：Job3 = (Job1.map).filter，整个过程是共享内存的，而不需要中间结果存放在可靠的分布式文件系统中。 2 Resilient Distributed Datasets 分布式 RDD支持分区，可以运行在集群中 弹性 RDD支持高效的容错 RDD中的数据即可以缓存在内存中，也可以缓存在磁盘中，也可以缓存在外部存储中 数据集 RDD可以不保存具体数据，只保留创建自己的必备信息，例如依赖和计算函数 RDD也可以缓存起来，相当于存储具体数据 3 Feature RDD是数据集 RDD不仅是数据集，也是编程模型 RDD的算子大致分为两类： Transformation转化操作，例如：map、flatMap、filter等 Action动作操作，例如：reduce、collect、show等 RDD是编程模型 RDD相互之间有依赖关系 RDD是可以分区的 RDD是只读的 RDD需要容错，可以惰性求值，可以移动计算，所以很难支持修改，显著降低问题的复杂度。 sparkRdd 4 sparkContext SparkContext是spark功能的主要入口。其代表与spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。每个JVM里只能存在一个处于激活状态的SparkContext，在创建新的SparkContext之前必须调用stop()来关闭之前的SparkContext。 每一个Spark应用都是一个SparkContext实例，可以理解为一个SparkContext就是一个spark application的生命周期，一旦SparkContext创建之后，就可以用这个SparkContext来创建RDD、累加器、广播变量，并且可以通过SparkContext访问Spark的服务，运行任务。spark context设置内部服务，并建立与spark执行环境的连接。 @Test def sparkContext(): Unit = { // Spark Context 编写 // 创建SparkConf val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"spark_context\") // 2.创建SparkContext val sc = new SparkContext(conf) //SparkContext身为大入口API，应该能够创建RDD，并且设置参数，设置Jar包 //sc... //2. 关闭SparkContext，释放集群资源 } 5 Creation Way 三种RDD的创建方式 通过本地集合创建RDD @Test def rddCreationLocal(): Unit = { val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"spark_context\") val sc = new SparkContext(conf) val rdd1 = sc.parallelize(Seq(\"Hello1\", \"Hello2\", \"Hello3\"), 2) val rdd2 = sc.makeRDD(seq, 2) // parallelize和makeRDD区别：parallelize可以不指定分区数 } 2. 通过外部数据创建RDD @Test def rddCreationFiles(): Unit = { sc.textFile(\"/.../...\") //testFile: 传入* hdfs:// file:// /.../...(这种方式分为在集群还是本地执行，在集群中读的是hdfs，本地读本地文件) //2.是否支持分区：支持，在hdfs中由hdfs文件的block决定 //3.支持什么平台：支持aws和阿里云... } 3. 通过RDD衍生新的RDD @Test def rddCreationFromRDD(): Unit = { val rdd1 = sc.parallelize(Seq(1,2,3)) //通过在rdd上执行算子操作，会生成新的rdd //非原地计算：str.substr 返回新的字符串，非原地计算。字符串不可变，RDD也不可变 val rdd2: RDD[Int] = rddmap(item =\u003e item) } ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:4:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Transformation Operator map() @Test def mapTest(): Unit = { //创建RDD val rdd1 = sc.parallelize(Seq(1,2,3)) //2.执行map操作 val rdd2 = rdd1.map(item =\u003e item * 10) //3.得到结果 val result = rdd2.collect() result.foreach(item =\u003e println(item)) } flatmap() 把rdd中的数据转化成数组或集合形式 把集合展开 生成了多条数据 flatmap是一对多 @Test def flatMapTest(): Unit = { val rdd1 = sc.parallelize(Seq(\"Hello a\",\"Hello b\",\"Hello c\")) val rdd2 = rddf1.latMap( item =\u003e item.split(\" \")) val result = rdd2.collect() result.foreach(item =\u003e println(item)) sc.stop() } reducebykey() reduceByKey第一步先按照key分组，然后对每一组进行聚合，得到结果。 @Test def reduceBykeyTest(): Unit = { //创建RDD val rdd1 = sc.parallelize(Seq(\"Hello a\",\"Hello b\",\"Hello c\")) //2.处理数据 val rdd2 = rdd1.flatMap( item =\u003e item.split(\" \")) .map( item =\u003e (item,1) ) .reduceByKey( (curr, agg) =\u003e curr + agg)//curr是当前的总值，agg是单个item的值 //3.得到结果 val result = rdd2.collect() result.foreach(item =\u003e println(item)) //4.关闭sc sc.stop() } Q\u0026A 数据量过大，如何处理？ 集群中处理，利用集群多台计算机来并行处理 如何放在集群中运行? sparkPutFile2Cluster 并行计算就是同时使用多个计算资源解决一个问题，有四个要点 解决的问题可以分解为多个可以并发计算的部分 每个部分可以在不同处理器上被同时执行 需要一个共享内存的机制 需要一个总体上的协作机制来进行调度 如果放在集群中，如何对整个计算任务进行分解？ sparkFile2Cluster2 概述 对于HDFS中的文件，是分为不同的Block 在进行计算的时候，就可以按照Block来划分，每一个Block对应一个不同的计算单元 扩展 RDD并没有真实的存放数据，数据是从HDFS中读取的，在计算的过程中读取即可 RDD至少是需要可以分片的，因为HDFS中的文件就是分片的，RDD可以分片也意味着可以并行计算 移动数据不如移动计算是一个基础的优化，如何做到？ 每一个计算单元需要记录其存储单元的位置，尽量调度过去 集群中运行，需要多节点配合，出错的概率也更高，出错了怎么办？ RDD1-\u003eRDD2-\u003eRDD3这个过程中，RDD2出错了，有两种解决办法 缓存RDD2的数据，直接恢复RDD2，类似HDFS的备份机制 记录RDD2的依赖关系，通过其父级的RDD来恢复RDD2，这种方式会少很多数据的交互和保存 如何通过父级RDD恢复？ 记录RDD2的父亲是RDD1 记录RDD2的计算函数，例如RDD2 = RDD1.map(…)等计算函数 通过父级RDD和计算函数来恢复RDD2 任务特别复杂，流程特别长，有很多RDD之间有依赖关系，如何优化？ 上面提到了可以使用依赖关系来进行容错，但是如果依赖关系特别长的时候，这种方式其实也比较低效，这个时候就应该使用另外一种方式，也就是记录数据集的状态 在Spark中有两个手段可以做到 缓存 Checkpoint map() \u0026 mapPartitions() mapPartitions 和 map 算子是一样的，只不过map是针对每一条数据进行转换，mapPartitions针对一整个分区的数据进行转换 所以 map 的 func 参数是单条数据，mapPartitions 的 func 参数是一个集合(一个分区整个所有的数据) map 的 func 返回值也是单条数据，mapPartition 的 func 返回值是一个集合 mapPartitionWithIndex 和 mapPartition 的区别是 func 中多分区数量参数 filter() 保留满足条件的元素 sample() filter按照规律过滤，sample则是随机采样 def sample( withReplacement: Boolean, //是否重复取样 fraction: Double, //取样比例 seed: Long = Utils.random.nextLong): RDD[T] = {...} mapValues() mapValue也是map，map作用于全部数据，mapValue作用于value collection operation 交集：rdd1.intersection(rdd2) 并集：rdd1.union(rdd2) 差集：rdd1.subract(rdd2) groupByKey() 聚合操作： reduceByKey -\u003e按照key分组，然后把每一组数据reduce。reduceByKey在map端combiner能减少IO，一个分区放多个数据。 groupByKey 运算结果的格式：（k，（value1，value2）），没有减少IO sc.parallelize(Seq((\"a\",1),(\"a\",1),(\"b\",1))) .groupByKey() .collect() .foreach(println(_)) combineByKey() 接收三个参数： 转化数据的函数（初始函数，作用于第一条数据，用于开启整个计算） 在分区上进行聚合 把所有的分区的聚合结果聚合为最终结果 val result = rdd.combineBykey( createCombiner = curr =\u003e (curr,1), mergeValue = (curr: (Double, Int), nextValue: Double) =\u003e (curr._1 + nextValue, curr._2 + 1)), mergeCombiners = (curr: (Double,Int), agg: (Double, Int)) =\u003e (curr._1 + agg._1, curr._2 + agg._2) ) result.map(item =\u003e (item._1, item._2._1 / item._2._2)) foldByKey() 功能等同于reduceByKey()，增加了初始值。reduceByKey底层是combineByKey()，foldByKey()底层是aggregateByKey()。 aggregateByKey() join() 按照相同的Key进行连接 sortBy() 排序：sortBy()，sortByKey() coalesce() 一般涉及到分区操作的算子常见的有两个，repartition和coalesce，都可以调大或者调小分区数量 summary 所有的转化操作的算子都是惰性的，在执行时候不会调度运行求得结果，而只是生成了对应的RDD 只有在Action操作的时候，才会真的运行 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:5:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Action Operator reduce((T, T) - U) 对整个结果集规约，最终生成一条数据，是整个数据集的总汇 reduceByKey和reduce有什么区别： reduce是action算子，reduceByKey是一个转换算子 RDD里有一万条数据，大部分key是相同的，有10个不同的key生成10条数据 reduce生成1条数据 reduceByKey是按Key分组，然后把每组聚合 reduce是针对一整个数据集进行聚合 reduceByKey是对KV数据进行计算 reduce可针对所有类型数据 reduce算子是一个shuffle操作吗？ shuffle操作分为mapper和reducer，mapper将数据放入paritioner的函数计算，求得往哪个reducer里放 reduce操作没有mapper和reducer，因为reduce算子会作用于RDD中的每个分区，然后分区求得局部结果，最终汇总到Driver中求得最终结果 RDD有五大属性，partitioner在shuffle过程中使用 paritioner只有kv型的RDD才有 collect() 以数组的形式返回数据集中所有元素 countByKey() count和countByKey countByKey结果：Map(Key -\u003e Key的count) 调用Action会生成一个job，job会运行获取结果，所以在两个job中有大量的log 数据倾斜：解决数据倾斜的问题，需要先通过countByKey查看Key对应的数量 first() 返回第一个元素 take(N) 返回前N个元素 takeSample(withReplacement, num) 类似于sample，区别这是action，直接返回结果 withReplacement：取数据有无放回 first() first()速度相比其他方法最快 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:6:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Data Type in RDD RDD中存放的数据类型 基本类型，String，对象 KV类型 数字类型 ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:7:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Practice @Test def process(): Unit = { //1. 创建sc对象 val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"practice\") val sc = new SparkContext(conf) //2. 读取文件 //1,2010,1,1,0,4,NA,NA,NA,NA,-21,43,1021,-11,NW,1.79,0,0 val source = sc.textFile(\"dataset/parctive.csv\") //3. 处理数据 val resultRDD = source.map(item =\u003e ((item.split(\",\")(1), item.split(\",\")(2)),item.split(\",\")(6))) .filter(item =\u003e StringUtils.isNotEmpty(item._2) \u0026\u0026 ! item._2.equalsIgnoreCase(\"NA\")) .map(item =\u003e (item._1, item._2.toInt)) .reduceByKey((curr,agg) =\u003e curr + agg) .sortBy(item =\u003e item._2, ascending = false) //4. 获取结果 resultRDD.take(10).foreach(item =\u003e println(item)) //5. 关闭sc sc.stop() } ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:8:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"RDD Feature RDD’s shuffle and partition RDD经常需要通过读取外部系统的数据来创建，外部存储系统往往是支持分片的。RDD需要支持分区，来和外部系统的分片一一对应 RDD的分区是一个并行计算的实现手段 partition function RDD使用分区来分布式处理，当使用RDD读取数据时，会尽量在屋里上靠近数据源。比如读HDFS或Cassandra时，会尽量的保持RDD的分区和数据源的分区数，分区模式一一对应 shuffle 从mapper端到reducer端 Spark支持宽依赖的转换，例如groupByKey和reduceByKey。在这些依赖项中，计算单个分区中的记录所需的数据可以来自于父数据集的许多分区中。要执行这些转换，具有相同key的所有元组必须最终位于同一分区中，由同一任务处理。为了满足这一要求，Spark产生一个shuffle，它在集群内部传输数据，并产生一个带有一组新分区的新stage。 Hash base shuffle Reduce 找到每个Mapper中对应自己哈希桶拉取数据 缺点：过多占用资源占用 Sort base shuffle 先按照partition ID 排序， 后按照Key的HashCode排序 partition and shuffle relation 分区主要用来实现并行计算，和shuffle没什么关系，但数据处理时，例如reduceByKey，groupByKey等聚合操作，需要把Key相同的Value拉取到一起进行计算，这个时候因为这些Key的相同的Value可能会在不同的分区，所以理解分区才能理解shuffle的根本原理 shuffle feature 只有KV型的RDD才会有Shuffle操作 早期版本spark的shuffle算法是 hash base shuffle，后来改为 sort base shuffle，更适合大吞吐量的场景 check partition 指定分区数 通过本地集合创建的时候指定分区数 val conf = new SparkConf().setMaster(\"local[6]\").setAppName(\"practice\")//创建App并开启6个分区 val sc = new SparkContext(conf) 通过读取文件创建的时候指定分区数 val rdd1 = sc.parallelize(Seq(1, 2, 3, 4, 5, 6, 7), 3) //指定分区数3 val rdd2 = sc.testFile(\"hdfs://node01:8020/data/test.txt\", 6) //这里指定的是最小分区数6 查看方法 通过WebUI查看。端口：4040 通过partitions来查看。rdd1.partitions.size 重分区 coalesce(num, true) repartitions(num) RDD Cache //1. 取出IP val countRDD = source.map(item =\u003e (item.split(\" \")(0), 1)) //2. 数据清洗 val cleanRDD = countRDD.filter(item =\u003e StingUtils.isNotEmpty(item._1)) //3. 统计ip的出现次数 val aggRDD = cleanRDD.reduceBykey((curr,agg) =\u003e curr + agg) //4. 统计出现最少的ip val leastIP = aggRDD.sortBy(item =\u003e item._2, ascending = true).first() //5. 统计出现最多的ip val mostIP = aggRDD.sortBy(item =\u003e item._2, ascending = false).first() println(leastIP, mostIP) sc.stop() 第一次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey) 第二次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey) 转换算子的作用：生成RDD，以及RDD之间的依赖关系 Action算子的作用：生成job，执行job 全局执行了四个shuffle 使用缓存的意义： 减少shuffle操作 容错，减少开销：rdd1-\u003erdd2-\u003erdd3，若rdd3算错会再次计算rdd1和rdd2整个流程。 缓存API: cache()或persist(null/level) //1. 处理 val countRDD = source.map(item =\u003e (item.split(\" \")(0), 1)) val cleanRDD = countRDD.filter(item =\u003e StingUtils.isNotEmpty(item._1)) val aggRDD = cleanRDD.reduceBykey((curr,agg) =\u003e curr + agg) //2. cache aggRDD = aggRDD.cache() //3. 两个RDD的action操作 val leastIP = aggRDD.sortBy(item =\u003e item._2, ascending = true).first() val mostIP = aggRDD.sortBy(item =\u003e item._2, ascending = false).first() println(leastIP, mostIP) sc.stop() //1. 处理 val countRDD = source.map(item =\u003e (item.split(\" \")(0), 1)) val cleanRDD = countRDD.filter(item =\u003e StingUtils.isNotEmpty(item._1)) val aggRDD = cleanRDD.reduceBykey((curr,agg) =\u003e curr + agg) //2. cache aggRDD = aggRDD.persist(storageLevel.MEMORY_ONLY) //3. 两个RDD的action操作 val leastIP = aggRDD.sortBy(item =\u003e item._2, ascending = true).first() val mostIP = aggRDD.sortBy(item =\u003e item._2, ascending = false).first() println(leastIP, mostIP) sc.stop() 缓存级别： MEMORY_ONLY: CPU效率最高 MEMORY_ONLY_SER: 更加节省空间 Checkpoint 斩断RDD的依赖链，并且将数据存储在可靠的存储引擎中，例如HDFS HDFS的NameNode中主要职责就是维护两个文件，一个是edits，另一个是fsimage。 edits中主要存放Editlog，FsImage保存了当前系统中所有目录和文件的信息，这个FsImage其实就是一个Checkpoint。 每一次修改文件的时候，都会在Edits中添加一条记录。 在一定条件满足的情况下，把edits删掉添加一个新的FSimage，包含了系统当前最新的状态。好处：增加速度，提高稳定性 Checkpoint和Cache的区别： Cache可以吧RDD计算出来放到内存中，但RDD的依赖链(相当于NameNode中的Edits日志)是不能丢的，若出现错误，只能重计算出来。 Checkpoint把结果存放在HDFS这类存储中，就变成了可靠的数据，如果出错了，则通过复制HDFS中的文件来实现容错。 如何使用： 两步： val conf = new.SparkConf().setMaster(\"local[6]\").setAppName(\"debug_string\") //1. setCheckPointDir：设置保存目录，也可以设置为HDFS上的目录 sc.setCheckpointDir(\"checkpoint\") val interimRDD = sc.textFile(\"dataset/test.txt\") .map(item =\u003e (item.split(\" \")(0), 1)) .filter(item =\u003e StringUtils.isNotBlank(item._1)) .reduceByKey((curr, agg) =\u003e curr + agg) //2. setCheckPoint：是一个action操作，也就是说如果调用checkpoint，则会重新计算一下RDD，然后把结果存在HDFS或者本地目录中 interimRDD.checkpoint() interimRDD.collect().foreach(println(_)) sc.stop() ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:9:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Spark Running Process //1. 创建sc对象 //2. 创建数据集 val textRDD = sc.parallelize(Seq(\"hadoop spark\", \"hadoop flume\", \"spark soo\")) //3. 数据处理 // 1.拆词2.赋予初始词频3.聚合4.将结果转为字符串 val splitRDD = textRDD.flatMap(_.split(\" \")) val tupleRDD = splitRDD.map((_, 1)) val reduceRDD = tupleRDD.reduceByKey(_ + _) val strRDD = reduceRDD.map(item =\u003e s\"${item._1}, ${item._2}\") //4. 结果获取 strRDD.collect().foreach(item =\u003e println(_)) //5. 关闭sc sc.stop() 集群组成 Node1主节点: Master Daemon： 负责管理Master节点， 协调资源的获取，以及连接Worker节点来运行Executor，是spark集群中的协调节点 Node2: Worker Daemon： 也称之为Slaves，是spark集群中的计算节点，用于和Master交互和并管理Driver， 当一个spark job 提交后，会创建sparkContext，worker会启动对应的Executor Driver: ction算子操作获取的结果，会把结果存放在Driver中 Executor Backend： Worker用于控制Executor的启停，其实worker是通过 Executor Backend来进行控制的。 Executor Backend是一个进程（是一个JVM实例），持有一个Executor对象。 Executor Task1 Task2 Task3 逻辑执行图 val textRDD = sc.parallelize(Seq(\"hadoop spark\", \"hadoop flume\", \"spark soo\")) val splitRDD = textRDD.flatMap(_.split(\" \")) val tupleRDD = splitRDD.map((_, 1)) val reduceRDD = tupleRDD.reduceByKey(_ + _) val strRDD = reduceRDD.map(item =\u003e s\"${item._1}, ${item._2}\") println(strRDD.toDebugString) (8) MapPartitionsRDD[4] at map at test.scala:12 [] | ShuffledRDD[3] at reduceByKey at test.scala:11 [] +-(8) MapPartitionsRDD[2] at map at test.scala:10 [] | MapPartitionsRDD[1] at flatMap at test.scala:9 [] | ParallelCollectionRDD[0] at parallelize at test.scala:6 [] RDDlogic 物理执行图 当触发Action执行的时候，这一组互相依赖的RDD要被处理，所以要转化为可运行的物理执行图，调度到集群中执行。 因为大部分RDD是不真正存放数据的，只是数据从中流转，所以不能直接在集群中运行RDD，要有一种pipeline的思想，需要将这组RDD转为Stage和Task，从而运行Task，优化整体执行速度。 RDDphysic 小结： ① -\u003e ① -\u003e ① 在第一个stage中，每一个这样的执行流程是一个Task，也就是在同一个Stage中的所有RDD的对应分区，在同一个Task中执行 Stage的划分是由Shuffle操作来确定的，有Shuffle的地方，Stage断开 数据流动 val sc = ... val textRDD = sc.parallelize(Seq(\"Hadoop Spark\", \"Hadoop Flume\", \"Spark Squad\")) val splitRDD = textRDD.flatMap(_.split(\" \")) val tupleRDD = splitRDD.map((_,1)) val reduceRDD = tupleRDD.reduceByKey(_ + _) val strRDD = reduceRDD.map(item =\u003e s\"${item._1, ${item._2}}\") strRDD.collect.foreach(item =\u003e println(item)) Job和Stage的关系 Job是一个最大的调度单位，DAGScheduler会首先创建一个Job的相关信息，然后去调度Job，但是没办法直接调度Job。 ​ 为什么Job需要切分 因为job的含义是对整个RDD求值，但RDD之间可能有一些宽依赖 如果遇到宽依赖的话，两个RDD之间需要进行数据拉取和复制 那么一个RDD就必须等待它所依赖的RDD所有分区先计算完成，然后再进行拉取 所以，一个Job是无法计算完整的RDD血统的 ​ Stage和Task的关系 Stage中的RDD之间是窄依赖： 窄依赖RDD理论上可以放在同一个Pipeline中执行的 RDD还有分区： 一个RDD只是一个概念，而真正存放和处理数据时，都是以分区作为单位的 Stage对应的是多个整体上的RDD，而真正的运行是需要针对RDD的分区来进行的 一个Task对应一个RDD的分区： 一个比Stage粒度更细的单元叫做Task，Stage是由Task组成的，之所以有Task这个概念，是因为Stage针对整个RDD，而计算的时候，要针对RDD的分区。 总结： Job\u003eStage\u003eTask 一个Job由多个Stage组成(这个取决有多少个宽依赖)，一个Stage由多个Task组成（这个取决有多少个分区数量 而Stage中经常会有一组Task需要同时执行，所以针对每一个Task来进行调度太过频繁没有意义，所以每个Stage中的Task们会被收集起来，放入一个TaskSet集合中。 一个Stage有一个TaskSet TaskSet中Task的个数由Stage中的最大分区数决定 sparkFlow ","date":"2020-05-27","objectID":"/20200527_spark-guide1/:10:0","tags":["Java","Spark"],"title":"Spark Guide, Part Ⅰ","uri":"/20200527_spark-guide1/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-26","objectID":"/20200526_app-architecture-development/","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"The features of large applications: high availability, high concurrency and big data. High availability: system need to provide service without interruption. High concurrency: still stable under the big access. Big data: store and manage big data well. ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:0:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"简单的架构 开始数据量少，所以只需一台服务器 smapleArchitecture 应用程序、文件、数据库往往都部署在一台服务器上，应用程序部署在Tomcat服务器上，数据库可以使用MySQL ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:1:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"应用于数据服务分隔 随着业务越来越复杂，访问量越来越大，导致性能越来越差，存储空间严重不足，这时我们考虑把服务器增加到三台。分离出应用服务器、数据库服务器、文件服务器。 应用服务器需要处理大量的访问，所以需要性能更好的CPU 数据库服务器需要存储大量的数据以及快速的检索，所以需磁盘的检索速度较快以及存储空间大 文件服务器需要存储上传的文件，需要更大的磁盘；现在通常情况下会选择第三方的存储服务 appSeparateData ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:2:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"应用服务器集群 在高并发、大流量的情况下，一台服务器是肯定处理不过来的，这个时候增加服务器，部署集群提供服务，来分担每台服务器的压力。部署集群的另一个好处是可伸缩性。比如双十一增加服务器分摊流量，双十一过后再减少服务器。 Cluster 如果应用服务器是Tomcat，那么可以部署一个Tomcat的集群，外部在部署一个负载均衡器，可以采用随机、轮询或者一致性哈希算法达将用户的请求分发到不同应用服务集群；通常选择的免费的负载均衡是nginx。在这种架构下，应用服务器的负载将不会是整个应用的瓶颈点； 虽然应用程序的处理速度在这种架构下提升了许多，但是又会暴露一个问题，数据库的压力大大增大，导致访问响应延迟，影响整个应用的性能。这种架构还有个问题，通常应用是有状态的，需要记录用户的登录信息，如果每次用户的请求都是随机路由到后端的应用服务器，那么用户的会话将会丢失；解决这个问题两个方案： 采用一致性hash把用户的请求路由到同一个Tomcat，如果有一台服务器跪了，那么这台服务器上面的用户信息将会丢失 Tomcat集群之间通过配置session复制，达到共享，此方案效率较低 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:3:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"缓存 根据二八原则，80%的的业务都是集中访问20%的数据，这20%的数据通常称为热点数据，但是这20%的数据占用的内存也不会小，如果每个应用服务器都存放一份，有些浪费存储空间，所以这时候需要考虑加入分布式缓存服务器（常用的是Redis）；当引入了分布式缓存服务器，再来看上面那个方案的问题，就可以解决了，把用户的会话存放到缓存服务器，不仅可以防止用户数据丢失，效率也不低；架构图如下： cache 由于分布式缓存服务器毕竟存放在远程，需要经过网络，所以取数据还是要花一点时间；本地缓存访问速度更快，但是内存空间有限，并且还会出现和应用程序争抢资源；所以这种架构搭配了分布式缓存和本地缓存，本地缓存存放少量常用热点数据，当本地缓存中没有命中时在去集中式缓存取 在引进缓存之后，数据库的访问压力可以的一定的缓解 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:4:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"数据库读写分离 虽然在加入了缓存之后，部分数据可以直接走缓存，不需要访问数据库，但是任然会有一些请求，会访问数据库，比如：缓存失效，缓存未命中；当流量大的时候，数据库的访问量也不小。这时候我们需要考虑搭建数据库集群，读写分离 rwSeparate 当应用服务器有写操作时，访问主库，当应用程序有读操作时，访问从库；大多数的应用都是读的操作远远大于写的操作，所以可以配置数据库一主多从来分担数据库的压力；为了让应用程序对应主库和从库无感知，通常需要引入一些读写分离的框架做一个统一的数据访问模块。 这种架构通常需要警惕的一个问题是主从延迟，当在高并发的场景下，主库刚写成功，数据库还未成功同步完从库，这时候另一个请求进入读取数据发现不存在；解放方案是在应用程序中高并发的场景下设置强制走主库查询 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:5:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"反向代理和CDN 假如随着业务的不断扩大，全国各地都会使用到我们的应用，由于各地区的网络情况不同，所以有的人请求响应速度快，有的人请求响应速度慢，这会严重的影响到用户的体验。为了提高响应速度需要引入反向代理和CDN；CDN和反向代理都是采用的缓存，目的： 尽可能快的把数据呈现给用户 减轻后端服务器的压力 架构图如下： cdn CDN: 部署在网络提供商的机房，当用户来访问的时候，从距离用户最近的服务器返回数据，尽快呈现给用户；通常情况下在CDN中缓存的是静态资源（html,js,css），达到动静分离；但是有时候遇到了某些数据访问量特别大的时候，后端会生成静态资源放入到CDN，比如：商城的首页，每个用户进入都需要访问的页面，如果每次请求都进入到后端，那么服务器的压力肯定不小，这种情况下会把首页生成静态的文件缓存到cdn和反向代理服务器 反向代理：部署在应用的中心机房，通常也是缓存的静态资源，当用户通过CDN未请求到需要的数据时，先进入反向代理服务器，如果有缓存用户访问的数据，那么直接返回给用户；这里也有特殊情况，对于有些场景下的热点数据，在这里根据用户的请求去分布式缓存服务器中获取，能拿到就直接返回。 这种架构已经把缓存做到了4级 第一级：CDN 缓存静态资源 第二级：反向代理缓存静态资源以及部分热点数据 第三级：应用服务器的本地缓存 第四级：分布式缓存服务器 通常情况下经过了这4级缓存，能够进入到数据库的请求也不多了，很好的释放了数据库的压力 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:6:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"搜索引擎和NoSQL 随着业务的不断扩大，对于数据的存储和查询的需求也越来越复杂，通常情况我们需要引入非关系型数据库，比如搜索引擎和NoSQL数据库 NoSQL 有时候我们的查询场景很复杂，需要查询很多数据表，经过一系列的计算才能完成，这时候可以考虑通过数据同步工具（比如canal）拉去数据到大数据平台，使用批处理框架离线计算，把输出的结果存放到搜索引擎或者NoSQL数据库中，应用程序直接查询计算的结果返回给用户。也有可能我们需要汇总多个表的数据做一张宽表，方便应用程序查询 由于引入的数据存储方式增多，为了减轻应用程序的管理多个数据源的麻烦，需要封装统一数据访问模块，如果使用的时Java，可以考虑spring-data ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:7:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"业务纵向拆分 互联网公司通常的宗旨是小步迭代试错快跑，当业务发展到足够大，对于单体应用想要达到这个宗旨是有难度的，随着业务的发展，应用程序越来越大，研发、维护、发布的成本也越来越大，这时候就需要考虑根据业务把单体应用拆分为多个服务，服务之间可以通过RPC远程调用和消息队列来一起完成用户的请求。 由于业务的拆分，通常情况下也会相应的对数据库进行拆分，达到一个服务对应一个数据库的理想状态 vertical 引入MQ的好处： 提高系统的可用性：当消费服务器发送故障时，消息还在消息队列中，数据不会丢失 加快请求的响应：当用户请求到达服务器后，把请求中可以异步处理的数据放入到MQ，让系统逐一消费，不需要用户等待，加快了响应速度 削峰填谷：当大量请求都同时进入到系统之后，会全部放入到消息队列，系统逐一消费，不会对系统造成很大的冲击 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:8:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"总结 还有一个情况未谈及到，就是数据库的水平拆分，这也是数据库拆分的最后手段，只有当单表数据特别大，不能满足业务的需要才使用。使用最多的还是进行数据库的业务纵向拆分，把数据库中不同业务的数据放到不同的物理服务器上。 应用当前到底选择什么架构，一定要根据实际业务的需求进行灵活的选择，驱动技术架构发展的主要动力还是在于业务的发展，不要为了技术而技术。 ","date":"2020-05-26","objectID":"/20200526_app-architecture-development/:9:0","tags":["Distribution"],"title":"The Development of App Architecture","uri":"/20200526_app-architecture-development/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-26","objectID":"/20200526_final-key-word-java/","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"Final key word no doubt be mentioned most time in Java language. There are some points about final key word. First of all, final key word can modify three objects: first is modify variables, second is modify way, third is modify class. ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:0:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"final 关键字修饰变量 在使用final修饰变量时，又可以分为两种情况：一种是基本数据类型的变量，另一种是引用类型的变量。final关键字在修饰基本数据类型时必须对变量赋予初始值，因此final也常常和static关键字一起用来声明常量值。final正好限制了必须赋值，static声明了静态变量。 final static String str = \"Hollow world\"; 修饰引用变量时，该引用变量如果已经被赋值则不可以再被赋值，否则也会出现不能编译的情况 //定义一个main对象并实例化 final Main main = new Main(); //被final关键字修饰后，再次对Main对象进行赋值就会报错 main = new Main(); ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:1:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"final关键字修饰方法 通过final修饰的方法是不能被子类的方法重写的。一般情况下，一个方法确定好不再修改可以使用final，因为final修饰的方法是在程序编译的时候就被动态绑定了不用等到程序运行的时候被动态绑定，这样就大大提高了执行效率。 public final void method(){ System.out.println(\"It is final method\"); } ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:2:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"final关键字修饰类 被final修饰的类叫final类，final类是不能被继承的，这也就以为这final类的功能是比较完整的。因此，jdk中有很多类使用final修饰的，它们不需要被继承，其中，最常见的就是String类。 public final class String implements java.io.Serializable, Comparable\u003cString\u003e public final class Integer extends Number implements ... public final class Long extends Number implements ... 包括其他的装饰类都是被final关键字修饰的，它自身提供的方法和功能也是非常完备的。 ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:3:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"static关键字的不同 final static 成对出现的频率也是比较高的，使用static修饰的变量只会在类加载的时候被初始化，不会因为对象的再次创建而改变。 static double num = Math.random(); ","date":"2020-05-26","objectID":"/20200526_final-key-word-java/:4:0","tags":["Java"],"title":"The Final Key Word In Java","uri":"/20200526_final-key-word-java/"},{"categories":["Technology"],"content":"Article description.","date":"2020-05-19","objectID":"/20200519_zookeeper/","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"ZooKeeper is an open source distributed coordination framework. It is positioned to provide consistent services for distributed applications and is the administrator of the entire big data system. ZooKeeper will encapsulate key services that are complex and error-prone, and provide users with efficient, stable, and easy-to-use services. ","date":"2020-05-19","objectID":"/20200519_zookeeper/:0:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1. Introduce ZooKeeper 是一个开源的分布式协调框架，它的定位是为分布式应用提供一致性服务，是整个大数据体系的管理员。ZooKeeper 会封装好复杂易出错的关键服务，将高效、稳定、易用的服务提供给用户使用。 如果上面的官方言语你不太理解，你可以认为 ZooKeeper = 文件系统 + 监听通知机制。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1.1 File System filesys Zookeeper维护一个类似文件系统的树状数据结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为1M。每个子目录项如 NameService 都被称作为 znode(目录节点)。和文件系统一样，我们能够自由的增加、删除znode，在一个znode下增加、删除子znode，唯一的不同在于znode是可以存储数据的。默认有四种类型的znode： 持久化目录节点 PERSISTENT：客户端与zookeeper断开连接后，该节点依旧存在。 持久化顺序编号目录节点 PERSISTENT_SEQUENTIAL：客户端与zookeeper断开连接后，该节点依旧存在，只是Zookeeper给该节点名称进行顺序编号。 临时目录节点 EPHEMERAL：客户端与zookeeper断开连接后，该节点被删除。 临时顺序编号目录节点 EPHEMERAL_SEQUENTIAL：客户端与zookeeper断开连接后，该节点被删除，只是Zookeeper给该节点名称进行顺序编号。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1.2 Watcher Watcher 监听机制是 Zookeeper 中非常重要的特性，我们基于 Zookeeper 上创建的节点，可以对这些节点绑定监听事件，比如可以监听节点数据变更、节点删除、子节点状态变更等事件，通过这个事件机制，可以基于 Zookeeper 实现分布式锁、集群管理等功能。 Watcher 特性： 当数据发生变化的时候， Zookeeper 会产生一个 Watcher 事件，并且会发送到客户端。但是客户端只会收到一次通知。如果后续这个节点再次发生变化，那么之前设置 Watcher 的客户端不会再次收到消息。（Watcher 是一次性的操作）。可以通过循环监听去达到永久监听效果。 ZooKeeper 的 Watcher 机制，总的来说可以分为三个过程： 客户端注册 Watcher，注册 watcher 有 3 种方式，getData、exists、getChildren。 服务器处理 Watcher 。 客户端回调 Watcher 客户端。 监听流程： 首先要有一个main()线程 在main线程中创建Zookeeper客户端，这时就会创建两个线程，一个负责网络连接通信（connet），一个负责监听（listener）。 通过connect线程将注册的监听事件发送给Zookeeper。 在Zookeeper的注册监听器列表中将注册的监听事件添加到列表中。 Zookeeper监听到有数据或路径变化，就会将这个消息发送给listener线程。 listener线程内部调用了process()方法。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1.3 Feature feature 集群：Zookeeper是一个领导者（Leader），多个跟随者（Follower）组成的集群。 高可用性：集群中只要有半数以上节点存活，Zookeeper集群就能正常服务。 全局数据一致：每个Server保存一份相同的数据副本，Client无论连接到哪个Server，数据都是一致的。 更新请求顺序进行：来自同一个Client的更新请求按其发送顺序依次执行。 数据更新原子性：一次数据更新要么成功，要么失败。 实时性：在一定时间范围内，Client能读到最新数据。 从设计模式角度来看，zk是一个基于观察者设计模式的框架，它负责管理跟存储大家都关心的数据，然后接受观察者的注册，数据反生变化zk会通知在zk上注册的观察者做出反应。 Zookeeper是一个分布式协调系统，满足CP性，跟SpringCloud中的Eureka满足AP不一样。 分布式协调系统：Leader会同步数据到follower，用户请求可通过follower得到数据，这样不会出现单点故障，并且只要同步时间无限短，那这就是个好的 分布式协调系统。 CAP原则又称CAP定理，指的是在一个分布式系统中，一致性（Consistency）、可用性（Availability）、分区容错性（Partition tolerance）。CAP 原则指的是，这三个要素最多只能同时实现两点，不可能三者兼顾。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:1:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"2. Function 通过对 Zookeeper 中丰富的数据节点进行交叉使用，配合 Watcher 事件通知机制，可以非常方便的构建一系列分布式应用中涉及的核心功能，比如 数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、分布式锁和分布式队列 等功能。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"1. 数据发布/订阅 当某些数据由几个机器共享，且这些信息经常变化数据量还小的时候，这些数据就适合存储到ZK中。 数据存储：将数据存储到 Zookeeper 上的一个数据节点。 数据获取：应用在启动初始化节点从 Zookeeper 数据节点读取数据，并在该节点上注册一个数据变更 Watcher 数据变更：当变更数据时会更新 Zookeeper 对应节点数据，Zookeeper会将数据变更通知发到各客户端，客户端接到通知后重新读取变更后的数据即可。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"2. 分布式锁 关于分布式锁其实在 Redis 中已经讲过了，并且Redis提供的分布式锁是比ZK性能强的。基于ZooKeeper的分布式锁一般有如下两种。 保持独占 核心思想：在zk中有一个唯一的临时节点，只有拿到节点的才可以操作数据，没拿到的线程就需要等待。缺点：可能引发羊群效应，第一个用完后瞬间有999个同时并发的线程向zk请求获得锁。 控制时序 主要是避免了羊群效应，临时节点已经预先存在，所有想要获得锁的线程在它下面创建临时顺序编号目录节点，编号最小的获得锁，用完删除，后面的依次排队获取。 distributedLock ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3. 负载均衡 多个相同的jar包在不同的服务器上开启相同的服务，可以通过nginx在服务端进行负载均衡的配置。也可以通过ZooKeeper在客户端进行负载均衡配置。 多个服务注册 客户端获取中间件地址集合 从集合中随机选一个服务执行任务 ZooKeeper负载均衡和Nginx负载均衡区别： ZooKeeper不存在单点问题，zab机制保证单点故障可重新选举一个leader只负责服务的注册与发现，不负责转发，减少一次数据交换（消费方与服务方直接通信），需要自己实现相应的负载均衡算法。 Nginx存在单点问题，单点负载高数据量大,需要通过 KeepAlived + LVS 备机实现高可用。每次负载，都充当一次中间人转发角色，增加网络负载量（消费方与服务方间接通信），自带负载均衡算法。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4. 命名服务 命名服务是指通过指定的名字来获取资源或者服务的地址，利用 zk 创建一个全局唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5. 分布式协调/通知 对于系统调度来说，用户更改zk某个节点的value， ZooKeeper会将这些变化发送给注册了这个节点的 watcher 的所有客户端，进行通知。 对于执行情况汇报来说，每个工作进程都在目录下创建一个携带工作进度的临时节点，那么汇总的进程可以监控目录子节点的变化获得工作进度的实时的全局情况。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:5","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"6. 集群管理 大数据体系下的大部分集群服务好像都通过ZooKeeper管理的，其实管理的时候主要关注的就是机器的动态上下线跟Leader选举。 动态上下线： 比如在zookeeper服务器端有一个znode叫 /Configuration，那么集群中每一个机器启动的时候都去这个节点下创建一个EPHEMERAL类型的节点，比如server1 创建 /Configuration/Server1，server2创建**/Configuration /Server1**，然后Server1和Server2都watch /Configuration 这个父节点，那么也就是这个父节点下数据或者子节点变化都会通知到该节点进行watch的客户端。 Leader选举： 利用ZooKeeper的强一致性，能够保证在分布式高并发情况下节点创建的全局唯一性，即：同时有多个客户端请求创建 /Master 节点，最终一定只有一个客户端请求能够创建成功。利用这个特性，就能很轻易的在分布式环境中进行集群选举了。 就是动态Master选举。这就要用到 EPHEMERAL_SEQUENTIAL类型节点的特性了，这样每个节点会自动被编号。允许所有请求都能够创建成功，但是得有个创建顺序，每次选取序列号最小的那个机器作为Master 。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:2:6","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3. Choose Leader ZooKeeper集群节点个数一定是奇数个，一般3个或者5个就OK。为避免集群群龙无首，一定要选个大哥出来当Leader。这是个高频考点。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.1 预备知识 3.1.1. 节点四种状态。 LOOKING：寻 找 Leader 状态。当服务器处于该状态时会认为当前集群中没有 Leader，因此需要进入 Leader 选举状态。 FOLLOWING：跟随者状态。处理客户端的非事务请求，转发事务请求给 Leader 服务器，参与事务请求 Proposal(提议) 的投票，参与 Leader 选举投票。 LEADING：领导者状态。事务请求的唯一调度和处理者，保证集群事务处理的顺序性，集群内部个服务器的调度者(管理follower,数据同步)。 OBSERVING：观察者状态。3.0 版本以后引入的一个服务器角色，在不影响集群事务处理能力的基础上提升集群的非事务处理能力，处理客户端的非事务请求，转发事务请求给 Leader 服务器，不参与任何形式的投票。 3.1.2 服务器ID 既Server id，一般在搭建ZK集群时会在myid文件中给每个节点搞个唯一编号，编号越大在Leader选择算法中的权重越大，比如初始化启动时就是根据服务器ID进行比较。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.1.3 ZXID ZooKeeper 采用全局递增的事务 Id 来标识，所有 proposal(提议)在被提出的时候加上了ZooKeeper Transaction Id ，zxid是64位的Long类型，这是保证事务的顺序一致性的关键。zxid中高32位表示纪元epoch，低32位表示事务标识xid。你可以认为zxid越大说明存储数据越新。 每个leader都会具有不同的epoch值，表示一个纪元/朝代，用来标识 leader 周期。每个新的选举开启时都会生成一个新的epoch，新的leader产生的话epoch会自增，会将该值更新到所有的zkServer的zxid和epoch， xid是一个依次递增的事务编号。数值越大说明数据越新，所有 proposal（提议）在被提出的时候加上了zxid，然后会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.2 Leader选举 Leader的选举一般分为启动时选举跟Leader挂掉后的运行时选举。 3.2.1 启动时Leader选举 我们以5台机器为例，只有超过半数以上，即最少启动3台服务器，集群才能正常工作。 服务器1启动，发起一次选举。 服务器1投自己一票。此时服务器1票数一票，不够半数以上（3票），选举无法完成，服务器1状态保持为LOOKING。 服务器2启动，再发起一次选举。 服务器1和2分别投自己一票，此时服务器1发现服务器2的id比自己大，更改选票投给服务器2。此时服务器1票数0票，服务器2票数2票，不够半数以上（3票），选举无法完成。服务器1，2状态保持LOOKING。 服务器3启动，发起一次选举。 与上面过程一样，服务器1和2先投自己一票，然后因为服务器3id最大，两者更改选票投给为服务器3。此次投票结果：服务器1为0票，服务器2为0票，服务器3为3票。此时服务器3的票数已经超过半数（3票），服务器3当选Leader。服务器1，2更改状态为FOLLOWING，服务器3更改状态为LEADING； 服务器4启动，发起一次选举。 此时服务器1、2、3已经不是LOOKING状态，不会更改选票信息，交换选票信息结果。服务器3为3票，服务器4为1票。此时服务器4服从多数，更改选票信息为服务器3，服务器4并更改状态为FOLLOWING。 服务器5启动，发起一次选举 同4一样投票给3，此时服务器3一共5票，服务器5为0票。服务器5并更改状态为FOLLOWING； 最终 Leader是服务器3，状态为LEADING。其余服务器是Follower，状态为FOLLOWING。 3.2.2 运行时Leader选举 运行时候如果Master节点崩溃了会走恢复模式，新Leader选出前会暂停对外服务，大致可以分为四个阶段 选举、发现、同步、广播。 chooseLeader 每个Server会发出一个投票，第一次都是投自己，其中投票信息 = (myid，ZXID) 收集来自各个服务器的投票 处理投票并重新投票，处理逻辑：优先比较ZXID，然后比较myid。 统计投票，只要超过半数的机器接收到同样的投票信息，就可以确定leader，注意epoch的增加跟同步。 改变服务器状态Looking变为Following或Leading。 当 Follower 链接上 Leader 之后，Leader 服务器会根据自己服务器上最后被提交的 ZXID 和 Follower 上的 ZXID 进行比对，比对结果要么回滚，要么和 Leader 同步，保证集群中各个节点的事务一致。 集群恢复到广播模式，开始接受客户端的写请求。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"3.3 脑裂 脑裂问题是集群部署必须考虑的一点，比如在Hadoop跟Spark集群中。而ZAB为解决脑裂问题，要求集群内的节点数量为2N+1。当网络分裂后，始终有一个集群的节点数量过半数，而另一个节点数量小于N+1, 因为选举Leader需要过半数的节点同意，所以我们可以得出如下结论： 有了过半机制，对于一个Zookeeper集群，要么没有Leader，要没只有1个Leader，这样就避免了脑裂问题 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:3:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4. ZAB of Consistence 建议先看下 浅谈大数据中的2PC、3PC、Paxos、Raft、ZAB ，不然可能看的吃力。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.1 ZAB 协议介绍 ZAB (Zookeeper Atomic Broadcast 原子广播协议) 协议是为分布式协调服务ZooKeeper专门设计的一种支持崩溃恢复的一致性协议。基于该协议，ZooKeeper 实现了一种主从模式的系统架构来保持集群中各个副本之间的数据一致性。 分布式系统中leader负责外部客户端的写请求。follower服务器负责读跟同步。这时需要解决俩问题。 Leader 服务器是如何把数据更新到所有的Follower的。 Leader 服务器突然间失效了，集群咋办？ 因此ZAB协议为了解决上面两个问题而设计了两种工作模式，整个 Zookeeper 就是在这两个模式之间切换： 原子广播模式：把数据更新到所有的follower。 崩溃恢复模式：Leader发生崩溃时，如何恢复。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.2 原子广播模式 你可以认为消息广播机制是简化版的 2PC协议，就是通过如下的机制保证事务的顺序一致性的。 leader从客户端收到一个写请求后生成一个新的事务并为这个事务生成一个唯一的ZXID， leader将将带有 zxid 的消息作为一个提案(proposal)分发给所有 FIFO队列。 FIFO队列取出队头proposal给follower节点。 当 follower 接收到 proposal，先将 proposal 写到硬盘，写硬盘成功后再向 leader 回一个 ACK。 FIFO队列把ACK返回给Leader。 当leader收到超过一半以上的follower的ack消息，leader会进行commit请求，然后再给FIFO发送commit请求。 当follower收到commit请求时，会判断该事务的ZXID是不是比历史队列中的任何事务的ZXID都小，如果是则提交，如果不是则等待比它更小的事务的commit(保证顺序性) ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.3 崩溃恢复 消息广播过程中，Leader 崩溃了还能保证数据一致吗？当 Leader 崩溃会进入崩溃恢复模式。其实主要是对如下两种情况的处理。 Leader 在复制数据给所有 Follwer 之后崩溃，咋搞？ Leader 在收到 Ack 并提交了自己，同时发送了部分 commit 出去之后崩溃咋办？ 针对此问题，ZAB 定义了 2 个原则： ZAB 协议确保执行那些已经在 Leader 提交的事务最终会被所有服务器提交。 ZAB 协议确保丢弃那些只在 Leader 提出/复制，但没有提交的事务。 至于如何实现确保提交已经被 Leader 提交的事务，同时丢弃已经被跳过的事务呢？关键点就是依赖上面说到过的 ZXID了。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.4 ZAB 特性 一致性保证 可靠提交(Reliable delivery) ：如果一个事务 A 被一个server提交(committed)了，那么它最终一定会被所有的server提交 全局有序(Total order) 假设有A、B两个事务，有一台server先执行A再执行B，那么可以保证所有server上A始终都被在B之前执行 因果有序(Causal order) 如果发送者在事务A提交之后再发送B,那么B必将在A之后执行 高可用性 只要大多数（法定数量）节点启动，系统就行正常运行 可恢复性 当节点下线后重启，它必须保证能恢复到当前正在执行的事务 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"4.5 ZAB 和 Paxos 对比 相同点： 两者都存在一个类似于 Leader 进程的角色，由其负责协调多个 Follower 进程的运行. Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交. ZAB 协议中，每个 Proposal 中都包含一个 epoch 值来代表当前的 Leader周期，Paxos 中名字为 Ballot 不同点： ZAB 用来构建高可用的分布式数据主备系统（Zookeeper），Paxos 是用来构建分布式一致性状态机系统。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:4:5","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5. ZooKeeper 零散知识 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:0","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.1 常见指令 Zookeeper 有三种部署模式： 单机部署：一台机器上运行。 集群部署：多台机器运行。 伪集群部署：一台机器启动多个 Zookeeper 实例运行。 部署完毕后常见指令如下： 命令基本语法 功能描述 help 显示所有操作命令 ls path [watch] 显示所有操作命令 ls path [watch] 查看当前节点数据并能看到更新次数等数据 create 普通创建， -s 含有序列， -e 临时（重启或者超时消失） get path [watch] 获得节点的值 set 设置节点的具体值 stat 查看节点状态 delete 删除节点 rmr 递归删除节点 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:1","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.2 Zookeeper客户端 5.2.1. Zookeeper原生客户端 Zookeeper客户端是异步的哦！需要引入CountDownLatch 来确保连接好了再做下面操作。Zookeeper原生api是不支持迭代式的创建跟删除路径的，具有如下弊端。 会话的连接是异步的；必须用到回调函数 。 Watch需要重复注册：看一次watch注册一次 。 Session重连机制：有时session断开还需要重连接。 开发复杂性较高：开发相对来说比较琐碎。 5.2.2. ZkClient 开源的zk客户端，在原生API基础上封装，是一个更易于使用的zookeeper客户端，做了如下优化。 优化一 、在session loss和session expire时自动创建新的ZooKeeper实例进行重连。优化二、 将一次性watcher包装为持久watcher。 5.2.3. Curator 开源的zk客户端，在原生API基础上封装，apache顶级项目。是Netflix公司开源的一套Zookeeper客户端框架。了解过Zookeeper原生API都会清楚其复杂度。Curator帮助我们在其基础上进行封装、实现一些开发细节，包括接连重连、反复注册Watcher和NodeExistsException等。目前已经作为Apache的顶级项目出现，是最流行的Zookeeper客户端之一。 5.2.4. Zookeeper图形化客户端工具 ZooInspector工具 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:2","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.3 ACL 权限控制机制 ACL全称为Access Control List 即访问控制列表，用于控制资源的访问权限。zookeeper利用ACL策略控制节点的访问权限，如节点数据读写、节点创建、节点删除、读取子节点列表、设置节点权限等。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:3","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"5.4 Zookeeper使用注意事项 集群中机器的数量并不是越多越好，一个写操作需要半数以上的节点ack，所以集群节点数越多，整个集群可以抗挂点的节点数越多(越可靠)，但是吞吐量越差。集群的数量必须为奇数。 zk是基于内存进行读写操作的，有时候会进行消息广播，因此不建议在节点存取容量比较大的数据。 dataDir目录、dataLogDir两个目录会随着时间推移变得庞大，容易造成硬盘满了。建议自己编写或使用自带的脚本保留最新的n个文件。 默认最大连接数 默认为60，配置maxClientCnxns参数，配置单个客户端机器创建的最大连接数。 ","date":"2020-05-19","objectID":"/20200519_zookeeper/:5:4","tags":["Zookeeper","Distribution"],"title":"Key points of ZooKeeper","uri":"/20200519_zookeeper/"},{"categories":["Technology"],"content":"Article description.","date":"2020-04-27","objectID":"/20200427_kafka/","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Apache Kafka aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a “message set” abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This “leads to larger network packets, larger sequential disk operations, contiguous memory blocks […] which allows Kafka to turn a bursty stream of random message writes into linear writes.” ","date":"2020-04-27","objectID":"/20200427_kafka/:0:0","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Message queue ","date":"2020-04-27","objectID":"/20200427_kafka/:1:0","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"1. Why 为什么使用消息队列？ 从系统之间有通信需求开始，就自然产生了消息队列。 在计算机科学中，消息队列（英语：Message queue）是一种进程间通信或同一进程的不同线程间的通信方式，软件的贮列用来处理一系列的输入，通常是来自用户。消息队列提供了异步的通信协议，每一个贮列中的纪录包含详细说明的资料，包含发生的时间，输入设备的种类，以及特定的输入参数，也就是说：消息的发送者和接收者不需要同时与消息队列交互。消息会保存在队列中，直到接收者取回它。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:1","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"2. Feature 解耦： 允许你独立的扩展或修改两边的处理过程，只要确保它们遵守同样的接口约束。 冗余： 消息队列把数据进行持久化直到它们已经被完全处理，通过这一方式规避了数据丢失风险。许多消息队列所采用的”插入-获取-删除”范式中，在把一个消息从队列中删除之前，需要你的处理系统明确的指出该消息已经被处理完毕，从而确保你的数据被安全的保存直到你使用完毕。 扩展性： 因为消息队列解耦了你的处理过程，所以增大消息入队和处理的频率是很容易的，只要另外增加处理过程即可。 灵活性 \u0026 峰值处理能力： 在访问量剧增的情况下，应用仍然需要继续发挥作用，但是这样的突发流量并不常见。如果为以能处理这类峰值访问为标准来投入资源随时待命无疑是巨大的浪费。使用消息队列能够使关键组件顶住突发的访问压力，而不会因为突发的超负荷的请求而完全崩溃。 可恢复性： 系统的一部分组件失效时，不会影响到整个系统。消息队列降低了进程间的耦合度，所以即使一个处理消息的进程挂掉，加入队列中的消息仍然可以在系统恢复后被处理。 顺序保证： 在大多使用场景下，数据处理的顺序都很重要。大部分消息队列本来就是排序的，并且能保证数据会按照特定的顺序来处理。（Kafka 保证一个 Partition 内的消息的有序性） 缓冲： 有助于控制和优化数据流经过系统的速度，解决生产消息和消费消息的处理速度不一致的情况。 异步通信： 很多时候，用户不想也不需要立即处理消息。消息队列提供了异步处理机制，允许用户把一个消息放入队列，但并不立即处理它。想向队列中放入多少消息就放多少，然后在需要的时候再去处理它们。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:2","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"3. Usage 服务解耦： 下游系统可能只需要当前系统的一个子集，应对不断增加变化的下游系统，当前系统不停地修改调试与这些下游系统的接口，系统间耦合过于紧密。引入消息队列后，当前系统变化时发送一条消息到消息队列的一个主题中，所有下游系统都订阅主题，这样每个下游系统都可以获得一份实时完整的订单数据。 异步处理： 以秒杀为例：风险控制-\u003e库存锁定-\u003e生成订单-\u003e短信通知-\u003e更新统计数据 限流削峰/流量控制 一个设计健壮的程序有自我保护的能力，也就是说，它应该可以在海量的请求下，还能在自身能力范围内尽可能多地处理请求，拒绝处理不了的请求并且保证自身运行正常。使用消息队列隔离网关和后端服务，以达到流量控制和保护后端服务的目的。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:3","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"4. Realize 点对点： 系统 A 发送的消息只能被系统 B 接收，其他任何系统都不能读取 A 发送的消息。 日常生活的例子比如电话客服就属于这种模型： 同一个客户呼入电话只能被一位客服人员处理，第二个客服人员不能为该客户服务。 发布/订阅模型 这个模型可能存在多个发布者向相同的主题发送消息，而订阅者也可能存在多个，它们都能接收到相同主题的消息。 生活中的报纸订阅就是一种典型的发布 / 订阅模型。 ","date":"2020-04-27","objectID":"/20200427_kafka/:1:4","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Kafka ","date":"2020-04-27","objectID":"/20200427_kafka/:2:0","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"1. Intro kafka是一个分布式流处理平台。 类似一个消息系统，读写流式的数据 编写可扩展的流处理应用程序，用于实时事件响应的场景 安全的将流式的数据存储在一个分布式，有副本备份，容错的集群 ","date":"2020-04-27","objectID":"/20200427_kafka/:2:1","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"2. History Kafka从何而来?我们为什么要开发Kafka? Kafka到底是什么? Kafka 最初是 LinkedIn 的一个内部基础设施系统。我们发现虽然有很多数据库和系统可以用来存储数据，但在我们的架构里，刚好缺一个可以帮助处理持续数据流的组件。在开发Kafka之前，我们实验了各种现成的解决方案，从消息系统到日志聚合系统，再到ETL工具，它们都无法满足我们的需求。 最后，我们决定从头开发一个系统。我们不想只是开发一个能够存储数据的系统，比如传统的关系型数据库、键值存储引擎、搜索引擎或缓存系统，我们希望能够把数据看成是持续变化和不断增长的流，并基于这样的想法构建出一个数据系统。事实上，是一个数据架构。 这个想法实现后比我们最初预想的适用性更广。Kafka 一开始被用在社交网络的实时应用和数据流当中，而现在已经成为下一代数据架构的基础。大型零售商正在基于持续数据流改造他们的基础业务流程，汽车公司正在从互联网汽车那里收集和处理实时数据流，银行也在重新思考基于 Kafka 改造他们的基础。 它可以用于两大类别的应用: 构造实时流数据管道，它可以在系统或应用之间可靠地获取数据。(相当于message queue) 构建实时流式应用程序，对这些流数据进行转换或者影响。(就是流处理，通过kafka stream topic和topic之间内部进行变化) 版本号 备注 0.7 上古版本，提供了最基础的消息队列功能 0.8 引入了副本机制，成为了一个真正意义上完备的分布式高可靠消息队列解决方案 0.8.2 新版本 Producer API，即需要指定 Broker 地址的 Producer 0.9 增加了基础的安全认证 / 权限，Java 重写了新版本消费者 API 0.10.0.0 引入了 Kafka Streams 0.11.0.0 提供幂等性 Producer API 以及事务（Transaction） API，对 Kafka 消息格式做了重构。 1.0 Kafka Streams 的各种改进 2.0 Kafka Streams 的各种改进 ","date":"2020-04-27","objectID":"/20200427_kafka/:2:2","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"3. Item 消息：Record。这里的消息就是指 Kafka 处理的主要对象。 服务：Broker。一个 Kafka 集群由多个 Broker 组成，Broker 负责接收和处理客户端发送过来的请求，以及对消息进行持久化。 主题：Topic。主题是承载消息的逻辑容器，在实际使用中多用来区分具体的业务。 分区：Partition。一个有序不变的消息序列。每个主题下可以有多个分区。 消息位移：Offset。表示分区中每条消息的位置信息，是一个单调递增且不变的值。 副本：Replica。Kafka 中同一条消息能够被拷贝到多个地方以提供数据冗余，这些地方就是所谓的副本。副本还分为领导者副本和追随者副本，各自有不同的角色划分。副本是在分区层级下的，即每个分区可配置多个副本实现高可用。 生产者：Producer。向主题发布新消息的应用程序。 消费者：Consumer。从主题订阅新消息的应用程序。 消费者位移：Consumer Offset。表征消费者消费进度，每个消费者都有自己的消费者位移。 消费者组：Consumer Group。多个消费者实例共同组成的一个组，同时消费多个分区以实现高吞吐。 重平衡：Rebalance。消费者组内某个消费者实例挂掉后，其他消费者实例自动重新分配订阅主题分区的过程。Rebalance 是 Kafka 消费者端实现高可用的重要手段。 kafkaItem ","date":"2020-04-27","objectID":"/20200427_kafka/:2:3","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"4. Topic 日志 日志可能是一种最简单的不能再简单的存储抽象，只能追加、按照时间完全有序（totally-ordered）的记录序列。日志看起来的样子： kafkaLog 在日志的末尾添加记录，读取日志记录则从左到右。每一条记录都指定了一个唯一的顺序的日志记录编号。 日志记录的次序（ordering）定义了『时间』概念，因为位于左边的日志记录表示比右边的要早。日志记录编号可以看作是这条日志记录的『时间戳』。把次序直接看成是时间概念，刚开始你会觉得有点怪异，但是这样的做法有个便利的性质：解耦了 时间 和 任一特定的物理时钟（physical clock）。引入分布式系统后，这会成为一个必不可少的性质。 日志 和 文件或数据表（table）并没有什么大的不同。文件是一系列字节，表是由一系列记录组成，而日志实际上只是一种按照时间顺序存储记录的数据表或文件。 对于每一个topic， Kafka集群都会维持一个分区日志，如下所示： kafkaPartitionLog 实操 启动zk cd /usr/local/kara/kafka_2.13-2.6.0/bin zookeeper-server-start.sh ../config/zookeeper.properties 启动kafka服务器 kafka-server-start.sh ../config/server.properties 创建topic，4个分区，一个副本 kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 4 --topic partition_test 发送一些消息 kafka-console-producer.sh --broker-list localhost:9092 --topic partition_test 启动一个consumer kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic partition_test --from-beginning 分区 partition存储分布 一个topic下有多个不同partition，每个partition为一个目录，partiton命名规则为topic名称+有序序号，第一个partiton序号从0开始，序号最大值为partitions数量减1 partition文件存储 1.每个partion(目录)相当于一个巨型文件被平均分配到多个大小相等segment(段)数据文件中。但每个段segment file消息数量不一定相等，这种特性方便old segment file快速被删除。 2.每个partiton只需要支持顺序读写就行了，segment文件生命周期由服务端配置参数决定。 segment文件存储 segment file组成：由2大部分组成，分别为index file和data file，此2个文件一一对应，成对出现，分别表示为segment索引文件、数据文件. segment文件命名规则：partion全局的第一个segment从0开始，后续每个segment文件名为上一个segment文件最后一条消息的offset值。数值最大为64位long大小，19位数字字符长度，没有数字用0填充。segment index file采取稀疏索引存储方式，它减少索引文件大小，通过mmap可以直接内存操作，稀疏索引为数据文件的每个对应message设置一个元数据指针,它比稠密索引节省了更多的存储空间，但查找起来需要消耗更多的时间。 segment中的消息message物理结构字段说明 关键字 解释说明 8 byte offset 在parition(分区)内的每条消息都有一个有序的id号，这个id号被称为偏移(offset),它可以唯一确定每条消息在parition(分区)内的位置。即offset表示partiion的第多少message 4 byte message size message大小 4 byte CRC32 用crc32校验message 1 byte “magic” 表示本次发布Kafka服务程序协议版本号 1 byte “attributes” 表示为独立版本、或标识压缩类型、或编码类型。 4 byte key length 表示key的长度,当key为-1时，K byte key字段不填 K byte key 可选 value bytes payload 表示实际消息数据。 文件系统 Kafka 对消息的存储和缓存严重依赖于文件系统。人们对于“磁盘速度慢”具有普遍印象，事实上，磁盘的速度比人们预期的要慢的多，也快得多，这取决于人们使用磁盘的方式。 使用6个7200rpm、SATA接口、RAID-5的磁盘阵列在JBOD配置下的顺序写入的性能约为600MB/秒，但随机写入的性能仅约为100k/秒，相差6000倍以上。 线性的读取和写入是磁盘使用模式中最有规律的，并且由操作系统进行了大量的优化。 read-ahead 是以大的 data block 为单位预先读取数据 write-behind 是将多个小型的逻辑写合并成一次大型的物理磁盘写入 关于该问题的进一步讨论可以参考 ACM Queue article，他们发现实际上顺序磁盘访问在某些情况下比随机内存访问还要快！ 为了弥补这种性能差异，现代操作系统主动将所有空闲内存用作 disk caching（磁盘高速缓存），所有对磁盘的读写操作都会通过这个统一的 cache（ in-process cache）。 即使进程维护了 in-process cache，该数据也可能会被复制到操作系统的 pagecache 中，事实上所有内容都被存储了两份。 此外，Kafka 建立在 JVM 之上，任何了解 Java 内存使用的人都知道两点： 对象的内存开销非常高，通常是所存储的数据的两倍(甚至更多)。 随着堆中数据的增加，Java 的垃圾回收变得越来越复杂和缓慢。 kafka选择了一个非常简单的设计：相比于维护尽可能多的 in-memory cache，并且在空间不足的时候匆忙将数据 flush 到文件系统，我们把这个过程倒过来。所有数据一开始就被写入到文件系统的持久化日志中，而不用在 cache 空间不足的时候 flush 到磁盘。实际上，这表明数据被转移到了内核的 pagecache 中。 Pagecache页面缓存 Page cache（页面缓存） Page cache 也叫页缓冲或文件缓冲，是由好几个磁盘块构成，大小通常为4k，在64位系统上为8k，构成的几个磁盘块在物理磁盘上不一定连续，文件的组织单位为一页， 也就是一个page cache大小，文件读取是由外存上不连续的几个磁盘块，到buffer cache，然后组成page cache，然后供给应用程序。 Buffer cache（块缓存） Buffer cache 也叫块缓冲，是对物理磁盘上的一个磁盘块进行的缓冲，其大小为通常为1k，磁盘块也是磁盘的组织单位。设立buffer cache的目的是为在程序多次访问同一磁盘块时，减少访问时间。 Page cache（页面缓存）与Buffer cache（块缓存）的区别 磁盘的操作有逻辑级（文件系统）和物理级（磁盘块），这两种Cache就是分别缓存逻辑和物理级数据的。 我们通过文件系统操作文件，那么文件将被缓存到Page Cache，如果需要刷新文件的时候，Page Cache将交给Buffer Cache去完成，因为Buffer Cache就是缓存磁盘块的。 简单说来，page cache用来缓存文件数据，buffer cache用来缓存磁盘数据。在有文件系统的情况下，对文件操作，那么数据会缓存到page cache，如果直接采用dd等工具对磁盘进行读写，那么数据会缓存到buffer cache。 Buffer(Buffer Cache)以块形式缓冲了块设备的操作，定时或手动的同步到硬盘，它是为了缓冲写操作然后一次性将很多改动写入硬盘，避免频繁写硬盘，提高写入效率。 Cache(Page Cache)以页面形式缓存了文件系统的文件，给需要使用的程序读取，它是为了给读操作提供缓冲，避免频繁读硬盘，提高读取效率。 降低时间复杂度 消息系统使用的持久化数据结构通常是和 BTree 相关联的消费者队列或者其他用于存储消息源数据的通用随机访问数据结构。BTree 的操作复杂度是 O(log N)，通常我们认为 O(log N) 基本等同于常数时间，但这条在磁盘操作中不成立。 存储系统将非常快的cache操作和非常慢的物理磁盘操作混合在一起，当数据随着 fixed cache 增加时，可以看到树的性能通常是非线性的——比如数据翻倍时性能下降不只两倍。 kafka选择把持久化队列建立在简单的读取和向文件后追加两种操作之上，这和日志解决方案相同。这种架构的优点在于所有的操作复杂度都是O(1)，而且读操作不会阻塞写操作，读操作之间也不会互相影响。 在不产生任何性能损失的情况下能够访问几乎无限的硬盘空间，Kafka 可以让消息保留相对较长的一段时间(比如一周)，而不是试图在被消费后立即删除。 降低大量小型IO操作的影响 小型的 I/O 操作发生在客户端和服务端之间以及服务端自身的持久化操作中。 为了避免这种情况，kafka的协议是建立在一个 “消息块” 的抽象基础上，合理将消息分","date":"2020-04-27","objectID":"/20200427_kafka/:2:4","tags":["Kafka","Distribution"],"title":"Kafka \u0026 Message Queue","uri":"/20200427_kafka/"},{"categories":["Technology"],"content":"Article description.","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"The major difference between the two is Replication/Fault Tolerance. HDFS was designed to survive failures. NFS does not have any fault tolerance built in. Other than fault tolerance, HDFS does support multiple replicas of files. This eliminates (or eases) the common bottleneck of many clients accessing a single file. Since files have multiple replicas, on different physical disks, reading performance scales better than NFS. ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:0:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"NFS NFS (Network File system): A protocol developed that allows clients to access files over the network. NFS clients allow files to be accessed as if the files reside on the local machine, even though they reside on the disk of a networked machine. In NFS, the data is stored only on one main system. All the other systems in that network can access the data stored in that as if it was stored in their local system. But the problem with this is that, if the main system goes down, then the data is lost and also, the storage depends on the space available on that system. ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:1:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"HDFS HDFS (Hadoop Distributed File System): A file system that is distributed amongst many networked computers or nodes. HDFS is fault tolerant because it stores multiple replicas of files on the file system, the default replication level is 3. In HDFS, data is distributed among different systems called datanodes. Here, the storage capacity is comparatively high. HDFS is mainly used to store Big Data and enable fast data transaction. ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:2:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"Similarities 两者的文件系统数据均能够在相关系统内的多台机器上进行数据读取和写入，都是分布式文件系统 ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:3:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"Differences NFS是通过RPC通信协议进行数据共享的文件系统，所以NFS必须在运行的同时确保RPC能够正常工作。在不同的文件进行读取和写入时，实际上是对服务端的共享文件地址进行操作，一旦服务端出现问题，那么其他所有的机器无法进行文件读取和写入，并且数据无法找回。所以NFS系统的文件其实并没有备份，并且其服务端没有做高可用处理。 HDFS是通过数据备份进行的大数据存储文件系统。HDFS有系统备份，并且其namenode有secondnamenode进行备份处理，更加安全可靠。数据在经过多副本存储后，能够抵御各种灾难，只要有一个副本不丢失，数据就不会丢失。所以数据的安全性很高。 ","date":"2020-03-26","objectID":"/20200326_hdfs-nfs/:4:0","tags":["Hdfs","NFS","Distribution"],"title":"HDFS \u0026 NFS","uri":"/20200326_hdfs-nfs/"},{"categories":["Technology"],"content":"Article description.","date":"2020-02-08","objectID":"/20200208_scalaintroduction/","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"Scala combines object-oriented and functional programming in one concise, high-level language. Scala’s static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries. ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:0:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"1. REPL \u0026 Scaladoc Scala解释器读到一个表达式，对它进行求值，将它打印出来，接着再继续读下一个表达式。这个过程被称做Read-Eval-Print-Loop，即：REPL。 从技术上讲，scala程序并不是一个解释器。实际发生的是，你输入的内容被快速地编译成字节码，然后这段字节码交由Java虚拟机执行。正因为如此，大多数scala程序员更倾向于将它称做“REPL” scala api文档，包含了scala所有的api以及使用说明，class、object、trait、function、method、implicit等 为什么要查阅Scaladoc：如果只是写一些普通的Scala程序基本够用了；但是如果（在现在，或者未来，实际的工作环境中）要编写复杂的scala程序，那么还是需要参考Scaladoc的。（纯粹用scala开发spark应用程序，应该不会特别复杂；用scala构建类似于spark的公司内的分布式的大型系统） 以下是一些Scaladoc使用的tips： 直接在左上角的搜索框中，搜索你需要的寻找的包、类即可 C和O，分别代表了类和伴生对象的概念 t和O，代表了特制(trait)(类似于Java的接口) 标记为implicit的方法，代表的是隐式转换 举例：搜索StringOps，可以看到String的增强类，StringOps的所有方法说明 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:1:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"2. Data Type ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:2:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"2.1 Data Type 数据类型 描述 Byte 8位有符号补码整数。数值区间为 -128 到 127 Short 16位有符号补码整数。数值区间为 -32768 到 32767 Int 32位有符号补码整数。数值区间为 -2147483648 到 2147483647 Long 64位有符号补码整数。数值区间为 -9223372036854775808 到 9223372036854775807 Float 32 位, IEEE 754 标准的单精度浮点数 Double 64 位 IEEE 754 标准的双精度浮点数 Char 16位无符号Unicode字符, 区间值为 U+0000 到 U+FFFF String 字符序列 Boolean true或false Unit 表示无值，和其他语言中void等同。用作不返回任何结果的方法的结果类型。Unit只有一个实例值，写成()。 Null null 或空引用 Nothing Nothing类型在Scala的类层级的最底端；它是任何其他类型的子类型。 Any Any是所有其他类的超类 AnyRef AnyRef类是Scala里所有引用类(reference class)的基类 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:2:1","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"2.2 val、var \u0026 Lazy Value 内容是否可变：val修饰的是不可变的，var修饰是可变的 val修饰的变量在编译后类似于java中的中的变量被final修饰 lazy修饰符可以修饰变量，但是这个变量必须是val修饰的 ps. lazy相当于延迟加载（懒加载），当前变量使用lazy修饰的时候，只要变量不被调用，就不会进行初始化，什么时候调用，什么时候进行初始化 lazy val words = scala.io.Source.fromFile(\"/usr/share/dict/words\").mkString //当val被声明为lazy时，它的初始化将被推迟，直到我们首次对他取值 懒值对于开销大的初始化语句十分有用。它还可以用来应对其他初始化问题，比如循环依赖。更重要的是，它是开发懒数据结构的基础。 val words = ... //在words被定义时即被取值 lazy val words = ... //在words被首次使用时取值 def words = ... //在每一次words被使用时取值 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:2:2","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"3. Control and Function if表达式也有值 块也有值：是它最后一个表达式的值 Scala的for循环就像是“增强版”的Java for循环 分好在绝大数情况下不是必须的 void类型是Unit 避免在函数使用return 注意别再函数式定义中使用return 异常的工作方式和Java或C++基本一样，不同的是你在catch语句中使用“模式匹配” Scala没有受检异常 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:3:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"4. Array 若长度固定可用Array，若长度可能有变化则使用ArrayBuffer 提供初始值时不要使用new 用()来访问元素 用for(elem \u003c- arr)来遍历元素 用for(elem \u003c- arr if …) yield … 来将原数据转型为新数组 Scala数组和Java数组可以相互操作，用ArrayBuffer，使用scala.collection.JavaConversions中的转换函数 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:4:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"5. Map and Tuple Scala有十分易用的语法来创建、查询和遍历映射(Map) 你需要从可变的和不可变的映射中做出选择 默认情况下，你得到的是一个哈希映射(Hash Map)，不过你也可以指明要树形映射 你可以很容易地在Scala映射和Java映射之间来回切换 元组可以用来聚集值 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:5:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"6. Class 类中的字段自动带有getter方法和setter方法 可以用定制的getter/setter方法替换掉字段的定义，而不必修改使用类的客户端——这就是所谓的“统一访问原则” 用@BeanProperty注解来生成JavaBeans的get*/set*方法 每个类都有一个主要的构造器，这个构造器和类定义\"交织\"在一起。它的参数直接为类的字段。主构造器执行类体中所有的语句 辅助构造器是可选的。他们叫做this ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:6:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"7. Object 对象作为单例或存放工具方法 类可以拥有一个同名的伴生对象 对象可以扩展类或特质 对象的apply方法通常用来构造伴生类的新实例 如果不想显示定义main方法，可以用扩展App特质的对象 可以通过扩展Enumeration对象来实现枚举 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:7:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"8. Package 包也可以像内部类那样嵌套 包路径不是绝对路径 包声明链x.y.z并不自动将中间包x和x.y变成可见 位于文件顶部不带花括号的包声明在整个文件范围内有效 包对象可以持有函数和变量 引入语句可以引入包、类和对象 引入语句可以出现在任何位置 引入语句可以重命名和隐藏特定成员 java.lang、scala和predef总是被引入 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:8:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"9. Extends extends、final关键字和Java中相同 重写方法时必须用override 只有主构造器可以用超类的主构造器 可以重写字段 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:9:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"10. File\u0026Regex Source.fromFile(…).getLines.toArray将交出文件的所有行 Source.fromFile(…).mkString将以字符串形式交出文件内容 将字符串转化为数字，可以用toInt或toDouble方法 使用Java的PrintWriter来写入文本文件 “正则”.r是一个Regex对象 如果你的正则表达式包含反斜杠的话，用\"\"\"…\"\"\" 如果正则模式包含分组，你可以用如下语法来提取它们的内容for(regex(变量1,…,变量n) \u003c- 字符串) ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:10:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"11. Feature 类可以实现任意数量的特质 特质可以要求实现它们的类具备特定的字段、方法或超类 和Java接口不同，Scala特质可以提供方法和字段的实现 当将多个特质叠加在一起时，顺序很重要——其方法先被执行啊的特质排在更后面 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:11:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"12. Advanced function 函数可以直接赋值给变量，就和数字一样 可以创建匿名函数，通常还会把它们交给其他函数 函数参数可以给出需要稍后执行的行为 许多集合方法都接受函数参数，将函数应用到集合中的值 有很多语法上的简写让你以简短且易读的方式表达函数参数 可以创建操作代码块的函数，它们看上去就像是内建的控制语句 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:12:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"13. Collection 所有集合都扩展自Iterable特质 集合有三大类，分别为序列、集合映射 对于几乎所有集合类，Scala都同时提供了可变的和不可变的版本 Scala列表要么是空的，要么拥有一头一尾，其中尾部本身又是一个列表 集是无先后次序的集合 用LinkedHashSet保留插入顺序，或者用SortedSet按顺序进行迭代 +将元素添加到无先后次序的集合中；+:和:+向前或向后追加到序列；++将两个集合串接在一起；-和–移除元素 映射、折叠和拉链操作是很有用的技巧，用来将函数或操作应用到集合中的元素 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:13:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"14. Pattern match match表达式是一个更好的switch，不会有意外掉入下一个分支的问题 如果没有模式能够匹配，会抛出MatchError。可以用case _模式来避免 模式可以包含一个随意定义的条件，称作守卫（guard） 可以对表达式的类型进行匹配；优先选择模式匹配而不是isInstanceOf/asInstanceOf 可以匹配数组、元组和样例类的模式，然后将匹配到的不同部分绑定到变量 在for表达式中，不能匹配的情况会被安静地跳过 样例类是是编译器会为之自动产出模式匹配所需要的方法的类 样例类继承层级中的公共超类应该是sealed的 用Option来存放对于可能存在也可能不存在的值——比null更安全 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:14:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"15. Annotation 可以为类、方法、字段、局部变量、参数、表达式、类型参数以及各种类型定义添加注解 对于表达式和类型，注解跟在被注解的条目之后 注解的形式有 @Annotation、@Annotation(value)或@Annotation(name = value1, …) @volatile、@transient、@strictfp和@native分别生成等效的Java修饰符 用@throws来生成与Java兼容的throws规格说明 @tailrec注解让你校验某个递归函数使用了尾递归优化 assert函数利用了@elidable注解。你可以选择从Scala程序中移除所有断言 用@deprecated注解来标记已过时的特性 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:15:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"16. XML XML字面量this的类型为NodeSeq 可以在XML里字面量中嵌套Scala代码 Node的child属性交出的是子节点 Node的attributes属性交出的是包含节点属性的MetaData对象 \\和\\操作符执行类XPath匹配 ","date":"2020-02-08","objectID":"/20200208_scalaintroduction/:16:0","tags":["Scala","Java"],"title":"Scala Introduction","uri":"/20200208_scalaintroduction/"},{"categories":["Technology"],"content":"Article description.","date":"2020-01-22","objectID":"/20200122_java-static/","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"The static keyword can be used for variables, methods, code blocks, and inner classes to indicate that a particular member belongs only to a class itself, and not to an object of that class. ","date":"2020-01-22","objectID":"/20200122_java-static/:0:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"1. Static Variable 静态变量也叫类变量，它属于一个类，而不是这个类的对象。 public class Writer { private String name; private int age; public static int countOfWriters; public Writer(String name, int age) { this.name = name; this.age = age; countOfWriters++; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } } 其中，countOfWriters 被称为静态变量，它有别于 name 和 age 这两个成员变量，因为它前面多了一个修饰符 static。 这意味着无论这个类被初始化多少次，静态变量的值都会在所有类的对象中共享。 Writer w1 = new Writer(\"沉默王二\",18); Writer w2 = new Writer(\"沉默王三\",16); System.out.println(Writer.countOfWriters); 按照上面的逻辑，你应该能推理得出，countOfWriters 的值此时应该为 2 而不是 1。从内存的角度来看，静态变量将会存储在 Java 虚拟机中一个名叫“Metaspace”（元空间，Java 8 之后）的特定池中。 静态变量和成员变量有着很大的不同，成员变量的值属于某个对象，不同的对象之间，值是不共享的；但静态变量不是的，它可以用来统计对象的数量，因为它是共享的。就像上面例子中的 countOfWriters，创建一个对象的时候，它的值为 1，创建两个对象的时候，它的值就为 2。 Summary: 由于静态变量属于一个类，所以不能通过对象引用来访问，而应该直接通过类名来访问； w1.countOfWriters #不应该通过类实例访问静态成员 不需要初始化类就可以访问静态变量。 public static void main(String[] args) { System.out.println(Writer.countOfWriters); // 输出 0 } ","date":"2020-01-22","objectID":"/20200122_java-static/:1:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"2. Static Method 静态方法也叫类方法，它和静态变量类似，属于一个类，而不是这个类的对象。 public static void setCountOfWriters(int countOfWriters) { Writer.countOfWriters = countOfWriters; } setCountOfWriters() 就是一个静态方法，它由 static 关键字修饰。 如果你用过 java.lang.Math 类或者 Apache 的一些工具类（比如说 StringUtils）的话，对静态方法一定不会感动陌生。 Math 类的几乎所有方法都是静态的，可以直接通过类名来调用，不需要创建类的对象。 Math. random() abs(int a) sin(double a) cos(double a) ... Summary: Java 中的静态方法在编译时解析，因为静态方法不能被重写（方法重写发生在运行时阶段，为了多态）。 抽象方法不能是静态的。 static abstract void paly(); #修饰符abstract 和 static的组合非法 静态方法不能使用 this 和 super 关键字。 成员方法可以直接访问其他成员方法和成员变量。 成员方法也可以直接方法静态方法和静态变量。 静态方法可以访问所有其他静态方法和静态变量。 静态方法无法直接访问成员方法和成员变量。 ","date":"2020-01-22","objectID":"/20200122_java-static/:2:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"3. Static Code Block 静态代码块可以用来初始化静态变量，尽管静态方法也可以在声明的时候直接初始化，但有些时候，我们需要多行代码来完成初始化。 public class StaticBlockDemo { public static List\u003cString\u003e writes = new ArrayList\u003c\u003e(); static { writes.add(\"a\"); writes.add(\"b\"); writes.add(\"c\"); System.out.println(\"第一块\"); } static { writes.add(\"d\"); writes.add(\"e\"); System.out.println(\"第二块\"); } } writes 是一个静态的 ArrayList，所以不太可能在声明的时候完成初始化，因此需要在静态代码块中完成初始化。 Summary: 一个类可以有多个静态代码块。 静态代码块的解析和执行顺序和它在类中的位置保持一致。为了验证这个结论，可以在 StaticBlockDemo 类中加入空的 main 方法，执行完的结果如下所示： 第一块 第二块 ","date":"2020-01-22","objectID":"/20200122_java-static/:3:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"4. Static Inner Class Java 允许我们在一个类中声明一个内部类，它提供了一种令人信服的方式，允许我们只在一个地方使用一些变量，使代码更具有条理性和可读性。 常见的内部类有四种，成员内部类、局部内部类、匿名内部类和静态内部类。 静态内部类： public class Singleton { private Singleton() {} private static class SingletonHolder { public static final Singleton instance = new Singleton(); } public static Singleton getInstance() { return SingletonHolder.instance; } } 以上这段代码是不是特别熟悉，对，这就是创建单例的一种方式，第一次加载 Singleton 类时并不会初始化 instance，只有第一次调用 getInstance() 方法时 Java 虚拟机才开始加载 SingletonHolder 并初始化 instance，这样不仅能确保线程安全也能保证 Singleton 类的唯一性。不过，创建单例更优雅的一种方式是使用枚举。 Summary: 静态内部类可以访问外部类的所有成员变量，包括私有变量。 外部类不能声明为 static。 ","date":"2020-01-22","objectID":"/20200122_java-static/:4:0","tags":["Java"],"title":"Static Keyword in Java","uri":"/20200122_java-static/"},{"categories":["Technology"],"content":"Article description.","date":"2020-01-14","objectID":"/20200114_leetcode/","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Algorithms are used in every part of computer science. They form the field’s backbone. In computer science, an algorithm gives the computer a specific set of instructions, which allows the computer to do everything, be it running a calculator or running a rocket. 异或运算 特点： 与自己异或等于0，与0异或等于自己 遵循交换律和结合律 例： //1.两数交换： arr[i] = arr[i] ^ arr[j]; // a = a ^ b arr[j] = arr[i] ^ arr[j]; // b = (a ^ b) ^ b = a arr[i] = arr[i] ^ arr[j]; // a = (a ^ b) ^ a = b //2.数组中有一个出现奇数次的数 int eor = 0; for(int cur : arr){ eor ^= cur; //[a,a,b,b,c,c,c] =\u003e a^a^b^b^c^c^c = c } System.out.println(eor); //3.数组中有两个出现奇数次的数 int eor = 0; for(int cur : arr){ eor ^= cur; //[a,a,b,b,b,c,c,c] =\u003e a^a^b^b^b^c^c^c = b^c } //eor = b ^ c //eor != 0 //eor必然有个一位置上是1, 说明这个位置上b和c不同 int rightOne = eor \u0026 (~eor + 1); //（取反 + 1） \u0026 自己 = 提取出只保留最右侧的1 int result1 = 0; for(int cur : arr){ if((cur \u0026 rightOne) == 0){ //已知在rightOne这个位置上b和c不同，通过(\u0026 rightOne)把b和c区分开,别的数是偶数个，异或起来是0不同管 result1 ^= cur; //这样result1是b或者c } } int result2 = eor ^ result1; ","date":"2020-01-14","objectID":"/20200114_leetcode/:0:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"排序 冒泡排序 bubble_sort 选择排序 select_sort ","date":"2020-01-14","objectID":"/20200114_leetcode/:1:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"插入排序 insert-sort 概念： 将后部分的数据(从第二开始)按照一定的顺序一个一个的插入到前部分有序的表中 时间复杂度：O(N^2) 额外空间复杂度：O(1) 数据状况不同，时间复杂度不同: 时间复杂度是按照最差情况考虑 public static void insertionSort(int[] arr) { if (arr == null || arr.length \u003c 2){ return; } for (int i = 1; i \u003c arr.length; i++){ for(int j = i - 1; j \u003e= 0 \u0026\u0026 arr[j] \u003e arr[j+1]; j--){ swap(arr, j, j+1); } } } public static void swap(int[] arr, int i, int j){ arr[i] = arr[i] ^ arr[j]; arr[j] = arr[i] ^ arr[j]; arr[i] = arr[i] ^ arr[j]; } 递归 public static int getMax(int[] arr){ return process(arr, 0, arr.length - 1); } public static int process(int[] arr, int L, int R){ if(L == R){ return arr[L]; } int mid = L + ((R - L) \u003e\u003e 1); int leftMax = process(arr, L, mid); int rightMax = process(arr, mid + 1, R); return Math.max(leftMax, rightMax); } 递归时间复杂度估算 Master公式： T(N) = a * T (N / b) + 0 (N ^ d) log(b, a) \u003e d, 则复杂度为O(N ^ log(b, a)) log(b, a) = d, 则复杂度为O(N ^ d * logN) log(b, a) \u003c d, 则复杂度为O(N ^ d) ","date":"2020-01-14","objectID":"/20200114_leetcode/:1:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"归并排序 merge-sort 概念：将n个元素分成个含n/2个元素的子序列，用合并排序法对两个子序列递归的排序，合并两个已排序的子序列已得到排序结果。 时间复杂度：使用Master公式： T(N) = NlogN 代码实现： public static void process(int[] arr, int L, int R){ if(L == R){ return; } int mid = L + ((R - L) \u003e\u003e 1); process(arr, L, mid); process(arr, mid + 1, R); merge(arr, L, mid, R); } public static void merge(int[] arr, int L, int M, int R){ int[] extra = new int[R - L + 1]; int i = 0; int p1 = L; int p2 = M + 1; while(p1 \u003c= M \u0026\u0026 p2 \u003c= R){ extra[i++] = arr[p1] \u003c= arr[p2] ? arr[p1++]: arr[p2++]; } while(p1 \u003c= M){ extra[i++] = arr[p1++]; } while(p2 \u003c= R){ extra[i++] = arr[p2++]; } for(i = 0; i \u003c extra.length; i++){ arr[L + i] = extra[i]; } } 小和问题、逆序对问题 [1, 3, 4, 2, 5] 的小和是 0 + 1 + 4 + 1 +10 = 16（ 在一个数组中,每一个数左边比当前数小的数累加起来,叫做这个数组的小和。） 归并排序思路，目的就是减少对比次数。增加一个外部空间，记录在一次merge中的小和。 public static int process(int[] arr, int l, int r){ if (l == r){ return 0; } int mid = 1 + ((r - l) \u003e\u003e 1); return process(arr, 1 , mid) + process(arr, mid + 1 , r) + merge(arr, 1 , mid, r); } public static void merge(int[] arr, int L, int mid, int R){ int[] extr = new int[R - L + 1]; int i = 0; int p1 = L;1 int p2 = M + 1; int res = 0; while(p1 \u003c= M \u0026\u0026 p2 \u003c= R){ res += arr[p1] \u003c arr[p2] ? (r - p2 + 1) * arr[p1] : 0 extr[i++] = arr[p1] \u003c arr[p2] ? arr[p1++]: arr[p2++]; } while(p1 \u003c= M){ extr[i++] = arr[p1++]; } while(p2 \u003c= R){ extr[i++] = arr[p2++]; } for(i = 0; i \u003c extra.length; i++){ arr[L + i] = extra[i]; } return res; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:1:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"快速排序 quick_sort 时间复杂度：O(NlogN)，因为有随机概率行为，所以时间复杂度为期望值。 空间复杂度：O(logN) 荷兰国旗问题 问题1：给一个 arr 和 num，要求把小于等num的数放左，大于num放右，时间复杂度为O(N)，空间为O(1) 问题2：给一个 arr 和 num，要求把小于等num的数放左，等于放中间，大于num放右，时间复杂度为O(N)，空间为O(1) public static void quickSort(int[] arr, int L, int R){ if(L \u003c R){ swap(arr, L + (int)(Math.random() * (R - L + 1)), R);//随机选一个数和最后位置交换 int[] p = partition(arr, L, R);//=区域的左右边界 quickSort(arr, L, p[0] - 1); quickSort(arr, p[1] + 1, R); } } public static int[] partition(int[] arr, int L, int R){ int less = L - 1;// \u003c区域的右边界 int more = R; //\u003e区的左边界 while(L \u003c more){ // L表示当前数的位置 if(arr[L] \u003c arr[R]){ swap(arr, ++less, L++); }else if(arr[L] \u003e arr[R]){ swap(arr, --more, L); }else{ L++; } } swap(arr, more, R); return new int[]{ less + 1, more }; } private static void swap(int[] arr, int i, int j) { if(i != j){ arr[i] = arr[i] ^ arr[j]; arr[j] = arr[i] ^ arr[j]; arr[i] = arr[i] ^ arr[j]; } } ","date":"2020-01-14","objectID":"/20200114_leetcode/:1:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"堆排序 heap_sort 时间复杂度：O(NlogN) 大根堆：在完全二叉树（最后一层的结点都连续集中在最左边，其余层满节点）的前提下，每一棵子树的最大值是头结点的数。 小根堆：在完全二叉树（最后一层的结点都连续集中在最左边，其余层满节点）的前提下，每一棵子树的最小值是头结点的数。 优先级队列结构，就是堆结构 //处在index位置上的数，往上继续移动 public static void heapInsert(int[] arr, int index){ while(arr[index] \u003e arr[(index - 1) / 2]) { swap(arr, index, (index - 1) / 2); index = (index - 1) / 2; } } 返回最大值（arr[0]上的数），并删除最大值： 返回根节点。 把 arr[heapSize] 的数放到根节点。 heapSize–，最后一个节点即与堆断连。 while 循环，与子节点比较，小则替换，直到满足大根堆。 // 处在index位置上的数，往下继续移动 public static void heapify(int[] arr, int index, int heapSize){ int left = index * 2 + 1; //节点左孩子下标 while(left \u003c heapSize){ //判断下方是否有孩子 //两个孩子中，谁的值大，把下标给最大的 int largest = left + 1 \u003c heapSize \u0026\u0026 arr[left + 1] \u003e arr[left] ? left + 1 : left; //节点与孩子比较，谁的值大，把下标给最大的 largest = arr[largest] \u003e arr[index] ? largest : index; if(largest == index){ break; } swap(arr, largest, index); index = largest; left = index * 2 + 1; } } HeapSort public static void heapSort(int[] arr){ if(arr == null || arr.length \u003c 2){ return; } for(int i = 0; i \u003c arr.length; i++){ heapInsert(arr, i);//数组整体范围变成大根堆 } int heapSize = arr.length; swap(arr, 0, --heapSize);//大根堆的root(最大值)放到arr最后 while(heapSize \u003e 0){ heapify(arr, 0, heapSize); swap(arr, 0, --heapSize); } } 一个数组如果把它排好序的话，每个元素移动的距离不超过k，k相对于数组长度比较小，选择一个合适的排序算法排序 方法： 最小堆排方法，先取出前k个数字组成最小堆，根位置为0位置上的数；添加一个数，堆化，根位置为第二个位置上的数。以此类推 PriorityQueue\u003cInteger\u003e heap = new PriorityQueue\u003c\u003e();//优先级队列结构，就是堆结构 扩容方式：双倍扩容 public void sortedArrDistanceLessK(int[] arr, int k){ PriorityQueue\u003cInteger\u003e heap = new PriorityQueue\u003c\u003e();//默认小根堆 int index = 0; for(; index \u003c= Math.min(arr.length, k); index++){//生成前k个数的最小堆 heap.add(arr[index]); } int i = 0; for(;index \u003c arr.length; i++, index++){//弹一个，放入一个 heap.add(arr[index]); arr[i] = heap.poll(); } while(!heap.isEmpty()){ arr[i++] = heap.poll(); } } ","date":"2020-01-14","objectID":"/20200114_leetcode/:1:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"桶排序 计数排序： 基数排序： radix_sort public static void radixSort(int[] arr, int L, int R, int digit){//digit: 最大数的位数 final int radix = 10; int i = 0, j = 0; int[] bucket = new int[R - L + 1];//有多少个数准备多少个辅助空间 for(int d = 1; d \u003c= digit; d++){//最大位数有多少位就进出桶多少次 int[] count = new int[radix]; //count[0..9] //count[0] 当前(d)位是0的数字有多少个 //count[1] 当前(d)位小于等于1的数字有多少个 //count[2] 当前(d)位小于等于2的数字有多少个 //count[i] 当前(d)位小于等于i的数字有多少个 for(i = L; i \u003c= R; i++){ count[getDigit(arr[i], d)]++;//先统计每个数字(num)出现了几次(times)，先记录在count[num] = times } for(i = 1; i \u003c radix; i++){ count[i] = count[i] + count[i - 1];//再统计小于等于出现的次数 } for(i = R; i \u003e= L; i--){//从右向左 j = getDigit(arr[i], d); bucket[count[j] - 1] = arr[i]; } for(i = L, j = 0; i \u003c= R; i++, j++){ arr[i] = bucket[j];//将桶中数据倒回数组 } } } ","date":"2020-01-14","objectID":"/20200114_leetcode/:1:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"总结 时间复杂度 空间复杂度 稳定性 选择 O(N2) O(1) x 冒泡 O(N2) O(1) √ 插入 O(N2) O(1) √ 归并 O(NlogN) O(N) √ 快排 O(NlogN) O(logN) x 堆排 O(NlogN) O(1) x 一般优先选择 快速排序，快排的复杂度在常数项是最低的。 对空间有要求选择 堆排序。 对稳定性有要求选择 归并排序。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:1:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 排序 ","date":"2020-01-14","objectID":"/20200114_leetcode/:2:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"快速选择 用于求解 Kth Element 问题，也就是第 K 个元素的问题。 可以使用快速排序的 partition() 进行实现。需要先打乱数组，否则最坏情况下时间复杂度为 O(N2)。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:2:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"堆 用于求解 TopK Elements 问题，也就是 K 个最小元素的问题。使用最小堆来实现 Top 问题，最小堆使用大顶堆来实现，大顶堆的堆顶元素为当前的最大元素。 实现过程：不断地往大顶堆中插入新元素，当堆中元素的数量大于 k 时，移除堆顶元素，也就是当前堆中最大的元素，剩下的元素都为当前添加过的元素中最小的K个元素。插入和移除堆顶元素的时间复杂度都为 log2N 快速选择也可以求解 TopK Elements 问题，因为找到了 Kth Element 之后，所有小于等于 Kth Element 的元素都是 TopK Elements。 快速选择和堆排序都可以求解 Kth Element 和 TopK Elements 问题。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:2:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目一：Kth Element Kth Largest Element in an Array (Medium) Input: [3,2,1,5,6,4] and k = 2 Output: 5 题目描述：找到倒数第 k 个的元素。 解题： 方法一：排序：时间复杂度 O(NlogN)，空间复杂度 O(1) sort() 方法：元素少于NSERTION_SORT_THRESHOLD⽤插⼊排序，大于NSERTION_SORT_THRESHOLD，使用快排。 public int findKthLargest(int[] nums, int k) { Arrays.sort(nums); return nums[nums.length - k]; } 方法二：堆：时间复杂度 O(NlogK)，空间复杂度 O(K)。 public int findKthLargest(int[] nums, int k) { PriorityQueue\u003cInteger\u003e pq = new PriorityQueue\u003c\u003e(); //小项堆 for(int val : nums){ pq.add(val); if(pq.size() \u003e k) pq.poll(); //维护堆的大小为K，弹出最小项，留下的 k 项为数组中最大的。 } return pq.peek(); } 方法三：快速选择 ：时间复杂度 O(N)，空间复杂度 O(1) public int findKthLargest(int[] nums, int k) { k = nums.length - k; int l = 0, h = nums.length - 1; while (l \u003c h) { int j = partition(nums, l, h); if (j == k) { break; } else if (j \u003c k) { l = j + 1; } else { h = j - 1; } } return nums[k]; } private int partition(int[] a, int l, int h) { int i = l, j = h + 1; while (true) { while (a[++i] \u003c a[l] \u0026\u0026 i \u003c h) ; while (a[--j] \u003e a[l] \u0026\u0026 j \u003e l) ; if (i \u003e= j) { break; } swap(a, i, j); } swap(a, l, j); return j; } private void swap(int[] a, int i, int j) { int t = a[i]; a[i] = a[j]; a[j] = t; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:2:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目二：出现频率最多的 k 个元素 Top K Frequent Elements (Medium) Given [1,1,1,2,2,3] and k = 2, return [1,2]. 解题：桶排序 设置若干个桶，每个桶存储出现频率相同的数。桶的下标表示数出现的频率，即第 i 个桶中存储的数出现的频率为 i。 把数都放到桶之后，从后向前遍历桶，最先得到的 k 个数就是出现频率最多的的 k 个数。 public int[] topKFrequent(int[] nums, int k) { Map\u003cInteger, Integer\u003e frequencyForNum = new HashMap\u003c\u003e(); for(int num : nums){ frequencyForNum.put(num, frequencyForNum.getOrDefault(num, 0) + 1); } List\u003cInteger\u003e[] buckets = new ArrayList[nums.length + 1]; for(int key : frequencyForNum.keySet()){ int frequency = frequencyForNum.get(key); if (buckets[frequency] == null){ buckets[frequency] = new ArrayList\u003c\u003e(); } buckets[frequency].add(key); } List\u003cInteger\u003e topK = new ArrayList\u003c\u003e();//res for(int i = buckets.length - 1; i \u003e= 0 \u0026\u0026 topK.size() \u003c k; i--){ if(buckets[i] == null){ continue; } if(buckets[i].size() \u003c= (k - topK.size())){ topK.addAll(buckets[i]); }else{ topK.addAll(buckets[i].subList(0, k - topK.size())); } } int[] res = new int[k]; for (int i = 0; i \u003c k; i++) { res[i] = topK.get(i); } return res; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:2:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目三：按照字符出现次数对字符串排序 Sort Characters By Frequency (Medium) Input: \"tree\" Output: \"eert\" Explanation: 'e' appears twice while 'r' and 't' both appear once. So 'e' must appear before both 'r' and 't'. Therefore \"eetr\" is also a valid answer. 解题： public String frequencySort(String s) { Map\u003cCharacter, Integer\u003e frequencyForNum = new HashMap\u003c\u003e(); for (char c : s.toCharArray()) frequencyForNum.put(c, frequencyForNum.getOrDefault(c, 0) + 1); List\u003cCharacter\u003e[] frequencyBucket = new ArrayList[s.length() + 1]; for (char c : frequencyForNum.keySet()) { int f = frequencyForNum.get(c); if (frequencyBucket[f] == null) { frequencyBucket[f] = new ArrayList\u003c\u003e(); } frequencyBucket[f].add(c); } StringBuilder str = new StringBuilder(); for (int i = frequencyBucket.length - 1; i \u003e= 0; i--) { if (frequencyBucket[i] == null) { continue; } for (char c : frequencyBucket[i]) { for (int j = 0; j \u003c i; j++) { str.append(c); } } } return str.toString(); } ","date":"2020-01-14","objectID":"/20200114_leetcode/:2:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"荷兰国旗问题：快排 partition Input: [2,0,2,1,1,0] Output: [0,0,1,1,2,2] 解题： public void sortColors(int[] nums) { int zero = -1, one = 0, two = nums.length; while (one \u003c two) { if (nums[one] == 0) { swap(nums, ++zero, one++); } else if (nums[one] == 2) { swap(nums, --two, one); } else { ++one; } } } private void swap(int[] nums, int i, int j) { int t = nums[i]; nums[i] = nums[j]; nums[j] = t; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:2:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 二分查找 时间复杂度：O(logN) (logN默认指log以2为底) ","date":"2020-01-14","objectID":"/20200114_leetcode/:3:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"求中间数 mid = (L + R) / 2 // L + R 可能会越界65535，这时mid算出负数 mid = L + (R - L) / 2 //不会越界 mid = L + (R - L) \u003e\u003e 1 //右移一位比除2快 ","date":"2020-01-14","objectID":"/20200114_leetcode/:3:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"binarySearch public int binarySearch(int[] nums, int key) { int l = 0, h = nums.length; while (l \u003c h) { //int m = l + (h - l) / 2; int m = l + (h - l) \u003e 2; if (nums[m] \u003e= key) { h = m; } else { l = m + 1; } } return l; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:3:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目一：求开方 Sqrt(x) (Easy) Input: 4 Output: 2 Input: 8 Output: 2 Explanation: The square root of 8 is 2.82842..., and since we want to return an integer, the decimal part will be truncated. 解题： 一个数 x 的开方 sqrt 一定在 0 ~ x 之间，并且满足 sqrt == x / sqrt。可以利用二分查找在 0 ~ x 之间查找 sqrt。 对于 x = 8，它的开方是 2.82842…，最后应该返回 2 而不是 3。在循环条件为 l \u003c= h 并且循环退出时，h 总是比 l 小 1，也就是说 h = 2，l = 3，因此最后的返回值应该为 h 而不是 l。 public int mySqrt(int x){ if(x \u003c= 1){ return x; } int l = 1, h = x; while(l \u003c= h){ int mid = l + (h - l) \u003e\u003e 2; int sqrt = x / mid; if (sqrt == mid){ return mid; } else if (mid \u003e sqrt){ h = mid - 1; } else { l = mid + 1; } } return h; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:3:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目二：大于给定元素的最小元素 Find Smallest Letter Greater Than Target (Easy) Input: letters = [\"c\", \"f\", \"j\"] target = \"d\" Output: \"f\" Input: letters = [\"c\", \"f\", \"j\"] target = \"k\" Output: \"c\" 解题： public char nextGreatestLetter(char[] letters, char target) { int n = letters.length; int l = 0, h = n - 1; while (l \u003c= h) { int m = l + (h - l) / 2; if (letters[m] \u003c= target) { l = m + 1; } else { h = m - 1; } } return l \u003c n ? letters[l] : letters[0]; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:3:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目三：有序数组的 Single Element in a Sorted Array (Medium) Input: [1, 1, 2, 3, 3, 4, 4, 8, 8] Output: 2 题目描述：一个有序数组只有一个数不出现两次，找出这个数。要求以 O(logN) 时间复杂度求解，因此不能遍历数组并进行异或操作来求解，这么做的时间复杂度为O(N) 解题： 令 index 为 Single Element 在数组中的位置。在 index 之后，数组中原来存在的成对状态被改变。如果 m 为偶数，并且 m + 1 \u003c index，那么 nums[m] == nums[m + 1]；m + 1 \u003e= index，那么 nums[m] != nums[m + 1]。 public int singleNonDuplicate(int[] nums) { int l = 0, h = nums.length - 1; while (l \u003c h) { int m = l + (h - l) / 2; if (m % 2 == 1) { m--; // 保证 l/h/m 都在偶数位，使得查找区间大小一直都是奇数 } if (nums[m] == nums[m + 1]) { l = m + 2; } else { h = m; } } return nums[l]; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:3:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目：找一个局部最小（i-1 \u003c i \u003c i+1）的数，复杂度小于 O(N) 例题： 在一个有序数组中，找某个数是否存在 在一个有序数组中，找 \u003e= 某个数最左侧的位置：二分查找直到左侧没有数 无序，相邻数一定不相等，找一个局部最小(i-1 \u003c i \u003c i+1)的数，复杂度能否好于O(N) 先判断首尾项是否满足要求，若首尾项不满足要求，一定是\\进/出，中间一定有低谷拐点。 然后判断中间点是否满足要求，若不满足，分为两种情况： 1. 斜坡，则一方可与起点或终点组成\\进/出。 2. 顶峰，则两方向都满足\\进/出条件，两个方向都可继续进行二分法 直到判断出某个二分法中点满足要求。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:3:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 哈希表 - 有序表 哈希表介绍 哈希表使用层面上可以理解为一种集合结构。 有无伴随数据，是HashMap和HashSet唯一的区别，底层的数据结构一样。 使用哈希表 put，remove，put，get 的操作，可以认为时间复杂度为O(1)，但常数时间比较大。 哈希表使用 O(N) 空间复杂度存储数据，并且以 O(1) 时间复杂度求解问题。 Java 中的 HashSet 用于存储一个集合，可以查找元素是否在集合中。如果元素有穷，并且范围不大，那么可以用一个布尔数组来存储一个元素是否存在。例如对于只有小写字符的元素，就可以用一个长度为 26 的布尔数组来存储一个字符集合，使得空间复杂度降低为 O(1)。 Java 中的 HashMap 主要用于映射关系，从而把两个元素联系起来。HashMap 也可以用来对元素进行计数统计。在对一个内容进行压缩或者其他转换时，利用 HashMap 可以把原始内容和转换后的内容联系起来。例如在一个简化 url 的系统中。 利用 HashMap 就可以存储精简后的 url 到原始 url 的映射，使得不仅可以显示简化的 url，也可以根据简化的 url 得到原始 url 从而定位到正确的资源。 有序表介绍 有序表在使用层面上可以理解为一种集合结构。 有无伴随数据，是TreeSet和TreeMap唯一的区别，底层的数据结构一样。 有序表和哈希表的区别是，有序表把key按照顺序组织起来，而哈希表完全不组织。 红黑树、AVL树、size-balance-tree和跳表都属于有序表结构，只是底层具体实现不同。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:4:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"1. 数组中两个数的和为给定值 Two Sum (Easy) 解题： 可以先对数组进行排序，然后使用双指针方法或者二分查找方法。这样做的时间复杂度为 O(NlogN)，空间复杂度为O(1)。 用 HashMap 存储数组元素和索引的映射，在访问到 nums[i] 时，判断 HashMap 中是否存在 target - nums[i]，如果存在说明 target - nums[i] 所在的索引和 i 就是要找的两个数。该方法的时间复杂度为 O(N)，空间复杂度为O(N)，使用空间来换取时间。 public int[] twoSum(int[] nums, int target){ HashMap\u003cIntger, Intger\u003e indexForNum = new HashMap\u003c\u003e(); for(int i = 0; i \u003c nums.length; i++){ if(indexForNum.containsKey(target - nums[i])){ return new int[]{indexForNum.get(target - nums[i]), i}; } else { indexForNum.put(nums[i], i); } } return null; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:4:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"2. 判断数组是否含有重复元素 Contains Duplicate (Easy) public boolean containsDuplicate(int[] nums){ Set\u003cInteger\u003e set = new HashSet\u003c\u003e(); for (int num : nums){ set.add(num); } return set.size() \u003c nums.length; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:4:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"3. 最长和谐序列 Longest Harmonious Subsequence (Easy) 和谐序列中最大数和最小数之差正好为 1，应该注意的是序列的元素不一定是数组的连续元素。 Input: [1,3,2,2,5,2,3,7] Output: 5 Explanation: The longest harmonious subsequence is [3,2,2,2,3]. public int findLHS(int[] nums){ Map\u003cInteger, Integer\u003e countForNum = new HashMap\u003c\u003e(); for(int num : nums){ countForNum.put(num, countForNum.getOrDefault(nums, 0) + 1); } int longest = 0; for(int num : countForNum.keySet()){ if(countForNum.containsKey(num + 1)){ longest = Math.max(longest, countForNum.get(num + 1) + countForNum.get(num)); } } return longest; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:4:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"4. 最长连续序列 Longest Consecutive Sequence (Hard) ","date":"2020-01-14","objectID":"/20200114_leetcode/:4:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"链表 笔试 面试区分，面试时需考虑空间复杂度。 题目一：判断一个链表是否回文 方法一：笔试用：进栈出栈(先进后出) public static boolean isPalindrome1(Node head) { Stack\u003cNode\u003e stack = new Stack\u003cNode\u003e(); Node cur = head; while (cur != null) { stack.push(cur); cur = cur.next; } while (head != null) { if (head.value != stack.pop().value) { return false; } head = head.next; } return true; } 方法2：快慢指针：空间复杂度O(1)，使用了有限几个变量 public static boolean isPalindrome3(Node head) { if (head == null || head.next == null) { return true; } Node n1 = head; Node n2 = head; while (n2.next != null \u0026\u0026 n2.next.next != null) { n1 = n1.next; //n1 -\u003e mid n2 = n1.next.next; //n2 -\u003e end } n2 = n1.next; //n2 -\u003e right part first node Node n3 = null; while (n2 != null) { //right part convert n3 = n2.next; // save next node n2.next = n1; // right direct to left direct n1 = n2; n2 = n3; } n3 = n1; // last node n2 = head; // fist node boolean res = true; while (n1 != null \u0026\u0026 n2 != null) { if (n1.value != n2.value) { res = false; break; } n1 = n1.next; n2 = n2.next; } n1 = n3.next; // recover list n3.next = null; while (n1 != null) { n2 = n1.next; n1.next = n3; n3 = n1; n1 = n2; } return res; } 题目二：单链表按某值划分成左边小，中间相等，右边大 方法一：（笔试）放到数组里，在数组里partition 方法二：（面试）创建6个空节点，每两个一组，作为三个区域的首尾节点。遍历原链表，放到不同的区域并调整各组首尾节点，最后三个拼装。 题目三：复制含有随机指针节点的链表 class Node { int value; Node next; Node rand; Node(int val) { value = val; } } 方法一：hashmap public static Node copyListWithRand1(Node head) { HashMap\u003cNode, Node\u003e map = new HashMap\u003cNode, Node\u003e(); Node cur = head; while (cur != null) { map.put(cur, new Node(cur.value)); cur = cur.next; } cur = head; while (cur != null) { map.get(cur).next = map.get(cur.next); map.get(cur).rand = map.get(cur.rand); cur = cur.next; } return map.get(head); } 方法二：利用位置关系省去哈希表。 当前节点的下一个就放克隆节点。 curCopy = cur.next; curCopy.rand = cur.rand != null ? cur.rand.next:null; 跳过旧链表。 题目四：两个单链表相交的一系列问题 先判断有无环： 方法一：Hashset。get一个，看之前是否加入过，否则put进set。 方法二：快慢指针。不会走到空节点，快慢指针一定会相遇，而且快指针在环中不会超过两圈。相遇后快指针回到开头，然后两个指针都走一步，一定会在入环节点相遇。 情况一：两链表都无环 判断链表长度 判断end节点是否是一个，不同则不相交。 长链表先走差值步，然后两链表一起走，一定会一起走到第一个相交点。 情况二：两链表都有环 情况二之一：链表无交集 情况二之二：入环节点是同一个节点。 情况二之三：入环节点不同。 public static Node bothLoop(Node head1, Node head2, Node loop1, Node loop2) {//两链表头节点，入环节点 Node cur1 = null; Node cur2 = null; if (loop1 == loop2) { //情况二之二：入环节点是同一个节点。 cur1 = head1; cur2 = head2; int n = 0; while (cur1 != loop1) { //判断到达入环节点的长度 n++; cur1 = cur1.next; } while (cur2 != loop2) { //判断是1链表头结点到入环节点长度和2链表到入环节点长度的差值 n--; cur2 = cur2.next; } cur1 = n \u003e 0 ? head1 : head2; cur2 = cur1 == head1 ? head2 : head1; n = Math.abs(n); while (n != 0) {// 使两个链表到入环点长度一样 n--; cur1 = cur1.next; } while (cur1 != cur2) { cur1 = cur1.next; cur2 = cur2.next; } return cur1; } else { // 情况二之三：入环节点不同 或 情况二之一：链表无交集 cur1 = loop1.next; while (cur1 != loop1) { // 限定只转一圈，碰不到loop2就是无交集 if (cur1 == loop2) { return loop1; } cur1 = cur1.next; } return null; } } ","date":"2020-01-14","objectID":"/20200114_leetcode/:5:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 双指针 ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目一：有序数组的 Two Sum Two Sum II - Input array is sorted (Easy) Input: numbers={2, 7, 11, 15}, target=9 Output: index1=1, index2=2 题目描述：在有序数组中找出两个数，使它们的和为 target。 解题： 使用双指针，一个指针指向最小的元素，一个指针指向最大的元素，两指针向中间遍历 如果两指针的和 sum == garget，return 结果； 如果 sum \u003e target，移动较大的元素，使 sum 变小一些； 如果 sum \u003c target，移动较小的元素，使 sum 变小一些。 最多遍历一遍，时间复杂度为 O(N)。只是用两个额外的变量，空间按复杂度为 O(1) public int[] twoSum(int[] numbers, int target){ if (numbers == null) return null; int i = 0,j = numbers.length - 1; while(i \u003c j){ int sum = numbers[i] + numbers[j]; if (sum == target) { return new int[]{i + 1, j + 1}; } else if (sum \u003c target) { i++; } else { j--; } } return null; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目二：两数平方和 Sum of Square Numbers (Easy) 题目描述：判断一个非负整数是否为两个整数的平方和。 解题： 可以看成是在元素为 0 ~ target 的有序数组中查找两个数，使得这两个数的平方和为 target。 与 题一 逻辑一致，不同的是：左指针从 0位置上的0开始，右指针从 sqrt(target) 位置开始； 时间复杂度 O(sqrt(target))，时间复杂度 O(1) ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目三：反转字符串中的元音字符 Reverse Vowels of a String (Easy) Given s = \"leetcode\", return \"leotcede\". 解题： 使用双指针，一个指针从头遍历，一个指针从尾遍历，当两个指针都遇到元音时，交换这两个元音字符。 为了快速判断字符是不是元音字符，将元音字符添加到集合 HashSet 中，从而以 O(1) 的时间复杂度进行该操作 private final static HashSet\u003cCharacter\u003e vowels = new HashSet\u003c\u003e( Arrays.asList('a', 'e', 'i', 'o', 'u', 'A', 'E', 'I', 'O', 'U')); 时间复杂度 O(N)，空间复杂度 O(1) ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目四：回文字符串 Valid Palindrome II (Easy) Input: \"abca\" Output: True Explanation: You could delete the character 'c'. 题目描述：可以删除一个字符，判断是否能构成回文字符串。 使用 双指针（stack进栈出栈、快慢指针（空间复杂度为O(1)））容易判断一个字符串是否是回文字符串。 本题的关键是处理删除一个字符。在使用双指针遍历字符串时，如果出现两个指针指向的字符不相等的情况，我们就试着删除一个字符，再判断删除完之后的字符串是否是回文字符串。 在判断是否为回文字符串时，我们不需要判断整个字符串，因为左指针左边和右指针右边的字符之前已经判断过具有对称性质，所以只需要判断中间的子字符串即可。 在试着删除字符时，我们既可以删除左指针指向的字符，也可以删除右指针指向的字符。 public boolean validPalindrome(String s){ for(int i = 0, j = s.length() - 1; i \u003c j; i++,j--){ if(s.charAt(i) != s.charAt(j)){ return isPalindrome(s, i, j - 1) || isPalindrome(s, i + 1, j); } } return true; } private bool isPalindrome(String s, int i, int j) { while (i \u003c j){ if(s.charAt(i++) != s.charAt(j--)){ return false; } } return ture; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目五：归并两个有序数组 Merge Sorted Array (Easy) 题目描述：把归并结果存到第一个数组上。 Input: nums1 = [1,2,3,0,0,0], m = 3 nums2 = [2,5,6], n = 3 Output: [1,2,2,3,5,6] 解题： 类似归并排序合并字符串。 public void merge(int[] nums1, int m, int[] nums2, int n) { int index1 = m - 1, index2 = n - 1; int indexMerge = m + n - 1; while (index2 \u003e= 0) { if (index1 \u003c 0) { nums1[indexMerge--] = nums2[index2--]; } else if (index2 \u003c 0) { nums1[indexMerge--] = nums1[index1--]; } else if (nums1[index1] \u003e nums2[index2]) { nums1[indexMerge--] = nums1[index1--]; } else { nums1[indexMerge--] = nums2[index2--]; } } } //归并排序merge方法 public static void merge(int[] arr, int L, int M, int R){ int[] extra = new int[R - L + 1]; int i = 0; int p1 = L; int p2 = M + 1; while(p1 \u003c= M \u0026\u0026 p2 \u003c= R){ extra[i++] = arr[p1] \u003c= arr[p2] ? arr[p1++]: arr[p2++]; } while(p1 \u003c= M){ extra[i++] = arr[p1++]; } while(p2 \u003c= R){ extra[i++] = arr[p2++]; } for(i = 0; i \u003c extra.length; i++){ arr[L + i] = extra[i]; } } ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目六：判断链表是否存在环 Linked List Cycle (Easy) 解题： 使用快慢指针。如果存在环，那么这两个指针一定会相遇。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目七：最长子序列 Longest Word in Dictionary through Deleting (Medium) Input: s = \"abpcplea\", d = [\"ale\",\"apple\",\"monkey\",\"plea\"] Output: \"apple\" 题目描述：删除 s 中的一些字符，使得它构成字符串列表 d 中的一个字符串，找出能构成的最长字符串。如果有多个相同长度的结果，返回字典序的最小字符串。 解题： 通过删除字符串 s 中的一个字符能得到字符串 t，可以认为 t 是 s 的子序列，我们可以使用双指针来判断一个字符串是否为另一个字符串的子序列。 public String findLongestWord(String s, List\u003cString\u003e d) { String longestWord = \"\"; for(String target: d){ int l1 = longestWord.length(), l2 = target.length(); if(l1 \u003e l2 || (l1 == l2 \u0026\u0026 longestWord.compareTo(target) \u003c 0)){ continue; } if(isSubstr(s,target)){ } } return longestWord; } private boolean isSubstr(String s,String target){ int i = 0,j = 0; while(i \u003c s.length() \u0026\u0026 j \u003c target.length()){ if(s.charAt(i) == target.charAt(j)){ j++; } i++; } return j == target.length(); } ","date":"2020-01-14","objectID":"/20200114_leetcode/:6:7","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 树 递归遍历： public static void orderRecur(Node head){ if(head == null){ return; } //operation(...) //先序遍历 orderRecur(head.left); //operation(...) //中序遍历 orderRecur(head.right); //operation(...) //后序遍历 } 深度优先遍历：中序遍历 宽度优先遍历并求最大宽度：队列 LinkedList public static void weight(Node head) { if (head == null) { return; } Queue\u003cNode\u003e queue = new LinkedList\u003c\u003e(); queue.add(head); HashMap\u003cNode, Integer\u003e levelMap = new HashMap\u003c\u003e();//记录行数 levelMap.put(head, 1); int curLevel = 1; int curLevelNodes = 0; int max = Integer.MIN_VALUE; while (!queue.isEmpty()) { Node cur = queue.poll(); int curNodeLevel = levelMap.get(cur); if (curNodeLevel == curLevel) { curLevelNodes++; } else { max = Math.max(max, curLevelNodes); curLevel++; curLevelNodes = 0; } System.out.println(cur.value); if (cur.left != null) { levelMap.put(cur.left, curNodeLevel + 1); queue.add(cur.left); } if (cur.right != null) { levelMap.put(cur.right, curNodeLevel + 1); queue.add(cur.right); } } } 判断是否是搜索二叉树：左孩子小于父节点，右孩子大于父节点。 中序遍历：中间打印节点值的地方改成和前节点值比较。 判断是否是完全二叉树：每一层是满的，或者最后一层节点都在左边。 非递归方法： 同时满足 任一节点有右无左 –\u003e false 遇到一个左右子树不全的节点，后续皆是叶节点。 public static boolean isCBT(Node head){ if(head == null){ return true; } LinkedList\u003cNode\u003e queue = new LinkedList\u003c\u003e(); //是否遇到过左右两个孩子不双全的节点 boolean leaf = false; Node l = null; node r = null; queue.add(head); while(!queue.isEmpty()){ head = queue.poll(); l = head.left; r = head.right; if( //节点不双全，又发现有孩子 (leaf \u0026\u0026 (l ！= null || r ！= null)) || (l == null \u0026\u0026 r != null) ){ return false; } if (l != null){ queue.add(l); } if (r != null){ queue.add(r); } if (l == null || r == null){ leaf = true; } return true; } } 判断满二叉树： 二叉树DP题目固定套路：递归 二叉树DP题目: 可以通过从左右树要信息解决问题可以使用这个固定方法。 public static class Info { public int height; public int nodes; public Info(int h, int n) { height = h; nodes = n; } } public static Info f(Node x) { if (x == null) { return new Info(0, 0); } Info leftData = f(x.left); Info rightData = f(x.right); int height = Math.max(leftData.height, rightData.height) + 1; int nodes = leftData.nodes + rightData.nodes + 1; return new Info(height, nodes); } 判断平衡二叉树：左树高度和右树高度的差都不超过1 同时满足： 左子树是平衡二叉树 右子树是平衡二叉树 左树高度和右树高度的差不超过1 public static class ReturnType{ public boolean isBalanced; public int height; public ReturnType(boolean isB, int hei){ isBalanced = isB; height = hei; } } public static ReturnType Process(Node x){ if(x = null){ return new ReturnType(true, 0); } ReturnType leftData = process(x.left); ReturnType rightData = process(x.right); int height = Math.max(leftData.height, rightData.height) + 1; boolean isBalanced = leftData.isBalanced \u0026\u0026 rightData.isBalanced \u0026\u0026 Math.abs(leftData.height - rightData.height) \u003c 2; return new ReturnType(isBalanced, height); } ","date":"2020-01-14","objectID":"/20200114_leetcode/:7:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"递归 1. 树的高度 Maximum Depth of Binary Tree (Easy) public int maxDepth(TreeNode root){ if(root == null) return 0; return Math.max(maxDepth(root.left), maxDepth(root.right)) + 1; } 2. 平衡树 Balanced Binary Tree (Easy) 平衡树左右子树高度差都小于等于 1 private boolean result = true; public boolean isBalanced(TreeNode root){ maxDepth(root); return result; } public int maxDepth(TreeNode root){ if(root == null) return 0; int l = maxDepth(root.left); int r = maxDepth(root.right); if(Math.abs(l - r) \u003e 1) result = false; return Math.max(l, r) + 1; } 3. 两节点的最长路径 Diameter of Binary Tree (Easy) Input: 1 / \\ 2 3 / \\ 4 5 Return 3, which is the length of the path [4,2,1,3] or [5,2,1,3]. private int max = 0; public int diameterOfBinaryTree(TreeNode root){ depth(root); return max; } private int depth(TreeNode root){ if(root == null) return 0; int l = depth(root.left); int r = depth(root.right); max = Math.max(max, l + r); return Math.max(l, r) + 1; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:7:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"BFS层次遍历 使用 BFS 时，不需要使用两个队列来分别存储当前层的节点和下一层的节点，因为在开始遍历一层的节点时，当前队列的节点数就是当前层的节点数，只要控制遍历这么多节点数，就能保证这次遍历的都是当前层的节点。 1. 一棵树每层节点的平均数 Average of Levels in Binary Tree (Easy) public List\u003cDouble\u003e averageOfLevels (TreeNode root){ List\u003cDouble\u003e ret = new ArrayList\u003c\u003e(); if (root == null) return ret; Queue\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); queue.add(root); while (!queue.isEmpty()){ int cnt = queue.size(); double sum = 0; for(int i = 0; i \u003c cnt; i++){ TreeNode node = queue.poll(); sum += node.val; if (node.left != null) queue.add(node.left); if (node.right != null) queue.add(node.right); } ret.add(sum / cnt); } return ret; } 2. 得到左下角的节点 Find Bottom Left Tree Value (Easy) Input: 1 / \\ 2 3 / / \\ 4 5 6 / 7 Output: 7 解题： 宽度优先遍历 BFS，每一行从左至右改成从右至左，然后最后一个数 public int findBottomLeftValue(TreeNode root) { Queue\u003cTreeNode\u003e queue = new LinkedList\u003c\u003e(); queue.add(root); while(!queue.isEmpty()){ root = queue.poll(); if (root.right != null) queue.add(root.right); if (root.left != null) queue.add(root.left); } return root.val; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:7:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"前中后序遍历 非递归实现前中后序遍历 前序遍历、后续遍历 public List\u003cInteger\u003e preOrderTraversal(TreeNode root){ List\u003cInteger\u003e ret = new ArrayList\u003c\u003e(); Stack\u003cTreeNode\u003e stack = new Stack\u003c\u003e(); stack.push(root); while(!stack.isEmpty()){ TreeNode node = stack.pop(); if (node == null) continue; ret.add(node.val); stack.push(node.right); //先右后左，保证左子树先遍历。先左后右即为后续遍历。 stack.push(node.left); } return ret; } 中序遍历 public List\u003cInteger\u003e inorderTraversal(TreeNode root) { List\u003cInteger\u003e ret = new ArrayList\u003c\u003e(); if (root == null) return ret; Stack\u003cTreeNode\u003e stack = new Stack\u003c\u003e(); TreeNode cur = root; while(cur != null || !stack.isEmpty()){ while(cur != null){ stack.push(cur); cur = cur.left; } TreeNode node = stack.pop(); ret.add(node.val); cur = node.right; } return ret; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:7:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"BST二叉查找树 二叉查找树（BST）：根节点大于等于左子树所有节点，小于等于右子树所有节点。 二叉查找树中序遍历有序。 1. 修剪二叉查找树 Trim a Binary Search Tree Input: 3 / \\ 0 4 \\ 2 / 1 L = 1 R = 3 Output: 3 / 2 / 1 题目描述：只保留值在 L ~ R 之间的节点 解题： public TreeNode trimBST(TreeNode root, int L, int R){ if(root == null) return null; if(root.val \u003e R) return trimBST(root.left, L, R); if(root.val \u003c L) return trimBST(root.right, L, R); root.left = trimBST(root.left, L, R); root.right = trimBST(root.right, L, R); return root; } 2.寻找二叉查找树第K个元素 Kth Smallest Element in a BST (Medium) 中序遍历解法： private int cnt = 0; private int val; public int kthSmallest(TreeNode root, int k){ if(node == null) return; inOrder(node.left, k); //中序遍历查找树有序，左子树走到底为最小数 cnt++; if(cnt == k){ val = node.val; return; } inOrder(node.right, k); //中序遍历向右子树 } 3. 把二叉查找树每个节点的值都加上比它大的节点的值 Convert BST to Greater Tree (Easy) Input: The root of a Binary Search Tree like this: 5 / \\ 2 13 Output: The root of a Greater Tree like this: 18 / \\ 20 13 解题： 先遍历右子树。反向中序遍历即从大到小. private int sum = 0; public TreeNode convertBST(TreeNode root){ traver(root); return root; } private void traver(TreeNode node){ if(node == null) return; traver(node.right); //到最大的节点 sum += node.val; node.val = sum; traver(node.left); } 4. 二叉查找树的最近公共祖先 Lowest Common Ancestor of a Binary Search Tree (Easy) _______6______ / \\ ___2__ ___8__ / \\ / \\ 0 4 7 9 / \\ 3 5 For example, the lowest common ancestor (LCA) of nodes 2 and 8 is 6. Another example is LCA of nodes 2 and 4 is 2, since a node can be a descendant of itself according to the LCA definition. 解题： 前序遍历，第一个满足区间中的数即为最近根。 public TreeNode lowestCommonAncestor(TreeNode root, TreeNode p, TreeNode q){ if(root.val \u003e p.val \u0026\u0026 root.val \u003e q.val) return lowestCommonAncestor(root.left, p, q); if(root.val \u003c p.val \u0026\u0026 root.val \u003c q.val) return lowestCommonAncestor(root.right, p, q); } 6. 从有序数组中构造二叉查找树 Convert Sorted Array to Binary Search Tree (Easy) public TreeNode sortedArrayToBST(int[] nums){ return toBST(nums, 0, nums.length - 1); } private TreeNode toBST(int[] nums, int sIdx, int eIdx){ if(sIdx \u003e eIdx) return null; int mIdx = (sIdx + eIdx) / 2; TreeNode root = new TreeNode(nums[mIdex]); root.left = toBST(nums, sIdx, mIdx - 1); root.right = toBST(nums, mIdx + 1, eIdx); return root; } 7. 根据有序链表构造平衡的二叉查找树 Convert Sorted List to Binary Search Tree (Medium) 8. 在二叉查找树中寻找两个节点，使它们的和为一个给定值 Two Sum IV - Input is a BST (Easy) Input: 5 / \\ 3 6 / \\ \\ 2 4 7 Target = 9 Output: True 解题： 使用中序遍历得到有序数组之后，再利用双指针对数组进行查找。 应该注意到，这一题不能用分别在左右子树两部分来处理这种思想，因为两个待求的节点可能分别在左右子树中。 public boolean findTarget(TreeNode root, int k) { List\u003cInteger\u003e nums = new ArrayList\u003c\u003e(); inOrder(root, nums); int i = 0, j = nums.size() - 1; while (i \u003c j) { int sum = nums.get(i) + nums.get(j); if (sum == k) return true; if (sum \u003c k) i++; else j--; } return false; } private void inOrder(TreeNode root, List\u003cInteger\u003e nums) { if (root == null) return; inOrder(root.left, nums); nums.add(root.val); inOrder(root.right, nums); } 9. 在二叉查找树中查找两个节点之差的最小绝对值 Minimum Absolute Difference in BST (Easy) Input: 1 \\ 3 / 2 Output: 1 解题： 利用二叉查找树的中序遍历有序的性质，计算中序遍历中临近的两个节点之差的绝对值，取最小值。 private int minDiff = Integer.MAX_VALUE; private TreeNode preNode = null; public int getMinimumDifference(TreeNode root){ inOrder(root); return minDiff; } private void inOrder(TreeNode node){ if (node == null) return; inOrder(node.left); if (preNode != null) minDiff = Math.min(minDiff, node.val - preNode.val); preNode = node; inOrder(node.right); } 10. 寻找二叉查找树中出现次数最多的值 Find Mode in Binary Search Tree (Easy) private int curCnt = 1; private int maxCnt = 1; private TreeNode preNode = null; public int[] findMode(TreeNode root){ List\u003cInteger\u003e maxCntNums = new ArrayList\u003c\u003e(); inOrder(root, maxCntNums); int[] ret = new int[maxCntNums.size()]; int idx = 0; for(int num : maxCntNums){ ret[idx++] = num; } return ret; } private void inOrder(TreeNode node, List\u003cInteger\u003e nums){ if(node == null) return; inOrder(node.left, nums); if(preNode != null){ if (preNode.val == node.val) curCnt++; else curCnt = 1; } if(curCnt \u003e maxCnt){ maxCnt = curCnt; nu","date":"2020-01-14","objectID":"/20200114_leetcode/:7:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Trie 前缀树 Trie 概念：单词集合生成由字母组成的树。又称前缀树或字典树，用于判断字符串是否存在或者是否具有某种字符串前缀。 trie 1.实现一个 Trie Implement Trie (Prefix Tree) (Medium) class Trie{ private class Node{ Node[] childs = new Node[26]; boolean isLeaf; } private Node root = new Node(); public Trie(){ } public void insert(String word, Node node){ insert(word, root); } private void insert(String word, Node node){ if(node == null) return; if(word.length() == 0){ node.isLeaf = true; return; } int index = indexForChar(word.charAt(0)); if(node.childs[index] == null){ node.childs[index] = new Node(); } insert(word.substring(1), node.childs[index]); } public boolean search(String word){ return search(word, root); } private boolean search(String word, Node node){ if(node == null) return false; if(word.length() == 0) node.isLeaf; int index = indexForChar(word.charAt(0)); return search(word.substring(1), node.childs[index]); } public boolean startsWith(String prefix){ return startWith(prefix, root); } private boolean startWith(String prefix, Node node){ if(node == null) return false; if(prefix.length() == 0) return true; int index = indexForChar(prefix.charAt(0)); return startWith(prefix.substring(1), node.childs[index]); } private int indexForChar(char c) { return c - 'a'; } } ","date":"2020-01-14","objectID":"/20200114_leetcode/:7:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 栈和队列 ","date":"2020-01-14","objectID":"/20200114_leetcode/:8:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"1. 用栈实现队列 Implement Queue using Stacks (Easy) 解题： 栈的顺序为后进先出，而队列的顺序为先进先出。使用两个栈实现队列，一个元素需要经过两个栈才能出队列，在经过第一个栈时元素顺序被反转，经过第二个栈时再次被反转。 class MyQueue{ private Stack\u003cInteger\u003e in = new Stack\u003c\u003e(); private Stack\u003cInteger\u003e out = new Stack\u003c\u003e(); public void push(int x){ in.push(x); } public int pop(){ in2out(); return out.pop(); } public int peek(){ in2out(); return out.peek(); } private void in2out(){ if(out.isEmpty()){ while(!in.isEmpty()){ out.push(in.pop()); } } } public boolean empty(){ return in.isEmpty() \u0026\u0026 out.isEmpty(); } } ","date":"2020-01-14","objectID":"/20200114_leetcode/:8:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"2. 用队列实现栈 Stack using Queues (Easy) 解题： 在将一个元素x插入队列时，为了维护原来的后进先出顺序，需要让x插入队列首部。而队列默认插入顺序是队列尾部，因此在将x插入队列尾部之后，需要让除了x之外的所有元素出队列，再入队列。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:8:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"3. 最小值栈 Min Stack (Easy) class MinStack { private Stack\u003cInteger\u003e dataStack; private Stack\u003cInteger\u003e minStack; private int min; public MinStack() { dataStack = new Stack\u003c\u003e(); minStack = new Stack\u003c\u003e(); min = Integer.MAX_VALUE; } public void push(int x) { dataStack.add(x); min = Math.min(min, x); minStack.add(min); } public void pop() { dataStack.pop(); minStack.pop(); min = minStack.isEmpty() ? Integer.MAX_VALUE : minStack.peek(); } public int top() { return dataStack.peek(); } public int getMin() { return minStack.peek(); } } 对于实现最小值队列问题，可以先将队列使用栈来实现，然后就将问题转换为最小值栈。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:8:3","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"4. 用栈实现括号匹配 Valid Parentheses (Easy) \"()[]{}\" Output : true public boolean isValid(String s){ Stack\u003cCharacter\u003e stack = new Stack\u003c\u003e(); for(char c : s.toCharArray()){ if(c == '(' || c == '{' || c == '['){ stack.push(c); } else { if(stack.isEmpty()){ return false; } char cStack = stack.pop(); boolean b1 = c == ')' \u0026\u0026 cStack != '('; boolean b2 = c == ']' \u0026\u0026 cStack != '['; boolean b3 = c == '}' \u0026\u0026 cStack != '{'; if(b1 || b2 || b3){ return false; } } } return stack.isEmpty(); } ","date":"2020-01-14","objectID":"/20200114_leetcode/:8:4","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"5. 数组中元素与下一个比它大的元素之间的距离 Daily Temperatures (Medium) Input: [73, 74, 75, 71, 69, 72, 76, 73] Output: [1, 1, 4, 2, 1, 1, 0, 0] 解题： 在遍历数组时用栈把数组中的数存起来，如果当前遍历的数比栈顶元素来的大，说明栈顶元素的下一个比它大的数就是当前元素。 public int[] dailyTemperatures(int[] temperatures){ int n = temperatures.length; int[] dist = new int[n]; Stack\u003cInteger\u003e indexs = new Stack\u003c\u003e(); for(int curIndex = 0; curIndex \u003c n; curIndex ++){ while(!indexs.isEmpty() \u0026\u0026 temperatures[curIndex] \u003e temperatures[indexs.peek()]){ int preIndex = indexs.pop(); dist[preIndex] = curIndex - preIndex; } indexs.add(curIndex); } return dist; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:8:5","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"6. 循环数组中比当前元素大的下一个元素 Next Greater Element II (Medium) Input: [1,2,1] Output: [2,-1,2] Explanation: The first 1's next greater number is 2; The number 2 can't find next greater number; The second 1's next greater number needs to search circularly, which is also 2. 解题： 与 Daily Temperatures (Medium) 不同的是，数组是循环数组，并且最后要求的不是距离而是下一个元素。 public int[] nextGreaterElements(int[] nums){ int n = nums.length; int[] next = new int[n]; Stack\u003cInteger\u003e pre = new Stack\u003c\u003e(); for(int i = 0; i \u003c n * 2; i++){ int num = nums[i % n]; while(!pre.isEmpty() \u0026\u0026 nums[pre.peek()] \u003c num){ next[pre.pop()] = num; } if(i \u003c n){ pre.push(i); } } return next; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:8:6","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 分治 ","date":"2020-01-14","objectID":"/20200114_leetcode/:9:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目一：给表达式加括号 Different Ways to Add Parentheses (Medium) Input: \"2-1-1\". ((2-1)-1) = 0 (2-(1-1)) = 2 Output : [0, 2] 解题： 利用分治算法，在顺序扫描表达式的时候，当遇到一个运算符，该运算符将整个表达式分成了两部分，表达式左右均是完整的表达式，可以对两边的表达式分开处理，分别得到两个表达式的结果数组，然后根据该运算符对两边的结果数组做相应的处理，再对整体表达式的下一个运算符做上述处理，直到所有运算符。 public List\u003cInteger\u003e diffWaysToCompute(String input) { List\u003cInteger\u003e ways = new ArrayList\u003c\u003e(); for (int i = 0; i \u003c input.length(); i++) { char c = input.charAt(i); if (c == '+' || c == '-' || c == '*') { List\u003cInteger\u003e left = diffWaysToCompute(input.substring(0, i)); List\u003cInteger\u003e right = diffWaysToCompute(input.substring(i + 1)); for (int l : left) { for (int r : right) { switch (c) { case '+': ways.add(l + r); break; case '-': ways.add(l - r); break; case '*': ways.add(l * r); break; } } } } } if (ways.size() == 0) { ways.add(Integer.valueOf(input)); } return ways; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:9:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"题目二：不同的二叉搜索树 Unique Binary Search Trees II (Medium) 给定一个数字 n，要求生成所有值为 1…n 的二叉搜索树。 二叉搜索树：对于树中每个节点： 若其左子树存在，则其左子树中每个节点的值都不大于该节点值； 若其右子树存在，则其右子树中每个节点的值都不小于该节点值。 Input: 3 Output: [ [1,null,3,2], [3,2,null,1], [3,1,null,null,2], [2,1,3], [1,null,2,null,3] ] Explanation: The above output corresponds to the 5 unique BST's shown below: 1 3 3 2 1 \\ / / / \\ \\ 3 2 1 1 3 2 / / \\ \\ 2 1 2 3 解题： public List\u003cTreeNode\u003e generateTrees(int n) { if (n \u003c 1) { return new LinkedList\u003cTreeNode\u003e(); } return generateSubtrees(1, n); } private List\u003cTreeNode\u003e generateSubtrees(int s, int e) { List\u003cTreeNode\u003e res = new LinkedList\u003cTreeNode\u003e(); if (s \u003e e) { res.add(null); return res; } for (int i = s; i \u003c= e; ++i) { List\u003cTreeNode\u003e leftSubtrees = generateSubtrees(s, i - 1); List\u003cTreeNode\u003e rightSubtrees = generateSubtrees(i + 1, e); for (TreeNode left : leftSubtrees) { for (TreeNode right : rightSubtrees) { TreeNode root = new TreeNode(i); root.left = left; root.right = right; res.add(root); } } } return res; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:9:2","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 动态规划 递归和动态规划的区别 都是将原问题拆成多个子问题然后求解，他们之间最本质的区别是，动态规划保存了子问题的解，避免重复计算。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:10:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"斐波那契数列 题目一：爬楼梯 Climbing Stairs (Easy) 题目描述：有 N 阶楼梯，每次可以上一阶或者两阶，求有多少种上楼梯的方法。 解题： 定义一个数组 dp 存储上楼梯的方法数（为了方便讨论，数组下标从 1 开始），dp[i] 表示走到第 i 个楼梯的方法数目。 第 i 个楼梯可以从第 i-1 和 i-2 个楼梯再走一步到达，走到第 i 个楼梯的方法数为走到第 i-1 和第 i-2 个楼梯的方法数之和。 dp[i] = dp[i - 1] + dp[i - 2] 考虑到 dp[i] 只与 dp[i - 1] 和 dp[i - 2] 有关，因此可以只用两个变量来存储 dp[i - 1] 和 dp[i - 2]，使得原来的 O(N) 空间复杂度优化为 O(1) 复杂度。 public int climbStairs(int n){ if(n \u003c= 2){ return n; } int pre2 = 1, pre1 = 2; for(int i = 2;i \u003c n;i++){ int cur = pre1 + pre2; pre2 = pre1; pre1 = cur; } return pre1; } 题目二：强盗抢劫 House Robber (Easy) 题目描述：抢劫一排住户，但是不能抢邻近的住户，求最大抢劫量。 解题： ","date":"2020-01-14","objectID":"/20200114_leetcode/:10:1","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"图 图的描述方式: public class Node { public int value; //点的编号 public int in; //入度的个数 public int out; //出度的个数 public ArrayList\u003cNode\u003e nexts; //邻居点 public ArrayList\u003cEdge\u003e edges; //邻居边 public Node(int =value) { this.value = value; in = 0; out = 0; nexts = new ArrayList\u003c\u003e(); edges = new ArrayList\u003c\u003e(); } } public class Edge { public int weight; public Node from; public Node to; public Edge(int weight, Node from, Node to) { this.weight = weight; this.from = from; this.to = to; } } public class Graph { public HashMap\u003cInteger, Node\u003e nodes; //点集 public HashSet\u003cEdge\u003e edges; //边集 public Graph() { nodes = new HashMap\u003c\u003e(); edges = new HashSet\u003c\u003e(); } } 图的宽度优先遍历： 利用队列实现 从源节点开始依次按照宽度进队列，然后弹出 每弹出一个点，把该节点所有没有进过队列的邻接点放入队列 直到队列变空 public static void bfs(Node node){ if (node == null){ return; } Queue\u003cNode\u003e queue = new LinkedList\u003c\u003e(); HashSet\u003cNode\u003e set = new HashSet\u003c\u003e(); //放入已处理的节点，检查点是否重复。 queue.add(node); set.add(node); while(!queue.isEmpty()){ Node cur = queue.poll(); System.out.println(cur.value); for(Node next : cur.nexts){ if(!set.contains(nexts)){ set.add(next); queue.add(next); } } } } 图的广度优先遍历： 利用栈实现 从源节点开始依次按照深度进队列，然后弹出 每弹出一个点，把该节点所有没有进过队列的邻接点放入队列 直到队列变空 public static void dfs(Node node){ if (node == null){ return; } Stack\u003cNode\u003e stack = new Stack\u003c\u003e(); HashSet\u003cNode\u003e set = new HashSet\u003c\u003e(); stack.add(node); set.add(node); System.out.println(node.value); while(!stack.isEmpty()){ Node cur = stack.pop(); for(Node next : cur.nexts){ if(!set.contains(next)){ stack.push(cur); stack.push(next); set.add(next); System.out.println(next.value); break; } } } } 拓扑排序问题： 实际问题：编译依赖是个有向无环图。如何决定编译顺序。 方法：找到入度为0的点输出，然后擦掉该节点和该节点边，再找到入度为0点输出。以此往复。 public static List\u003cNode\u003e sortedTopoloty(Graph graph){ HashMap\u003cNode, Integer\u003e inMap = new HashMap\u003c\u003e(); // key: 某一个Node； value：剩余的入度 Queue\u003cNode\u003e zeroInQueue = new LinkedList\u003c\u003e(); // 入度为0放入这个队列。 for(Node node: graph.nodes.values()){ inMap.put(node, node.in); if(node.in == 0){ zeroInQueue.add(node); } } // 拓扑排序结果，依次加入result List\u003cNode\u003e result = new ArrayList\u003c\u003e(); while(!zeroInQueue.isEmpty()){ Node cur = zeroInQueue.poll(); result.add(cur); for(Node next : cur.nexts){ inMap.put(next, inMap.get(next) - 1); if(inMap.get(next) == 0){ zeroInQueue.add(next); } } } return result; } kruskal算法：适用范围要求无向图 prim算法：适用范围要求无向图 Dijkstra算法：适用范围没有权值为负数的边 ","date":"2020-01-14","objectID":"/20200114_leetcode/:11:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"哈希函数 问题：2的32次方个由 0 - 2 的32次方组成的随机数，用1G内存，返回出现次数最多的数 如果直接使用哈希表 {key ：次数}，会占用8 * 2 32 个字节（32G）。 可以将每个数算出哈希值后%100 后存入哈希表，这样 2 32 个 key：value 变成了一百个key：value。 再将次数最多的 key 进行统计。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:12:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"布隆过滤器 解决问题： 黑名单系统（100亿 URL 组成的黑名单，每次访问要判断是否在黑名单中，用 HashSet 则占用 640G 内存） 爬虫去重问题等 （使用1000个线程爬虫，不希望爬已经爬过的网站，需要把已经爬过的网站做成集合） 功能： 只有添加和搜索功能，没有删除功能 使用很少的内存，允许有一定的失误 位图代码： int [] arr = new int[10]; //每个 int 4个字节32位，10个int数组可以表示 32bit * 10 -\u003e 320bit //例：想拿到第178位的状态 int numIndex = 178 / 32; int bitIndex = 178 % 32; //拿到178位的状态 int s = ((arr[numIndex] \u003e\u003e (bitIndex)) \u0026 1); //把178位的状态改成1 arr[numIndex] = arr[numIndex] | (1 \u003c\u003c (bitIndex)); //把178位的状态改成0 arr[numIndex] = arr[numIndex] \u0026 (~ (1 \u003c\u003c (bitIndex))); 布隆过滤器逻辑： 生成布隆过滤器： //1. 生成 m 长度的位图 //2. 加入黑名单：URL1 通过k个 hash 函数算出k个 hash 值 %m ，确立k个位数值位 1 。更新到位图上 //3. 此时位图已记录 URL1 。以此重复, 已经为1继续为1，直到写入 100亿 URL 查询： // URL 通过k个 hash 函数算出k个 hash 值 %m，在位图上验证，都为 1 则满足 m和k值如何确定： // m 越大越好，k 过大过小都不好。有计算公式 ","date":"2020-01-14","objectID":"/20200114_leetcode/:13:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"一致性哈希 解决问题： 数据服务器如何组织，解决分布式数据库负载均衡，高低频查询。 传统分布式存储： 海量数据分布式存储：数据通过key值算出哈希值 % 机器数量 决定数据存放服务器节点。 当服务器硬盘大小不够，需要增加服务器。这时需要数据全量迁移，而且重新将hash值 % 新的服务器数量 通过一致性哈希存储保证不 % ，并减少迁移代价： 如有 3 台机器分布式存储数据。通过 MD5 作哈希值。 将1 ~ 264-1 想象成一个环，将 [0 , 264 / 3] 分给节点1，[264 / 3 , 264 / 3 * 2] 分给节点2，[264 / 3 * 2 , 264] 分给节点3。 当增加节点时，比如节点放在节点2和节点3正中间。只用从节点2上移动 264 / 3 / 2 个数据到新节点上。 问题1：开始如何均分数据 问题2：增加减少节点导致负载不均衡 虚拟节点技术 ：按比例去抢环，增加节点时，按比例从每个节点拿数据放到新节点。删除节点也按比例分配到别的节点。 一致性哈希按比例同时可以解决：性能强的机器数据量大，性能弱的机器数据少。 【题目】: 岛问题 【题目】矩阵中只有0和1两种值，每个位置都可以和自己的上下左右四个位置相连，相邻的一片1叫做一个岛，求一个矩阵中有多少个岛？ 001010 111010 100100 000000 \u003e 这个矩阵中有三个岛 【解题】顺序遍历节点，遇到1则进入感染函数，将与该1一起的岛改成2，岛数++。 感染函数： public static void infect (int[][]m, int i, int j, int N, int M){ //(i,j)位置 N,M矩形长宽 if(i \u003c 0 || i \u003e= N || j \u003c 0 || j \u003e= M || m[i][j] != 1){ //i,j没越界，并且当前位置是1 return; } m[i][j] = 2; infect(m, i + 1, j, N, M); infect(m, i - 1, j, N, M); infect(m, i, j + 1, N, M); infect(m, i, j - 1, N, M); } 时间复杂度：O(N * M) 【进阶】如何设计一个并行算法解决这个问题 并查集： 解决两个集合查重与合并。使用 链表 和 hashSet 效率都不高。链表 合并的复杂度是 O(1)，查重是 O(n2)。使用 hashSet 查重 复杂度是 O(n)，合并 复杂度是 O(n)。 //找到头结点函数：判断集合是否是一个集合只用判断头结点是否相同。 //找头结点时同时优化：将该节点到头节点路径上的节点拍平，直接插到头结点上。当调用 findHead 函数次数越多，时间复杂度越接近 O(1) private Element\u003cV\u003e findHead(Element\u003cV\u003e element){ Stack\u003cElement\u003cV\u003e\u003e path = new Stack\u003c\u003e(); while (element != fatherMap.get(element)){ path.push(element); element = fatherMap.get(element); } while (!path.isEmpty()){ fatherMap.put(path.pop(), element); } return element; } //判断是否为同一集合函数 public boolean isSameSet(V a, V b){ if (elementMap.containsKey(a) \u0026\u0026 elementMap.containsKey(b)){ return findHead(elementMap.get(a)) == findHead(elementMap.get(b)); } return false; } //合并集合函数, 小集合插到大集合尾部 public void union(V a, V b){ if (elementMap.containsKey(a) \u0026\u0026 elementMap.containsKey(b)){ ... } } 并行解决方案： 如有两个线程，将岛图切分成 2 块，分别进行顺序遍历感染。这样统计出分散的岛视为不同的集合，将这些集合进行并查集合并操作，算出合并后的岛数。 ","date":"2020-01-14","objectID":"/20200114_leetcode/:14:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Leetcode 题解 - 贪心思想 笔试面试都很少出现 概念：保证每次操作都是局部最优的，并且最后得到的结果是全局最优的。 因为变化太多，笔试面试都很少出现。 题目一：分配饼干 Assign Cookies (Easy) Input: grid[1,3], size[1,2,4] Output: 2 题目描述：每个孩子都有一个满足度 grid，每个饼干都有一个大小 size，只有饼干的大小大于等于一个孩子的满足度，该孩子才会获得满足。求解最多可以获得满足的孩子数量。 解题： public int findContentChildren(int[] grid, int[] size) { if (grid == null || size == null) return 0; Arrays.sort(grid); Arrays.sort(size); int gi = 0, si = 0; while (gi \u003c grid.length \u0026\u0026 si \u003c size.length) { if (grid[gi] \u003c= size[si]) { gi++; } si++; } return gi; } ","date":"2020-01-14","objectID":"/20200114_leetcode/:15:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"SQL 题目一：登录日志，计算每个人连续登录的最大天数(断一天也算连续) val tx_sql = spark.createDataFrame(Seq( (\"1002\",\"2021-08-01\"), (\"1002\",\"2021-08-02\"), (\"1002\",\"2021-08-04\"), (\"1002\",\"2021-08-05\"), (\"1002\",\"2021-08-06\"), (\"1003\",\"2021-08-01\"), (\"1003\",\"2021-08-04\"), (\"1003\",\"2021-08-05\") )).toDF(\"id\",\"time\") tx_sql.createOrReplaceTempView(\"tx_sql\") 思路一：等差数列 开窗函数，按照id分组同时按照时间排序，求rank select id,time,rank() over(partition by id order by time) rk from tx_sql 将每行日期减去rk值，如果之前是连续的日期，则相减之后为相同日期 select if,time,data_sub(time, rk) flag from (select id,time,rank() over(partition by id order by time) rk from tx_sql) t1 计算绝对连续的天数，相同的日期个数即为连续天数 select id,flag,count(*) days from (select id,time,date_sub(time, rk) flag from (select id,time,rank() over(partition by id order by time) rk from tx_sql)t1)t2 group by id,flag 再计算连续问题，使用相同方法再计算一次 select id,flag,days,rank() over(partition by id order by flag) newFlag from ()t3 日期差1则可以相加 select id,flag,sum(days)+count(*)-1 days from t5 group by id,flag 取最大值 思路二：数据 lag 下移做差值 将每行数据下移 select id,time,lag(1,time,'1970-01-01') over(partition by id order by time) lagDt from tx 做差值 select id,time,datediff(time,lagDt) dtDiff from t1 差值为 1或2 都视为连续，大于2则为断开，分到另一组 select id,time,sum(if(dtDiff \u003e 2, 1, 0)) over(partition by id order by time) flag from t2 计算每一组的最大最小值差加一 select id,flag,datediff(max(dt),min(dt)) + 1 from t3 group by id,flag 求连续天数的最大值 TIPS HiveOnSpark bug: Datediff over 子查询 =\u003e Nullpoint 解决方案： 换 MR 引擎 将时间字段由 String 类型改为 Date 类型 ","date":"2020-01-14","objectID":"/20200114_leetcode/:16:0","tags":["Interview","leetcode","Algorithm"],"title":"Leetcode","uri":"/20200114_leetcode/"},{"categories":["Technology"],"content":"Article description.","date":"2020-01-01","objectID":"/20200101_java-interview/","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"JVM, Garbage Collection, Multi - thread, Redis ","date":"2020-01-01","objectID":"/20200101_java-interview/:0:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"JVM Java从编码到执行 javac 将 x.java(任何语言) 文件编码成 x.class 文件 JVM 中的 ClassLoader 将 x.class 文件装载到内存里，通常也会把 java类库也装载到内存里 JVM 调用 字节码解释器 或 JIT即时编译器(常用的代码使用即时编译，第二次编译的时候直接调用) 来进行解释或编译 *jvm是解释执行也是编译执行的，也可以混合执行 JVM 执行引擎开始执行 JVM jvm 是一种规范 它是虚构出来的一台计算机 ClassFileFormat 二进制字节流 CLASS文件结构 前4个字节：magic 两个字节：minor version 两个字节：major version 两个字节：constant pool count 常量池个数 ，2个字节，最多65535个 常量池结构： constant_utf8_info: tag:1,占用空间一个字节。length: utf-8字符串占用的字节数。bytes: 长度位length的字符串 constant_integer_info: tag:3,占用空间一个字节。bytes: 4个字节，big-endian（高位在前）存储的int值 constant_methodref_info: tag:10 index:2字节，指向声明方法的类或者接口描述符constant_class_info的索引项 index: 2字节，指向字段描述符constant_nameAndType的索引项 class/this_class/super_class interfaces fields methods Class Loading Linking Initializing class文件load到内存的过程 loading: class文件放到内存 双亲委派，安全 lazyLoading 五中情况 classLoader：findInCache -\u003e parent.loadClass -\u003e findClass() 自定义类加载器 extends ClassLoader overwrite findClass() -\u003e defineClass(byte[] -\u003e Class clazz) 混合执行 linking verification：文件符合jvm规定：0xCAFEBABE… preparation：静态成员变量赋默认值，int = 0… resolution：类、方法、属性符号应用解析为直接引用，常量池中的各个符号引用解析为指针、偏移量等内存地址的直接引用 initializing：静态变量这时才赋为初始值 load 静态成员变量 -\u003e 默认值 -\u003e 初始值 new Object -\u003e 申请内存 -\u003e 默认值 -\u003e 初始值：单例模式中，volatile 确保指令执行顺序，先初始化后再把内存给 t ，否则有可能先指向内存再初始化 类加载器 JVM 是按需动态加载，采用双亲委派机制 （自顶向下，进行实际查找和加载child方向） Bootstrap -----\u003e 加载lib/rt.jar charset.jar 等核心类，C++实现 | Extension -----\u003e 加载扩展jar包(jre/lib/ext/*.jar)，或由 Djava.ext.dirs指定 | Application -----\u003e 加载classpath指定内容 | Custom ClassLoader -----\u003e 自定义 classLoader （自底向上检查该类是否已经加载parent方向） 双亲委派 父加载器 -父加载器不是\"类加载器的加载器\"，！！也不是\"类加载器的父类加载器\" 双亲委派是一个孩子向父亲方向，然后父亲向孩子方向的双亲委派过程 classLoadProcess 为什么要做双亲委派 安全 双亲委派机制能够保证类加载器加载某个类时，最终都是由一个加载器加载，确保最终加载结果相同 比如 java.long.object, 从 custom classLoader 从下向上开始，到Bootstrap 发现已经加载过了，就不再加载了 补充 class load到内存中有两块东西，一是class二进制文件，二是class对象。其他自己写的对象访问内存中的class对象，通过class对象访问内存中的二进制文件 method Area ： class对象是存储在 method Area 中，method Area 在内存是存储在 metaspace中，也就是 permanent generation。1.8之前叫 permanent generation， 1.8之后叫 metaspace 什么时候需要调用loadclass函数？ spring 里有动态代理，spring 就调用 loadclass 把 class 加载到缓存里 tomcat，load自定义的class 热部署，热启动 类编译 解释器 -bytecode intepreter JIT -Just In Time compiler 混合模式： -混合使用解释器 + 热点代码编译 -起始阶段采用解释执行 -热点代码检测 多次被调用的方法（方法计数器：监测方法执行频率） 多次被调用的方法（循环计数器：检测循环执行频率） 进行编译 Xmixed: 默认为混合模式，开始解释执行，启动速度较快，对热点代码实行检测和编译 Xint: 解释模式，启动很快执行稍慢 Xcomp: 纯编译模式，执行很快，启动很慢 懒加载 new/ get static / put static / invoke static 指令，访问 final 变量除外 java.lang.reflect 对类进行反射调用时 初始化子类的时候，父类首先初始化 jvm启动时，被执行的主类必须初始化 动态语言支持REF_putstatic/REF_getstatic/REF_invokestatic的方法句柄时，该类必须初始化 Initializing //例：求 T.count main(){ System.out.println(T.count); } class T { public static T t = new T(); public static int count = 2; private T(){ count ++; } } //T.class load，然后linking， //到preparation，静态成员变量赋值默认值。t为null，count为0 //然后到initializing，t先执行构造方法，count赋值为2, count为2 class T { public static int count = 2; public static T t = new T(); private T(){ count ++; } } //initializing，count赋值为2，t再执行构造方法 count = 2++ = 3 ","date":"2020-01-01","objectID":"/20200101_java-interview/:1:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"GC 熟悉GC常用算法，熟悉常见垃圾收集器，具有实际JVM调优实战经验 gc1 程序的栈和堆 栈： 每个线程一个栈，栈中照先进先出，存放方法 堆： 动态内存块，比如 new 对象 什么是垃圾 没有引用指向他了就是垃圾 回收垃圾的方法 引用计数法（reference count） 当引用指向为0，回收 缺点：当三个内存垃圾互相指向，无法回收 Python 根可达算法（root searching） GC roots: 线程栈变量，静态变量，常量池，JNI指针 GC 的演化 随着内存大小的不断增长而演进 堆内存逻辑分区 分代模型： 刚刚诞生的对象优先放在新生代内存区， 随着GC器的扫描新生代，新生代内存若多次没被回收(在Surviver两个区反复横跳多次)会变成老年代(gc正常不管这片区域) GC 算法 Mark - Sweep (标记清除) 标记分为：存货对象，未使用内存区，可回收内存区 缺点：碎片化严重，分大块内存时不便 Copying 基于标记，整齐拷贝到新区域，原内存整体性回收 缺点：浪费内存 Mark - Compact (标记压缩) 基于copying，回收时直接整理内存 缺点：效率最低 GC 器 Serial GC: 优点：单线程精简的GC实现，无需维护复杂的数据结构，初始化简单，是client模式下JVM默认选项。最古老的GC。 缺点：会进入\"Stop-The World\"状态。 ParNew GC： 新生代GC实现，是SerialGC的多线程版本，最常见的应用场景是配合老年代的CMS GC 工作 CMS（Concurrent Mark Sweep）GC : 初始标记 (STW) -\u003e 并发标记 -\u003e 重新标记 (STW) (三色标记)-\u003e 并发清理 三色标记算法： 黑色：自己已经标记，子节点都标记完成。下次扫描不扫描 灰色：自己已经标记，子节点还没标记。下次扫描只扫描子节点 白色：没有遍历到的节点。 Incremental update 优点： 基于标记-清除（Mark-Sweep）算法，尽量减少停顿时间。 缺点： Incremental update天然bug，会有漏标的问题，所以CMS的remark阶段，必须重头扫描一遍，STW是所有时间最长的。存在碎片化问题，在长时间运行的情况下会发生full GC，导致恶劣停顿。会占用更多的CPU资源，和用户争抢线程。在JDK 9中被标记为废弃。 Parrallel GC（parallel Scavenge + parallel old）： 在JDK8等版本中，是server模式JVM的默认GC选择，也被称为吞吐量优先的GC，算法和Serial GC相似，特点是老生代和新生代GC并行进行，更加高效。 G1 GC： 兼顾了吞吐量和停顿时间的GC实现，是Oracle JDK 9后默认的GC 可以直观的设值停顿时间，相对于CMS GC ，G1未必能做到CMS最好情况下的延时停顿，但比最差情况要好得多 G1 仍存在年代的概念，G1物理上不分代，逻辑分代，使用了Region棋盘算法，实际上是标记-整理（Mark-Compact）算法，可以避免内存碎片，尤其是堆非常大的时候，G1优势更明显。 G1 吞吐量和停顿表现都非常不错。 ZGC： colored Pointer + Load Barrier 不再分代 shenandoah： 和 ZGC 类似 GC 调优 java -Xms200M -Xmx200m -XX:+PrintGC com.jvm -Xms200M -Xmx200m : 防止内存抖动，消耗资源 cpu 占用率居高不下如何调试 jvm 项目中，产生内存泄露的问题， 频繁GC但是回收不到内存，通过定位发现泄露是因为一个类创建了海量的对象 top #查看哪个进程占CPU比较高 top -Hp PID #查看进程里哪个线程占CPU jstack/PrintGC #找到进程，看是 VM GC 进程还是业务进程 #若是 GC 则一定是频繁的 full GC，使用 PrintGC 查看GC每次回收是否正常 #java -printgc -heapDumpOnOutOfMemoryError, OOM会下载dump文件 jmap #查看堆中对象占用内存的情况；查看堆转储文件 MAT/jhat/jvisualbm #进行dump文件分析 jmap 为了把里面的对象全输出出来，会 STW，让整个JVM卡死 jmap 命令在压测环境上观察的 机器做了负载均衡，发现问题后把有问题的机器从负载环境摘出来，再把堆转储文件导出来 使用tcpcopy复制两份，一份到生产环境，一份到测试环境 arthas： java -jar arthas-boot.jar # 运行后会自动找机器中java的进程 # 性能上有所降低，但不会stw dashboard # 展示线程占用、年代堆栈内存 heapdump # 替代jmap命令 thread -b # 查找线程中死锁，代替 jstack jvm jad # 在线反编译，在线定位一些问题 redefine # 在线修改class，临时解决版本bug。多台服务器写个脚本批量修改 trace # 查看方法所用时间 ","date":"2020-01-01","objectID":"/20200101_java-interview/:2:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"Multi Thread 启动线程的三种方式: extends Thread implements Runnable Executors.newCachedThread 线程的状态： classLoadProcess JVM内存模型 存储器的层次结构 l0:寄存器 l1:高速缓存 l2:高速缓存--------------cpu内部 -------------------------------- l3:高速缓存 l4:主存 l5:硬盘 l6:远程文件存储-----------cpu共享 -------------------------------- cache line 的概念 / 缓存行对其 / 伪共享 缓存行： 缓存行越大，局部性空间效率越高，但读取时间慢 缓存行越小，局部性空间效率越低，但读取时间快 目前用：64字节 缓存对其，在cpu内部的L2高速缓存处理时多线程，属于硬件问题： [x y main memory] [x y L3 cache] [x y L2][x y L2] [x y L1][x y L1] [计算单元与寄存器][计寄存器] 高速缓存数据一致性解决方法 老的CPU：总线锁 大大降低了性能 新的CPU：MESI Cache 数据一致性协议等(intel 用 MESI) + 总线锁 Modified Exclusive Shared Invalid 有些无法被缓存的数据，或者跨越多个缓存行的数据，依然必须使用总线锁 缓存行对其 / 伪共享 位于同一缓存行的两个不同数据，被两个不同CPU锁定，产生互相影响的伪共享问题 解决方法： 缓存行对其：扩大字节数，使其不在统一缓存行 private static class Padding { public long p1, p2, p3, p4, p5, p6, p7; //cache line padding private volatile long cursor = INITIAL_CURSOR_VALUE; public long p8, p9, p10, p11, p12, p13, p14; //cache line padding } 乱序问题 cpu为了提高指令执行效率，去同时执行另一条指令（前提两条指令没有依赖关系:int a = 0; a++;）这样的cpu的执行就是乱序的。 而且 必须使用Memory Barrier来做好指令排序 volatile的底层就是这么实现的(windows 是 lock 指令) 如何保证特定情况下不乱序 volatile 有序： 使用 CPU 内存屏障， 原理： sfence指令前的写操作必须在sfence指令后的写操作前完成 Ifence指令前的读操作必须在Ifence指令后的读操作前完成 mfence指令前的读写操作必须在mfence指令后的读写操作前完成 实际使用的是 Intel lock 汇编指令 volatile 作用： 保持线程间的可见性 禁止指令重排序（通过内存屏障） 关于 Object o = new Object() 解释对象的创建过程（半初始化） T t = new T() jvm 编译成 class 汇编码5条指令: new #2 \u003cT\u003e //申请内存空间, 成员变量是默认值 dup invokespecial #3 \u003cT.\u003cinit\u003e\u003e //调用初始化方法，成员变量初始化 astore //t 和 new 出的对象进行关联 return 由于指令重排的存在 invokespecial #3 \u003cT.\u003cinit\u003e\u003e astore //有可能乱序执行,先指向内存再初始化变量 DCL与volatile问题（指令重排） 1.Double Check Lock public static volatile T INSTANCE; public static T getInstance(){ if (INSTANCE == null){ synchronized(T.class){ if(INSTANCE == null){ try{ Thread.sleep(1); }catch(InterruptedException e) { e.printStackTrace(); } INSTANCE = new T(); } } } } 2.DCL单例必须要加volatile 指令重排，对象初始化时先指向内存，再初始化赋值 若此时线程2进入，则直接拿到未初始化赋值的对象 对象在内存中的存储布局（对象与数组的存储不同） 对象头具体包括什么（markword klasspointer） synchronized锁信息 对象怎么定位（直接 间接） 对象怎么分配（栈上 线程本地 Eden Old） Object o = new Object()在内存中占用多少字节 ","date":"2020-01-01","objectID":"/20200101_java-interview/:3:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"Redis 项目中Redis的应用场景 五大value类型 基本上就是缓存 为的是服务无状态，（延伸：看项目中有哪些数据结构，如分布式锁，抽出来放到Redis） 无锁化 Redis是单线程还是多线程 无论哪个版本，工作线程就是一个 6.x 高版本出现了IO多线程 关于各种框架优化的回答 问法：做过什么优化？ 解决过什么问题？遇到哪些问题？ 说明业务场景； 遇到了什么问题 –\u003e 往往通过监控工具结合报警系统； 排查问题； 解决手段； 问题被解决。 ","date":"2020-01-01","objectID":"/20200101_java-interview/:4:0","tags":["Interview"],"title":"Java Basic","uri":"/20200101_java-interview/"},{"categories":["Technology"],"content":"Article description.","date":"2019-12-29","objectID":"/20191229_copy-in-java/","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Cloning is a process of creating an exact copy of an existing object in the memory. In java, clone() method of java.lang.Object class is used for cloning process. This method creates an exact copy of an object on which it is called through field-by-field assignment and returns the reference of that object. Not all the objects in java are eligible for cloning process. The objects which implement Cloneable interface are only eligible for cloning process. Cloneable interface is a marker interface which is used to provide the marker to cloning process. Click here to see more info on clone() method in java. ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:0:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Preface Object 类中有方法clion()，具体方法如下： protected native Object clone() throws CloneNotSupportedException; 该方法由 protected 修饰，java中所有类默认是继承Object类的，重载后的clone()方法为了保证其他类都可以正常调用，修饰符需要改成public。 该方法是一个native方法，被native修饰的方法实际上是由非Java代码实现的，效率要高于普通的java方法。 该方法的返回值是Object对象，因此我们需要强转成我们需要的类型。 该方法抛出了一个CloneNotSupportedException异常，意思就是不支持拷贝，需要我们实现Cloneable接口来标记，这个类支持拷贝。 为了演示，我们新建两个实体类Dept 和 User，其中User依赖了Dept，实体类代码如下： Dept 类： @Data @AllArgsConstructor @NoArgsConstructor public class Dept { private int deptNo; private String name; } User 类： @Data @AllArgsConstructor @NoArgsConstructor public class User { private int age; private String name; private Dept dept; } ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:1:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Shallow Copy 对于基本类型的的属性，浅拷贝会将属性值复制给新的对象，而对于引用类型的属性，浅拷贝会将引用复制给新的对象。而像String，Integer这些引用类型，都是不可变的，拷贝的时候会创建一份新的内存空间来存放值，并且将新的引用指向新的内存空间。不可变类型是特殊的引用类型，我们姑且认为这些被final标记的引用类型也是复制值。 shallowCopy 浅拷贝功能实现 @Data @AllArgsConstructor @NoArgsConstructor public class User implements Cloneable{ private int age; private String name; private Dept dept; @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } } 如何验证我们的结论呢？首先对比被拷贝出的对象和原对象是否相等，不等则说明是新拷贝出的一个对象。其次修改拷贝出对象的基本类型属性，如果原对象的此属性发生了修改，则说明基本类型的属性是同一个，最后修改拷贝出对象的引用类型对象即Dept属性，如果原对象的此属性发生了改变，则说明引用类型的属性是同一个。清楚测试原理后，我们写一段测试代码来验证我们的结论。 public static void main(Strign[] args) thows Exception{ Dept dept = new Dept(12,\"市场部\"); User user = new User(18,\"Java\", dept); User user1 = (User)user.clone(); System.out.println(user == user1); user1.setAge(20); System.out.println(user); System.out.println(user1); dept.setName(\"研发部\"); System.out.println(user); System.out.println(user1); } 运行结果如下： false User{age=18, name='Java', dept=Dept{deptNo=12, name='市场部'}} User{age=20, name='Java', dept=Dept{deptNo=12, name='市场部'}} User{age=18, name='Java', dept=Dept{deptNo=12, name='研发部'}} User{age=20, name='Java', dept=Dept{deptNo=12, name='研发部'}} ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:2:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Technology"],"content":"Deep Copy 相较于浅拷贝而言，深拷贝除了会将基本类型的属性复制外，还会将引用类型的属性也会复制。 DeepCopy 深拷贝功能实现 在拷贝user的时候，同事将user中的dept属性进行拷贝。 dept 类： @Data @AllArgsConstructor @NoArgsXonstructor public class Dept implements Cloneable{ private int deptNo; private String name; @Override public Object clone() throws CloneNotSupportedException{ return super.clone(); } } user 类： @Data @AllArgsConstructor @NoArgsConstructor public class User implements Cloneable{ private int age; private String name; private Dept dept; @Override protected Object clone() throws CloneNotSupportedException { User user = (User) super.clone(); user.dept = (Dept) dept.clone(); return user; } } 使用浅拷贝的测试代码继续测试，运行结果如下： false User{age=18, name='Java旅途', dept=Dept{deptNo=12, name='市场部'}} User{age=20, name='Java旅途', dept=Dept{deptNo=12, name='市场部'}} User{age=18, name='Java旅途', dept=Dept{deptNo=12, name='研发部'}} User{age=20, name='Java旅途', dept=Dept{deptNo=12, name='市场部'}} 除此之外，还可以利用反序列化实现深拷贝，先将对象序列化成字节流，然后再将字节流序列化成对象，这样就会产生一个新的对象。 ","date":"2019-12-29","objectID":"/20191229_copy-in-java/:3:0","tags":["Java"],"title":"Shallow Copy and Deep Copy in JAVA","uri":"/20191229_copy-in-java/"},{"categories":["Life"],"content":"Article description.","date":"2019-12-18","objectID":"/20191218_2019-summary-video/","tags":["Life","Video","USA","Graduation"],"title":"Memories of 2019","uri":"/20191218_2019-summary-video/"},{"categories":["Life"],"content":"Photos and videos recorded in 2019. ","date":"2019-12-18","objectID":"/20191218_2019-summary-video/:0:0","tags":["Life","Video","USA","Graduation"],"title":"Memories of 2019","uri":"/20191218_2019-summary-video/"},{"categories":["Technology"],"content":"Article description.","date":"2019-11-26","objectID":"/20191126_git-contribution/","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Contribution graph shows activity from public repositories. You can choose to show activity from both public and private repositories, with specific details of your activity in private repositories anonymized. ","date":"2019-11-26","objectID":"/20191126_git-contribution/:0:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Update local git config git config --global user.name “github’s Name” git config --global user.email \"github@*.com\" ","date":"2019-11-26","objectID":"/20191126_git-contribution/:1:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Update commit history If you do not want to waste your commit history. we can use ‘git log’ to see the git record git log we need to edit all of the history of commit and push. git filter-branch -f --env-filter ' if [ \"$GIT_AUTHOR_NAME\" = \"oldName\" ] then export GIT_AUTHOR_NAME=\"newName\" export GIT_AUTHOR_EMAIL=\"newEmail\" fi ' HEAD git filter-branch -f --env-filter ' if [ \"$GIT_COMMITTER_NAME\" = \"oldName\" ] then export GIT_COMMITTER_NAME=\"newName\" export GIT_COMMITTER_EMAIL=\"newEmail\" fi ' HEAD 如果无差别把所有都改的话去掉if..fi git filter-branch -f --env-filter \" GIT_AUTHOR_NAME='newName'; GIT_AUTHOR_EMAIL='newEmail'; GIT_COMMITTER_NAME='newName'; GIT_COMMITTER_EMAIL='newEmail' \" HEAD ","date":"2019-11-26","objectID":"/20191126_git-contribution/:2:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Update Git push 你这里将你本地git的账户和邮箱重新设置了,但是github并没有那么智能就能判断你是原来你系统默认的用户. 也就是说你新配置的用户和你默认的被github识别成两个用户. 这样你以后操作的时候commit 或者 push的时候有可能产生冲突. Solution: 使用强制push的方法: git push -u origin master -f 这样会使远程修改丢失，一般是不可取的，尤其是多人协作开发的时候。 push前先将远程repository修改pull下来 git pull origin master git push -u origin master 若不想merge远程和本地修改，可以先创建新的分支： git branch [name] #然后push git push -u origin [name] ","date":"2019-11-26","objectID":"/20191126_git-contribution/:3:0","tags":["Git"],"title":"Git Contribution","uri":"/20191126_git-contribution/"},{"categories":["Technology"],"content":"Article description.","date":"2019-10-07","objectID":"/20191007_u32/","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"C/C++ provides various data types that can be used in your programs.In general, you’d commonly use: int for most variables and “countable” things (for loop counts, variables, events). char for characters and strings. float for general measurable things (seconds, distance, temperature). uint32 for bit manipulations, especially on 32-bit registers. int for most variables and “countable” things (for loop counts, variables, events) char for characters and strings float for general measurable things (seconds, distance, temperature) uint32_t for bit manipulations, especially on 32-bit registers ","date":"2019-10-07","objectID":"/20191007_u32/:0:0","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Integer Data Types C type alias Bits Sign Range char int8 8 Signed -128 .. 127 unsigned char uint8 8 Unsigned 0 .. 255 short int16 16 Signed -32,768 .. 32,767 unsigned short uint16 16 Unsigned 0 .. 65,535 int int32 32 Signed -2,147,483,648 .. 2,147,483,647 unsigned int uint32 32 Unsigned 0 .. 4,294,967,295 long long int64 64 Signed -9,223,372,036,854,775,808 .. 9,223,372,036,854,775,807 unsigned long long uint64 64 Unsigned 0 .. 18,446,744,073,709,551,615 ","date":"2019-10-07","objectID":"/20191007_u32/:0:1","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Floating Point Data Types C type IEE754 Name Bits Range float Single Precision 32 -3.4E38 .. 3.4E38 double Double Precision 64 -1.7E308 .. 1.7E308 ","date":"2019-10-07","objectID":"/20191007_u32/:0:2","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Print format %[flags][width][.prec][length]type %[标志][最小宽度][.精度][类型长度]类型。 1. Type 字符 对应数据类型 含义 示例 d/i int 输出十进制有符号32bits整数，i是老式写法 printf(\"%i\",123);输出123 o unsigned int 无符号8进制(octal)整数(不输出前缀0) printf(\"0%o\",123);输出0173 u unsigned int 无符号10进制整数 printf(\"%u\",123);输出123 x/X unsigned int 无符号16进制整数，x对应的是abcdef，X对应的是ABCDEF（不输出前缀0x) printf(\"0x%x 0x%X\",123,123);输出0x7b 0x7B f/lf float(double) 单精度浮点数用f,双精度浮点数用lf(printf可混用，但scanf不能混用) printf(\"%.9f %.9lf\",0.000000123,0.000000123);输出0.000000123 0.000000123。注意指定精度，否则printf默认精确到小数点后六位 F float(double) 与f格式相同，只不过 infinity 和 nan 输出为大写形式。 例如printf(\"%f %F %f %F\\n\",INFINITY,INFINITY,NAN,NAN);输出结果为inf INF nan NAN e/E float(double) 科学计数法，使用指数(Exponent)表示浮点数，此处”e”的大小写代表在输出时“e”的大小写 printf(\"%e %E\",0.000000123,0.000000123);输出1.230000e-07 1.230000E-07 g float(double) 根据数值的长度，选择以最短的方式输出，%f或%e printf(\"%g %g\",0.000000123,0.123);输出1.23e-07 0.123 G float(double) 根据数值的长度，选择以最短的方式输出，%f或%E printf(\"%G %G\",0.000000123,0.123);输出1.23E-07 0.123 c char 字符型。可以把输入的数字按照ASCII码相应转换为对应的字符 printf(\"%c\\n\",64)输出A s char* 字符串。输出字符串中的字符直至字符串中的空字符（字符串以空字符’\\0‘结尾） printf(\"%s\",\"测试test\");输出：测试test S wchar_t* 宽字符串。输出字符串中的字符直至字符串中的空字符（宽字符串以两个空字符’\\0‘结尾） setlocale(LC_ALL,\"zh_CN.UTF-8\"); wchar_t wtest[]=L\"测试Test\"; printf(\"%S\\n\",wtest); 输出：测试test p void* 以16进制形式输出指针 printf(\"%010p\",\"lvlv\");输出：0x004007e6 n int* 什么也不输出。%n对应的参数是一个指向signed int的指针，在此之前输出的字符数将存储到指针所指的位置 int num=0; printf(\"lvlv%n\",\u0026num); printf(\"num:%d\",num); 输出:lvlvnum:4 m 无 打印errno值对应的出错内容 printf(\"%m\\n\"); a/A float(double) 十六进制p计数法输出浮点数，a为小写，A为大写 printf(\"%a %A\",15.15,15.15);输出：0x1.e4ccccccccccdp+3 0X1.E4CCCCCCCCCCDP+3 2. Flags 字符 名称 说明 - 减号 结果左对齐，右边填空格。默认是右对齐，左边填空格。 + 加号 输出符号(正号或负号) space 空格 输出值为正时加上空格，为负时加上负号 # 井号 type是o、x、X时，增加前缀0、0x、0X。 type是a、A、e、E、f、g、G时，一定使用小数点。默认的，如果使用.0控制不输出小数部分，则不输出小数点。 type是g、G时，尾部的0保留。 0 数字零 将输出的前面补上0，直到占满指定列宽为止（不可以搭配使用“-”） example: printf(\"%5d\\n\",1000); //默认右对齐,左边补空格 //output:1000 printf(\"%-5d\\n\",1000); //左对齐,右边补空格 //output:1000 printf(\"%+d %+d\\n\",1000,-1000); //输出正负号 //output:+1000 -1000 printf(\"% d % d\\n\",1000,-1000); //正号用空格替代，负号输出 //output:1000 -1000 printf(\"%x %#x\\n\",1000,1000); //输出0x //output:3e8 0x3e8 printf(\"%.0f %#.0f\\n\",1000.0,1000.0)//当小数点后不输出值时依然输出小数点 //output:1000 1000. printf(\"%g %#g\\n\",1000.0,1000.0); //保留小数点后后的0 //output:1000 1000.00 printf(\"%05d\\n\",1000); //前面补0 //01000 3. width 输出最小宽度 width 描述 示例 数值 十进制整数 printf(\"%06d\",1000);输出:001000 * 星号。不显示指明输出最小宽度，而是以星号代替，在printf的输出参数列表中给出 printf(\"%0*d\",6,1000);输出:001000 4. precision .precision 描述 .数值 十进制整数。 (1)对于整型（d,i,o,u,x,X）,precision表示输出的最小的数字个数，不足补前导零，超过不截断。(2)对于浮点型（a, A, e, E, f ），precision表示小数点后数值位数，默认为六位，不足补后置0，超过则截断。 (3)对于类型说明符g或G，表示可输出的最大有效数字。 (4)对于字符串（s），precision表示最大可输出字符数，不足正常输出，超过则截断。 precision不显示指定，则默认为0 .* 以星号代替数值，类似于width中的*，在输出参数列表中指定精度。 5. length 类型长度指明待输出数据的长度。因为相同类型可以有不同的长度，比如整型有16bits的short int，32bits的int，也有64bits的long int，浮点型有32bits的单精度float和64bits的双精度double。为了指明同一类型的不同长度，于是乎，类型长度（length）应运而生，成为格式控制字符串的一部分。 typeTable ","date":"2019-10-07","objectID":"/20191007_u32/:0:3","tags":["C"],"title":"Various data types in C","uri":"/20191007_u32/"},{"categories":["Technology"],"content":"Article description.","date":"2019-08-25","objectID":"/20190825_struct-size/","tags":["C"],"title":"The size of structure in C","uri":"/20190825_struct-size/"},{"categories":["Technology"],"content":"The sizeof for a struct is not always equal to the sum of sizeof of each individual member. This is because of the padding added by the compiler to avoid alignment issues. Padding is only added when a structure member is followed by a member with a larger size or at the end of the structure. Different compilers might have different alignment constraints as C standards state that alignment of structure totally depends on the implementation. Case 1: struct A { // sizeof(int) = 4 int x; // Padding of 4 bytes // sizeof(double) = 8 double z; // sizeof(short int) = 2 short int y; // Padding of 6 bytes }; Output: ​ Size of struct: 24 struct_sizeof_ex1 The red portion represents the padding added for data alignment and the green portion represents the struct members. In this case, x (int) is followed by z (double), which is larger in size as compared to x. Hence padding is added after x. Also, padding is needed at the end for data alignment. Case 2: struct B { // sizeof(double) = 8 double z; // sizeof(int) = 4 int x; // sizeof(short int) = 2 short int y; // Padding of 2 bytes }; Output: ​ Size of struct: 16 struct_sizeof_ex2 In this case, the members of the structure are sorted in decreasing order of their sizes. Hence padding is required only at the end. Case 3: struct C { // sizeof(double) = 8 double z; // sizeof(short int) = 2 short int y; // Padding of 2 bytes // sizeof(int) = 4 int x; }; Output: ​ Size of struct: 16 struct_sizeof_ex3 In this case, y (short int) is followed by x (int) and hence padding is required after y. No padding is needed at the end in this case for data alignment. C language doesn’t allow the compilers to reorder the struct members to reduce the amount of padding. In order to minimize the amount of padding, the struct members must be sorted in a descending order (similar to the case 2). ","date":"2019-08-25","objectID":"/20190825_struct-size/:0:0","tags":["C"],"title":"The size of structure in C","uri":"/20190825_struct-size/"},{"categories":["Technology"],"content":"Article description.","date":"2019-07-17","objectID":"/20190717_typedef/","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"A typedef is a C keyword that defines a new name for a data type, including internal data types (int, char, etc.) and custom data types (struct, etc.). A typedef is itself a type of stored class keyword that cannot appear in the same expression as the keywords auto, extern, static, register, etc. ","date":"2019-07-17","objectID":"/20190717_typedef/:0:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"1. 概述 typedef为C语言的关键字，作用是为一种数据类型定义一个新名字，这里的数据类型包括内部数据类型（int，char等）和自定义的数据类型（struct等）。 typedef本身是一种存储类的关键字，与auto、extern、static、register等关键字不能出现在同一个表达式中。 ","date":"2019-07-17","objectID":"/20190717_typedef/:1:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"2. 作用及用法 ","date":"2019-07-17","objectID":"/20190717_typedef/:2:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"2.1 typedef的用法 使用typedef定义新类型的方法：在传统的变量声明表达式里用（新的）类型名替换变量名，然后把关键字typedef加在该语句的开头就行了。 下面以两个示例，描述typedef的用法步骤。 示例1： int a; ———— 传统变量声明表达式 typedef int myint_t; ———— 使用新的类型名myint_t替换变量名a。在语句开头加上typedef关键字，myint_t就是我们定义的新类型 示例2： void (*pfunA)(int a); ———— 传统变量（函数）声明表达式 typedef void (*PFUNA)(int a); ———— 使用新的类型名PFUNA替换变量名pfunA。在语句开头加上typedef关键字，PFUNA就是我们定义的新类型 ","date":"2019-07-17","objectID":"/20190717_typedef/:2:1","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"2.2 typedef的作用 typedef的作用有以下几点： 1）typedef的一个重要用途是定义机器无关的类型。例如，定义一个叫REAL的浮点类型，该浮点类型在目标机器上可以获得最高的精度： typedef long double REAL; 如果在不支持 long double 的机器上运行相关代码，只需要对对应的typedef语句进行修改，例如： typedef double REAL; 或者： typedef float REAL; 2）使用typedef为现有类型创建别名，给变量定义一个易于记忆且意义明确的新名字。 例如: typedef unsigned int UINT 3）使用typedef简化一些比较复杂的类型声明。 例如： typedef void (*PFunCallBack)(char* pMsg, unsigned int nMsgLen); 上述声明引入了PFunCallBack类型作为函数指针的同义字，该函数有两个类型分别为char*和unsigned int参数，以及一个类型为int的返回值。通常，当某个函数的参数是一个回调函数时，可能会用到typedef简化声明。 例如，承接上面的示例，我们再列举下列示例： RedisSubCommand(const string\u0026 strKey, PFunCallBack pFunCallback, bool bOnlyOne); 注意：类型名PFunCallBack与变量名pFunCallback的大小写区别。 RedisSubCommand函数的参数是一个PFunCallBack类型的回调函数，返回某个函数（pFunCallback）的地址。在这个示例中，如果不用typedef，RedisSubCommand函数声明如下： RedisSubCommand(const string\u0026 strKey, void (*pFunCallback)(char* pMsg, unsigned int nMsgLen), bool bOnlyOne); 从上面两条函数声明可以看出，不使用typedef的情况下，RedisSubCommand函数的声明复杂得多，不利于代码的理解，并且增加的出错风险。 所以，在某些复杂的类型声明中，使用typedef进行声明的简化是很有必要的。 ","date":"2019-07-17","objectID":"/20190717_typedef/:2:2","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"3. typedef与#define 两者的区别如下： #define进行简单的进行字符串替换。 #define宏定义可以使用#ifdef、#ifndef等来进行逻辑判断，还可以使用#undef来取消定义。 typedef是为一个类型起新名字。typedef符合（C语言）范围规则，使用typedef定义的变量类型，其作用范围限制在所定义的函数或者文件内（取决于此变量定义的位置），而宏定义则没有这种特性。 通常，使用typedef要比使用#define要好，特别是在有指针的场合里。 下面列举几个示例。 3.1 示例1 代码如下： typedef　char*　pStr1; #define　pStr2　char*　pStr1　s1, s2; pStr2　s3, s4; 在上述的变量定义中，s1、s2、s3都被定义为char类型；但是s4则定义成了char类型，而不是我们所预期的指针变量char，这是因为#define只做简单的字符串替换，替换后的相关代码等同于为： char*　s3, s4; 而使用typedef为char*定义了新类型pStr1后，相关代码等同于为： char *s3, *s4; 3.1 示例2 代码如下： typedef char *pStr; char string[5]=\"test\"; const char *p1=string; const pStr p2=string; p1++; p2++; error:increment of read-only variable 'p2' 根据错误信息，能够看出p2为只读的常量了，所以p2++出错了。这个问题再一次提醒我们：typedef和#define不同，typedef不是简单的文本替换，上述代码中const pStr p2并不等于const char * p2，pStr是作为一个类型存在的，所以const pStr p2实际上限制了pStr类型的p2变量，对p2常量进行了只读限制。也就是说，const pStr p2和pStr const p2本质上没有区别（可类比const int p2和int const p2），都是对变量p2进行只读限制，只不过此处变量p2的数据类型是我们自己定义的（pStr），而不是系统固有类型（如int）而已。 所以，const pStr p2的含义是：限定数据类型为char *的变量p2为只读，因此p2++错误。 注意：在本示例中，typedef定义的新类型与编译系统固有的类型没有差别。 ","date":"2019-07-17","objectID":"/20190717_typedef/:3:0","tags":["C"],"title":"The descriptions of typedef","uri":"/20190717_typedef/"},{"categories":["Technology"],"content":"Article description.","date":"2019-06-11","objectID":"/20190611_conditional-compilation/","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"Conditional compilation is static compilation of code based on the actual definition of macros (some kind of condition). Compilation conditions can be determined based on the value of an expression or whether a particular macro is defined. C language conditional compilation related precompilation instructions, including #define, #undef, #ifdef, #ifndef, #if, #elif, #else, #endif, defined. ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:0:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"预编译指令 #define 定义一个预处理宏 #undef 取消宏的定义 #if 编译预处理中的条件命令，相当于C语法中的if语句 #ifdef 判断某个宏是否被定义，若已定义，执行随后的语句 #ifndef 与#ifdef相反，判断某个宏是否未被定义 #elif 若#if, #ifdef, #ifndef或前面的#elif条件不满足，则执行#elif之后的语句，相当于C语法中的else-if #else 与#if, #ifdef, #ifndef对应, 若这些条件不满足，则执行#else之后的语句，相当于C语法中的else #endif #if, #ifdef, #ifndef这些条件命令的结束标志. #defined 与#if, #elif配合使用，判断某个宏是否被定义 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:1:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"条件编译 条件编译是根据实际定义宏（某类条件）进行代码静态编译的手段。可根据表达式的值或某个特定宏是否被定义来确定编译条件。 最常见的条件编译是防止重复包含头文件的宏，形式跟下面代码类似： #ifndef ABCD_H #define ABCD_H // ... some declaration codes #endif // #ifndef ABCD_H 在实现文件中通常有如下类似的定义： #ifdef _DEBUG // ... do some operations #endif #ifdef _WIN32 // ... use Win32 API #endif ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:2:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"预编译指令应用举例 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:0","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"1. #define、#undef #define命令定义一个宏: #define MACRO_NAME[(args)] [tokens[(opt)]] 之后出现的MACRO_NAME将被替代为所定义的标记(tokens)。宏可带参数，而后面的标记也是可选的。 宏定义，按照是否带参数通常分为对象宏、函数宏两种。 对象宏: 不带参数的宏被称为\"对象宏(objectlike macro)\"。对象宏多用于定义常量、通用标识。例如： // 常量定义 #define MAX_LENGTH 100 // 通用标识，日志输出宏 #define SLog printf // 预编译宏 #define _DEBUG 函数宏：带参数的宏。利用宏可以提高代码的运行效率: 子程序的调用需要压栈出栈, 这一过程如果过于频繁会耗费掉大量的CPU运算资源。 所以一些代码量小但运行频繁的代码如果采用带参数宏来实现会提高代码的运行效率。但多数c++程序不推荐使用函数宏，调试上有一定难度，可考虑使用c++的inline代替之。例如： // 最小值函数 #define MIN(a,b) ((a)\u003e(b)? (a):(b)) // 安全释放内存函数 #define SAFE_DELETE(p) {if(NULL!=p){delete p; p = NULL;}} #undef可以取消宏定义，与#define对应。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:1","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"2. defined defined用来测试某个宏是否被定义。defined(name): 若宏被定义，则返回1，否则返回0。 它与#if、#elif、#else结合使用来判断宏是否被定义，乍一看好像它显得多余, 因为已经有了#ifdef和#ifndef。defined可用于在一条判断语句中声明多个判别条件；#ifdef和#ifndef则仅支持判断一个宏是否定义。 #if defined(VAX) \u0026\u0026 defined(UNIX) \u0026\u0026 !defined(DEBUG) 和#if、#elif、#else不同，#ifdef、#ifndef、defined测试的宏可以是对象宏，也可以是函数宏。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:2","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"3. #ifdef、#ifndef、#else、#endif 条件编译中相对常用的预编译指令。模式如下： #ifdef ABC // ... codes while definded ABC #elif (CODE_VERSION \u003e 2) // ... codes while CODE_VERSION \u003e 2 #else // ... remained cases #endif #ifdef用于判断某个宏是否定义，和#ifndef功能正好相反，二者仅支持判断单个宏是否已经定义，上面例子中二者可以互换。如果不需要多条件预编译的话，上面例子中的#elif和#else均可以不写。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:3","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"4. #if、#elif、#else、#endif 在判断某个宏是否被定义时，应当避免使用#if，因为该宏的值可能就是被定义为0。而应当使用#ifdef或#ifndef。 注意: #if、#elif之后的宏只能是对象宏。如果宏未定义，或者该宏是函数宏，则编译器可能会有对应宏未定义的警告。 ","date":"2019-06-11","objectID":"/20190611_conditional-compilation/:3:4","tags":["C"],"title":"Conditional compilation directives","uri":"/20190611_conditional-compilation/"},{"categories":["Technology"],"content":"Article description.","date":"2019-05-10","objectID":"/20190510_pythonintroduction/","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. Python is often described as a “batteries included” language due to its comprehensive standard library. ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:0:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"1. 数据类型转换 整型 浮点 bool 复数 string list tuple set dict 整型 \\ 加.0 int(0) 加0j 任意 × × × × 浮点 去小数 \\ 0.0 加0j 数据 × × × × bool true1 false0 true1.0 false0.0 \\ true1+0j 可以 × × × × 复数 × × 0j \\ 转换 × × × × string 纯数字 纯数字 '' 纯数\u0026+0j \\ 每个字符转成每个值 每个字符转成每个值 每个字符转成每个值+去重 × list × × [] × 成为 \\ 内容不变 随机+去重 有2个数据的二级列表 tuple × × () × str 内容不变 \\ 随机+去重 有2个数据的二级列表 set × × set() × 格式 内容不变+随机 内容不变+随机 \\ 有2个数据的二级列表 dict × × {} × 数据 仅保留键 仅保留键 仅保留键 \\ ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:1:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"2. 身份运算 判断地址是否相同 x is y x is not y 1.字符串：字符串值相同，ID相同。 2.列表、字典、集合：无论什么情况ID都不同 ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:2:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"3. 成员检测运算 val1 in val2 val1 not in val2 //检测一个数据是否在容器中 ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:3:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"4. 流程控制 1. if ___: elif ___: elif ___: else: 2. while ___: else: 3. for x in 容器: ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:4:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"5. 函数 def func( name, sex = “male”, *args, like = “乒乓”, **kwargs); ↓ ↓ ↓ ↓ ↓ ↓ func( “刘佳锐”, “男”, “乐色”, “垃圾”, like = “羽毛球”, skin = “yellow”, hobby = “hello”) print(locals()) //获取当前作用域的局部变量 print(globals()) //获取当前作用域的全局变量 ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:5:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"6. 迭代器 能被next()函数调用并不断返回下一个值的对象 特征：迭代器会生成惰性序列，通过计算把值依次返回 优点：需要数据的时候，一次取一个可以很大节省内存 检测： from collections import Iterable, Iterator isinstance()判断数据类型，返回bool值 print(isinstance(list1,list1)) Iter：使用iter可把迭代数据变为迭代器 list1 = [1,2,3] result = iter(list1) print(next(result)) ","date":"2019-05-10","objectID":"/20190510_pythonintroduction/:6:0","tags":["Python"],"title":"Python Introduction","uri":"/20190510_pythonintroduction/"},{"categories":["Technology"],"content":"Article description.","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"Performance optimization methods and ideas for large C projects. Performance optimization strategies for x86 projects that encounter performance bottlenecks when porting to low performance processors. ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:0:0","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"通常优化方法 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:0","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"1. 宏定义或内联 短的、调用频繁的函数改为宏定义或内联函数，减少调用层级 可能编译器已经做了部分优化，效果不一定明显 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:1","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"2. 固定次数的短循环展开 循环语句如果循环次数已知，且是短循环，可以将语句展开 编译器可能已做优化，效果不一定明显 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:2","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"3. 减少内存分配和释放的次数 频繁使用的变量能用全局变量的尽量不用局部变量 函数体内部的局部变量，如果大小不是特别大，尽量不用动态分配空间 数据结构中，尽量不用指针变量 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:3","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"4. 移位代替乘除 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:4","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"5. 条件语句优化 根据分支被执行的频率将频繁执行的分支放在前面部分 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:5","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"6. 数据结构/算法 数据结构中尽量减少需要动态分配空间的指针，改用联合、结构体或固定大小的缓存区 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:6","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"7. 使用并行程序 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:1:7","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"代码重构原则 维护一套代码，使用宏定义控制 小步前进，重构一部分进行充分测试后再重构下一部分 ","date":"2019-04-09","objectID":"/20190409_cperformance-optimization/:2:0","tags":["Tuning","C"],"title":"C Project Performance Tuning","uri":"/20190409_cperformance-optimization/"},{"categories":["Technology"],"content":"Article description.","date":"2019-03-04","objectID":"/20190304_gdb-debug/","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"GDB is a powerful program debugging tool based on command line under UNIX/LINUX operating system released by GNU Source Organization. For a C/C ++ programmer working on Linux, GDB is an essential tool. ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:0:0","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"gdb的使用 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:0","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"启动gdb 编译一个测试程序，-g表示可以调试 gcc -g demo.c -o demo 启动gdb gdb test 或者 gdb -q test //表示不打印gdb版本信息，界面较为干净； ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:1","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"查看源码 list ：简记为 l ，其作用就是列出程序的源代码，默认每次显示10行。 list 行号：将显示当前文件以“行号”为中心的前后10行代码，如：list 12 list 函数名：将显示“函数名”所在函数的源代码，如：list main list ：不带参数，将接着上一次 list 命令的，输出下边的内容。 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:2","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"运行程序 run：简记为 r ，其作用是运行程序，当遇到断点后，程序会在断点处停止运行，等待用户输入下一步的命令。 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:3","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"设置参数 set args 参数1 参数2 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:4","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"设置断点 break n （简写b n）:在第n行处设置断点.可以带上代码路径和代码名称： b demo.cpp:578 b fn1 if a＞b：条件断点设置 break func（break缩写为b）：在函数func()的入口处设置断点，如：break cb_button delete 断点号n：删除第n个断点 disable 断点号n：暂停第n个断点 enable 断点号n：开启第n个断点 clear 行号n：清除第n行的断点 info b （info breakpoints）：显示当前程序的断点设置情况 delete breakpoints：清除所有断点 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:5","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"单步执行 continue （简写c ）：继续执行，到下一个断点处（或运行结束） next：（简写 n），单步跟踪程序，当遇到函数调用时，也不进入此函数体；此命令同 step 的主要区别是，step 遇到用户自定义的函数，将步进到函数中去运行，而 next 则直接调用函数，不会进入到函数体内。 step （简写s）：单步调试如果有函数调用，则进入函数；与命令n不同，n是不进入调用的函数的 until：当你厌倦了在一个循环体内单步跟踪时，这个命令可以运行程序直到退出循环体。 until+行号： 运行至某行，不仅仅用来跳出循环 finish： 运行程序，直到当前函数完成返回，并打印函数返回时的堆栈地址和返回值及参数值等信息。 call 函数(参数)：调用程序中可见的函数，并传递“参数”，如：call gdb_test(55) ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:6","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"查看变量 print 表达式：简记为 p ，其中“表达式”可以是任何当前正在被测试程序的有效表达式，比如当前正在调试C语言的程序，那么“表达式”可以是任何C语言的有效表达式，包括数字，变量甚至是函数调用。 print a：将显示整数 a 的值 print ++a：将把 a 中的值加1,并显示出来 print name：将显示字符串 name 的值 print gdb_test(22)：将以整数22作为参数调用 gdb_test() 函数 print gdb_test(a)：将以变量 a 作为参数调用 gdb_test() 函数 display 表达式：在单步运行时将非常有用，使用display命令设置一个表达式后，它将在每次单步进行指令后，紧接着输出被设置的表达式及值。如： display a watch 表达式：设置一个监视点，一旦被监视的“表达式”的值改变，gdb将强行终止正在被调试的程序。如： watch a whatis ：查询变量或函数 info function： 查询函数 扩展info locals： 显示当前堆栈页的所有变量 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:7","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"查看运行信息 where/bt ：当前运行的堆栈列表 bt backtrace 显示当前调用堆栈 up/down 改变堆栈显示的深度 set args 参数:指定运行时的参数 show args：查看设置好的参数 info program： 来查看程序的是否在运行，进程号，被暂停的原因。 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:8","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"分割窗口 layout：用于分割窗口，可以一边查看代码，一边测试 layout src：显示源代码窗口 layout asm：显示反汇编窗口 layout regs：显示源代码/反汇编和CPU寄存器窗口 layout split：显示源代码和反汇编窗口 Ctrl + L：刷新窗口 ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:9","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"退出gdb quit：简记为 q ，退出gdb ","date":"2019-03-04","objectID":"/20190304_gdb-debug/:1:10","tags":["Linux","C"],"title":"GDB debug guide","uri":"/20190304_gdb-debug/"},{"categories":["Technology"],"content":"Article description.","date":"2019-02-03","objectID":"/20190203_huge-pages/","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"Reduce page tables by increasing the size of operating system pages to avoid fast table misses. Large page memory optimizer is designed for malloc mechanism, which means allocating large pages to increase TLB hit ratio. ","date":"2019-02-03","objectID":"/20190203_huge-pages/:0:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"原理 大页内存的原理涉及到操作系统的虚拟地址到物理地址的转换过程。操作系统为了能同时运行多个进程，会为每个进程提供一个虚拟的进程空间，在32位操作系统上，进程空间大小为4G，64位系统为2^64（实际可能小于这个值）。事实上，每个进程的进程空间都是虚拟的，这和物理地址还不一样。两个进行访问相同的虚拟地址，但是转换到物理地址之后是不同的。这个转换就通过页表来实现，涉及的知识是操作系统的分页存储管理。 分页存储管理将进程的虚拟地址空间，分成若干个页，并为各页加以编号。相应地，物理内存空间也分成若干个块，同样加以编号。页和块的大小相同。 在操作系统中设置有一个页表寄存器，其中存放了页表在内存的始址和页表的长度。进程未执行时，页表的始址和页表长度放在本进程的PCB中；当调度程序调度该进程时，才将这两个数据装入页表寄存器。 当进程要访问某个虚拟地址中的数据时，分页地址变换机构会自动地将有效地址（相对地址）分为页号和页内地址两部分，再以页号为索引去检索页表，查找操作由硬件执行。若给定的页号没有超出页表长度，则将页表始址与页号和页表项长度的乘积相加，得到该表项在页表中的位置，于是可以从中得到该页的物理块地址，将之装入物理地址寄存器中。与此同时，再将有效地址寄存器中的页内地址送入物理地址寄存器的块内地址字段中。这样便完成了从虚拟地址到物理地址的变换。 由于页表是存放在内存中的，这使CPU在每存取一个数据时，都要两次访问内存。第一次时访问内存中的页表，从中找到指定页的物理块号，再将块号与页内偏移拼接，以形成物理地址。第二次访问内存时，才是从第一次所得地址中获得所需数据。因此，采用这种方式将使计算机的处理速度降低近1/2。 为了提高地址变换速度，可在地址变换机构中，增设一个具有并行查找能力的特殊高速缓存，也即快表（TLB），用以存放当前访问的那些页表项。具有快表的地址变换机构如图四所示。由于成本的关系，快表不可能做得很大，通常只存放16~512个页表项。 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:1:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"大页内存的配置和使用 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"1. 安装libhugetlbfs库 libhugetlbfs库实现了大页内存的访问。安装可以通过apt-get或者yum命令完成，如果系统没有该命令，还可以git clone, 然后make生成libhugetlbfs.so文件. 直接使用makefile进行编译：make BUILDTYPE=NATIVEONLY一定要加最后的参数 BUILDTYPE=NATIVEONLY否则会遇见各种错误 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:1","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"2. 配置grub启动文件 具体就是在kernel选项的最后添加几个启动参数：transparent_hugepage=never default_hugepagesz=1G hugepagesz=1Ghugepages=123 这四个参数中，最重要的是后两个，hugepagesz用来设置每页的大小，我们将其设置为1G，其他可选的配置有4K，2M（其中2M是默认）。 vim /boot/grub/grub.cfg 修改完grub.conf后，重启系统。然后运行命令查看大页设置是否生效 cat /proc/meminfo|grep Huge ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:2","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"3. mount 执行mount，将大页内存映像到一个空目录。可以执行下述命令： mount -t hugetlbfs hugetlbfs /mnt/huge ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:3","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"4. 运行应用程序 为了能启用大页，不能按照常规的方法启动应用程序，需要按照下面的格式启动： HUGETLB_MORECORE=yes LD_PRELOAD=libhugetlbfs.so ./your_program 这种方法会加载libhugetlbfs库，用来替换标准库。具体的操作就是替换标准的malloc为大页的malloc。此时，程序申请内存就是大页内存了。 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:2:4","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"大页内存的使用场景 任何优化手段都有它适用的范围，大页内存也不例外。只有耗费的内存巨大、访存随机而且访存是瓶颈的程序大页内存才会带来很明显的性能提升。 ","date":"2019-02-03","objectID":"/20190203_huge-pages/:3:0","tags":["Linux","C"],"title":"HugePages","uri":"/20190203_huge-pages/"},{"categories":["Technology"],"content":"Article description.","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"In development, we just need to know that lib is needed at compile time and DLL is needed at run time. If you want to compile source code, lib is all you need. If you want dynamically connected programs to run, you need only a DLL. This article will more clearly understand the difference, generation, use of the two. ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:0:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"lib和dll区别 首先介绍静态库（静态链接库）、动态库（动态链接库）的概念，首先两者都是代码共享的方式。 静态库：在链接步骤中，连接器将从库文件取得所需的代码，复制到生成的可执行文件中，这种库称为静态库，其特点是可执行文件中包含了库代码的一份完整拷贝；缺点就是被多次使用就会有多份冗余拷贝。即静态库中的指令都全部被直接包含在最终生成的 EXE 文件中了。在vs中新建生成静态库的工程，编译生成成功后，只产生一个.lib文件。 动态库：动态链接库是一个包含可由多个程序同时使用的代码和数据的库，DLL不是可执行文件。动态链接提供了一种方法，使进程可以调用不属于其可执行代码的函数。函数的可执行代码位于一个 DLL 中，该 DLL 包含一个或多个已被编译、链接并与使用它们的进程分开存储的函数。在vs中新建生成动态库的工程，编译成功后，产生一个.lib文件和一个.dll文件。 静态库和动态库中的lib有什么区别呢？ 静态库中的lib：该LIB包含函数代码本身（即包括函数的索引，也包括实现），在编译时直接将代码加入程序当中。 动态库中的lib：该LIB包含了函数所在的DLL文件和文件中函数位置的信息（索引），函数实现代码由运行时加载在进程空间中的DLL提供。 总之，lib是编译时用到的，dll是运行时用到的。如果要完成源代码的编译，只需要lib；如果要使动态链接的程序运行起来，只需要dll。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:1:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"静态链接 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"1. 为什么进行静态链接 在我们的实际开发中，不可能将所有代码放在一个源文件中，所以会出现多个源文件，而且多个源文件之间不是独立的，而会存在多种依赖关系，如一个源文件可能要调用另一个源文件中定义的函数，但是每个源文件都是独立编译的，即每个*.c文件会形成一个*.o文件，为了满足前面说的依赖关系，则需要将这些源文件产生的目标文件进行链接，从而形成一个可以执行的程序。这个链接的过程就是静态链接 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:1","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"2. 静态链接的原理 由很多目标文件进行链接形成的是静态库，反之静态库也可以简单地看成是一组目标文件的集合，即很多目标文件经过压缩打包后形成的一个文件。 以下面这个图来简单说明一下从静态链接到可执行文件的过程，根据在源文件中包含的头文件和程序中使用到的库函数，如stdio.h中定义的printf()函数，在libc.a中找到目标文件printf.o(这里暂且不考虑printf()函数的依赖关系)，然后将这个目标文件和我们hello.o这个文件进行链接形成我们的可执行文件。 链接器在链接静态链接库的时候是以目标文件为单位的。比如我们引用了静态库中的printf()函数，那么链接器就会把库中包含printf()函数的那个目标文件链接进来，如果很多函数都放在一个目标文件中，很可能很多没用的函数都被一起链接进了输出结果中。由于运行库有成百上千个函数，数量非常庞大，每个函数独立地放在一个目标文件中可以尽量减少空间的浪费，那些没有被用到的目标文件就不要链接到最终的输出文件中。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:2","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"3. 静态链接的优缺点 静态链接的缺点很明显，一是浪费空间，因为每个可执行程序中对所有需要的目标文件都要有一份副本，所以如果多个程序对同一个目标文件都有依赖，如多个程序中都调用了printf()函数，则这多个程序中都含有printf.o，所以同一个目标文件都在内存存在多个副本；另一方面就是更新比较困难，因为每当库函数的代码修改了，这个时候就需要重新进行编译链接形成可执行程序。但是静态链接的优点就是，在可执行程序中已经具备了所有执行程序所需要的任何东西，在执行的时候运行速度快。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:2:3","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"动态链接 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:0","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"1. 为什么进行动态链接 动态链接出现的原因就是为了解决静态链接中提到的两个问题，一方面是空间浪费，另外一方面是更新困难。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:1","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"2. 动态链接的原理 动态链接的基本思想是把程序按照模块拆分成各个相对独立部分，在程序运行时才将它们链接在一起形成一个完整的程序，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文件。下面简单介绍动态链接的过程： 假设现在有两个程序program1.o和program2.o，这两者共用同一个库lib.o,假设首先运行程序program1，系统首先加载program1.o，当系统发现program1.o中用到了lib.o，即program1.o依赖于lib.o，那么系统接着加载lib.o，如果program1.o和lib.o还依赖于其他目标文件，则依次全部加载到内存中。当program2运行时，同样的加载program2.o，然后发现program2.o依赖于lib.o，但是此时lib.o已经存在于内存中，这个时候就不再进行重新加载，而是将内存中已经存在的lib.o映射到program2的虚拟地址空间中，从而进行链接（这个链接过程和静态链接类似）形成可执行程序。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:2","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"3. 动态链接的优缺点 动态链接的优点显而易见，就是即使需要每个程序都依赖同一个库，但是该库不会像静态链接那样在内存中存在多分，副本，而是这多个程序在执行时共享同一份副本；另一个优点是，更新也比较方便，更新时只需要替换原来的目标文件，而无需将所有的程序再重新链接一遍。当程序下一次运行时，新版本的目标文件会被自动加载到内存并且链接起来，程序就完成了升级的目标。但是动态链接也是有缺点的，因为把链接推迟到了程序运行时，所以每次执行程序都需要进行链接，所以性能会有一定损失。 ​ 据估算，动态链接和静态链接相比，性能损失大约在5%以下。经过实践证明，这点性能损失用来换区程序在空间上的节省和程序构建和升级时的灵活性是值得的。 ","date":"2019-01-25","objectID":"/20190125_dynamic-static-link-library/:3:3","tags":["C","C++"],"title":"Dynamic link library and static link library","uri":"/20190125_dynamic-static-link-library/"},{"categories":["Technology"],"content":"read binary file to char *bytes in c.","date":"2018-12-20","objectID":"/20181220_read-binary-file/","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"Read the binary file (any file will do; this article uses binary as an example) and read the entire contents of the binary file into a char* string. With fseek() and fread() functions to achieve file reading advanced methods. 需求 使用fwrite(dbdata, dblength, 1,fp)把字节流写入二进制文件。在新程序读取二进制文件遇到问题：二进制内容不能向文本一样行读取，也不知道二进制文件长度，在fread()函数中无从下手。 ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:0:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"1.creat FILE pointer and set mode as ‘rb’ FILE *f = fopen(inputFN, \"rb\"); ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:1:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"2.check the FILE pointer is not null if (!f) { fprintf(stderr, \"ERROR: unable to open file \\\"%s\\\": %s\\n\", inputFN,strerror(errno)); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:2:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"3.use fseek/ftell to get data length fseek(f,0,SEEK_END) put the pointer to the end of the file. ftell(f) can get the current offset. then use fseek(f,0,SEEK_SET) put the pointer to the start of file. if (fseek(f, 0, SEEK_END) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } long datalen = ftell(f); if (dataLen \u003c 0) { fprintf(stderr, \"ERROR: ftell() failed: %s\\n\", strerror(errno)); fclose(f); return NULL; } if (fseek(f, 0, SEEK_SET) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:3:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"4.check the datalen if ((unsigned long)dataLen \u003e UINT_MAX) { dataLen = UINT_MAX; printf(\"WARNING: clipping data to %ld bytes\\n\", dataLen); } else if (dataLen == 0) { fprintf(stderr, \"ERROR: input file \\\"%s\\\" is empty\\n\", inputFN); fclose(f); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:4:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"5.malloc memory to char *inputData char *inputData = static_cast\u003cchar *\u003e(malloc(dataLen)); if (!inputData) { fprintf(stderr, \"ERROR: unable to malloc %ld bytes\\n\", dataLen); fclose(f); return NULL; } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:5:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"6.read the bin data // create a pointer p to point the begin of the inputData char *p = inputData; // create a bytesLeft to record the moving of offset size_t bytesLeft = dataLen; while (bytesLeft) { //fread will return the bytes of read size_t bytesRead = fread(p, 1, bytesLeft, f); bytesLeft -= bytesRead; p += bytesRead; if (ferror(f) != 0) { fprintf(stderr, \"ERROR: fread() failed\\n\"); free(inputData); fclose(f); return NULL; } } ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:6:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"7.close the File stream fclose(f); ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:7:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"8.return length \u0026 inputData //change the parameter of \u0026length *length = (unsigned int)dataLen; return inputData; ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:8:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"9.完整代码 FILE *f = fopen(inputFN, \"rb\"); if (!f) { fprintf(stderr, \"ERROR: unable to open file \\\"%s\\\": %s\\n\", inputFN,strerror(errno)); return NULL; } if (fseek(f, 0, SEEK_END) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } long dataLen = ftell(f); if (dataLen \u003c 0) { fprintf(stderr, \"ERROR: ftell() failed: %s\\n\", strerror(errno)); fclose(f); return NULL; } if (fseek(f, 0, SEEK_SET) != 0) { fprintf(stderr, \"ERROR: unable to seek file \\\"%s\\\": %s\\n\", inputFN, strerror(errno)); fclose(f); return NULL; } if ((unsigned long)dataLen \u003e UINT_MAX) { dataLen = UINT_MAX; printf(\"WARNING: clipping data to %ld bytes\\n\", dataLen); } else if (dataLen == 0) { fprintf(stderr, \"ERROR: input file \\\"%s\\\" is empty\\n\", inputFN); fclose(f); return NULL; } char *inputData = static_cast\u003cchar *\u003e(malloc(dataLen)); if (!inputData) { fprintf(stderr, \"ERROR: unable to malloc %ld bytes\\n\", dataLen); fclose(f); return NULL; } char *p = inputData; size_t bytesLeft = dataLen; while (bytesLeft) { size_t bytesRead = fread(p, 1, bytesLeft, f); bytesLeft -= bytesRead; p += bytesRead; if (ferror(f) != 0) { fprintf(stderr, \"ERROR: fread() failed\\n\"); free(inputData); fclose(f); return NULL; } } fclose(f); *length = (unsigned int)dataLen; ","date":"2018-12-20","objectID":"/20181220_read-binary-file/:9:0","tags":["C","File"],"title":"The Advanced method of reading files","uri":"/20181220_read-binary-file/"},{"categories":["Technology"],"content":"Article description.","date":"2018-11-18","objectID":"/20181118_sqlcipher/","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"SQLCipher is based on SQLite, and thus, the majority of the accessible API is identical to the C/C++ interface for SQLite 3. However, SQLCipher does add a number of security specific extensions in the form of PRAGMAs, SQL Functions and C Functions. ","date":"2018-11-18","objectID":"/20181118_sqlcipher/:0:0","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"1. Build SQLcipher from source $ git clone https://github.com/sqlcipher/sqlcipher.git $ cd sqlcipher $ ./configure –enable-tempstore=yes CFLAGS=\"-DSQLITE_HAS_CODEC -DSQLITE_TEMP_STORE=2\" LDFLAGS=\"-lcrypto\" $ make $ make install #if you want to do a system wide install of SQLCipher Mark the output of make install, especially the following lines: libtool: install: /usr/bin/install -c .libs/libsqlcipher.a /usr/local/lib/libsqlcipher.a /usr/bin/install -c -m 0644 sqlite3.h /usr/local/include/sqlcipher these are the folders of SQLcipher headers and the library necessary when building proper C project. ","date":"2018-11-18","objectID":"/20181118_sqlcipher/:1:0","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"2. Building minimal C project example In SQLite_example.c put the following lines: #include \"sqlite3.h\" //We want to SQLCipher extension, rather then a system wide SQLite header rc = sqlite3_open(\"test.db\",\u0026db); //open SQLite database test.db rc = sqlite3_key(db,\"1q2w3e4r\",8); //apply encryption to previously opened database Build you example: $gcc SQLite_example.c -o SQLtest -I /path/to/local/folder/with/sqlcipher/header/files/ -L /path/to/local/folder/with/sqlcipher/library.a -l sqlcipher e.g. with paths extracted from the output of $make install $gcc SQLite_example.c -o SQLtest -I /usr/local/include/sqlcipher -L /usr/local/lib/libsqlcipher.a -lsqlcipher Finally, make sure that your SQLCipher library is in the system wide library path e.g. for (Arch)Linux: $ export LD_LIBRARY_PATH=/usr/local/lib/ Run your test code ((Arch)Linux): $ ./SQLtest ","date":"2018-11-18","objectID":"/20181118_sqlcipher/:2:0","tags":["SQL","C"],"title":"SQLcipher Guide","uri":"/20181118_sqlcipher/"},{"categories":["Technology"],"content":"Article description.","date":"2018-10-17","objectID":"/20181017_sql-introduction/","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"In computer programming, create, read, update, and delete (CRUD) are the four basic functions of persistent storage. Alternate words are sometimes used when defining the four basic functions of CRUD, such as retrieve instead of read, modify instead of update, or destroy instead of delete. CRUD is also sometimes used to describe user interface conventions that facilitate viewing, searching, and changing information, often using computer-based forms and reports. ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:0","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"1. 查询语句 select … from … where … group by … having … order by … limit … 次序 4 1 2 3 5 6 7 ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:1","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"2. Group by GROUP BY 语句用于结合聚合函数，根据一个或多个列对结果集进行分组。 实例： mysql\u003e SELECT * FROM access_log; +-----+---------+-------+------------+ | aid | site_id | count | date | +-----+---------+-------+------------+ | 1 | 1 | 45 | 2016-05-10 | | 2 | 3 | 100 | 2016-05-13 | | 3 | 1 | 230 | 2016-05-14 | | 4 | 2 | 10 | 2016-05-14 | | 5 | 5 | 205 | 2016-05-14 | | 6 | 4 | 13 | 2016-05-15 | | 7 | 3 | 220 | 2016-05-15 | | 8 | 5 | 545 | 2016-05-16 | | 9 | 3 | 201 | 2016-05-17 | +-----+---------+-------+------------+ 9 rows in set (0.00 sec) SELECT site_id, SUM(access_log.count) AS nums FROM access_log GROUP BY site_id; +---------+------+ | site_id | nums | +---------+------+ | 1 | 275 | | 2 | 10 | | 3 | 521 | | 4 | 13 | | 5 | 750 | +---------+------+ ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:2","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"3. 聚集函数 1 2 3 4 5 6 count sum max min avg group_concat ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:3","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"4. Having HAVING 子句可以让我们筛选分组后的各组数据。 查询每个班中人数大于2的班级号： select count(1) as n, classid from stu group by classid having n\u003e2; or select classid from stu group by classid having count(1)\u003e2; ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:4","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"5. Order by ... order by n, classid; 1.先按n排序 2.在不改n排序的情况下排classid ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:5","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"6. Limit ... limit 1, 10; # 检索记录行 2-10 ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:6","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"7. Join stu join class on classid = class.id # join 会把左表的每一行分别与右表每一行拼接 # on 做筛选 ","date":"2018-10-17","objectID":"/20181017_sql-introduction/:0:7","tags":["SQL"],"title":"SQL Introduction","uri":"/20181017_sql-introduction/"},{"categories":["Technology"],"content":"Article description.","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefiles define a set of rules that specify which files need to be compiled first, which files need to be compiled later, which files need to be recompiled, and even more complex functional operations, because makefiles are like Shell scripts that also execute operating system commands. One of the benefits of Makefiles is that they are “automatically compiled”. The entire project is automatically compiled, greatly improving the efficiency of software development. ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:0:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"关于程序的编译和链接 无论是C还是C++，首先要把源文件编译成中间代码文件，在Windows下也就是 .obj 文件，UNIX下是 .o 文件，即 Object File，这个动作叫做编译（compile）。然后再把大量的Object File合成执行文件，这个动作叫作链接（link）。 编译时，编译器需要的是语法的正确，函数与变量的声明的正确。对于后者，通常需要告诉编译器头文件的所在位置（头文件中应该只是声明，而定义应该放在C/C++文件中），只要所有的语法正确，编译器就可以编译出中间目标文件。一般来说，每个源文件都应该对应于一个中间目标文件（O文件或是OBJ文件）。 链接时，主要是链接函数和全局变量，所以，我们可以使用这些中间目标文件（O文件或是OBJ文件）来链接我们的应用程序。链接器并不管函数所在的源文件，只管函数的中间目标文件（Object File），在大多数时候，由于源文件太多，编译生成的中间目标文件太多，而在链接时需要明显地指出中间目标文件名，这对于编译很不方便，所以，我们要给中间目标文件打个包，在Windows下这种包叫“库文件”（Library File)，也就是 .lib 文件，在UNIX下，是Archive File，也就是 .a 文件。 总结一下，源文件首先会生成中间目标文件，再由中间目标文件生成执行文件。在编译时，编译器只检测程序语法，和函数、变量是否被声明。如果函数未被声明，编译器会给出一个警告，但可以生成Object File。而在链接程序时，链接器会在所有的Object File中找寻函数的实现，如果找不到，那到就会报链接错误码（Linker Error），在VC下，这种错误一般是：Link 2001错误，意思说是说，链接器未能找到函数的实现。需要指定函数的ObjectFile. ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:1:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile 介绍 make命令执行时，需要一个 Makefile 文件，以告诉make命令需要怎么样的去编译和链接程序。 首先，我们用一个示例来说明Makefile的书写规则。以便给大家一个感兴认识。这个示例来源于GNU的make使用手册，在这个示例中，我们的工程有8个C文件，和3个头文件，我们要写一个Makefile来告诉make命令如何编译和链接这几个文件。我们的规则是： 如果这个工程没有编译过，那么我们的所有C文件都要编译并被链接。 如果这个工程的某几个C文件被修改，那么我们只编译被修改的C文件，并链接目标程序。 如果这个工程的头文件被改变了，那么我们需要编译引用了这几个头文件的C文件，并链接目标程序。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. Makefile 规则 target... : prerequisites ... command ... target也就是一个目标文件，可以是Object File，也可以是执行文件。还可以是一个标签（Label） prerequisites就是，要生成那个target所需的文件或是目标。 command也就是make需要执行的命令。(任意的shell命令) 这是一个文件的依赖关系，也就是说，target这一个或多个的目标文件依赖于prerequisites中的文件，其生成规则定义在command中。也就是说，prerequisites中如果有一个以上的文件比target文件要新的话，command所定义的命令就会被执行。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2. $@, $^, $\u003c 含义 $@–目标文件 $^–所有的依赖文件 $\u003c–第一个依赖文件 一个示例 如果一个工程有3个头文件，和8个C文件，我们为了完成前面所述的那三个规则，我们的Makefile应该是下面的这个样子的。 edit : main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o cc -o edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o main.o : main.c defs.h cc -c main.c kbd.o : kbd.c defs.h command.h cc -c kbd.c command.o : command.c defs.h command.h cc -c command.c display.o : display.c defs.h buffer.h cc -c display.c insert.o : insert.c defs.h buffer.h cc -c insert.c search.o : search.c defs.h buffer.h cc -c search.c files.o : files.c defs.h buffer.h command.h cc -c files.c utils.o : utils.c defs.h cc -c utils.c clean : rm edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o 反斜杠（\\）是换行符的意思。这样比较便于Makefile的易读。 在这个makefile中，目标文件（target）包含：执行文件edit和中间目标文件（*.o），依赖文件（prerequisites）就是冒号后面的那些 .c 文件和 .h文件。每一个 .o 文件都有一组依赖文件，而这些 .o 文件又是执行文件 edit 的依赖文件。依赖关系的实质上就是说明了目标文件是由哪些文件生成的，换言之，目标文件是哪些文件更新的。 在定义好依赖关系后，后续的那一行定义了如何生成目标文件的操作系统命令，一定要以一个Tab键作为开头。记住，make并不管命令是怎么工作的，他只管执行所定义的命令。make会比较targets文件和prerequisites文件的修改日期，如果prerequisites文件的日期要比targets文件的日期要新，或者target不存在的话，那么，make就会执行后续定义的命令。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. make是如何工作的 在默认的方式下，也就是我们只输入make命令。那么， make会在当前目录下找名字叫“Makefile”或“makefile”的文件。 如果找到，它会找文件中的第一个目标文件（target），在上面的例子中，他会找到“edit”这个文件，并把这个文件作为最终的目标文件。 如果edit文件不存在，或是edit所依赖的后面的 .o 文件的文件修改时间要比edit这个文件新，那么，他就会执行后面所定义的命令来生成edit这个文件。 如果edit所依赖的.o文件也存在，那么make会在当前文件中找目标为.o文件的依赖性，如果找到则再根据那一个规则生成.o文件。 当然，C文件和H文件是存在的啦，于是make会生成 .o 文件，然后再用 .o 文件声明make的终极任务，也就是执行文件edit了。 这就是整个make的依赖性，make会一层又一层地去找文件的依赖关系，直到最终编译出第一个目标文件。在找寻的过程中，如果出现错误，比如最后被依赖的文件找不到，那么make就会直接退出，并报错，而对于所定义的命令的错误，或是编译不成功，make根本不理。make只管文件的依赖性 于是在我们编程中，如果这个工程已被编译过了，当我们修改了其中一个源文件，比如file.c，那么根据我们的依赖性，我们的目标file.o会被重编译（也就是在这个依性关系后面所定义的命令），于是file.o的文件也是最新的啦，于是file.o的文件修改时间要比edit要新，所以edit也会被重新链接了（详见edit目标文件后定义的命令）。 而如果我们改变了“command.h”，那么，kdb.o、command.o和files.o都会被重编译，并且，edit会被重链接。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. makefile中使用变量 以edit的规则为例： edit:main.o kbd.o command.o display.o \\ insert.o search.o files.o utills.o cc -o edit main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o 看到[.o]文件的字符串被重复了两次，如果我们的工程需要加入新的[.o]文件，那么需要在两个地方加。因为此时的makefile并不复杂。当makefile变得复杂我们就有可能忘掉某个地方，而导致编译失败。所以为了makefile的易维护，在makefile中可以使用变量。 比如我们声明一个变量，objects, OBJECTS,objs,OBJS,或OBJ，表示obj文件。在makefile一开始就定义 objects = main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o 于是，就可以方便的在makefile中以\"$(objects)“的方式来使用这个变量了。改良后的makefile如下： objects = main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o edit : $(objects) cc -o edit $(objects) main.o : main.c defs.h cc -c main.c command.o : command.c defs.h command.h cc -c command.c display.o : display.c defs.h buffer.h cc -c display.c insert.o : insert.c defs.h buffer.h cc -c insert.c search.o : search.c defs.h buffer.h cc -c search.c files.o : files.c defs.h buffer.h command.h cc -c files.c utils.o : utils.c defs.h cc -c utils.c clean : rm edit $(objects) 于是如果有新的 .o 文件加入，只需修改一下 objects 变量就可以了。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"5. make自动推导 GNU的make很强大，它可以自动推导文件以及文件依赖关系后面的命令，于是我们没有必要在每个[.o]文件写上依赖关系。make会自动识别，并自己推导命令。 只要make找到一个[.o]文件，它就会自动的把[.c]文件加在依赖关系中，如果make找到一个whatever.o，那么whatever.c等whatever.o的依赖文件，并且cc -c whatever.c也会被推导出来。于是makefile可以又一次简化。 objects = main.o kbd.o command.o display.o \\ insert.o seaerch.o files.o utils.o edit : $(objects) cc -o edit $(objects) main.o : defs.h kbd.o : defs.h command.h command.o : defs.h command.h display.o : defs.h buffer.h insert.o : defs.h buffer.h search.o : defs.h buffer.h files.o : defs.h buffer.h command.h utils.o : defs.h .PHONY : clean clean : rm edit $(objects) 这就是make的“隐晦规则”。 .PHONY表示，clean是个伪目标文件 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:5","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"6. makefile的收缩 收拢起来 objects = main.o kbd.o command.o display.o \\ insert.o search.o files.o utils.o edit : $(objects) cc -o edit $(objects) $(objects) : defs.h kbd.o command.o files.o : command.h display.o insert.o search.o files.o : buffer.h .PHONY : clean clean: rm edit $(objects) 虽然makefile变得很简单，但我们的文件依赖关系会显得凌乱，新增.o文件不好管理 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:6","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"7. 清空目标文件的规则 每个Makefile中都应该写一个清空目标文件(.o和执行文件)的规则。 一般的风格是： clean: rm edit $(objects) 更稳健的做法是： .PHONY: clean clean: -rm edit $(objects) .PHONY表示clean是一个“伪目标”，向make说明，不管是否有这个文件，这个目标就是“伪目标”。只要有这个声明，不管是否有“clean”文件，要运行“clean”这个目标，只有“make clean”这样。 而在rm命令前面加了一个减号表示也许某些文件出现问题，但不用管，继续往后执行。 当然，clean的规则不要放在文件的开头，不然，这就会变成make的默认目标。 “clean从来都是放在文件的最后” ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:2:7","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile总述 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. Makefile里有什么 Makefile主要包含了五个东西：显示规则，隐晦规则，变量定义，文件指示和注释。 显示规则：显示规则说明了如何生成一个或多个的目标文件。这是由Makefile书写者明显指出要生成的文件、文件的依赖文件、生成的命令。 隐晦规则：由于我们的make有自动推导功能，所以隐晦的规则可以让我们比较粗糙地简略书写Makefile，这是make支持的。 变量的定义：在Makefile中我们要定义一系列的变量，变量一般都是字符串，这个有点像C语言中的宏，当Makefile被执行时，其中的变量都会被扩展到相应的引用位置上。 文件指示：其中包括三个部分，一个是在Makefile中引用另一个Makefile，就像C中的#include一样；另一个是指根据某些情况指定Makefile中的有效部分，就像C语言中的预编译#if一样；还有就是定义一个多行的命令。 注释：Makefile中只有行注释，和UNIX的Shell脚本一样，其注释是用“#”字符，这个就像C/C++中的“//”一样。如果要在Makefile中使用“#”字符，可以用反斜框进行转义，如：“#”。 最后，在Makefile中的命令，必须要以[Tab]键开始。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2. Makefile的文件名 默认的情况下，make命令会在当前目录下按顺序找寻文件名为**“GNUmakefile”、“makefile”、“Makefile”**的文件，找到了解释这个文件。在这三个文件名中，最好使用“Makefile”这个文件名，因为，这个文件名第一个字符为大写，这样有一种显目的感觉。最好不要用“GNUmakefile”，这个文件是GNU的make识别的。有另外一些make只对全小写的“makefile”文件名敏感，但是基本上来说，大多数的make都支持“**makefile”和“Makefile”**这两种默认文件名。 当然，可以使用别的文件名来书写Makefile，比如：“Make.Linux”，“Make.Solaris”，“Make.AIX”等，如果要****指定特定的Makefile，可以使用make的“-f”和“–file”参数****，如：make -f Make.Linux或make –file Make.AIX。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. 引用其他的Makefile 在Makefile使用include关键字可以把别的Makefile包含进来，这很像C语言的#include，被包含的文件会原模原样的放在当前文件的包含位置。include的语法是： include filename #filename可以是当前操作系统Shell的文件模式（可以保含路径和通配符） 在include前面可以有一些空字符，但是绝不能是[Tab]键开始。include和可以用一个或多个空格隔开。 举个例子，有这样几个Makefile：a.mk、b.mk、c.mk，还有一个文件叫foo.make，以及一个变量$(bar)，其包含了e.mk和f.mk，那么，下面的语句： include foo.make *.mk $(bar) #等价于： include foo.make a.mk b.mk c.mk e.mk f.mk make命令开始时，会把找寻include所指出的其它Makefile，并把其内容安置在当前的位置。就好像C/C++的#include指令一样。如果文件都没有指定绝对路径或是相对路径的话，make会在当前目录下首先寻找，如果当前目录下没有找到，那么，make还会在下面的几个目录下找： 如果make执行时，有“-I”或“–include-dir”参数，那么make就会在这个参数所指定的目录下去寻找。 如果目录/include（一般是：/usr/local/bin或/usr/include）存在的话，make也会去找。 如果有文件没有找到的话，make会生成一条警告信息，但不会马上出现致命错误。它会继续载入其它的文件，一旦完成makefile的读取，make会再重试这些没有找到，或是不能读取的文件，如果还是不行，make才会出现一条致命信息。如果想让make不理那些无法读取的文件，而继续执行，可以在include前加一个减号“-”。如： -include\u003cfilename\u003e 其表示，无论include过程中出现什么错误，都不要报错继续执行。和其它版本make兼容的相关命令是sinclude，其作用和这一个是一样的。这个变量中的值是其它的Makefile，用空格分隔。只是，它和include不同的是，从这个环境变量中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. 环境变量Makefiles 如果当前环境中定义了环境变量Makefiles，那么make会把这个变量中的值做一个类似于include的动作。这个变量中的值是其它的Makefile，用空格分隔。只是，它和include不同的是，从这个环境变中引入的Makefile的“目标”不会起作用，如果环境变量中定义的文件发现错误，make也会不理。 但还是建议不要使用这个环境变量，因为只要这个变量一旦被定义，那么当使用make时，所有的Makefile都会受到它的影响，这绝不是想看到的。在这里提这个事，只是为了告诉大家，也许有时候Makefile出现了怪事，那么可以看看当前环境中有没有定义这个变量。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"5. make的工作方式 GNU的make工作时执行步骤如下： 读入所有的makefile 读入被include的其他makefile 初始化文件中的变量 推导隐晦规则，并分析所有规则 为所有的目标文件创建依赖关系链 根据依赖关系，决定哪些目标文件重新生成 执行生成命令 1-5步为第一个阶段，6-7为第二个阶段。 第一个阶段中，如果定义被使用了，那么make会把其展开在使用的位置。但make并不会完全马上展开，如果变量出现在依赖关系的规则中，那么仅当这条依赖被决定要使用了，变量才会在内部展开 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:3:5","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile书写规则 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. Makefile规则 规则包含两个部分，一个是依赖关系，一个是生成目标的方法。 在Makefile中，规则的顺序是很重要的，因为，Makefile中只应该有一个最终目标，其它的目标都是被这个目标所连带出来的，所以一定要让make知道你的最终目标是什么。一般来说，定义在Makefile中的目标可能会有很多，但是第一条规则中的目标将被确立为最终的目标。如果第一条规则中的目标有很多个，那么，第一个目标会成为最终的目标。make所完成的也就是这个目标。 规则举例 foo.o: foo.c defs.h # foo模块 cc -c -g foo.c foo.o是我们的目标，foo.c和defs.h是目标所依赖的源文件，而只有一个命令“cc -c -g foo.c”（以Tab键开头） 文件的依赖关系，foo.o依赖于foo.c和defs.h的文件，如果foo.c和defs.h的文件日期要比foo.o文件日期要新，或是foo.o不存在，那么依赖关系发生。 如果生成（或更新）foo.o文件。也就是那个cc命令，其说明了，如何生成foo.o这个文件。（当然foo.c文件include了defs.h文件） ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2. 规则的语法 targets : prerequisites command #或： targets : prerequisites ; command command targets是文件名，以空格分开，可以使用通配符。一般来说，我们的目标基本上是一个文件，但也有可能是多个文件。 command是命令行，如果其不与“target:prerequisites”在一行，那么，必须以[Tab键]开头，如果和prerequisites在一行，那么可以用分号做为分隔 prerequisites也就是目标所依赖的文件（或依赖目标）。如果其中的某个文件要比目标文件要新，那么，目标就被认为是“过时的”，被认为是需要重生成的。 如果命令太长，你可以使用反斜框（‘\\’）作为换行符。make对一行上有多少个字符没有限制。规则告诉make两件事，文件的依赖关系和如何成成目标文件。 一般来说，make会以UNIX的标准Shell，也就是/bin/sh来执行命令。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. 在规则中使用通配符 make支持三各通配符：“*”，“?”和“[…]”。这是和Unix的B-Shell是相同的。 “~”：波浪号（“~”）字符在文件名中也有比较特殊的用途。如果是“~/test”，这就表示当前用户的$HOME目录下的test目录。而“~hchen/test”则表示用户hchen的宿主目录下的test目录。（这些都是Unix下的小知识了，make也支持）而在Windows或是MS-DOS下，用户没有宿主目录，那么波浪号所指的目录则根据环境变量“HOME”而定。 “*”：通配符代替了你一系列的文件，如“.c”表示所以后缀为c的文件。一个需要我们注意的是，如果我们的文件名中有通配符，如：“”，那么可以用转义字符“\\”，如“*”来表示真实的“*”字符，而不是任意长度的字符串。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. 静态模式 静态模式可以更加容易地定义多目标的规则，可以让我们的规则变得更加的有弹性和灵活。语法： \u003ctargets...\u003e: \u003ctarget-pattern\u003e: \u003cprereq-patterns ...\u003e \u003ccommands\u003e targets定义了一系列的目标文件，可以有通配符。是目标的一个集合。 target-parrtern是指明了targets的模式，也就是的目标集模式。 prereq-parrterns是目标的依赖模式，它对target-parrtern形成的模式再进行一次依赖目标的定义。 说明: 如果我们的定义成“%.o”，意思是我们的集合中都是以“.o”结尾的。 而如果我们的定义成“%.c”，意思是对所形成的目标集进行二次定义，其计算方法是，取模式中的“%”（也就是去掉了[.o]这个结尾），并为其加上[.c]这个结尾，形成的新集合。 所以，我们的“目标模式”或是“依赖模式”中都应该有“%”这个字符 实例： objects = foo.o bar.o all: $(objects) $(objects): %.o: %.c $(CC) -c $(CFLAGS) $\u003c -o $@ 例子中，指明了目标从$objects中获取 “%.o”表明要所有以“.o”结尾的目标，也就是\"foo.o bar.o”，也就是变量$object集合的模式 而依赖模式“%.c”则取模式“%.o”的“%”，也就是“foobar”，并为其加下“.c”的后缀，于是，我们的依赖目标就是“foo.c bar.c” 而命令中的“$\u003c”和“$@”则是自动化变量，“$\u003c”表示所有的依赖目标集（也就是“foo.c bar.c”），“$@”表示目标集（也褪恰癴oo.o bar.o”） 上面的规则展开后等价于： foo.o : foo.c $(CC) -c $(CFLAGS) foo.c -o foo.o bar.o : bar.c $(CC) -c $(CFLAGS) bar.c -o bar.o ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:4:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Makefile书写命令 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:0","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"1. 显示命令 通常，make会把其要执行的命令行在命令执行前输出到屏幕上。当我们用“@”字符在命令行前，那么，这个命令将不被make显示出来，最具代表性的例子是，我们用这个功能来像屏幕显示一些信息。如： @echo 正在编译XXX模块…… ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:1","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"2.赋值命令 # = 是最基本的赋值 # := 是覆盖之前的值 # ?= 是如果没有被赋值过就赋予等号后面的值 # += 是添加等号后面的值 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:2","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"3. 指定目标 1. “all” 这个伪目标是所有目标的目标，其功能一般是编译所有的目标。 2. “clean” 这个伪目标功能是删除所有被make创建的文件。 3. “install” 这个伪目标功能是安装已编译好的程序，其实就是把目标执行文件拷贝到指定的目标中去。 4. “print” 这个伪目标的功能是例出改变过的源文件。 5. “tar” 这个伪目标功能是把源程序打包备份。也就是一个tar文件。 6. “dist” 这个伪目标功能是创建一个压缩文件，一般是把tar文件压成Z文件。或是gz文件。 7. “TAGS” 这个伪目标功能是更新所有的目标，以备完整地重编译使用。 8. “check”和“test” 这两个伪目标一般用来测试makefile的流程。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:3","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"4. 自动化变量 $@表示规则中的目标文件集。在模式规则中，如果有多个目标，那么，\"$@“就是匹配于目标中模式定义的集合。 $%仅当目标是函数库文件中，表示规则中的目标成员名。例如，如果一个目标是\"foo.a(bar.o)\"，那么，\"$%“就是\"bar.o”，\"$@“就是\"foo.a”。如果目标不是函数库文件（Unix下是 [.a]，Windows下是[.lib]），那么，其值为空。 $\u003c依赖目标中的第一个目标名字。如果依赖目标是以模式（即”%\"）定义的，那么\"$\u003c“将是符合模式的一系列的文件集。注意，其是一个一个取出来的。 $?所有比目标新的依赖目标的集合。以空格分隔。 $^所有的依赖目标的集合。以空格分隔。如果在依赖目标中有多个重复的，那个这个变量会去除重复的依赖目标，只保留一份。 $+这个变量很像”$^\"，也是所有依赖目标的集合。只是它不去除重复的依赖目标。 $*这个变量表示目标模式中\"%“及其之前的部分。如果目标是\"dir/a.foo.b”，并且目标的模式是\"a.%.b\"，那么，\"$*“的值就是\"dir /a.foo”。这个变量对于构造有关联的文件名是比 较有较。如果目标中没有模式的定义，那么\"$*“也就不能被推导出，但是，如果目标文件的后缀是 make所识别的，那么”$*“就是除了后缀的那一部分。例如：如果目标是\"foo.c” ，因为\".c\"是make所能识别的后缀名，所以，\"$*“的值就是\"foo”。这个特性是GNU make的，很有可能不兼容于其它版本的make，所以，你应该尽量避免使用\"$*\"，除非是在隐含规则或是静态模式中。如果目标中的后缀是make所不能识别的，那么\"$*“就是空值。 ","date":"2018-09-15","objectID":"/20180915_makefile-cmakelist/:5:4","tags":["C","C++"],"title":"Makefile Guide","uri":"/20180915_makefile-cmakelist/"},{"categories":["Technology"],"content":"Article description.","date":"2018-08-12","objectID":"/20180812_hyperscan/","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"Hyperscan is a high-performance regular expression matching library from Intel. It is based on the X86 platform based on PCRE prototype development. While supporting most of the syntax of PCRE, Hyperscan adds specific syntax and working modes to ensure its usefulness in real-world network scenarios. ","date":"2018-08-12","objectID":"/20180812_hyperscan/:0:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"1. 概述 Hyperscan demo中使用libpcap从pcap文件中读取数据包，并根据一个规则文件中指定的多个正则表达式对报文进行匹配，并输出匹配结果和一些统计信息。Hyperscan增加了特定的语法和工作模式来保证其在真实网络场景下的实用性。与此同时，大量高效算法及IntelSIMD*指令的使用实现了Hyperscan的高性能匹配。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:1:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"2. KeyWords \u0026 KeyFunc Patterns: 规则，用来匹配关键词的规则，支持PCRE的大部分语法（正则表达式c十六进制.etc） id: 与规则绑定,一个规则对应一个不同的id，匹配命中时返回pattern对应的id flags: 与规则绑定,对pattern进行特殊操作，如：与或非逻辑运算(绑定多个pattern),忽略大小写，多行匹配，单次匹配.etc hs_compile_*(): 将patterns生成无向连通图(database)，匹配时把数据往连通图里迭代遍历 Scratch()：在扫描数据时，Hyperscan需要少量的临时内存来存储动态内部数据。但database的数量太大了，无法装入堆栈，特别是对于嵌入式应用程序，而且动态分配内存过于昂贵，因此必须为扫描函数提供预先分配的“Scratch”空间。 Serialization(): 将生成的database序列化成二进制文件，再由凡序列化拿到databse。 Scan(): 匹配，将需要匹配的数据放入database无向连通图中匹配，命中后调用回调函数。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:2:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"3. 原理 Hyperscan以自动机理论为基础，其工作流程主要分成两个部分：编译期(compiletime)和运行期(run-time)。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:3:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"3.1编译期 Hyperscan 自带C++编写的正则表达式编译器。如图1所示，它将正则表达式作为输入，针对不同的平台，用户定义的模式及特殊语法，经过复杂的图分析及优化过程，生成对应的数据库。另外，生成的数据库可以被序列化后保存在内存中，以供运行期提取使用。 Hyperscan-fig-1 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:3:1","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"3.2运行期 Hyperscan的运行期是通过C语言来开发的。图2展示了Hyperscan在运行期的主要流程。用户需要预先分配一段内存来存储临时匹配状态信息，之后利用编译生成的数据库调用Hyperscan内部的匹配引擎(NFA, DFA等)来对输入进行模式匹配。Hyperscan在引擎中使用Intel处理器所具有的SIMD指令进行加速。同时，用户可以通过回调函数来自定义匹配发生后采取的行为。由于生成的数据库是只读的，用户可以在多个CPU核或多线程场景下共享数据库来提升匹配扩展性。 Hyperscan-fig-2 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:3:2","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4. Hyperscan伪代码 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:0","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.1 编译 函数buildDatabase用来编译规则文件中的多个正则表达式，参数mode指定了是BLOCK,STREAM,或者向量模式。 static hs_database_t *buildDatabase(const vector\u003cconst char *\u003e \u0026expressions, const vector\u003cunsigned\u003e flags, const vector\u003cunsigned\u003e ids, unsigned int mode) { hs_database_t *db; hs_compile_error_t *compileErr; hs_error_t err; Clock clock; clock.start(); err = hs_compile_multi(expressions.data(), flags.data(), ids.data(), expressions.size(), mode, nullptr, \u0026db, \u0026compileErr); clock.stop(); if (err != HS_SUCCESS) { if (compileErr-\u003eexpression \u003c 0) { // The error does not refer to a particular expression. cerr \u003c\u003c \"ERROR: \" \u003c\u003c compileErr-\u003emessage \u003c\u003c endl; } else { cerr \u003c\u003c \"ERROR: Pattern '\" \u003c\u003c expressions[compileErr-\u003eexpression] \u003c\u003c \"' failed compilation with error: \" \u003c\u003c compileErr-\u003emessage \u003c\u003c endl; } // As the compileErr pointer points to dynamically allocated memory, if // we get an error, we must be sure to release it. This is not // necessary when no error is detected. hs_free_compile_error(compileErr); exit(-1); } //... } 其中的核心代码是hs_compile_multi的调用，此函数用来编译多个正则表达式，从代码可见除了mode参数，BLOCK和STREAM模式都使用这一API。它的原型是 hs_error_t hs_compile_multi(const char *const * expressions, const unsigned int * flags, const unsigned int * ids, unsigned int elements, unsigned int mode, const hs_platform_info_t * platform, hs_database_t ** db, hs_compile_error_t ** error) 其中，expressions是多个正则表达式字符串，flags和ids分别是expressions对应的flag和id数组；elements是表达式字符串的个数；其余参数与上一个例子中提到的hs_compile的参数涵义相同。 这里要注意的一个事情是参数ids，它是正则表达式的ID数组。每个表达式都有一个唯一ID，这样命中的时候匹配回调函数可以得到此ID，告诉调用者哪个表达式命中了。如果ids传入NULL，则所有表达式的ID都为0。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:1","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.2 准备匹配临时数据 为接下来的匹配分配足够的临时数据空间(scratch space） public: Benchmark(const hs_database_t *streaming, const hs_database_t *block) : db_streaming(streaming), db_block(block), scratch(nullptr), matchCount(0) { // Allocate enough scratch space to handle either streaming or block // mode, so we only need the one scratch region. hs_error_t err = hs_alloc_scratch(db_streaming, \u0026scratch); if (err != HS_SUCCESS) { cerr \u003c\u003c \"ERROR: could not allocate scratch space. Exiting.\" \u003c\u003c endl; exit(-1); } // This second call will increase the scratch size if more is required // for block mode. err = hs_alloc_scratch(db_block, \u0026scratch); if (err != HS_SUCCESS) { cerr \u003c\u003c \"ERROR: could not allocate scratch space. Exiting.\" \u003c\u003c endl; exit(-1); } } ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:2","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.3 匹配 4.3.1 BLOCK模式 // Scan each packet (in the ordering given in the PCAP file) through // Hyperscan using the block-mode interface. void scanBlock() { for (size_t i = 0; i != packets.size(); ++i) { const std::string \u0026pkt = packets[i]; hs_error_t err = hs_scan(db_block, pkt.c_str(), pkt.length(), 0, scratch, onMatch, \u0026matchCount); if (err != HS_SUCCESS) { cerr \u003c\u003c \"ERROR: Unable to scan packet. Exiting.\" \u003c\u003c endl; exit(-1); } } } 其中，db就是上一步编译的databas；data和length分别是要匹配的数据和数据长度；flags用来在未来版本中控制函数行为，目前未使用；scratch是匹配时要用的临时数据，之前已经分配好；onEvent非常关键，即匹配时调用的回调函数，由用户指定；context是用户自定义指针。 4.3.2 匹配回调函数 匹配回调函数的原型是 typedef (* match_event_handler)(unsigned int id, unsigned long long from, unsigned long long to, unsigned int flags, void *context) 其中，id是命中的正则表达式的ID，对于使用hs_compile编译的唯一表达式来说，此值为0；如果在编译时指定了相关模式选项(hs_compile中的mode参数），则此值将会设为匹配特征的起始位置，否则会设为0；to是命中数据的下一个字节的偏移；flags目前未用；context是用户自定义指针。 返回值为非0表示停止匹配，否则继续；在匹配的过程中，每次命中时都将同步调用匹配回调函数，直到匹配结束。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:3","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"4.4 清理资源 包括关闭流（hs_close_stream）、释放database等。 ","date":"2018-08-12","objectID":"/20180812_hyperscan/:4:4","tags":["C","C++","Hyperscan"],"title":"Hyperscan: high-performance multiple regex matching library","uri":"/20180812_hyperscan/"},{"categories":["Technology"],"content":"Article description.","date":"2018-06-10","objectID":"/20180610_c-getline/","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"Getline ()/get()/read() will read one more line. The cause may be a problem with the file itself or the getline() function. You can judge getLine ()/get()/read() while checking, and then process the data if you get it. Insert Lead paragraph here. ","date":"2018-06-10","objectID":"/20180610_c-getline/:0:0","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"1. 问题原因 ","date":"2018-06-10","objectID":"/20180610_c-getline/:1:0","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"1. 问题1: 文件末尾存在回车 while (!feof(fp)) { fgets(buffer,256,fp); j++; } feof（）这个函数是用来判断指针是否已经到达文件尾部的。若fp已经指向文件末尾，则feof（fp）函数值为“真”，即返回非零值；否则返回0。 如果文件还有换行或者空格的时候， 他会继续循环。 ","date":"2018-06-10","objectID":"/20180610_c-getline/:1:1","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"2.问题2: getline(s,1024,’\\n’)函数 while(!feof(s)) { infile.getline(s,1024,'\\n'); } 最后语句infile.getline(s,1024,’\\n’)未读到内容，出错后，变量s的内容并没改变，程序仍可继续执行，使s中的原数据再使用了一次。 ","date":"2018-06-10","objectID":"/20180610_c-getline/:1:2","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"2. 解决方法 ","date":"2018-06-10","objectID":"/20180610_c-getline/:2:0","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"1. fgets放到while里判断 while (fgets(buffer,256,fp)) { j++; } ","date":"2018-06-10","objectID":"/20180610_c-getline/:2:1","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"2.getline放到while里判断 while(infile.getline(s,1024,'\\n')) { ....... } 即infile.getline(s,1024,’\\n’)正确读到数据后再处理。 同理，对get()/read()等都类似处理。 ","date":"2018-06-10","objectID":"/20180610_c-getline/:2:2","tags":["C","File"],"title":"The problem about file reading one more line","uri":"/20180610_c-getline/"},{"categories":["Technology"],"content":"Article description.","date":"2018-05-07","objectID":"/20180507_mxml/","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"MXML (Minimal XML) is a small, fast and versatile library that reads a whole XML file and puts it in a DOM tree. ","date":"2018-05-07","objectID":"/20180507_mxml/:0:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"1.XML XML特点和作用 : XML 指可扩展标识语言（ eXtensible Markup Language） XML 的设计宗旨是传输数据， 而非显示数据 XML 标签没有被预定义。 您需要自行定义标签。 作为一种通用的数据存储和通信格式被广泛应用。 描述的数据作为一棵树型的结构而存在。 \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cbooks\u003e \u003cbook id=\"001\"\u003e \u003cauthor\u003ewanger\u003c/author\u003e \u003c/book\u003e \u003c/books\u003e ","date":"2018-05-07","objectID":"/20180507_mxml/:1:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"2.mxml写入操作 mxml既可以创建写入xml文件，也可以解析xml文件数据 写入示例: #include\"mxml.h\" int main() { // create a new xml mxml_node_t *xml = mxmlNewXML(\"1.0\"); //add a node name books mxml_node_t *books = mxmlNewElement(xml,\"books\"); //add a node under books name book mxml_node_t *book = mxmlNewElement(books,\"book\"); //set attr mxmlElementSetAttr(book,\"id\",\"001\"); mxml_node_t *author = mxmlNewElement(book,\"author\"); mxmlNewText(author,0,\"wanger\"); FILE* fp = fopen(\"book.xml\",\"wb\"); mxmlSaveFile(xml,fp,MXML_NO_CALLBACK); fclose(fp); mxmlDelete(xml); return 0; } ","date":"2018-05-07","objectID":"/20180507_mxml/:2:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"3.mxml解析操作 解析步骤： 步骤： 打开一个xml文件 把文件加载到内存中，mxml_node_t mxmlLoadFile(mxml_node_t *top, FILE *fp,mxml_type_t (*cb)(mxml_node_t )); 查找待提取的节点标签：mxml_node_t *mxmlFindElement(mxml_node_t *node, mxml_node_t *top,const char *name, const char *attr,const char *value, int descend); 获取标签属性：const char *mxmlElementGetAttr(mxml_node_t *node, const char *name); 获取标签的文本内容： const char *mxmlGetText(mxml_node_t *node, int *whitespace); 释放节点，关闭文件 示例： 解析的文本： \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cbooks\u003e \u003cbook id=\"001\"\u003e \u003cauthor\u003ewanger\u003c/author\u003e \u003c/book\u003e \u003c/books\u003e 代码： #include\"mxml.h\" #include\u003cstdio.h\u003e int main() { FILE* fp = fopen(\"book.xml\",\"r\"); // jiazai xml mxml_node_t* xml = mxmlLoadFile(NULL,fp,MXML_NO_CALLBACK); mxml_node_t* book = NULL; mxml_node_t* author = NULL; // find note book = mxmlFindElement(xml,xml,\"book\",\"id\",NULL,MXML_DESCEND); //get attr author = mxmlFindElement(book,xml,\"author\",NULL,NULL,MXML_DESCEND); if(author == NULL) { printf(\"author error\\n\"); } else { printf(\"book id is:%s\\n\",mxmlElementGetAttr(book,\"id\")); printf(\"author is:%s\\n\",mxmlGetText(author,NULL)); book = mxmlFindElement(xml,xml,\"book\",\"id\",NULL,MXML_DESCEND); } mxmlDelete(xml); fclose(fp); return 0; } 结果： book id is:001 author is:wanger ","date":"2018-05-07","objectID":"/20180507_mxml/:3:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"4.mxml for循环解析长文件 xml文件: \u003c?xml version=\"1.0\" encoding=\"utf-8\"?\u003e \u003cproto-meta-dump\u003e \u003cproto name=\"AH\"\u003e \u003cproto-class\u003ecrypto\u003c/proto-class\u003e \u003cproto-suit\u003eTCP/IP\u003c/proto-suit\u003e \u003cproto-desc\u003e\u003c/proto-desc\u003e \u003cdump-name\u003e\u003c/dump-name\u003e \u003cmeta-data name=\"\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003etotMeta\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.spi\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003eSPI\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.sequence\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003eseqNum\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.length\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003epayLen\u003c/dump-name\u003e \u003c/meta-data\u003e \u003cmeta-data name=\"ah.icv\"\u003e \u003cdump-on\u003e\u003c/dump-on\u003e \u003cdump-format\u003e\u003c/dump-format\u003e \u003cdump-name\u003eICV\u003c/dump-name\u003e \u003c/meta-data\u003e \u003c/proto\u003e ... \u003c/proto-meta-dump\u003e 解析代码： //open xml file FILE* fp = fopen(fileName,\"r\"); //create mxml tree head node mxml_node_t* tree = mxmlLoadFile(NULL,fp,MXML_NO_CALLBACK); //build head node:proto_meta_dump //第一层 mxml_node_t* proto = NULL; //第二层 mxml_node_t* class = NULL; mxml_node_t* suit = NULL; mxml_node_t* desc = NULL; mxml_node_t* name = NULL; //第三层 mxml_node_t* meta_data = NULL; mxml_node_t* dump_on = NULL; mxml_node_t* dump_format = NULL; mxml_node_t* dump_name = NULL; //遍历第一层proto，mxmlFindElement函数：寻找下一个proto节点 for(proto = mxmlFindElement(tree,tree,\"proto\",\"name\",NULL,MXML_DESCEND);proto!=NULL;proto = mxmlFindElement(proto,tree,\"proto\",\"name\",NULL,MXML_DESCEND)){ class = mxmlFindElement(proto,tree,\"proto-class\",NULL,NULL,MXML_DESCEND); suit = mxmlFindElement(proto,tree,\"proto-suit\",NULL,NULL,MXML_DESCEND); desc = mxmlFindElement(proto,tree,\"proto-desc\",NULL,NULL,MXML_DESCEND); name = mxmlFindElement(proto,tree,\"dump-name\",NULL,NULL,MXML_DESCEND); //拿到第一层数据 printf(\"|proto:%s\\n\",mxmlElementGetAttr(proto,\"name\")); printf(\" |----class:%s\\n\",mxmlGetText(class,NULL)); printf(\" |----suit:%s\\n\",mxmlGetText(suit,NULL)); printf(\" |----desc:%s\\n\",mxmlGetText(desc,NULL)); printf(\" |----name:%s\\n\",mxmlGetText(name,NULL)); //遍历第二层meta-data for(meta_data = mxmlFindElement(proto,proto,\"meta-data\",\"name\",NULL,MXML_DESCEND);meta_data!=NULL;meta_data = mxmlFindElement(meta_data,proto,\"meta-data\",\"name\",NULL,MXML_DESCEND)){ //拿到第二层数据 dump_on = mxmlFindElement(meta_data,proto,\"dump-on\",NULL,NULL,MXML_DESCEND); dump_id = mxmlFindElement(meta_data,proto,\"dump-id\",NULL,NULL,MXML_DESCEND); dump_format = mxmlFindElement(meta_data,proto,\"dump-format\",NULL,NULL,MXML_DESCEND); dump_name = mxmlFindElement(meta_data,proto,\"dump-name\",NULL,NULL,MXML_DESCEND); printf(...,mxmlGetText(...)); ... } } mxmlDelete(tree); fclose(fp); ","date":"2018-05-07","objectID":"/20180507_mxml/:4:0","tags":["C"],"title":"The realization of the MXML","uri":"/20180507_mxml/"},{"categories":["Technology"],"content":"Article description.","date":"2018-04-05","objectID":"/20180405_hashcode/","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"Hash table is a very important data structure, which is useful in many application scenarios. This paper will simply analyze the principle of hash table, and use C language to achieve a complete HashMap. ","date":"2018-04-05","objectID":"/20180405_hashcode/:0:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"1. 什么是HashMap？ 存储方式主要有两种线性存储和链式存储，常见的线性存储例如数组，常见的链式存储如链表、二叉树等。哈希表的存储主干为线性存储，这也是它在理想状态(无冲突)下时间复杂度为O(1)的关键所在。普通线性存储的存储内容与索引地址之间没有任何的联系，只能通过索引地址推算出存储内容，不能从存储内容推算出索引地址，是一个单向不可逆的过程，而HashMap存储的是一个\u003ckey, value\u003e的键值对，通过key和索引地址建立了一层关系，这层关系称之为哈希函数(或散列函数)，这样既可以通过key推算出索引地址，也可以通过推算出的索引地址直接定位到键值对，这是一个双向可逆的过程。需要注意的一点是HashMap并不直接暴露出键值对的索引地址，但是可以通过哈希函数推算出HashCode，其实HashCode就是真实的索引地址。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:1:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"2. 定义键值对结构 typedef struct entry { void * key; // 键 void * value; // 值 struct entry * next; // 冲突链表 }*Entry; #define newEntry() NEW(struct entry) #define newEntryList(length) (Entry)malloc(length * sizeof(struct entry)) 哈希冲突是指两个不同的key值得到了一个相同的HashCode，这种情况称之为哈希冲突，一个好的哈希函数很大程度上决定了哈希表的性能，不存在一种适合所有哈希表的哈希函数，在很多特定的情景下，需要有针对性的设计哈希函数才能达到理想的效果。当然啦，还是有一些优秀的哈希函数可以应对大多数情况的，对于性能要求不是很高的场景用这些就可以了。使用HashMap的时候难免会发生冲突，常用的方法主要分为两类：再散列法和链地址法。再散列法就是发生冲突时使用另一个哈希函数重新推算，一直到不冲突为止，这种方法有时候会造成数据堆积，就是元素本来的HashCode被其它元素再散列的HashCode占用，被迫再散列，如此恶性循环。链地址法就是在冲突的位置建立一个链表，将冲突值放在链表中，检索的时候需要额外的遍历冲突链表，本文采用的就是链地址法。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:2:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"3. 定义HashMap结构体 HashMap结构的存储本体是一个数组，建立一个Entry数组作为存储空间，然后根据传入的key计算出HashCode，当做数组的索引存入数据，读取的时候通过计算出的HashCode可以在数组中直接取出值。 size是当前存储键值对的数量，而listSize是当前数组的大小，仔细观察键值对结构会发现，数组的每一项其实都是冲突链表的头节点。因为冲突的存在，就有可能导致size大于listSize，当size大于listSize的时候一定发生了冲突，这时候就会扩容。 在结构体中放了一些常用的方法，因为C语言本身并没有类的概念，为了便于内部封装(经常会有一个方法调用另一个方法的时候)，可以让用户自定义一个方法而不影响其它方法的调用。举个简单的例子，put方法中调用了hashCode函数，如果想自定义一个hashCode方法，迫不得已还要再实现一个put方法，哪怕put中只改了一行代码。 结构体定义如下： // 哈希结构 typedef struct hashMap *HashMap; #define newHashMap() NEW(struct hashMap) // 哈希函数类型 typedef int(*HashCode)(HashMap, void * key); // 判等函数类型 typedef Boolean(*Equal)(void * key1, void * key2); // 添加键函数类型 typedef void(*Put)(HashMap hashMap, void * key, void * value); // 获取键对应值的函数类型 typedef void * (*Get)(HashMap hashMap, void * key); // 删除键的函数类型 typedef Boolean(*Remove)(HashMap hashMap, void * key); // 清空Map的函数类型 typedef void(*Clear)(HashMap hashMap); // 判断键值是否存在的函数类型 typedef Boolean(*Exists)(HashMap hashMap, void * key); typedef struct hashMap { int size; // 当前大小 int listSize; // 有效空间大小 HashCode hashCode; // 哈希函数 Equal equal; // 判等函数 Entry list; // 存储区域 Put put; // 添加键的函数 Get get; // 获取键对应值的函数 Remove remove; // 删除键 Clear clear; // 清空Map Exists exists; // 判断键是否存在 Boolean autoAssign; // 设定是否根据当前数据量动态调整内存大小，默认开启 }*HashMap; // 默认哈希函数 static int defaultHashCode(HashMap hashMap, void * key); // 默认判断键值是否相等 static Boolean defaultEqual(void * key1, void * key2); // 默认添加键值对 static void defaultPut(HashMap hashMap, void * key, void * value); // 默认获取键对应值 static void * defaultGet(HashMap hashMap, void * key); // 默认删除键 static Boolean defaultRemove(HashMap hashMap, void * key); // 默认判断键是否存在 static Boolean defaultExists(HashMap hashMap, void * key); // 默认清空Map static void defaultClear(HashMap hashMap); // 创建一个哈希结构 HashMap createHashMap(HashCode hashCode, Equal equal); // 重新构建 static void resetHashMap(HashMap hashMap, int listSize); HashMap的所有属性方法都有一个默认的实现，创建HashMap时可以指定哈希函数和判等函数(用于比较两个key是否相等)，传入NULL时将使用默认函数。这些函数都被设置为了static，在文件外不可访问。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:3:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"4. 哈希函数 int defaultHashCode(HashMap hashMap, let key) { IN_STACK; string k = (string)key; unsigned long h = 0; while (*k) { h = (h \u003c\u003c 4) + *k++; unsigned long g = h \u0026 0xF0000000L; if (g) { h ^= g \u003e\u003e 24; } h \u0026= ~g; } OUT_STACK; return h % hashMap-\u003elistSize; } key的类型为void *，是一个任意类型，HashMap本身也没有规定key值一定是string类型，上面的哈希函数只针对string类型，可以根据实际需要替换成其他。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:4:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"5. put函数 用于在哈希表中存入一个键值对，首先先推算出HashCode，然后判断该地址是否已经有数据，如果已有的key值和存入的key值相同，改变value即可，否则为冲突，需要挂到冲突链尾部，该地址没有数据时直接存储。实现如下： void resetHashMap(HashMap hashMap, int listSize) { if (listSize \u003c 8) return; // 键值对临时存储空间 Entry tempList = newEntryList(hashMap-\u003esize); HashMapIterator iterator = createHashMapIterator(hashMap); int length = hashMap-\u003esize; for (int index = 0; hasNextHashMapIterator(iterator); index++) { // 迭代取出所有键值对 iterator = nextHashMapIterator(iterator); tempList[index].key = iterator-\u003eentry-\u003ekey; tempList[index].value = iterator-\u003eentry-\u003evalue; tempList[index].next = NULL; } freeHashMapIterator(\u0026iterator); // 清除原有键值对数据 hashMap-\u003esize = 0; for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { Entry current = \u0026hashMap-\u003elist[i]; current-\u003ekey = NULL; current-\u003evalue = NULL; if (current-\u003enext != NULL) { while (current-\u003enext != NULL) { Entry temp = current-\u003enext-\u003enext; free(current-\u003enext); current-\u003enext = temp; } } } // 更改内存大小 hashMap-\u003elistSize = listSize; Entry relist = (Entry)realloc(hashMap-\u003elist, hashMap-\u003elistSize * sizeof(struct entry)); if (relist != NULL) { hashMap-\u003elist = relist; relist = NULL; } // 初始化数据 for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { hashMap-\u003elist[i].key = NULL; hashMap-\u003elist[i].value = NULL; hashMap-\u003elist[i].next = NULL; } // 将所有键值对重新写入内存 for (int i = 0; i \u003c length; i++) { Array x = tempList[i].value; hashMap-\u003eput(hashMap, tempList[i].key, tempList[i].value); } free(tempList); } void defaultPut(HashMap hashMap, let key, let value) { if (hashMap-\u003eautoAssign \u0026\u0026 hashMap-\u003esize \u003e= hashMap-\u003elistSize) { // 内存扩充至原来的两倍 // *注: 扩充时考虑的是当前存储元素数量与存储空间的大小关系，而不是存储空间是否已经存满， // 例如: 存储空间为10，存入了10个键值对，但是全部冲突了，所以存储空间空着9个，其余的全部挂在一个上面， // 这样检索的时候和遍历查询没有什么区别了，可以简单这样理解，当我存入第11个键值对的时候一定会发生冲突， // 这是由哈希函数本身的特性(取模)决定的，冲突就会导致检索变慢，所以这时候扩充存储空间，对原有键值对进行 // 再次散列，会把冲突的数据再次分散开，加快索引定位速度。 resetHashMap(hashMap, hashMap-\u003elistSize * 2); } int index = hashMap-\u003ehashCode(hashMap, key); if (hashMap-\u003elist[index].key == NULL) { hashMap-\u003esize++; // 该地址为空时直接存储 Array x = value; hashMap-\u003elist[index].key = key; hashMap-\u003elist[index].value = value; } else { Entry current = \u0026hashMap-\u003elist[index]; while (current != NULL) { if (hashMap-\u003eequal(key, current-\u003ekey)) { // 对于键值已经存在的直接覆盖 current-\u003evalue = value; return; } current = current-\u003enext; }; // 发生冲突则创建节点挂到相应位置的next上 Entry entry = newEntry(); entry-\u003ekey = key; entry-\u003evalue = value; entry-\u003enext = hashMap-\u003elist[index].next; hashMap-\u003elist[index].next = entry; hashMap-\u003esize++; } } put函数还有一个重要的功能，当size大于listSize时要主动扩容，这个判定条件看似有些不合理，当size大于listSize的时候可能因为冲突的存在，数组并没有存满，这时候就扩容不是浪费存储空间吗？事实确实如此，但这其实是为了加快检索速度一种妥协的办法，上文提到过，当size大于listSize时一定会发生冲突，因为哈希函数为了不越界，都会将计算出的HashCode进行取余操作，这就导致HashCode的个数一共就listSize个，超过这个个数就一定会冲突，冲突的越多，检索速度就越向O(n)靠拢，为了保证索引速度消耗一定的空间还是比较划算的，扩容时直接将容量变为了当前的两倍，这是考虑到扩容时需要将所有重新计算所有元素的HashCode，较为消耗时间，所以应该尽量的减少扩容次数。 ","date":"2018-04-05","objectID":"/20180405_hashcode/:5:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"6. 其它函数 let defaultGet(HashMap hashMap, let key) { int index = hashMap-\u003ehashCode(hashMap, key); Entry entry = \u0026hashMap-\u003elist[index]; while (entry-\u003ekey != NULL \u0026\u0026 !hashMap-\u003eequal(entry-\u003ekey, key)) { entry = entry-\u003enext; } return entry-\u003evalue; } Boolean defaultRemove(HashMap hashMap, let key) { int index = hashMap-\u003ehashCode(hashMap, key); Entry entry = \u0026hashMap-\u003elist[index]; if (entry-\u003ekey == NULL) { return False; } Boolean result = False; if (hashMap-\u003eequal(entry-\u003ekey, key)) { hashMap-\u003esize--; if (entry-\u003enext != NULL) { Entry temp = entry-\u003enext; entry-\u003ekey = temp-\u003ekey; entry-\u003evalue = temp-\u003evalue; entry-\u003enext = temp-\u003enext; free(temp); } else { entry-\u003ekey = entry-\u003evalue = NULL; } result = True; } else { Entry p = entry; entry = entry-\u003enext; while (entry != NULL) { if (hashMap-\u003eequal(entry-\u003ekey, key)) { hashMap-\u003esize--; p-\u003enext = entry-\u003enext; free(entry); result = True; break; } p = entry; entry = entry-\u003enext; }; } // 如果空间占用不足一半，则释放多余内存 if (result \u0026\u0026 hashMap-\u003eautoAssign \u0026\u0026 hashMap-\u003esize \u003c hashMap-\u003elistSize / 2) { resetHashMap(hashMap, hashMap-\u003elistSize / 2); } return result; } Boolean defaultExists(HashMap hashMap, let key) { int index = hashMap-\u003ehashCode(hashMap, key); Entry entry = \u0026hashMap-\u003elist[index]; if (entry-\u003ekey == NULL) { return False; } if (hashMap-\u003eequal(entry-\u003ekey, key)) { return True; } if (entry-\u003enext != NULL) { do { if (hashMap-\u003eequal(entry-\u003ekey, key)) { return True; } entry = entry-\u003enext; } while (entry != NULL); return False; } else { return False; } } void defaultClear(HashMap hashMap) { for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { // 释放冲突值内存 Entry entry = hashMap-\u003elist[i].next; while (entry != NULL) { Entry next = entry-\u003enext; free(entry); entry = next; } hashMap-\u003elist[i].next = NULL; } // 释放存储空间 free(hashMap-\u003elist); hashMap-\u003elist = NULL; hashMap-\u003esize = -1; hashMap-\u003elistSize = 0; } HashMap createHashMap(HashCode hashCode, Equal equal) { HashMap hashMap = newHashMap(); hashMap-\u003esize = 0; hashMap-\u003elistSize = 8; hashMap-\u003ehashCode = hashCode == NULL ? defaultHashCode : hashCode; hashMap-\u003eequal = equal == NULL ? defaultEqual : equal; hashMap-\u003eexists = defaultExists; hashMap-\u003eget = defaultGet; hashMap-\u003eput = defaultPut; hashMap-\u003eremove = defaultRemove; hashMap-\u003eclear = defaultClear; hashMap-\u003eautoAssign = True; // 起始分配8个内存空间，溢出时会自动扩充 hashMap-\u003elist = newEntryList(hashMap-\u003elistSize); Entry p = hashMap-\u003elist; for (int i = 0; i \u003c hashMap-\u003elistSize; i++) { p[i].key = p[i].value = p[i].next = NULL; } return hashMap; } ","date":"2018-04-05","objectID":"/20180405_hashcode/:6:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"7. Iterator接口 Iterator接口提供了遍历HashMap结构的方法，基本定义如下： // 迭代器结构 typedef struct hashMapIterator { Entry entry; // 迭代器当前指向 int count; // 迭代次数 int hashCode; // 键值对的哈希值 HashMap hashMap; }*HashMapIterator; #define newHashMapIterator() NEW(struct hashMapIterator) // 创建一个哈希结构 HashMap createHashMap(HashCode hashCode, Equal equal); // 创建哈希结构迭代器 HashMapIterator createHashMapIterator(HashMap hashMap); // 迭代器是否有下一个 Boolean hasNextHashMapIterator(HashMapIterator iterator); // 迭代到下一次 HashMapIterator nextHashMapIterator(HashMapIterator iterator); // 释放迭代器内存 void freeHashMapIterator(HashMapIterator * iterator); 实现如下: HashMapIterator createHashMapIterator(HashMap hashMap) { HashMapIterator iterator = newHashMapIterator(); iterator-\u003ehashMap = hashMap; iterator-\u003ecount = 0; iterator-\u003ehashCode = -1; iterator-\u003eentry = NULL; return iterator; } Boolean hasNextHashMapIterator(HashMapIterator iterator) { return iterator-\u003ecount \u003c iterator-\u003ehashMap-\u003esize ? True : False; } HashMapIterator nextHashMapIterator(HashMapIterator iterator) { if (hasNextHashMapIterator(iterator)) { if (iterator-\u003eentry != NULL \u0026\u0026 iterator-\u003eentry-\u003enext != NULL) { iterator-\u003ecount++; iterator-\u003eentry = iterator-\u003eentry-\u003enext; return iterator; } while (++iterator-\u003ehashCode \u003c iterator-\u003ehashMap-\u003elistSize) { Entry entry = \u0026iterator-\u003ehashMap-\u003elist[iterator-\u003ehashCode]; if (entry-\u003ekey != NULL) { iterator-\u003ecount++; iterator-\u003eentry = entry; break; } } } return iterator; } void freeHashMapIterator(HashMapIterator * iterator) { free(*iterator); *iterator = NULL; } ","date":"2018-04-05","objectID":"/20180405_hashcode/:7:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"},{"categories":["Technology"],"content":"8. 使用测试 #define Put(map, key, value) map-\u003eput(map, (void *)key, (void *)value); #define Get(map, key) (char *)map-\u003eget(map, (void *)key) #define Remove(map, key) map-\u003eremove(map, (void *)key) #define Existe(map, key) map-\u003eexists(map, (void *)key) int main() { HashMap map = createHashMap(NULL, NULL); Put(map, \"asdfasdf\", \"asdfasdfds\"); Put(map, \"sasdasd\", \"asdfasdfds\"); Put(map, \"asdhfgh\", \"asdfasdfds\"); Put(map, \"4545\", \"asdfasdfds\"); Put(map, \"asdfaasdasdsdf\", \"asdfasdfds\"); Put(map, \"asdasg\", \"asdfasdfds\"); Put(map, \"qweqeqwe\", \"asdfasdfds\"); printf(\"key: 4545, exists: %s\\n\", Existe(map, \"4545\") ? \"true\" : \"false\"); printf(\"4545: %s\\n\", Get(map, \"4545\")); printf(\"remove 4545 %s\\n\", Remove(map, \"4545\") ? \"true\" : \"false\"); printf(\"remove 4545 %s\\n\", Remove(map, \"4545\") ? \"true\" : \"false\"); printf(\"key: 4545, exists: %s\\n\", Existe(map, \"4545\") ? \"true\" : \"false\"); HashMapIterator iterator = createHashMapIterator(map); while (hasNextHashMapIterator(iterator)) { iterator = nextHashMapIterator(iterator); printf(\"{ key: %s, value: %s, hashcode: %d }\\n\", (char *)iterator-\u003eentry-\u003ekey, (char *)iterator-\u003eentry-\u003evalue, iterator-\u003ehashCode); } map-\u003eclear(map); freeHashMapIterator(\u0026iterator); return 0; } 运行结果： key: 4545, exists: true 4545: asdfasdfds remove 4545 true remove 4545 false key: 4545, exists: false { key: asdfasdf, value: asdfasdfds, hashcode: 2 } { key: asdhfgh, value: asdfasdfds, hashcode: 2 } { key: sasdasd, value: asdfasdfds, hashcode: 2 } { key: asdfaasdasdsdf, value: asdfasdfds, hashcode: 6 } { key: asdasg, value: asdfasdfds, hashcode: 7 } { key: qweqeqwe, value: asdfasdfds, hashcode: 9 } ","date":"2018-04-05","objectID":"/20180405_hashcode/:8:0","tags":["C","HashCode"],"title":"C language realize HashMap","uri":"/20180405_hashcode/"}]