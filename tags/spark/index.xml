<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark on JerrysBlog</title>
    <link>https://Jerrysmd.github.io/tags/spark/</link>
    <description>Recent content in spark on JerrysBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 03 Aug 2020 16:10:33 +0800</lastBuildDate><atom:link href="https://Jerrysmd.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spark 简介: Spark Guide, Part Ⅲ</title>
      <link>https://Jerrysmd.github.io/post/20200803sparkguide3/sparkguide3/</link>
      <pubDate>Mon, 03 Aug 2020 16:10:33 +0800</pubDate>
      
      <guid>https://Jerrysmd.github.io/post/20200803sparkguide3/sparkguide3/</guid>
      <description>
        
          &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
&lt;h2 id=&#34;project&#34;&gt;Project&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;业务场景&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;统计出租车利用率(有乘客乘坐的时间和无乘客空跑的时间比例)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;技术要点&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据清洗&lt;/li&gt;
&lt;li&gt;Json解析&lt;/li&gt;
&lt;li&gt;地理位置信息处理&lt;/li&gt;
&lt;li&gt;探索性数据分析&lt;/li&gt;
&lt;li&gt;会话分析&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;数据读取&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class TaxiAnalysisRunner{
    def main(args: Array[String]): Unit = {
        //1创建SparkSession
        val spark = SparkSession.builder()
        	.master(&amp;quot;local[6]&amp;quot;)
        	.appName(&amp;quot;taxi&amp;quot;)
        	.getOrCreate()
        
        //2导入隐式转换和函数
        import spark.implicits._
        import org.apache.spark.sql.functions._
        
        //3数据读取
        val taxiRaw: Dataset[Row] = spark.read
        	.option(&amp;quot;header&amp;quot;, value = true)
        	.csv(&amp;quot;dataset/half_trip.csv&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;抽象数据类&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//dataframe是ROW类型的dataset
//读取的dataframe是row类型的，如果是dataset[trip]把类型抽象，对数据方便处理
case class Trip(
	license: String,
    pickUpTime: Long,
    dropOffTime: Long,
    pickUpX: Double,
    pickUpy: Double,
    dropOffX: Double,
    dropOffY: Double
)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;转换DF类型、清洗异常数据&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//dataframe[Row] =&amp;gt; dataset[]
val taxiParsed:RDD[Either[Trip,(Row,Exception)]] = taxiRaw.rdd.map(safe(parse))
//异常数据
val exceptionResult = taxiParsed.filter(e =&amp;gt; e.isRight)
	.map(e =&amp;gt; e.right.get._1)
val taxi Good: Dataset[Trip] = taxiParsed.map(either =&amp;gt; either.left.get).toDS()


def parse(row: Row): Trip = {
    val richRow = new richRow(row)
    val license = richRow.getAs[String](&amp;quot;hack_license&amp;quot;).orNull
    val pickUpTime = parseTime(richRow, &amp;quot;...&amp;quot;)
    val dropOffTime = parseTime(richRow, &amp;quot;...&amp;quot;)
    val pickUpX = parseLocation(richRow, &amp;quot;...&amp;quot;)
    val pickUpy = parseLocation(richRow, &amp;quot;...&amp;quot;)
    val dropOffX = parseLocation(richRow, &amp;quot;...&amp;quot;)
    val dropOffY = parseLocation(richRow, &amp;quot;...&amp;quot;)
    Trip(license, pickUpTime, dropOffTime, pickUpX, pickUpy, dropOffX, dropOffY)
}

class RichRow(row: Row){
    def getAs[T](field: String): Option[T] = {
        if(row.isNullAt(row.fieldIndex(field))){
            None
        }else{
            Some(row.getAs[T](field))
        }
    }
}
def parseTime(row: RichRow, field: String): Long = {
    //规定格式
    val pattern = &amp;quot;yyyy-MM-dd HH:mm:ss&amp;quot;
    val formatter = new SimpleDateFormat(pattern, locale.ENGLISH)
    //执行转换
    val time = row.getAs[String](field)
    val timeOption = time.map(time =&amp;gt; formatter.parse(time).getTime)
    //Option代表某个方法，结果可能为空，使得方法调用出必须处理为null的情况
    //Option对象本身提供了一些对于null的支持
    timeOption.getOrElse(0L)
}

def parseLocation(row: RickRow, field: String): Double = {
    val location = row.getAs[String](fiecld)
    val locationOption = location.map(loc =&amp;gt; loc.toDouble)
    locationOption.getOrElse(0D)
}

//parse异常处理
//出现异常-&amp;gt;返回异常信息，和当前调用
def safe[P, R](f: P =&amp;gt; R): P =&amp;gt; Either[R,(P,Exception)]={
    new Function[P, Either[R,(P,Exception)]] with Serializable {
        override def apply(param: P): Either[R, (P, Exception)] = {
            try{
                Left(param)
            }catch{
                case e: Exception =&amp;gt; Right((param, e))
            }
        }
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;统计分布&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//编写udf，将毫秒转为小时单位
val hours = (pickUpTime: Long, dropOffTime: Long) =&amp;gt; {
    val duration = dropOffTime - pickUpTime
    val hours = TimeUnit.HOURS.convert(duration, TimeUnit.MILLISECONDS)
    hours
}
val hoursUDF = udf(hours)
//统计
taxiGood.groupBy(hoursUDF($&amp;quot;pickUpTime&amp;quot;,$&amp;quot;dropOffTime&amp;quot;) as &amp;quot;duration&amp;quot;)
	.count()
	.sort(&amp;quot;duration&amp;quot;)
	.show()
//直方图
spark.udf.register(&amp;quot;hours&amp;quot;, hours)
val taxiClean = taxiGood.where(&amp;quot;hours(pickUpTime, dropOffTime) BETWEEN 0 AND 3&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;JSON地理信息&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class FeatureCollection(features: List[Feature])

case class Feature(Properties: Map[String, String], geometry: JObject) {
    def getGeometry(): Geometry = {
        import org.json4s._
        import org.json4s.jackson.JsonMethods._
        val mapGeo = GeometryEngine.geoJsonToGeometry(compact(render(geometry)), 0, Geometry.Type.Unknown)
        mapGeo.getGeometry
    }
}

object FeatureExtraction{
    
    //JSON解析
    def parseJson(json: String): FeatureCollection = {
        //1导入一个formats隐式转换
        implicit val formats = Serialization.formats(NoTypeHints)
        //2JSON -&amp;gt; Obj
        import org.json4s.jackson.Serialization.read
        val featureCollection = read[FeatureCollection](json)
        featureCollection
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//链接行政区信息
//1读取数据
val geoJson = Source.fromFile(&amp;quot;dataset/districts.geojson&amp;quot;).mkString
val featureColleciton = FeatureExtraction.parseJson(geoJson)
//2排序
//理论上大的区域数量多，把大的区域放在前面，减少搜索次数
val sortedFeatures = featureCollection.features.sortBy(feature =&amp;gt; {
    (feature.properties(&amp;quot;boroughCode&amp;quot;), - feature.getGeometry().calculateArea2D())
})
//3广播
val featuresBC = spark.sparkContext.broadcast(sortedFeatures)
//4UDF
val boroughLookUp = (x: Double, y: Double) =&amp;gt; {
    //1搜索经纬度所在的区域
    val featureHit: Option[Feature] = featuresBC.value.find(feature =&amp;gt; {
        GeometryEngine.contains(feature.getGeometry(), new Point(x, y), SpatialReference.create(4326))
    })
    //2转为区域信息
    val borough = featureHit.map(feature =&amp;gt; feature.properties(&amp;quot;borought&amp;quot;)).getOrElse(&amp;quot;NA&amp;quot;)
    borough
}
//5统计信息
val broughUDF = udf(boroughLookUp)
taxiClean.groupBy(broughUDF(&#39;dropOffX,&#39;dropOff))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;会话统计&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//过滤没有经纬度的数据
val sessions = taxiClean.where(&amp;quot;dropOffX != 0 and dropOffY != 0 and pickUp...&amp;quot;)
	.repartition(&#39;license)
	.sortWithinPartitions(&#39;license, &#39;pickUpTime)

//求得时间差
def boroughDuration(t1: Trip, t2: Trip): (String, Long) = {
    val borough = boroughLookUp(t1.dropOffX, t1.dropOffY)
    val duration = (t2.pickUpTime - t1.dropOffTime)/1000
    (borough, duration)
}

val boroughtDuration = sessions.mapPartitions(trips =&amp;gt; {
    trips.sliding(2)//长度为2的窗口，移动
    	.filter(_.size == 2)
    	.filter(p =&amp;gt; p.head.license == p.last.license)
    viter.map(p =&amp;gt; boroughDuration(p.head, p.last))
}).toDF(&amp;quot;borough&amp;quot;, &amp;quot;seconds&amp;quot;)

boroughtDuration.where(&amp;quot;seconds &amp;gt; 0&amp;quot;)
	.groupBy(&amp;quot;borough&amp;quot;)
	.agg(avg(&#39;seconds), stddev(&#39;seconds))
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;spark-streaming&#34;&gt;Spark Streaming&lt;/h2&gt;
&lt;p&gt;Spark Streaming 的特点&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Spark Streaming 并不是实时流，而是按时间切分小批量，一个一个的小批处理&lt;/li&gt;
&lt;li&gt;Spark Streaming 对数据是按照时间切分为一个又一个的RDD，然后针对RDD进行处理&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;处理架构&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;批处理：HDFS&lt;/p&gt;
&lt;p&gt;流处理：Kafka&lt;/p&gt;
&lt;p&gt;混合处理：流式计算和批处理结合&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Netcat&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Netcat以在两台设备上面相互交互，即侦听模式/传输模式&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;功能：Telnet功能、获取banner信息、传输文本信息、传输文件/目录、加密传输文件，默认不加密、远程控制、加密所有流量、流媒体服务器、远程克隆硬盘&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;object StreamingWordCount {
    def main(args: Array[String]): Unit = {
        val sparkConf = new SparkConff().setAppName(&amp;quot;stream word count&amp;quot;).setMaster(&amp;quot;local[6]&amp;quot;)
        val ssc = new StreamingContext(sparkConf, Seconds(1))//批次时间，每1秒收集一次数据
        //在创建Streaming Context的时候也要用到conf，说明Spark Streaming是基于Spark Core的
        //在执行master的时候，不能指定一个线程：因为在Streaming运行的时候，需要开一个新的线程去一直监听数据的获取
        //socketTextStream方法会创建一个DStream，监听Socket输入，当做文本处理
        //DStream可以理解是一个流式的RDD
        val lines: ReceiverInputDStream[String] = ssc.socketTextStream(
           hostnmae = &amp;quot;192.168.169.101&amp;quot;,
            port = 9999,
            storageLevel = StoreageLevel.MEMORY_AND_DISK_SER
        )
        
        //2数据处理
        //	1拆分单词
        val words = lines.flatMap(_.split(&amp;quot; &amp;quot;))
        //	2转换单词
        val tuples = words.map((_, 1))
        //	3词频reduce
        val counts = tuples.reduceByKey(_ + _)
        
        ssc.start()
        
        // main方法执行完毕后整个程序就会退出，所以需要阻塞主线程
        ssc.awaitTermination()
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;容错&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;热备&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;当Receiver获取数据，交给BlockManager存储&lt;/li&gt;
&lt;li&gt;如果设置了StorageLevel.MEMORY_AND_DISK_SER，则意味着BlockManager 不仅会在本机存储，也会发往其它的主机存储，本质就是冗余备份&lt;/li&gt;
&lt;li&gt;如果某一个计算失败了，通过冗余的备份，再次进行计算即可&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;冷备&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;WAL 预写日志&lt;/li&gt;
&lt;li&gt;当数据出错时，根据Redo log去重新处理数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;重放&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;有一些上游的外部系统是支持重放的，如 Kafka&lt;/li&gt;
&lt;li&gt;Kafka 可以根据Offset来获取数据&lt;/li&gt;
&lt;li&gt;出错时，只需通过Kafka再次读取即可&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;h2 id=&#34;structured-streaming&#34;&gt;Structured Streaming&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;编程模型演进&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;RDD：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;针对自定义的数据对象进行处理，可以处理任意类型的对象，比较符合面向对象&lt;/li&gt;
&lt;li&gt;RDD处理数据速较慢&lt;/li&gt;
&lt;li&gt;RDD无法感知数据的结构，无法针对数据结构进行编程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DataFrame：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;保留元信息，针对数据结构进行处理，例如根据某一列进行排序或者分组&lt;/li&gt;
&lt;li&gt;DF在执行的时候会经过catalyst进行优化，并且序列化更加高效，性能会更好&lt;/li&gt;
&lt;li&gt;DF无法处理非结构化数据，因为DF内部使用Row对象保存数据&lt;/li&gt;
&lt;li&gt;DF的读写框架更加强大，支持多种数据源&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DataSet：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;DS结合了RDD和DF的特点，可以处理结构化数据，也可以处理非结构化数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;序列化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;将对象的内容变成二进制或存入文件中保存&lt;/p&gt;
&lt;p&gt;数据场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;持久化对象数据&lt;/li&gt;
&lt;li&gt;网络中不能传输Java对象，只能将其序列化后传输二进制数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;序列化应用场景&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Task分发：Master的driver往Worker的Executor任务分发&lt;/li&gt;
&lt;li&gt;RDD缓存：序列化后分布式存储&lt;/li&gt;
&lt;li&gt;广播变量：序列化后分布式存储&lt;/li&gt;
&lt;li&gt;Shuffle过程&lt;/li&gt;
&lt;li&gt;Spark Streaming 的 Receiver：kafka传入的数据是序列化的数据&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;RDD的序列化&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Kryo是Spark引入的一个外部的序列化工具，可以增快RDD的运行速度&lt;/p&gt;
&lt;p&gt;因为Kryo序列化后的对象更小，序列化和反序列化速度非常快&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val conf = new SparkConf()
	.setMaster(&amp;quot;local[2]&amp;quot;)
	.setAppName(&amp;quot;KyroTest&amp;quot;)

conf.set(&amp;quot;spark.serializer&amp;quot;, &amp;quot;org.apache.spark.serializer.KryoSerializer&amp;quot;)
conf.registerKryoClasses(Array(classOf[Person]))

val sc = new SparkContext(conf)
rdd.map(arr =&amp;gt; Person(arr(0), arr(1), arr(2)))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;StructuredStreaming区别&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;StructuredStreaming相比于SparkStreaming的进步类似于RDD到Dataset的进步&lt;/li&gt;
&lt;li&gt;StructuredStreaming支持连续流模型，类似于Flink那样的实时流&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;StructuredStreaming Project&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;需求&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对流式数据进行累加词频统计&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整体结构&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Socket Server 发送数据， Structured Streaming 程序接受数据&lt;/li&gt;
&lt;li&gt;Socket Server 使用 Netcat nc 来实现&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//1.创建sparkSession
//2.数据读取
val source: DataFrame = spark.readStream
	.format(&amp;quot;socket&amp;quot;)
	.option(&amp;quot;host&amp;quot;, &amp;quot;192.168.168.101&amp;quot;)
	.option(&amp;quot;port&amp;quot;, 9999)
	.load()
val sourceDS: Dataset[String] = source.as[String]
//3.数据处理
val words = sourceDS.flatMap(_.split(&amp;quot; &amp;quot;))
	.map((_, 1))
	.groupByKey(_._1)
	.count()
//4.结果生成
words.writeStream
	.outputMode(OutputMode.Complete())
	.format(&amp;quot;console&amp;quot;)
	.start()
	.awaitTermination()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;# 开启Netcat
nc -lk 9999
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;StreamExecution 分为三个重要的部分&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Source 从外部数据源读取数据，例如kafka&lt;/li&gt;
&lt;li&gt;LogicalPlan 逻辑计划，在流上查询计划，根据源头DF处理生成逻辑计划&lt;/li&gt;
&lt;li&gt;Sink 写入结果&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;StateStore&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Structured Streaming 虽然从API角度上模拟出来的是一个无线扩展的表，但其内部还是增量处理。&lt;/li&gt;
&lt;li&gt;每一批次处理完成，会将结果写入状态。每一批次处理之前，拉出来最新的状态，合并到处理过程中&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;structured-streaming-hdfs&#34;&gt;Structured Streaming HDFS&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;场景&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sqoop&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;MySQL -&amp;gt; Sqoop -&amp;gt; HDFS[增量数据1，增量数据2，&amp;hellip; ] -&amp;gt; Structured Streaming -&amp;gt; Hbase&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Ngix&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Ngix[log1, log2，&amp;hellip; ] -&amp;gt; Flume -&amp;gt; HDFS[增量数据1，增量数据2，&amp;hellip; ] -&amp;gt; Structured Streaming -&amp;gt; Hbase&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;特点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;会产生大量小文件在HDFS上&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Project&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;Python程序生成数据到HDFS&lt;/li&gt;
&lt;li&gt;Structured Streaming 从HDFS中获取数据&lt;/li&gt;
&lt;li&gt;Structured Streaming 处理数据&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Python程序生成数据到HDFS
import os
for index in range(100):
    #1.文件内容
    content = &amp;quot;&amp;quot;&amp;quot;
    {&amp;quot;name&amp;quot;: &amp;quot;Michael&amp;quot;}
    {&amp;quot;name&amp;quot;: &amp;quot;Andy&amp;quot;, &amp;quot;age&amp;quot;: 30}
    {&amp;quot;name&amp;quot;: &amp;quot;Justin&amp;quot;, &amp;quot;age&amp;quot;: 19}
    &amp;quot;&amp;quot;&amp;quot;
    
    #2.文件路径
    file_name = &amp;quot;/export/dataset/text{0}.json&amp;quot;.format(index)
    
    #3.打开文件，写入内容
    with open(file_name, &amp;quot;w&amp;quot;) as file:
        file.write(content)
	
    #4.执行HDFS命令，创建HDFS目录，上传文件到HDFS中
    os.system(&amp;quot;/export/servers/haddop/bin/hdfs dfs -mkdir -p /dataset/dataset/&amp;quot;)
    os.system(&amp;quot;/export/servers/haddop/bin/hdfs dfs -put {0} /dataset/dataset&amp;quot;.format(file_name))
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//Structured Streaming 从HDFS中获取数据
object HDFSSource{
    def main(args: Array[String]): Unit = {
        System.setProperty(&amp;quot;hadoop.home.dir&amp;quot;, &amp;quot;C:\\winutil&amp;quot;)
        
        //1.创建SparkSession
        val spark = SparkSession.builder()
        	.appName(&amp;quot;hdfs_souce&amp;quot;)
        	.master(&amp;quot;local[6]&amp;quot;)
        	.getOrCreate()
        //2.数据读取
        val schema = new StructType()
        	.add(&amp;quot;name&amp;quot;, &amp;quot;string&amp;quot;)
        	.add(&amp;quot;age&amp;quot;, &amp;quot;integer&amp;quot;)
        
        val souce = spark.readStream
        	.scheme(schema)
        	.json(&amp;quot;hdfs://node01:8020/dataset/dataset&amp;quot;)
        
        //3.输出结果
        source.writeStream
        	.outputMode(OutputMode.Append())
        	.format(&amp;quot;console&amp;quot;)
        	.start()
        	.awaitTermination()
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;structured-streaming-kafka&#34;&gt;Structured Streaming Kafka&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Kafka是一个 Pub/Sub 系统&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Publisher / Subscriber 发布订阅系统&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;发布者  --&amp;gt;  kafka  --&amp;gt;  订阅者
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;发布订阅系统可以有多个Publisher对应一个Subscriber，例如多个系统都会产生日志，一个日志处理器可以简单的获取所有系统产生的日志&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;用户系统  --&amp;gt;
订单系统  --&amp;gt;  kafka  --&amp;gt;  日志处理器
内容系统  --&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;发布订阅系统也可以一个Publisher对应多个Subscriber， 这样就类似于广播了，例如通过这样的方式可以非常轻易的将一个订单的请求分发给所有感兴趣的系统，减少耦合性&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;                      --&amp;gt;  日志处理器
 用户系统  --&amp;gt;  kafka  --&amp;gt;  日志处理器
                      --&amp;gt;  日志处理器
&lt;/code&gt;&lt;/pre&gt;
&lt;ul&gt;
&lt;li&gt;大数据系统中，消息系统往往可以作为整个数据平台的入口，左边对接业务系统各个模块，右边对接数据系统各个计算工具&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code&gt;  业务系统                     数据系统
 [用户系统]  --&amp;gt;         --&amp;gt;  [HDFS]
 [订单系统]  --&amp;gt;  kafka  --&amp;gt;  [Structured Streaming]
 [服务系统]  --&amp;gt;         --&amp;gt;  [MapReduce]
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kafka 的特点&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Kafka 非常重要的应用场景就是对接业务系统和数据系统，作为一个数据管道，其需要流通的数据量惊人，所以 Kafka 一定有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高吞吐量&lt;/li&gt;
&lt;li&gt;高可靠性&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Topic 和 Partitions&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;消息和事件经常是不同类型的，例如用户注册是一种消息，订单创建也是一种消息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;创建订单事件  --&amp;gt;
                  kafka  --&amp;gt;  structured Streaming
用户注册事件  --&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka 中使用 Topic 来组织不同类型的消息&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;创建订单事件  --&amp;gt;  Topic Order
                               --&amp;gt;  structured Streaming
用户注册事件  --&amp;gt;  Topic Order
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Kafka 中的 Topic 要承受非常大的吞吐量，所以 Topic 应该是可以分片的，应该是分布式的&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Anatomy of a Topic

Partition 0 [0][1][2][3]
Partition 1 [0][1]
Partition 3 [0][1][2]

Old  --&amp;gt;  New
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Kafka 和 Structured Streaming 整合的结构&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Structured Streaming 中使用 Source 对接外部系统，对接 Kafka 的 Source 叫做 KafkaSource&lt;/li&gt;
&lt;li&gt;KafkaSource 中会使用 KafkaSourceRDD 来映射外部 Kafka 的 Topic，两者的 Partition 一一对应&lt;/li&gt;
&lt;li&gt;Structured Streaming 会并行的从 Kafka 中获取数据&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Structured Streaming 读取 Kafka 消息的三种方式&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Earlist 从每个 Kafka 分区最开始处开始获取&lt;/li&gt;
&lt;li&gt;Assign 手动指定每个 Kafka 分区中的 Offset&lt;/li&gt;
&lt;li&gt;Latest 不再处理之前的消息，只获取流计算启动后新产生的数据&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;PROJECT&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;需求&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;模拟物联网系统的数据统计&lt;/li&gt;
&lt;li&gt;使用生产者在 Kafka 的 Topic：Streaming-test 中输入 JSON 数据&lt;/li&gt;
&lt;li&gt;使用 Structured Streaming 过滤出来家里有人的数据&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;创建 Topic 并输入数据到 Topic&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;使用命令创建 Topic&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/kafka-topics.sh --create streaming-test --replication-factor 1 --partitions 3 --zookeeper node01:2181
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;开启 Producer&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/kafka-console-producer.sh --broker-list node01:9092,node02:9092,node03:9092 -topic streaming-test
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;把 Json 转为单行输入&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;Spark 读取 kafka 的 Topic&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;object KafkaSource{
    def main(args: Array[String]): Unit = {
        //1.创建 SparkSession
        
        //2.读取 Kafka 数据
        val source: Dadaset[String] = spark.readSteam
        	.format(&amp;quot;kafka&amp;quot;)
        	.option(&amp;quot;kafka.bootstrap.servers&amp;quot;, &amp;quot;node01:9092,node02:9092,node03:9092&amp;quot;)
        	.option(&amp;quot;subscribe&amp;quot;, &amp;quot;streaming_test_1&amp;quot;)
        	.option(&amp;quot;startingOffsets&amp;quot;, &amp;quot;earliest&amp;quot;)
        	.load()
        	.selectExpr(&amp;quot;CAST(value AS STRING) as value&amp;quot;)
        	.as[String]
        
        //3.处理数据，Dataset(String) -&amp;gt; Dataset(id, name, category)
        //1::Toy Story (1995)::Animation|Children&#39;s|Comedy
        source.map(item =&amp;gt; {
            val arr = item.split(&amp;quot;::&amp;quot;)
            (arr(0).toInt, arr(1).toString, arr(2).toString)
        }).as[(Int, String, String)].toDF(&amp;quot;id&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;category&amp;quot;)
        
        //4.Sink to HDFS
        result.writeStream
        	.format(&amp;quot;parquet&amp;quot;)
        	.option(&amp;quot;path&amp;quot;, &amp;quot;/dataset/streaming/movies/&amp;quot;)
        	.option(&amp;quot;checkpointLocation&amp;quot;, &amp;quot;checkpoint&amp;quot;)
        	.start()
        	.awaitTermination()
        
        //4.Sink to Kafka
        result.writeStream
        	.format(&amp;quot;kafka&amp;quot;)
        	.outputMode(OutputMode.Append())
        	.option(&amp;quot;checkpointLocation&amp;quot;, &amp;quot;checkpoint&amp;quot;)
        	.option(&amp;quot;kafka.bootstrap.servers&amp;quot;, &amp;quot;node01:9092, node2:9092&amp;quot;)
        	.option(&amp;quot;topic&amp;quot;, &amp;quot;streaming_test_3&amp;quot;)
        	.start()
        	.awaitTermination()
        
        //4.Sink to Mysql
        //使用foreachWriter
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Sink Trigger&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;微批次&lt;/p&gt;
&lt;p&gt;默认一秒间隔&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;连续流&lt;/p&gt;
&lt;p&gt;Trigger.Continuous(&amp;ldquo;1 second&amp;rdquo;)，只支持Map类的类型操作，不支持聚合，Source和Sink只支持Kafka&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
        
      </description>
    </item>
    
    <item>
      <title>Spark 简介: Spark Guide, Part Ⅱ</title>
      <link>https://Jerrysmd.github.io/post/20200707sparkguide2/sparkguide2/</link>
      <pubDate>Tue, 07 Jul 2020 11:14:54 +0800</pubDate>
      
      <guid>https://Jerrysmd.github.io/post/20200707sparkguide2/sparkguide2/</guid>
      <description>
        
          &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
&lt;h2 id=&#34;advanced-operation&#34;&gt;Advanced Operation&lt;/h2&gt;
&lt;p&gt;closure&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def test(): Unit = {
    val f = closure()
    f(5)
}
def closure(): Int =&amp;gt; Double = {
    val factor = 3.14
    val areaFunction = (r: int) =&amp;gt; {
        math.pow(r,2) * factor
    }
    areaFunction
}
&lt;/code&gt;&lt;/pre&gt;
&lt;ol&gt;
&lt;li&gt;f就是闭包，闭包的本质就是一个函数&lt;/li&gt;
&lt;li&gt;在scala中函数是一个特殊的类型，FunctionX&lt;/li&gt;
&lt;li&gt;闭包也是一个FunctionX类型的对象&lt;/li&gt;
&lt;li&gt;闭包是一个对象&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class MyClass{
    val field = &amp;quot;Hello&amp;quot;
    def doStuff(rdd: RDD[String]): RDD [String] = {
        rdd.map(x =&amp;gt; field + x)
        //引用Myclass对象中的一个成员变量，说明其可以访问MyClass这个类的总用域，也是一个闭包。封闭的是MyClass这个作用域。
        //在将其分发的不同的Executor中执行的时候，其依赖MyClass这个类当前的对象，因为其封闭了这个作用域。MyClass和函数都要一起被序列化。发到不同的结点中执行。
        //1. 如果MyClass不能被序列化，将会报错
        //2. 如果在这个闭包中，依赖了一个外部很大的集合，那么这个集合会随着每一个Task分发
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Global accumulator&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;在任意地方创建long accumulator&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;累加&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;结果&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val counter = sc.longAccumulator(&amp;quot;counter&amp;quot;)
val result = sc.parallelize(Seq(1,2,3,4,5)).foreach(counter.add(_))
counter.value
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Broadcast&lt;/p&gt;
&lt;p&gt;广播变量允许将一个Read-Only的变量缓存到集群中的每个节点上，而不是传递给每一个Task一个副本&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;集群中的每个节点指的是一个机器&lt;/li&gt;
&lt;li&gt;每一个Task，一个Task是一个Stage中的最小处理单元，一个Executor中可以有多个Stage，每个Stage有多个Task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以在需要多个Stage的多个Task中使用相同数据的情况下，广播特别有用&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val v = Map(&amp;quot;Spark&amp;quot; -&amp;gt; &amp;quot;http[123]&amp;quot;, &amp;quot;scala&amp;quot; -&amp;gt; &amp;quot;http[456]&amp;quot;)

val config = new SparkConf().setMaster(&amp;quot;local[6]&amp;quot;).setAppName(&amp;quot;bc&amp;quot;)
val sc = new SparkContext(config)

//创建广播
val bc = sc.broadcast(v)

val r = sc.parallelize(Seq(&amp;quot;Spark&amp;quot;, &amp;quot;Scala&amp;quot;))

//使用广播变量代替直接引用集合，只会复制和executor一样的数量
//在使用广播之前，复制map了task数量份
//在使用广播之后，复制次数和executor数量一致
val result = r.map(item =&amp;gt; bc.value(item)).collect()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;sparksql&#34;&gt;SparkSQL&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Spark的RDD主要用于处理非结构化数据和半结构化数据&lt;/li&gt;
&lt;li&gt;SparkSQL主要用于处理结构化数据&lt;/li&gt;
&lt;li&gt;SparkSQL支持：命令式、SQL&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;优势：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;虽然SparkSQL是基于RDD的，但是SparkSQL的速度比RDD要快很多&lt;/li&gt;
&lt;li&gt;SparkSQL提供了更好的外部数据源读写支持&lt;/li&gt;
&lt;li&gt;SparkSQL提供了直接访问列的能力&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Person(name: String, age: Int)

val spark: SparkSession = new sql.SparkSession.Builder()
	.appName(&amp;quot;hello&amp;quot;)
	.master(&amp;quot;local[6]&amp;quot;)
	.getOrCreate()

impart spark.implicits._

val personRDD: RDD[people] = spark.sparkContext.parallelize(Seq(Person(&amp;quot;zs&amp;quot;, 10),Person(&amp;quot;ls&amp;quot;, 15)))
val personDS: Dataset[Person] = PersonRDD.toDS()
val teenagers: Dataset[String] = PersonDS.where(&#39;age &amp;gt; 10)
	.where(&#39;age &amp;lt; 20)
	.select(&#39;name)
	.as[String]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;RDD和SparkSQL运行时的区别&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;RDD的运行流程：&lt;/p&gt;
&lt;p&gt;RDD-&amp;gt;DAGScheduler-&amp;gt;TaskScheduleri-&amp;gt;Worker&lt;/p&gt;
&lt;p&gt;先将RDD解析为由Stage组成的DAG，后将Stage转为Task直接运行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SparkSQL的运行流程：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;解析SQL，并且生成AST（抽象语法树）&lt;/li&gt;
&lt;li&gt;在AST中加入元数据信息，做这一步主要是为了一些优化，例如 col = col 这样的条件&lt;/li&gt;
&lt;li&gt;对已经加入元数据的AST，输入优化器，进行优化（例如：谓词下推，列值裁剪）&lt;/li&gt;
&lt;li&gt;生成的AST其实最终还没办法直接运行，这个AST是逻辑计划，结束后，需要生成物理计划，从而生成RDD来运行。&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Dataset &amp;amp; DataFrame&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RDD 优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;JVM对象组成的分布式数据集合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不可变并且有容错能力&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可处理机构化和非结构化的数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持函数式转换&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;RDD缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;没有Schema&lt;/li&gt;
&lt;li&gt;用户自己优化程序&lt;/li&gt;
&lt;li&gt;从不同的数据源读取数据非常困难&lt;/li&gt;
&lt;li&gt;合并多个数据源中的数据也非常困难&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;DataFrame:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DataFrame类似一张关系型数据的表&lt;/li&gt;
&lt;li&gt;在DataFrame上的操作，非常类似SQL语句&lt;/li&gt;
&lt;li&gt;DataFrame中有行和列，Schema&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;DataFrame的优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Row对象组成的分布式数据集&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;不可变并且有容错能力&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;处理结构化数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;自带优化器Catalyset,可自动优化程序&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Data source API&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DataFrame让Spark对结构化数据有了处理能力&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;DataFrame的缺点：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;编译时不能类型转化安全检查，运行时才能确定是否有问题&lt;/li&gt;
&lt;li&gt;对于对象支持不友好，rdd内部数据直接以java对象存储，dataframe内存存储的是row对象而不能是自定义对象&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Dataset的优点：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;DateSet整合了RDD和DataFrame的优点，支持结构化和非结构化数据&lt;/li&gt;
&lt;li&gt;和RDD一样，支持自定义对象存储&lt;/li&gt;
&lt;li&gt;和DataFrame一样，支持结构化数据的sql查询&lt;/li&gt;
&lt;li&gt;采用堆外内存存储，gc友好&lt;/li&gt;
&lt;li&gt;类型转化安全，代码友好&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def dataset1(): Unit = {
    //1.创建SparkSession
    val spark = new sql.SparkSession.Builder()
    	.master(&amp;quot;local[6]&amp;quot;)
    	.appName(&amp;quot;dateset1&amp;quot;)
    	.getOrCreate()
    //2.导入隐式转化
    import spark.implicits._
    
    //3.demo
    val sourceRDD = spark.sparkContext.parallelize(Seq(Person(&amp;quot;zs&amp;quot;, 10),Person(&amp;quot;ls&amp;quot;, 15)))
    val dataset = sourceRDD.toDS()
    
    //Dataset支持强类型API
    dataset.filter(item =&amp;gt; item.age &amp;gt; 10).show()
    //Dataset支持弱类型API
    dataset.filter( &#39;age &amp;gt; 10 ).show()
    dataset.filter( $&amp;quot;age&amp;quot; &amp;gt; 10 ).show()
    //Dataset可以直接编写SQL表达式
    dataset.filter( &amp;quot;age &amp;gt; 10&amp;quot;).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;DataFrame Practice:&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def dataframe1(): Unit = {
    //1. 创建SparkSession
    val spark = SparkSession.builder()
    	.master(&amp;quot;local[6]&amp;quot;)
    	.appName(&amp;quot;pm analysis&amp;quot;)
    	.getOrCreate()
    //2.读取数据集
    val souceDF = spark.read
    	.option(&amp;quot;header&amp;quot;, value = true)
    	.csv(&amp;quot;dataset/beijingPM.csv&amp;quot;)
    //3.处理数据集
    sourceDF.select(&#39;year, &#39;month, &#39;PM_Dongsi)
    	.where(&#39;PM_Dongsi =!= &amp;quot;NA&amp;quot;)
    	.groupBy(&#39;year, &#39;month)
    	.count()
    	.show()
    
    spark.stop()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DataFrame &amp;amp; Dataset 区别：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;DataFrame是Dataset的一种特殊情况，DataFrame是Dataset[Row]的别名&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame表达的含义是一个支持函数式操作的表，而Dataset表达是一个类似RDD的东西，Dataset可以处理任何对象&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;DataFrame中存放的是Row对象，而Dataset中可以存放任何类型的对象&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;DataFrame是弱类型，Dataset是强类型。DataFrame的操作方式和Dataset是一样的，但是对于强类型的操作而言，他们处理的类型是不同的&lt;/p&gt;
&lt;p&gt;DataFrame在进行强类型操作的时候，例如map算子，所处理的数据类型永远是Row&lt;/p&gt;
&lt;p&gt;而Dataset，其中是什么类型，他就处理什么类型。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val df: DataFrame = personList.toDF()
df.map( (row: Row) =&amp;gt; Row(row.get(0), row,getAs[Int](1) * 2))(RowEncoder.apply(df.schema))

val ds: Dataset[person] = personList.toDS()
ds.map((person: Person =&amp;gt; Person(person.name, person.age * 2)))
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;
&lt;p&gt;DataFrame只能做到运行时类型检查，Dataset能做到编译和运行都有类型检查&lt;/p&gt;
&lt;p&gt;DataFrame弱类型是编译时不安全(df.groupBy(&amp;ldquo;name, school&amp;rdquo;))&lt;/p&gt;
&lt;p&gt;Dataset所代表的操作，是类型安全的，编译时安全的(ds.filter(person =&amp;gt; person.name))&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Row&lt;/p&gt;
&lt;p&gt;DataFrame就是Row集合加上Schema信息&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;case class Person(name: String, age: Int)
def row(): Unit = {
    //1.Row如何创建，是什么
    //row对象必须配合Schema对象才会有列名
    val person = Person(&amp;quot;zs&amp;quot;, 15)
    val row = Row(&amp;quot;zs&amp;quot;, 15)
    
    //2.如何从Row中获取数据
    row.getString(0)
    row.getInt(1)
    
    //3.Row也是样例类
    row match{
        case Row(name, age) =&amp;gt; println(name, age)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Reader&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def reader1(): Unit = {
    //1.create SparkSession
    val spark = SparkSession.builder()
    	.master(&amp;quot;local[6]&amp;quot;)
    	.appName(&amp;quot;reader1&amp;quot;)
    	.getOrCreate()
    
   //2.firstWay
    spark.read
    	.format(&amp;quot;csv&amp;quot;)
    	.option(&amp;quot;header&amp;quot;, value = true)
    	.option(&amp;quot;inferSchema&amp;quot;, value = true)
    	.load(&amp;quot;dataset/bjPM.csv&amp;quot;)
    	.show(10)
    
    //3.sencendWay
    spark.read
    	.option(&amp;quot;header&amp;quot;, value = true)
    	.option(&amp;quot;inferSchema&amp;quot;, value = true)
    	.csv(&amp;quot;dataset/bjPM.csv&amp;quot;)
    	.show(10)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Writer&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def writer1(): Unit = {
    System.setProperty(&amp;quot;hodoop.home.dir&amp;quot;,&amp;quot;c:\\winutils&amp;quot;)
    //1.create SparkSession
    val spark = SparkSession.builder()
    	.master(&amp;quot;local[6]&amp;quot;)
    	.appName(&amp;quot;reader1&amp;quot;)
    	.getOrCreate()
    //2.read data
    val df = spark.read.option(&amp;quot;header&amp;quot;, true).csv(&amp;quot;dataset/bjPM.csv&amp;quot;)
    //3.writer
    df.write.json(&amp;quot;dataset/bjPM.json&amp;quot;)
    
    df.write.format(&amp;quot;json&amp;quot;).save(&amp;quot;dataset/bjPM2.json&amp;quot;)
    
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Parquet&lt;/p&gt;
&lt;p&gt;Parquet属于Hadoop生态圈的一种&lt;strong&gt;新型列式存储&lt;/strong&gt;格式，既然属于Hadoop生态圈，因此也兼容大多圈内计算框架（Hadoop、Spark），另外Parquet是平台、语言无关的，这使得它的适用性很广，只要相关语言有对应支持的类库就可以用；&lt;/p&gt;
&lt;p&gt;Parquet的优劣对比：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;支持嵌套结构，这点对比同样是列式存储的OCR具备一定优势；&lt;/li&gt;
&lt;li&gt;适用于OLAP场景，对比CSV等行式存储结构，列示存储支持&lt;strong&gt;映射下推&lt;/strong&gt;和&lt;strong&gt;谓词下推&lt;/strong&gt;，减少磁盘IO；&lt;/li&gt;
&lt;li&gt;同样的压缩方式下，列式存储因为每一列都是同构的，因此可以使用更高效的压缩方法；&lt;/li&gt;
&lt;/ul&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def parquet(): Unit = {
    //read
    val df = spark.read.option(&amp;quot;header&amp;quot;, true).csv(&amp;quot;dataset/bjPM.csv&amp;quot;)
    //把数据写为parquet格式
    df.write
    	.format(&amp;quot;parquet&amp;quot;)
    	.mode(Savemode.Overwrite)
    	.save(&amp;quot;dataset/bj_PM&amp;quot;)
    //读取Parquet格式文件
    spark.read
    	.load(&amp;quot;dataset/bj_PM&amp;quot;)
    	.show()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Partition&lt;/p&gt;
&lt;p&gt;表分区的概念不仅在parquet上有，其他格式的文件也可以指定表分区&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def parquetPartions(): Unit ={
    val df = spark.read
    	.option(&amp;quot;header&amp;quot;,value = true)
    	.csv(&amp;quot;dataset/BJPM.csv&amp;quot;)
    
    //分区表形式写文件
    df.write
    .partitionBy(&amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;)
    .save(dataset/bjPM4)
    
    //读文件
    //写分区的时候，分区列不会包含在生成的文件中
    //直接通过文件来进行读取的话，分区信息会丢失
    //spark SQL自动发现分区
    spark.read
    	.parquet(&amp;quot;dataset/bjPM4&amp;quot;)
    	.printSchema()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;JSON&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;toJSON: 把Dataset[Object]转为Dataset[JsonString]&lt;/li&gt;
&lt;li&gt;可以直接从RDD读取JSON的DataFrame，把RDD[JsonString]转为Dataset[Object]&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hive&lt;/p&gt;
&lt;p&gt;整合什么内容&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;MetaStore，元数据存储&lt;/p&gt;
&lt;p&gt;SparkSql内置的有一个MetaStore。更成熟，功能更强，而且可以使用Hive的元信息&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;查询引擎&lt;/p&gt;
&lt;p&gt;SparkSQL内置了HiveSQL的支持&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hive的MetaStore&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Hive的MetaStore是Hive的一个组件。Hive中主要的组件组件就三个：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;HiveServer2负责接收外部系统的查询请求，列如JDBC，HiveServer2接收查询请求后，交给Driver处理&lt;/li&gt;
&lt;li&gt;Driver会首先询问MetaStore表在哪存，后Driver程序通过MR程序来访问HDFS从而获取结果返回给查询请求者&lt;/li&gt;
&lt;li&gt;MetaStore对SparkSQL的意义重大，如果SparkSQL可以直接访问Hive的MetaStore，则理论上可以做和Hive一样的事情，例如通过Hive表查询数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;而Hive的MetaStore的运行模式有三种：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;内嵌Derby数据库模式&lt;/p&gt;
&lt;p&gt;单链接，不支持并发&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;local模式&lt;/p&gt;
&lt;p&gt;local和remote都是访问MySQL数据库作为存储元数据的地方，但是local模式的MetaStore没有独立进程，依附于HiveServer2的进程&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Remote模式&lt;/p&gt;
&lt;p&gt;和Local模式一样，访问MySQL数据库存放元数据，但是Remote的MetaStore运行在独立的进程中&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Hive开启MetaStore&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;修改hive-sito.xml&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;启动Hive MetaStore&lt;/p&gt;
&lt;p&gt;nohup /export/servers/hive/bin/hive &amp;ndash;service metastore 2&amp;gt;&amp;amp;1 &amp;raquo; /var/log.log &amp;amp;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;SparkSQL整合Hive的MetaStore&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;即使不去整合MetaStore，spark也有一个内置的MetaStore，使用Derby数据库保存数据，但这种方式不适合生产环境。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;通过SparkSQL查询Hive的表&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;查询hive找那个的表可以通过spark。sql()来进行，可以直接在其中访问hive的MetaStore，前提是一定要将hive的配置文件拷贝到spark的conf目录&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;spark.sql(&amp;quot;use spark_integrition&amp;quot;)
val resultDF = spark.sql(&amp;quot;select * from student limit 10&amp;quot;)
resultDF.show
&lt;/code&gt;&lt;/pre&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;Spark访问Hive中的表&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在Hive中创建表&lt;/p&gt;
&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;将文件上传到集群hdfs上&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;hdfs dfs -mkdir -p /dataset
hdfs dfs -put studenttabl10k /dataset/
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;使用hive或者beeline执行sql&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;create database if not exists spark_integrition;
use spark_integrition;
create external table student
{
	name String,
	age INT,
	gpa String
}
Row format delimited
	fields terminated by &#39;\t&#39;
	lines terminated by &#39;\n&#39;
stored as textfile
location &#39;/dataset/hive&#39;

load Data INPATH &#39;/dataset/studenttab10k&#39; OVERWRITE INTO TABLE student;
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;JDBC&lt;/p&gt;
&lt;p&gt;MySQL的访问方式有两种：使用本地运行，提交到集群中运行&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-SCALA&#34;&gt;//读数据
val spark = SparkSession
	.builder()
	.appName(&amp;quot;jdbc example&amp;quot;)
	.master(&amp;quot;local[6]&amp;quot;)
	.getOrCreate()
val schema = StructType(
	List(
    	StructField(&amp;quot;name&amp;quot;, StringType),
        StructField(&amp;quot;age&amp;quot;, IntegerType),
        StructField(&amp;quot;gpa&amp;quot;, FloatType),
    )
)
val studentDF = spark.read
	.option(&amp;quot;delimiter&amp;quot;, &amp;quot;\t&amp;quot;)//读取文件的分隔符是制表符
	.schema(schema)
	.csv(&amp;quot;dataset/studenttab10k&amp;quot;)

//处理数据
val resultDF = studentDF.where(&amp;quot;age&amp;lt;30&amp;quot;)

//写数据
resultDF.write.format(&amp;quot;jdbc&amp;quot;).mode(SaveMode.Overwrite)
	.option(&amp;quot;url&amp;quot;, &amp;quot;jdbc:mysql://node01:3306/spark_test&amp;quot;)
	.option(&amp;quot;dbtable&amp;quot;, &amp;quot;student&amp;quot;)
	.option(&amp;quot;user&amp;quot;, &amp;quot;spark&amp;quot;)
	.option(&amp;quot;password&amp;quot;, &amp;quot;Spark123!&amp;quot;)
	.save()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-type-transformation&#34;&gt;Data Type Transformation&lt;/h2&gt;
&lt;p&gt;flatMap,map,mapPartitions,transform,as:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class TypedTransformation{
    //1.创建sparksession
    val spark = SparkSession.builder().master(&amp;quot;local[6]&amp;quot;).appName(&amp;quot;typed&amp;quot;).getOrCreate()
    import spark.implicits._
    
    @Test
    def trans():Unit = {
        //flatmap
        val ds = Seq(&amp;quot;hello spark&amp;quot;, &amp;quot;hello hadoop&amp;quot;).toDS
        ds.flatMap(item =&amp;gt; item.split(&amp;quot; &amp;quot;)).show()
        //map
        val ds2 = Seq(Persion(&amp;quot;zs&amp;quot;,15),Persion(&amp;quot;lisi&amp;quot;,20)).toDS()
        ds2.map(person =&amp;gt; Person(person.name, person.age*2)).show()
        //mappartitions
        ds2.mapPartitions{
            //iter 不能大到每个Executor的内存放不下，不然就会OOM
            //对每个元素进行转换，后生成一个新的集合
            iter =&amp;gt;{
                val result = iter.map(person =&amp;gt; Person(person.name, person.age * 2))
                result
            }
        }
    }
}

def trans1(): Unit = {
    val ds = spark.rage(10) //0-10
    ds.transform(dataset =&amp;gt; dataset.withColumn(&amp;quot;doubled&amp;quot;, &#39;id * 2&#39;))
      .show()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;DF转成DS&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;rdd.toDF -&amp;gt; DataFrame //toDF把rdd转成DF
dataFrame -&amp;gt; Dataset //DataFrame就是Dataset[Row]

case class Student(name:String, age:Int, gpa:Float)

//读取
val schema = StructType(
	Seq(
    	StructField(&amp;quot;name&amp;quot;,StringType),
        StructField(&amp;quot;age&amp;quot;,IntegerType),
        StructField(&amp;quot;gpa&amp;quot;,FloatType)
    )
)

val df = spark.read
	.schema(schema)
	.option(&amp;quot;delimiter&amp;quot;,&amp;quot;\t&amp;quot;)
	.csv(&amp;quot;dataset/studenttab10k&amp;quot;)
//转换
//本质上dataset[Row].as[Student] =&amp;gt; Dataset[Student]
val ds: Dataset[Student] = df.as[Student]

//输出
ds.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Filter&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def filter(): Unit = {
    val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20)).toDS()
    ds.filter(person =&amp;gt; person.age &amp;gt; 15).show()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Group&lt;/p&gt;
&lt;p&gt;groupByKey:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20)).toDS()
val grouped: KeyValueGroupedDataset[String, Person] = ds.groupByKey(person =&amp;gt; person.name)
val result: Dataset[(String, Long)] = grouped.count()

result.show()

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Split&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val ds = spark.range(15)
//randomSplit, the number of part, weight
val datasets: Array[Dataset[lang.Long]] =ds.randomSplit(Array(5,2,3))
datasets.foeach(_.show())

//split
ds.sample(withReplacement = false, fraction = 0.4).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Sort&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20),Person(&amp;quot;zs&amp;quot;,8)).toDS()
ds.orderBy(&#39;name.desc).show()
ds.sort(&#39;name.asc).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Distinct&lt;/p&gt;
&lt;p&gt;distinct,dropDuplicates:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def dropDuplicates(): Unit = {
    val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20),Person(&amp;quot;zs&amp;quot;,8)).toDS()
    //重复列完全匹配
    ds.distinct().show()
    //指定列去重
    ds.dropDuplicates(&amp;quot;age&amp;quot;).show()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Collection&lt;/p&gt;
&lt;p&gt;差集、交集、并集、limit&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def collection(): Unit ={
    val ds1 = spark.range(1,10)
    val ds2 = spark.range(5,15)
    
    ds1.except(ds2)
    ds1.intersect(ds2)
    ds1.union(ds2)
    ds1.limit(3)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;data-typeless-transformation&#34;&gt;Data Typeless Transformation&lt;/h2&gt;
&lt;p&gt;select&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20),Person(&amp;quot;zs&amp;quot;,8)).toDS()
ds.sort()
  ....
  .secect(&#39;name).show()

ds.selectExpr(&amp;quot;sum(age)&amp;quot;).show()

import org.apache.spark.sql.funcitons._

ds.select(exper(&amp;quot;sum(age)&amp;quot;)).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Column&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20),Person(&amp;quot;zs&amp;quot;,8)).toDS()

import org.apache.spark.sql.funcitons._
//如果想使用函数的功能
//1.使用functions.xx
//2.使用表达式，可以使用expr(&amp;quot;...&amp;quot;)随时编写表达式
ds.withColumn(&amp;quot;random&amp;quot;,expr(&amp;quot;rand()&amp;quot;)).show()
ds.withColumn(&amp;quot;name_new&amp;quot;,&#39;name + ...).show()
ds.withColumn(&amp;quot;name_jok&amp;quot;,&#39;name === &amp;quot;&amp;quot;).show()
ds.withColumnRenamed(&amp;quot;name&amp;quot;,&amp;quot;new_name&amp;quot;).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Drop&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20),Person(&amp;quot;zs&amp;quot;,8)).toDS()
ds.drop(&#39;age).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;GroupBy&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20),Person(&amp;quot;zs&amp;quot;,8)).toDS()
//为什么groupByKey是有类型的，最主要的原因是因为groupByKey所生成的对象中的算子是有类型的
ds.groupByKey(item =&amp;gt; item.name).mapValues()
//为什么groupBy是无类型的，因为groupBy所生成的对象中的算子是无类型的，针对列进行处理
ds.groupBy(&#39;name).agg(mean(&amp;quot;age&amp;quot;)).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;column&#34;&gt;Column&lt;/h2&gt;
&lt;p&gt;Creation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class Column{
    val spark = SparkSession.builder()
    	.master(&amp;quot;local[6]&amp;quot;)
    	.appName(&amp;quot;column&amp;quot;)
    	.getOrCreate()
    def creation():Unit = {
        val ds = Seq(Person(&amp;quot;zs&amp;quot;,15),Person(&amp;quot;ls&amp;quot;,20),Person(&amp;quot;zs&amp;quot;,8)).toDS()
        val df = Seq((&amp;quot;zs&amp;quot;,15),(&amp;quot;ls&amp;quot;,20),(&amp;quot;zs&amp;quot;,8)).toDF(&amp;quot;name&amp;quot;,&amp;quot;age&amp;quot;)
        //1. &#39; 必须导入spark的隐式转化才能使用str.intern()
        val column: Symbol = &#39;name
        
        //2. $ 必须导入spark的隐式转化才能使用
        val column1: ColumnName = $&amp;quot;name&amp;quot;
        
        //3. col 必须导入functions
        import org.apache.spark.sql.functions._
        val column2:sql.Column = col(&amp;quot;name&amp;quot;)
        
        //4. column 必须导入functions
        val column3:sql.Column = column(&amp;quot;name&amp;quot;)
        
        //Dataset可以，DataFrame可以使用column对象
        ds.select(column).show()
        df.select(column).show()
        
        //column有四种创建方式
        //column对象可以用作于Dataset和DataFrame中
        //column可以和命令式的弱类型的API配合使用:select where
        
        //5. dataset.col
        //使用dataset来获取column对象，会和某个dataset进行绑定，在逻辑计划中，就会有不同的表现
        val column4 = ds.col(&amp;quot;name&amp;quot;)
        val column5 = ds1.col(&amp;quot;name&amp;quot;)
        ds.select(column5).show()
        //为什么要和dataset来绑定呢？
        ds.join(ds1, ds.col(&amp;quot;name&amp;quot;) === ds1.col(&amp;quot;name&amp;quot;))
        
        //6. dataset.apply
        val column6 = ds.apply(&amp;quot;name&amp;quot;)
        val column7 = ds(&amp;quot;name&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Type&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;ds.select(&#39;name as &amp;quot;new_name&amp;quot;).show()
ds.select(&#39;age.as[Long]).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;API&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//添加新列
df.withColun(&amp;quot;age&amp;quot;, &#39;age * 2).show()
//模糊查询
ds.where(&#39;name like &amp;quot;zhang%&amp;quot;).show()
//排序
ds.sort(&#39;age asc).show()
//枚举判断
ds.where(&#39;name isin (&amp;quot;zs&amp;quot;,&amp;quot;wu&amp;quot;,&amp;quot;ls&amp;quot;)).show()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;na&#34;&gt;N/A&lt;/h2&gt;
&lt;p&gt;缺失值的处理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;丢弃缺失值的行&lt;/li&gt;
&lt;li&gt;替换初始值&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;DataFrameNaFunctions&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;创建&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val naf: DataFrameNaFunctions = df.na
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;功能&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;naf.drop&amp;hellip;	naf.fill &amp;hellip;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;df.na.drop.show()
df.na.fill.show()

class NullProcessor {
    @Test
    def nullAndNaN(): Unit = {
        //ss
        val spark = SparkSession.builder()
         .master(&amp;quot;local[6]&amp;quot;)
         .appName(&amp;quot;null processor&amp;quot;)
         .getOrCreate()
        //导入

        //读取
        //	1.通过spark-csv自动的推断类型来读取，推断数字的时候会将NaN推断为字符串
        spark.read
         .option(&amp;quot;header&amp;quot;, true)
         .option(&amp;quot;inferSchema&amp;quot;,true)
         .csv(dataset/ds)
        //	2.直接读取字符串，在后续的操作中使用map算子转换类型
        spark.read.csv().map(row =&amp;gt; row...)
        //	3.指定Schema,不要自动推断
        val schema = structType(
         list(
                StructField(&amp;quot;id&amp;quot;,LongType),
                StructField(&amp;quot;year&amp;quot;,IntegerType),
                StructField(&amp;quot;day&amp;quot;,IntegerType),
                StructField(&amp;quot;season&amp;quot;,IntegerType),
                StructField(&amp;quot;pm&amp;quot;,DoubleType)
            )
        )
        val sourceDF = spark.read
         .option(&amp;quot;header&amp;quot;, value = true)
         .schema(schema)
         .csv(&amp;quot;dataset/data.csv&amp;quot;)
         .show()
        //丢弃
        //	规则：
        //		1.any：只要有一个NaN就丢弃
        sourceDF.na.drop(&amp;quot;any&amp;quot;).show()
        sourceDF.na.drop().show()
        //		2.all: 所有数据NaN才丢弃
        sourceDF.na.drop(&amp;quot;all&amp;quot;).show()
        //		3.某些列
        sourceDF.na.drop(&amp;quot;any&amp;quot;,List(&amp;quot;year&amp;quot;,&amp;quot;month&amp;quot;,&amp;quot;day&amp;quot;)).show()
        //填充
        //	规则：
        //		1.针对所有列默认值填充
        sourceDF.na.fill(0).show()
        //		2.针对特定列填充
        sourceDF.na.fill(0,List(&amp;quot;year&amp;quot;, &amp;quot;month&amp;quot;)).show()
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;SparkSQL处理异常字符串:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def strProcessor(): Unit = {
    //1.丢弃
    import spark.implicits._
    sourceDF.where(&#39;PM_dongsi =!= &amp;quot;NA&amp;quot;).show()
    //2.替换
    import org.apache.spark.sql.functions._
    sourceDF.select(
     &#39;No as &amp;quot;id&amp;quot;, &#39;year, &#39;month, &#39;day,
        when(&#39;PM_Dongsi === &amp;quot;NA&amp;quot;, Double.NaN)
        .otherwise(&#39;PM_Dongsi cast DoubleType)
        .as(&amp;quot;pm&amp;quot;)
    ).show()

    sourceDF.na.replace(&amp;quot;PM_Dongsi&amp;quot;, Map(&amp;quot;NA&amp;quot; -&amp;gt; &amp;quot;NaN&amp;quot;, &amp;quot;NULL&amp;quot; -&amp;gt; &amp;quot;null&amp;quot;)).show()
}

&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;groupby&#34;&gt;groupBy&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;groupBy&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//分组
val groupedDF = cleanDF.groupBy($&amp;quot;year&amp;quot;,$&amp;quot;month&amp;quot;)
//使用functions函数来完成聚合
import org.apache.spark.sql.functions._
groupedDF.agg(avg($&amp;quot;pm&amp;quot;) as &amp;quot;pm_avg&amp;quot;)
	.orderBy($&amp;quot;pm_avg&amp;quot;.desc)

//分组第二种方式
groupedDF.avg(&amp;quot;pm&amp;quot;)
	.select($&amp;quot;avg(pm)&amp;quot; as &amp;quot;pm_avg&amp;quot;)
	.orderBy(&amp;quot;pm_avg&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;多维聚合&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//requirement 1:不同年，不同来源PM值的平均数
val postAndYearDF = pmFinal.groupBy(&#39;source,&#39;year)
	.agg(avg($pm) as &amp;quot;pm&amp;quot;)

//requirement 2:按照不同的来源统计PM值的平均数
val postDF = pmFinal.groupBy($source)
	.agg(avg($pm) as &amp;quot;pm&amp;quot;)
	.select($source, lit(null) as &amp;quot;year&amp;quot;, $pm)

//合并在同一个结果集中
postAndYearDF.union(postDF)
	.sort($source, $year asc_nulls_last, $pm)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;rollup&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;滚动分组：rollup(A, B)，生成三列：AB分组，A null分组，null(全局)的分组&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//requirement 1: 每个城市，每年的销售额
//requirement 2: 每个城市，一共的销售额
//requirement 3: 总体销售额
val sales = Seq(
    (&amp;quot;Bj&amp;quot;, 2016, 100),
    (&amp;quot;Bj&amp;quot;, 2017, 200),
    (&amp;quot;shanghai&amp;quot;, 2015, 50),
    (&amp;quot;shanghai&amp;quot;, 2016, 150),
    (&amp;quot;Guangzhou&amp;quot;, 2017, 50),
).toDF(&amp;quot;city&amp;quot;, &amp;quot;year&amp;quot;, &amp;quot;amount&amp;quot;)

sales.rollup($city, $year)
	.agg(sum($amount) as &amp;quot;amount&amp;quot;)
	.sort($city asc asc_nulls_last, $year.asc_nulls_last)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;cube&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;rollup对参数顺序有要求，cube是对rollup的弥补&lt;/p&gt;
&lt;p&gt;rollup(A, B)，生成四列：AB分组，A null分组，null B分组，null(全局)的分组&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;import org.apache.spark.sql.functions._
pmFinal.cube($source, $year)
	.agg(avg($pm) as &amp;quot;pm&amp;quot;)
	.sort($source.asc_nulls_last, $year.asc_nulls_last)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;RelationalGroupedDataset&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;groupBy, rollup, cube后的数据类型都是RelationalGroupedDataset&lt;/p&gt;
&lt;p&gt;RelationalGroupedDataset并不是DataFrame，所以其中并没有DataFrame的方法，只有如下一些聚合相关的方法，下列方法调用后会生成DataFrame对象，然后就可以再次使用DataFrame的算子进行操作&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;操作符&lt;/th&gt;
&lt;th&gt;解释&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;avg&lt;/td&gt;
&lt;td&gt;average&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;count&lt;/td&gt;
&lt;td&gt;count&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;max&lt;/td&gt;
&lt;td&gt;max&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;min&lt;/td&gt;
&lt;td&gt;min&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;mean&lt;/td&gt;
&lt;td&gt;average&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;sum&lt;/td&gt;
&lt;td&gt;sum&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;agg&lt;/td&gt;
&lt;td&gt;聚合，可以使用sql.funcitons中的函数来配合进行操作&lt;br&gt;&lt;code&gt;pmDf.groupBy($year).agg(avg($pm) as &amp;quot;pm_avg&amp;quot;)&lt;/code&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;table-join&#34;&gt;Table Join&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Join&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;class JoinProcessor{
    //create Spark
    //import implicits._
    @Test
    def introJoin(): Unit = {
        val person = Seq((0, &amp;quot;Lu&amp;quot;, 0), (1, &amp;quot;Li&amp;quot;, 0), (2,&amp;quot;Tim&amp;quot;, 0))
        	.toDF(&amp;quot;id&amp;quot;, &amp;quot;name&amp;quot;, &amp;quot;cityID&amp;quot;)
        val cities = Seq((0, &amp;quot;BJ&amp;quot;), (1, &amp;quot;SH&amp;quot;), (2,&amp;quot;GZ&amp;quot;))
        	.toDF(&amp;quot;id&amp;quot;, &amp;quot;name&amp;quot;)

        val df = person.join(cities, person.col(&amp;quot;cityID&amp;quot;) === cities.col(&amp;quot;id&amp;quot;))
        	.select(person.col(&amp;quot;id&amp;quot;),person.col(&amp;quot;name&amp;quot;),cities.col(&amp;quot;name&amp;quot;))
        
        df.createOrReplaceTempView(&amp;quot;user_city&amp;quot;)
    }
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;cross&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;交叉连接，笛卡尔积&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def crossJoin(): Unit = {
    person.crossJoin(cities)
    	.where(person.col(&amp;quot;cityId&amp;quot;) === cities.col(&amp;quot;id&amp;quot;))

    spark.sql(&amp;quot;select u.id, u.name, from person u cross join cities c&amp;quot; + &amp;quot;where u.cityId = c.id&amp;quot;) 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;inner&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;交集&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-sql&#34;&gt;select * from person inner join cities on person.cityId = cities.id
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;person.join(right = cities,
           joinExprs = person(&amp;quot;cityId&amp;quot;) === citeis(&amp;quot;id&amp;quot;),
           joinType = &amp;quot;inner&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;outer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;全外连接&lt;/p&gt;
&lt;p&gt;内连接的结果只有连接上的数据，而全外连接可以包含没有连接上的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;leftouter&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;左外连接&lt;/p&gt;
&lt;p&gt;全外连接含没有连接上的数据，左外连接只包含左边没有连接上的数据。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;semi&amp;amp;anti&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Semi-join&lt;br&gt;
通常出现在使用了exists或in的sql中，所谓semi-join即在两表关联时，当第二个表中存在一个或多个匹配记录时，返回第一个表的记录；&lt;br&gt;
与普通join的区别在于semi-join时，第一个表里的记录最多只返回一次；&lt;/p&gt;
&lt;p&gt;Anti-join&lt;br&gt;
而anti-join则与semi-join相反，即当在第二张表没有发现匹配记录时，才会返回第一张表里的记录；&lt;br&gt;
当使用not exists/not in的时候会用到，两者在处理null值的时候会有所区别&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用not in且相应列有not null约束&lt;/li&gt;
&lt;li&gt;not exists，不保证每次都用到anti-join代&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;udf&#34;&gt;UDF&lt;/h2&gt;
&lt;p&gt;自定义列操作函数&lt;/p&gt;
&lt;h2 id=&#34;over-rank&#34;&gt;Over Rank&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//1定义窗口
val window = Window.partitionBy($category)
	.orderBy($revenue.desc)

//2处理数据
import org.apache.spark.sql.functions._
source.select($production, $category, dense_rank() over window as &amp;quot;rank&amp;quot;)
	.where($rank &amp;lt;= 2)
	.show()
&lt;/code&gt;&lt;/pre&gt;
        
      </description>
    </item>
    
    <item>
      <title>Spark 简介: Spark Guide, Part Ⅰ</title>
      <link>https://Jerrysmd.github.io/post/20200527sparkguide1/sparkguide1/</link>
      <pubDate>Wed, 27 May 2020 17:20:52 +0800</pubDate>
      
      <guid>https://Jerrysmd.github.io/post/20200527sparkguide1/sparkguide1/</guid>
      <description>
        
          &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
&lt;h2 id=&#34;spark-introduction&#34;&gt;Spark Introduction&lt;/h2&gt;
&lt;p&gt;1 Spark Component&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spark提供了批处理（RDDs），结构化查询（DataFrame），流计算（SparkStreaming），机器学习（MLib），图计算（GraphX）等组件&lt;/li&gt;
&lt;li&gt;这些组件均是依托于通用的计算引擎RDDs而构建出，所以spark-core的RDDs是整个Spark的基础&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/picture/sparkStructure.png&#34; alt=&#34;sparkStructure&#34;&gt;&lt;/p&gt;
&lt;p&gt;2 Spark &amp;amp; Hadoop&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;&lt;/th&gt;
&lt;th&gt;Hadoop&lt;/th&gt;
&lt;th&gt;Spark&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;类型&lt;/td&gt;
&lt;td&gt;基础平台，包含计算，存储，调度&lt;/td&gt;
&lt;td&gt;分布式计算工具（主要代替Hadoop的计算功能）&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;场景&lt;/td&gt;
&lt;td&gt;大规模数据集上的批处理&lt;/td&gt;
&lt;td&gt;迭代计算，交互式计算，流计算&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;延迟&lt;/td&gt;
&lt;td&gt;大&lt;/td&gt;
&lt;td&gt;小&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;易用性&lt;/td&gt;
&lt;td&gt;API较为底层，算法适应性差&lt;/td&gt;
&lt;td&gt;API较为顶层，方便使用&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;价格&lt;/td&gt;
&lt;td&gt;性能要求低，便宜&lt;/td&gt;
&lt;td&gt;对内存要求高，相对较贵&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;spark-cluster&#34;&gt;Spark Cluster&lt;/h2&gt;
&lt;p&gt;1 Cluster  relation&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/picture/clusterManager.png&#34; alt=&#34;clusterManager&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Driver：该进程调用Spark程序的main方法，并且启动SparkContext&lt;/li&gt;
&lt;li&gt;Cluster Manager：该进程负责和外部集群工具打交道，申请或释放集群资源&lt;/li&gt;
&lt;li&gt;Worker：该进程是一个守护进程，负责启动和管理Executor&lt;/li&gt;
&lt;li&gt;Executor：该进程是一个JVM虚拟机，负责运行Spark Task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;运行一个Spark程序大致经历如下几个步骤：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;启动Driver，创建SparkContext&lt;/li&gt;
&lt;li&gt;Client提交程序给Drive，Drive向Cluster Manager申请集群资源&lt;/li&gt;
&lt;li&gt;资源申请完毕，在Worker中启动Executor&lt;/li&gt;
&lt;li&gt;Driver将程序转化为Tasks，分发给Executor执行&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;2 Build Cluster&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download Spark&lt;/li&gt;
&lt;li&gt;Upload&lt;/li&gt;
&lt;li&gt;Config&lt;/li&gt;
&lt;li&gt;HistoryServer&lt;/li&gt;
&lt;li&gt;Distribute:  scp -r spark node02: $PWD&lt;/li&gt;
&lt;li&gt;Start&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;3 High Availability&lt;/p&gt;
&lt;p&gt;对于 Spark Standalone 集群来说，当Worker调度出现问题时，会自动的弹性容错，将出错的Task调度到其他Worker执行。&lt;/p&gt;
&lt;p&gt;但对于Master来说，是会出现单点失败的，为了避免可能出现的单点失败问题，Spark提供了两种方式满足高可用&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;使用Zookeeper实现Master的主备切换(Zookeeper是一个分布式强一致性的协调服务，Zookeeper最基本的一个保证是：如果多个节点同时创建一个ZNode)只有一个能够成功创建，这个做法的本质使用的是Zookeeper的ZAB协议，能够在分布式环境下达成一致。&lt;/li&gt;
&lt;li&gt;使用文件系统做主备切换&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;running-process&#34;&gt;Running Process&lt;/h2&gt;
&lt;p&gt;1 Spark-Shell Run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd1 = sc.textFile(&amp;quot;/data/wordcount.txt&amp;quot;) //Hadoop默认读取hdfs路径：hdfs:///data/wordcount.txt
val rdd2 = rddflatMap(item =&amp;gt; item.split(&amp;quot; &amp;quot;))
val rdd3 = rdd2.map(item =&amp;gt; (item,1))
val rdd4 = rdd3.reduceByKey((curr,agg) =&amp;gt; curr + agg)
rdd4.collect()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/picture/sparkDemoRunProcess.png&#34; alt=&#34;sparkDemoRunProcess&#34;&gt;&lt;/p&gt;
&lt;p&gt;2 Local IDEA Run&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def main(args:Arrary[String]): Unit = {
    // 创建SparkContext
    val conf = new SparkConf().setMaster(&amp;quot;local[6]&amp;quot;).setAppName(&amp;quot;word_count&amp;quot;)
    val sc = new SparkContext(conf)
    
    //2. 加载文件
    //	准备文件
    //	2.读取文件
    val rdd1 = sc.testFile(path = &amp;quot;dataset/wordcount.txt&amp;quot;)
    
    //3. 处理
    //	拆分为多个单词
    val rdd2 = rddflatMap(item =&amp;gt; item.split(&amp;quot; &amp;quot;))
    //	2.把每个单词指定一个词频
    val rdd3 = rdd2.map(item =&amp;gt; (item,1))
    //	3.聚合
    val rdd4 = rdd3.reduceByKey((curr,agg) =&amp;gt; curr + agg)
    
    //4.得到结果
    val result = rdd4.collect()
    result.foreach(item =&amp;gt; println(item))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;3  Submit Run&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;修改代码
&lt;ol&gt;
&lt;li&gt;去掉master设置，并修改文件路径&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Maven打包上传&lt;/li&gt;
&lt;li&gt;在集群中运行&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-shell&#34;&gt;bin/spark -submit --class cn.demo.spark.rdd.WordCount --master spark://node01:7077 ~/original -spark-0.0.jar
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;rdd&#34;&gt;RDD&lt;/h2&gt;
&lt;p&gt;1 Cause of creation&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在RDD出现之前，MapReduce是比较主流的&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;但多个MapReduce任务之间没有基于内存的数据共享方式，只能通过磁盘来进行共享，这种方式明显比较低效。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;RDD如何解决迭代计算非常低效的问题&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;在Spark中，最终Job3从逻辑上的计算过程是：Job3 = (Job1.map).filter，整个过程是共享内存的，而不需要中间结果存放在可靠的分布式文件系统中。&lt;/p&gt;
&lt;p&gt;2 Resilient Distributed Datasets&lt;/p&gt;
&lt;p&gt;分布式&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;RDD支持分区，可以运行在集群中&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;弹性&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;RDD支持高效的容错&lt;/li&gt;
&lt;li&gt;RDD中的数据即可以缓存在内存中，也可以缓存在磁盘中，也可以缓存在外部存储中&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;数据集&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;RDD可以不保存具体数据，只保留创建自己的必备信息，例如依赖和计算函数&lt;/li&gt;
&lt;li&gt;RDD也可以缓存起来，相当于存储具体数据&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;3 Feature&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RDD是数据集&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;RDD不仅是数据集，也是编程模型&lt;/p&gt;
&lt;p&gt;RDD的算子大致分为两类：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Transformation转化操作，例如：map、flatMap、filter等&lt;/li&gt;
&lt;li&gt;Action动作操作，例如：reduce、collect、show等&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;ol&gt;
&lt;li&gt;RDD是编程模型&lt;/li&gt;
&lt;li&gt;RDD相互之间有依赖关系&lt;/li&gt;
&lt;li&gt;RDD是可以分区的&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;RDD是只读的&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;RDD需要容错，可以惰性求值，可以移动计算，所以很难支持修改，显著降低问题的复杂度。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/picture/sparkRdd.png&#34; alt=&#34;sparkRdd&#34;&gt;&lt;/p&gt;
&lt;p&gt;4 sparkContext&lt;/p&gt;
&lt;p&gt;SparkContext是spark功能的主要入口。其代表与spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。每个JVM里只能存在一个处于激活状态的SparkContext，在创建新的SparkContext之前必须调用stop()来关闭之前的SparkContext。&lt;/p&gt;
&lt;p&gt;每一个Spark应用都是一个SparkContext实例，可以理解为一个SparkContext就是一个spark application的生命周期，一旦SparkContext创建之后，就可以用这个SparkContext来创建RDD、累加器、广播变量，并且可以通过SparkContext访问Spark的服务，运行任务。spark context设置内部服务，并建立与spark执行环境的连接。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@Test
def sparkContext(): Unit = {
    //  Spark Context 编写
    // 		创建SparkConf
    val conf = new SparkConf().setMaster(&amp;quot;local[6]&amp;quot;).setAppName(&amp;quot;spark_context&amp;quot;)
    //		2.创建SparkContext
    val sc = new SparkContext(conf)
    
    //SparkContext身为大入口API，应该能够创建RDD，并且设置参数，设置Jar包
    //sc...
    
    //2. 关闭SparkContext，释放集群资源
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;5 Creation Way&lt;/p&gt;
&lt;p&gt;三种RDD的创建方式&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt; 通过本地集合创建RDD
@Test
def rddCreationLocal(): Unit = {
    val conf = new SparkConf().setMaster(&amp;quot;local[6]&amp;quot;).setAppName(&amp;quot;spark_context&amp;quot;)
    val sc = new SparkContext(conf)
    val rdd1 = sc.parallelize(Seq(&amp;quot;Hello1&amp;quot;, &amp;quot;Hello2&amp;quot;, &amp;quot;Hello3&amp;quot;), 2)
    val rdd2 = sc.makeRDD(seq, 2) // parallelize和makeRDD区别：parallelize可以不指定分区数
}
2. 通过外部数据创建RDD
@Test
def rddCreationFiles(): Unit = {
    sc.textFile(&amp;quot;/.../...&amp;quot;)
    //testFile: 传入* hdfs://   file://   /.../...(这种方式分为在集群还是本地执行，在集群中读的是hdfs，本地读本地文件)
    //2.是否支持分区：支持，在hdfs中由hdfs文件的block决定
    //3.支持什么平台：支持aws和阿里云...
}
3. 通过RDD衍生新的RDD
@Test
def rddCreationFromRDD(): Unit = {
    val rdd1 = sc.parallelize(Seq(1,2,3))
    //通过在rdd上执行算子操作，会生成新的rdd
    //非原地计算：str.substr 返回新的字符串，非原地计算。字符串不可变，RDD也不可变
    val rdd2: RDD[Int] = rddmap(item =&amp;gt; item)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;transformation-operator&#34;&gt;Transformation Operator&lt;/h2&gt;
&lt;p&gt;map()&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@Test
def mapTest(): Unit = {
    //创建RDD
    val rdd1 = sc.parallelize(Seq(1,2,3))
    //2.执行map操作
    val rdd2 = rdd1.map(item =&amp;gt; item * 10)
    //3.得到结果
    val result = rdd2.collect()
    result.foreach(item =&amp;gt; println(item))
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;flatmap()&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;把rdd中的数据转化成数组或集合形式&lt;/li&gt;
&lt;li&gt;把集合展开&lt;/li&gt;
&lt;li&gt;生成了多条数据&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;flatmap是一对多&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@Test
def flatMapTest(): Unit = {
    val rdd1 = sc.parallelize(Seq(&amp;quot;Hello a&amp;quot;,&amp;quot;Hello b&amp;quot;,&amp;quot;Hello c&amp;quot;))
    val rdd2 = rddf1.latMap( item =&amp;gt; item.split(&amp;quot; &amp;quot;))
    val result = rdd2.collect()
    result.foreach(item =&amp;gt; println(item))
    sc.stop()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;reducebykey()&lt;/p&gt;
&lt;p&gt;reduceByKey第一步先按照key分组，然后对每一组进行聚合，得到结果。&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@Test
def reduceBykeyTest(): Unit = {
    //创建RDD
    val rdd1 = sc.parallelize(Seq(&amp;quot;Hello a&amp;quot;,&amp;quot;Hello b&amp;quot;,&amp;quot;Hello c&amp;quot;))
    //2.处理数据
    val rdd2 = rdd1.flatMap( item =&amp;gt; item.split(&amp;quot; &amp;quot;))
    .map( item =&amp;gt; (item,1) )
    .reduceByKey( (curr, agg) =&amp;gt; curr + agg)//curr是当前的总值，agg是单个item的值
    //3.得到结果
    val result = rdd2.collect()
    result.foreach(item =&amp;gt; println(item))
    //4.关闭sc
    sc.stop() 
}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Q&amp;amp;A&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数据量过大，如何处理？&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;集群中处理，利用集群多台计算机来并行处理&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;如何放在集群中运行?&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/picture/sparkPutFile2Cluster.png&#34; alt=&#34;sparkPutFile2Cluster&#34;&gt;&lt;/p&gt;
&lt;p&gt;并行计算就是同时使用多个计算资源解决一个问题，有四个要点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;解决的问题可以分解为多个可以并发计算的部分&lt;/li&gt;
&lt;li&gt;每个部分可以在不同处理器上被同时执行&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;需要一个共享内存的机制&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;需要一个总体上的协作机制来进行调度&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;如果放在集群中，如何对整个计算任务进行分解？&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/picture/sparkFile2Cluster2.png&#34; alt=&#34;sparkFile2Cluster2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;概述&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;对于HDFS中的文件，是分为不同的Block&lt;/li&gt;
&lt;li&gt;在进行计算的时候，就可以按照Block来划分，每一个Block对应一个不同的计算单元&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;扩展&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;RDD并没有真实的存放数据，数据是从HDFS中读取的，在计算的过程中读取即可&lt;/li&gt;
&lt;li&gt;RDD至少是需要可以&lt;strong&gt;分片&lt;/strong&gt;的，因为HDFS中的文件就是分片的，RDD可以分片也意味着可以并行计算&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;移动数据不如移动计算是一个基础的优化，如何做到？&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;每一个计算单元需要记录其存储单元的位置，尽量调度过去&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;集群中运行，需要多节点配合，出错的概率也更高，出错了怎么办？&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;RDD1-&amp;gt;RDD2-&amp;gt;RDD3这个过程中，RDD2出错了，有两种解决办法&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;缓存RDD2的数据，直接恢复RDD2，类似HDFS的备份机制&lt;/li&gt;
&lt;li&gt;记录RDD2的依赖关系，通过其父级的RDD来恢复RDD2，这种方式会少很多数据的交互和保存&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;如何通过父级RDD恢复？&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;记录RDD2的父亲是RDD1&lt;/li&gt;
&lt;li&gt;记录RDD2的计算函数，例如RDD2 = RDD1.map(&amp;hellip;)等计算函数&lt;/li&gt;
&lt;li&gt;通过父级RDD和计算函数来恢复RDD2&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;任务特别复杂，流程特别长，有很多RDD之间有依赖关系，如何优化？&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;上面提到了可以使用依赖关系来进行&lt;strong&gt;容错&lt;/strong&gt;，但是如果依赖关系特别长的时候，这种方式其实也比较低效，这个时候就应该使用另外一种方式，也就是记录数据集的状态&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;在Spark中有两个手段可以做到&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;缓存&lt;/li&gt;
&lt;li&gt;Checkpoint&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;map() &amp;amp; mapPartitions()&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;mapPartitions 和 map 算子是一样的，只不过map是针对每一条数据进行转换，mapPartitions针对一整个分区的数据进行转换&lt;/p&gt;
&lt;p&gt;所以&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;map 的 func 参数是单条数据，mapPartitions 的 func 参数是一个集合(一个分区整个所有的数据)&lt;/li&gt;
&lt;li&gt;map 的 func 返回值也是单条数据，mapPartition 的 func 返回值是一个集合&lt;/li&gt;
&lt;li&gt;mapPartitionWithIndex 和 mapPartition 的区别是 func 中多分区数量参数&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;filter()&lt;/p&gt;
&lt;p&gt;保留满足条件的元素&lt;/p&gt;
&lt;p&gt;sample()&lt;/p&gt;
&lt;p&gt;filter按照规律过滤，sample则是随机采样&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;def sample(
    withReplacement: Boolean,	//是否重复取样
    fraction: Double,			//取样比例
    seed: Long = Utils.random.nextLong): RDD[T] = {...}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;mapValues()&lt;/p&gt;
&lt;p&gt;mapValue也是map，map作用于全部数据，mapValue作用于value&lt;/p&gt;
&lt;p&gt;collection operation&lt;/p&gt;
&lt;p&gt;交集：rdd1.intersection(rdd2)&lt;/p&gt;
&lt;p&gt;并集：rdd1.union(rdd2)&lt;/p&gt;
&lt;p&gt;差集：rdd1.subract(rdd2)&lt;/p&gt;
&lt;p&gt;groupByKey()&lt;/p&gt;
&lt;p&gt;聚合操作：&lt;/p&gt;
&lt;p&gt;reduceByKey -&amp;gt;按照key分组，然后把每一组数据reduce。reduceByKey在map端combiner能减少IO，一个分区放多个数据。&lt;/p&gt;
&lt;p&gt;groupByKey 运算结果的格式：（k，（value1，value2）），没有减少IO&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;sc.parallelize(Seq((&amp;quot;a&amp;quot;,1),(&amp;quot;a&amp;quot;,1),(&amp;quot;b&amp;quot;,1)))
  .groupByKey()
  .collect()
  .foreach(println(_))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;combineByKey()&lt;/p&gt;
&lt;p&gt;接收三个参数：&lt;/p&gt;
&lt;p&gt;转化数据的函数（初始函数，作用于第一条数据，用于开启整个计算）&lt;/p&gt;
&lt;p&gt;在分区上进行聚合&lt;/p&gt;
&lt;p&gt;把所有的分区的聚合结果聚合为最终结果&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val result = rdd.combineBykey(
	createCombiner = curr =&amp;gt; (curr,1), 
    mergeValue = (curr: (Double, Int), nextValue: Double) =&amp;gt; (curr._1 + nextValue, curr._2 + 1)),
    mergeCombiners = (curr: (Double,Int), agg: (Double, Int)) =&amp;gt; (curr._1 + agg._1, curr._2 + agg._2)
)
result.map(item =&amp;gt; (item._1, item._2._1 / item._2._2))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;foldByKey()&lt;/p&gt;
&lt;p&gt;功能等同于reduceByKey()，增加了初始值。reduceByKey底层是combineByKey()，foldByKey()底层是aggregateByKey()。&lt;/p&gt;
&lt;p&gt;aggregateByKey()&lt;/p&gt;
&lt;p&gt;join()&lt;/p&gt;
&lt;p&gt;按照相同的Key进行连接&lt;/p&gt;
&lt;p&gt;sortBy()&lt;/p&gt;
&lt;p&gt;排序：sortBy()，sortByKey()&lt;/p&gt;
&lt;p&gt;coalesce()&lt;/p&gt;
&lt;p&gt;一般涉及到分区操作的算子常见的有两个，repartition和coalesce，都可以调大或者调小分区数量&lt;/p&gt;
&lt;p&gt;summary&lt;/p&gt;
&lt;p&gt;所有的转化操作的算子都是惰性的，在执行时候不会调度运行求得结果，而只是生成了对应的RDD&lt;/p&gt;
&lt;p&gt;只有在Action操作的时候，才会真的运行&lt;/p&gt;
&lt;h2 id=&#34;action-operator&#34;&gt;Action Operator&lt;/h2&gt;
&lt;p&gt;reduce((T, T) - U)&lt;/p&gt;
&lt;p&gt;对整个结果集规约，最终生成一条数据，是整个数据集的总汇&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;reduceByKey和reduce有什么区别：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;reduce是action算子，reduceByKey是一个转换算子&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RDD里有一万条数据，大部分key是相同的，有10个不同的key生成10条数据&lt;/p&gt;
&lt;p&gt;reduce生成1条数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;reduceByKey是按Key分组，然后把每组聚合&lt;/p&gt;
&lt;p&gt;reduce是针对一整个数据集进行聚合&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;reduceByKey是对KV数据进行计算&lt;/p&gt;
&lt;p&gt;reduce可针对所有类型数据&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;reduce算子是一个shuffle操作吗？&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;shuffle操作分为mapper和reducer，mapper将数据放入paritioner的函数计算，求得往哪个reducer里放&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;reduce操作没有mapper和reducer，因为reduce算子会作用于RDD中的每个分区，然后分区求得局部结果，最终汇总到Driver中求得最终结果&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RDD有五大属性，partitioner在shuffle过程中使用&lt;/p&gt;
&lt;p&gt;paritioner只有kv型的RDD才有&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;p&gt;collect()&lt;/p&gt;
&lt;p&gt;以数组的形式返回数据集中所有元素&lt;/p&gt;
&lt;p&gt;countByKey()&lt;/p&gt;
&lt;p&gt;count和countByKey&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;countByKey结果：Map(Key -&amp;gt; Key的count)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;调用Action会生成一个job，job会运行获取结果，所以在两个job中有大量的log&lt;/p&gt;
&lt;p&gt;数据倾斜：解决数据倾斜的问题，需要先通过countByKey查看Key对应的数量&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;first()&lt;/p&gt;
&lt;p&gt;返回第一个元素&lt;/p&gt;
&lt;p&gt;take(N)&lt;/p&gt;
&lt;p&gt;返回前N个元素&lt;/p&gt;
&lt;p&gt;takeSample(withReplacement, num)&lt;/p&gt;
&lt;p&gt;类似于sample，区别这是action，直接返回结果&lt;/p&gt;
&lt;p&gt;withReplacement：取数据有无放回&lt;/p&gt;
&lt;p&gt;first()&lt;/p&gt;
&lt;p&gt;first()速度相比其他方法最快&lt;/p&gt;
&lt;h2 id=&#34;data-type-in-rdd&#34;&gt;Data Type in RDD&lt;/h2&gt;
&lt;p&gt;RDD中存放的数据类型&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;基本类型，String，对象&lt;/li&gt;
&lt;li&gt;KV类型&lt;/li&gt;
&lt;li&gt;数字类型&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;practice&#34;&gt;Practice&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;@Test
def process(): Unit = {
    //1. 创建sc对象
    val conf = new SparkConf().setMaster(&amp;quot;local[6]&amp;quot;).setAppName(&amp;quot;practice&amp;quot;)
    val sc = new SparkContext(conf)
    
    //2. 读取文件
    //1,2010,1,1,0,4,NA,NA,NA,NA,-21,43,1021,-11,NW,1.79,0,0
    val source = sc.textFile(&amp;quot;dataset/parctive.csv&amp;quot;)
    
    //3. 处理数据
    val resultRDD = source.map(item =&amp;gt; ((item.split(&amp;quot;,&amp;quot;)(1), item.split(&amp;quot;,&amp;quot;)(2)),item.split(&amp;quot;,&amp;quot;)(6)))
    .filter(item =&amp;gt; StringUtils.isNotEmpty(item._2) &amp;amp;&amp;amp; ! item._2.equalsIgnoreCase(&amp;quot;NA&amp;quot;))
    .map(item =&amp;gt; (item._1, item._2.toInt))
    .reduceByKey((curr,agg) =&amp;gt; curr + agg)
    .sortBy(item =&amp;gt; item._2, ascending = false)
    
    //4. 获取结果
    resultRDD.take(10).foreach(item =&amp;gt; println(item))
    
    //5. 关闭sc
    sc.stop()
}
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;rdd-feature&#34;&gt;RDD Feature&lt;/h2&gt;
&lt;p&gt;RDD&amp;rsquo;s shuffle and partition&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RDD经常需要通过读取外部系统的数据来创建，外部存储系统往往是支持分片的。RDD需要支持分区，来和外部系统的分片一一对应&lt;/li&gt;
&lt;li&gt;RDD的分区是一个并行计算的实现手段&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;partition function&lt;/p&gt;
&lt;p&gt;RDD使用分区来分布式处理，当使用RDD读取数据时，会尽量在屋里上靠近数据源。比如读HDFS或Cassandra时，会尽量的保持RDD的分区和数据源的分区数，分区模式一一对应&lt;/p&gt;
&lt;p&gt;shuffle&lt;/p&gt;
&lt;p&gt;从mapper端到reducer端&lt;/p&gt;
&lt;p&gt;Spark支持宽依赖的转换，例如groupByKey和reduceByKey。在这些依赖项中，计算单个分区中的记录所需的数据可以来自于父数据集的许多分区中。要执行这些转换，具有相同key的所有元组必须最终位于同一分区中，由同一任务处理。为了满足这一要求，Spark产生一个shuffle，它在集群内部传输数据，并产生一个带有一组新分区的新stage。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Hash base shuffle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reduce 找到每个Mapper中对应自己哈希桶拉取数据&lt;/p&gt;
&lt;p&gt;缺点：过多占用资源占用&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sort base shuffle&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;先按照partition ID 排序， 后按照Key的HashCode排序&lt;/p&gt;
&lt;p&gt;partition and shuffle relation&lt;/p&gt;
&lt;p&gt;分区主要用来实现并行计算，和shuffle没什么关系，但数据处理时，例如reduceByKey，groupByKey等聚合操作，需要把Key相同的Value拉取到一起进行计算，这个时候因为这些Key的相同的Value可能会在不同的分区，所以理解分区才能理解shuffle的根本原理&lt;/p&gt;
&lt;p&gt;shuffle feature&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;只有KV型的RDD才会有Shuffle操作&lt;/li&gt;
&lt;li&gt;早期版本spark的shuffle算法是 hash base shuffle，后来改为 sort base shuffle，更适合大吞吐量的场景&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;check partition&lt;/p&gt;
&lt;p&gt;指定分区数&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过本地集合创建的时候指定分区数&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val conf = new SparkConf().setMaster(&amp;quot;local[6]&amp;quot;).setAppName(&amp;quot;practice&amp;quot;)//创建App并开启6个分区
val sc = new SparkContext(conf)
&lt;/code&gt;&lt;/pre&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;通过读取文件创建的时候指定分区数&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val rdd1 = sc.parallelize(Seq(1, 2, 3, 4, 5, 6, 7), 3)	//指定分区数3
val rdd2 = sc.testFile(&amp;quot;hdfs://node01:8020/data/test.txt&amp;quot;, 6)	//这里指定的是最小分区数6
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;查看方法&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;通过WebUI查看。端口：4040&lt;/li&gt;
&lt;li&gt;通过partitions来查看。rdd1.partitions.size&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;重分区&lt;/p&gt;
&lt;p&gt;coalesce(num, true)&lt;/p&gt;
&lt;p&gt;repartitions(num)&lt;/p&gt;
&lt;p&gt;RDD Cache&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//1. 取出IP
val countRDD = source.map(item =&amp;gt; (item.split(&amp;quot; &amp;quot;)(0), 1))
//2. 数据清洗
val cleanRDD = countRDD.filter(item =&amp;gt; StingUtils.isNotEmpty(item._1))
//3. 统计ip的出现次数
val aggRDD = cleanRDD.reduceBykey((curr,agg) =&amp;gt; curr + agg)
//4. 统计出现最少的ip
val leastIP = aggRDD.sortBy(item =&amp;gt; item._2, ascending = true).first()
//5. 统计出现最多的ip
val mostIP = aggRDD.sortBy(item =&amp;gt; item._2, ascending = false).first()

println(leastIP, mostIP)
sc.stop()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;第一次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey)&lt;/p&gt;
&lt;p&gt;第二次统计job（一个Action算子）执行了两个shuffle(reduceByKey，sortByKey)&lt;/p&gt;
&lt;p&gt;转换算子的作用：生成RDD，以及RDD之间的依赖关系&lt;/p&gt;
&lt;p&gt;Action算子的作用：生成job，执行job&lt;/p&gt;
&lt;p&gt;全局执行了四个shuffle&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;使用缓存的意义：&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减少shuffle操作&lt;/li&gt;
&lt;li&gt;容错，减少开销：rdd1-&amp;gt;rdd2-&amp;gt;rdd3，若rdd3算错会再次计算rdd1和rdd2整个流程。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;缓存API:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;cache()或persist(null/level)&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//1. 处理
val countRDD = source.map(item =&amp;gt; (item.split(&amp;quot; &amp;quot;)(0), 1))
val cleanRDD = countRDD.filter(item =&amp;gt; StingUtils.isNotEmpty(item._1))
val aggRDD = cleanRDD.reduceBykey((curr,agg) =&amp;gt; curr + agg)

//2. cache
aggRDD = aggRDD.cache()

//3. 两个RDD的action操作
val leastIP = aggRDD.sortBy(item =&amp;gt; item._2, ascending = true).first()
val mostIP = aggRDD.sortBy(item =&amp;gt; item._2, ascending = false).first()

println(leastIP, mostIP)
sc.stop()
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//1. 处理
val countRDD = source.map(item =&amp;gt; (item.split(&amp;quot; &amp;quot;)(0), 1))
val cleanRDD = countRDD.filter(item =&amp;gt; StingUtils.isNotEmpty(item._1))
val aggRDD = cleanRDD.reduceBykey((curr,agg) =&amp;gt; curr + agg)

//2. cache
aggRDD = aggRDD.persist(storageLevel.MEMORY_ONLY)

//3. 两个RDD的action操作
val leastIP = aggRDD.sortBy(item =&amp;gt; item._2, ascending = true).first()
val mostIP = aggRDD.sortBy(item =&amp;gt; item._2, ascending = false).first()

println(leastIP, mostIP)
sc.stop()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;缓存级别：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;MEMORY_ONLY: CPU效率最高&lt;/p&gt;
&lt;p&gt;MEMORY_ONLY_SER: 更加节省空间&lt;/p&gt;
&lt;p&gt;Checkpoint&lt;/p&gt;
&lt;p&gt;斩断RDD的依赖链，并且将数据存储在可靠的存储引擎中，例如HDFS&lt;/p&gt;
&lt;p&gt;HDFS的NameNode中主要职责就是维护两个文件，一个是edits，另一个是fsimage。&lt;/p&gt;
&lt;p&gt;edits中主要存放Editlog，FsImage保存了当前系统中所有目录和文件的信息，这个FsImage其实就是一个Checkpoint。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;每一次修改文件的时候，都会在Edits中添加一条记录。&lt;/li&gt;
&lt;li&gt;在一定条件满足的情况下，把edits删掉添加一个新的FSimage，包含了系统当前最新的状态。好处：增加速度，提高稳定性&lt;/li&gt;
&lt;/ol&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Checkpoint和Cache的区别：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Cache可以吧RDD计算出来放到内存中，但RDD的依赖链(相当于NameNode中的Edits日志)是不能丢的，若出现错误，只能重计算出来。&lt;/p&gt;
&lt;p&gt;Checkpoint把结果存放在HDFS这类存储中，就变成了可靠的数据，如果出错了，则通过复制HDFS中的文件来实现容错。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;如何使用：&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;两步：&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val conf = new.SparkConf().setMaster(&amp;quot;local[6]&amp;quot;).setAppName(&amp;quot;debug_string&amp;quot;)
//1. setCheckPointDir：设置保存目录，也可以设置为HDFS上的目录
sc.setCheckpointDir(&amp;quot;checkpoint&amp;quot;)

val interimRDD = sc.textFile(&amp;quot;dataset/test.txt&amp;quot;)
				.map(item =&amp;gt; (item.split(&amp;quot; &amp;quot;)(0), 1))
				.filter(item =&amp;gt; StringUtils.isNotBlank(item._1))
				.reduceByKey((curr, agg) =&amp;gt; curr + agg)

//2. setCheckPoint：是一个action操作，也就是说如果调用checkpoint，则会重新计算一下RDD，然后把结果存在HDFS或者本地目录中
interimRDD.checkpoint()
interimRDD.collect().foreach(println(_))

sc.stop()
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;spark-running-process&#34;&gt;Spark Running Process&lt;/h2&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;//1. 创建sc对象
//2. 创建数据集
val textRDD = sc.parallelize(Seq(&amp;quot;hadoop spark&amp;quot;, &amp;quot;hadoop flume&amp;quot;, &amp;quot;spark soo&amp;quot;))
//3. 数据处理
//	1.拆词2.赋予初始词频3.聚合4.将结果转为字符串
val splitRDD = textRDD.flatMap(_.split(&amp;quot; &amp;quot;))
val tupleRDD = splitRDD.map((_, 1))
val reduceRDD = tupleRDD.reduceByKey(_ + _)
val strRDD = reduceRDD.map(item =&amp;gt; s&amp;quot;${item._1}, ${item._2}&amp;quot;)
//4. 结果获取
strRDD.collect().foreach(item =&amp;gt; println(_))
//5. 关闭sc
sc.stop()
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;集群组成&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Node1主节点:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Master Daemon&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;负责管理Master节点， 协调资源的获取，以及连接Worker节点来运行Executor，是spark集群中的协调节点&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Node2:&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Worker Daemon&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;也称之为Slaves，是spark集群中的计算节点，用于和Master交互和并管理Driver， 当一个spark job 提交后，会创建sparkContext，worker会启动对应的Executor&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Driver&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;ction算子操作获取的结果，会把结果存放在Driver中&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Executor Backend&lt;/strong&gt;：&lt;/p&gt;
&lt;p&gt;Worker用于控制Executor的启停，其实worker是通过 Executor Backend来进行控制的。 Executor Backend是一个进程（是一个JVM实例），持有一个Executor对象。&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Executor&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Task1   Task2   Task3&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;逻辑执行图&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val textRDD = sc.parallelize(Seq(&amp;quot;hadoop spark&amp;quot;, &amp;quot;hadoop flume&amp;quot;, &amp;quot;spark soo&amp;quot;))
val splitRDD = textRDD.flatMap(_.split(&amp;quot; &amp;quot;))
val tupleRDD = splitRDD.map((_, 1))
val reduceRDD = tupleRDD.reduceByKey(_ + _)
val strRDD = reduceRDD.map(item =&amp;gt; s&amp;quot;${item._1}, ${item._2}&amp;quot;)
println(strRDD.toDebugString)

(8) MapPartitionsRDD[4] at map at test.scala:12 []
 |  ShuffledRDD[3] at reduceByKey at test.scala:11 []
 +-(8) MapPartitionsRDD[2] at map at test.scala:10 []
    |  MapPartitionsRDD[1] at flatMap at test.scala:9 []
    |  ParallelCollectionRDD[0] at parallelize at test.scala:6 []
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/20200527sparkGuide1/RDDlogic.png&#34; alt=&#34;RDDlogic&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;物理执行图&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;当触发Action执行的时候，这一组互相依赖的RDD要被处理，所以要转化为可运行的物理执行图，调度到集群中执行。&lt;/p&gt;
&lt;p&gt;因为大部分RDD是不真正存放数据的，只是数据从中流转，所以不能直接在集群中运行RDD，要有一种pipeline的思想，需要将这组RDD转为Stage和Task，从而运行Task，优化整体执行速度。&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/20200527sparkGuide1/RDDphysic.png&#34; alt=&#34;RDDphysic&#34;&gt;&lt;/p&gt;
&lt;p&gt;小结：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;① -&amp;gt; ① -&amp;gt; ① 在第一个stage中，每一个这样的执行流程是一个Task，也就是在同一个Stage中的所有RDD的对应分区，在同一个Task中执行&lt;/li&gt;
&lt;li&gt;Stage的划分是由Shuffle操作来确定的，有Shuffle的地方，Stage断开&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;数据流动&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-scala&#34;&gt;val sc = ...
val textRDD = sc.parallelize(Seq(&amp;quot;Hadoop Spark&amp;quot;, &amp;quot;Hadoop Flume&amp;quot;, &amp;quot;Spark Squad&amp;quot;))
val splitRDD = textRDD.flatMap(_.split(&amp;quot; &amp;quot;))
val tupleRDD = splitRDD.map((_,1))
val reduceRDD = tupleRDD.reduceByKey(_ + _)
val strRDD = reduceRDD.map(item =&amp;gt; s&amp;quot;${item._1, ${item._2}}&amp;quot;)
strRDD.collect.foreach(item =&amp;gt; println(item))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Job和Stage的关系&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Job是一个最大的调度单位，DAGScheduler会首先创建一个Job的相关信息，然后去调度Job，但是没办法直接调度Job。&lt;/p&gt;
&lt;p&gt;​	&lt;strong&gt;为什么Job需要切分&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;因为job的含义是对整个RDD求值，但RDD之间可能有一些宽依赖&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果遇到宽依赖的话，两个RDD之间需要进行数据拉取和复制&lt;/p&gt;
&lt;p&gt;那么一个RDD就必须等待它所依赖的RDD所有分区先计算完成，然后再进行拉取&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;所以，一个Job是无法计算完整的RDD血统的&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;​	&lt;strong&gt;Stage和Task的关系&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Stage中的RDD之间是窄依赖：&lt;/p&gt;
&lt;p&gt;窄依赖RDD理论上可以放在同一个Pipeline中执行的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;RDD还有分区：&lt;/p&gt;
&lt;p&gt;一个RDD只是一个概念，而真正存放和处理数据时，都是以分区作为单位的&lt;/p&gt;
&lt;p&gt;Stage对应的是多个整体上的RDD，而真正的运行是需要针对RDD的分区来进行的&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个Task对应一个RDD的分区：&lt;/p&gt;
&lt;p&gt;一个比Stage粒度更细的单元叫做Task，Stage是由Task组成的，之所以有Task这个概念，是因为Stage针对整个RDD，而计算的时候，要针对RDD的分区。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;总结：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Job&amp;gt;Stage&amp;gt;Task&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个Job由多个Stage组成(这个取决有多少个宽依赖)，一个Stage由多个Task组成（这个取决有多少个分区数量&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;而Stage中经常会有一组Task需要同时执行，所以针对每一个Task来进行调度太过频繁没有意义，所以每个Stage中的Task们会被收集起来，放入一个TaskSet集合中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;一个Stage有一个TaskSet&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;TaskSet中Task的个数由Stage中的最大分区数决定&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://Jerrysmd.github.io/post/20200527sparkGuide1/sparkFlow.png&#34; alt=&#34;sparkFlow&#34;&gt;&lt;/p&gt;
        
      </description>
    </item>
    
  </channel>
</rss>
