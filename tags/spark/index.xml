<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0"><channel><title>Spark - Tag - JerrysBlog</title><link>https://jerrysmd.github.io/tags/spark/</link><description>Spark - Tag - JerrysBlog</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sat, 31 Dec 2022 14:43:47 +0800</lastBuildDate><atom:link href="https://jerrysmd.github.io/tags/spark/" rel="self" type="application/rss+xml"/><item><title>Spark Sharing: A Framework for Sharing Data and Resources Across Clusters</title><link>https://jerrysmd.github.io/20221231_spark-sharing/</link><pubDate>Sat, 31 Dec 2022 14:43:47 +0800</pubDate><author/><guid>https://jerrysmd.github.io/20221231_spark-sharing/</guid><description>&lt;p>Apache Spark™ is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters.&lt;/p></description></item><item><title>Spark Performance Tuning</title><link>https://jerrysmd.github.io/20220719_spark-optimization/</link><pubDate>Tue, 19 Jul 2022 14:54:16 +0800</pubDate><author/><guid>https://jerrysmd.github.io/20220719_spark-optimization/</guid><description>&lt;p>Spark SQL is the top active component in spark 3.0 release. Most of the resolved tickets are for Spark SQL. These enhancements benefit all the higher-level libraries, including structured streaming and MLlib, and higher level APIs, including SQL and DataFrames. Various related optimizations are added in latest release.&lt;/p></description></item><item><title>Parquet: A Columnar File Format for Big Data Analysis</title><link>https://jerrysmd.github.io/20201215_parquet-format/</link><pubDate>Tue, 15 Dec 2020 10:47:59 +0800</pubDate><author/><guid>https://jerrysmd.github.io/20201215_parquet-format/</guid><description>&lt;p>Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types. This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO.&lt;/p></description></item><item><title>Spark On Yarn: yarn-cluster, yarn-client</title><link>https://jerrysmd.github.io/20200811_yarn-clusteryarn-client/</link><pubDate>Tue, 11 Aug 2020 10:42:19 +0800</pubDate><author/><guid>https://jerrysmd.github.io/20200811_yarn-clusteryarn-client/</guid><description>&lt;p>YARN is a generic resource-management framework for distributed workloads; in other words, a cluster-level operating system. Although part of the Hadoop ecosystem, YARN can support a lot of varied compute-frameworks (such as Tez, and Spark) in addition to MapReduce.&lt;/p></description></item><item><title>Spark: A Framework for Large-Scale Data Processing, Part Ⅲ</title><link>https://jerrysmd.github.io/20200803_spark-guide3/</link><pubDate>Mon, 03 Aug 2020 16:10:33 +0800</pubDate><author/><guid>https://jerrysmd.github.io/20200803_spark-guide3/</guid><description>&lt;p>Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p></description></item><item><title>Spark: A Framework for Large-Scale Data Processing, Part Ⅱ</title><link>https://jerrysmd.github.io/20200707_spark-guide2/</link><pubDate>Tue, 07 Jul 2020 11:14:54 +0800</pubDate><author/><guid>https://jerrysmd.github.io/20200707_spark-guide2/</guid><description>&lt;p>Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p></description></item></channel></rss>