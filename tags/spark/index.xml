<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark on JerrysBlog</title>
    <link>https://jerrysmd.github.io/tags/spark/</link>
    <description>Recent content in spark on JerrysBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Tue, 15 Dec 2020 10:47:59 +0800</lastBuildDate><atom:link href="https://jerrysmd.github.io/tags/spark/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Parquet Format</title>
      <link>https://jerrysmd.github.io/post/20201215_parquet_format/</link>
      <pubDate>Tue, 15 Dec 2020 10:47:59 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201215_parquet_format/</guid>
      <description>
        
          
            &lt;p&gt;Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types. This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Elasticsearch Wildcard Search</title>
      <link>https://jerrysmd.github.io/post/20200911_es-wildcard-search/</link>
      <pubDate>Fri, 11 Sep 2020 10:57:33 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200911_es-wildcard-search/</guid>
      <description>
        
          
            &lt;p&gt;Elasticsearch is the distributed, RESTful search and analytics engine at the heart of the &lt;a href=&#34;https://www.elastic.co/products&#34;&gt;Elastic Stack&lt;/a&gt;. You can use Elasticsearch to store, search, and manage data for Logs，Metrics，A search backend，Application monitoring，Endpoint security.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark On Yarn: yarn-cluster, yarn-client</title>
      <link>https://jerrysmd.github.io/post/20200811_yarn-clusteryarn-client/</link>
      <pubDate>Tue, 11 Aug 2020 10:42:19 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200811_yarn-clusteryarn-client/</guid>
      <description>
        
          
            &lt;p&gt;YARN is a generic resource-management framework for distributed workloads; in other words, a cluster-level operating system. Although part of the Hadoop ecosystem, YARN can support a lot of varied compute-frameworks (such as Tez, and Spark) in addition to MapReduce.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅲ</title>
      <link>https://jerrysmd.github.io/post/20200803_sparkguide3/</link>
      <pubDate>Mon, 03 Aug 2020 16:10:33 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200803_sparkguide3/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅱ</title>
      <link>https://jerrysmd.github.io/post/20200707_sparkguide2/</link>
      <pubDate>Tue, 07 Jul 2020 11:14:54 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200707_sparkguide2/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅰ</title>
      <link>https://jerrysmd.github.io/post/20200527_sparkguide1/</link>
      <pubDate>Wed, 27 May 2020 17:20:52 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200527_sparkguide1/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
