<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JerrysBlog</title>
    <link>https://jerrysmd.github.io/</link>
    <description>Recent content on JerrysBlog</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 18 Dec 2021 10:01:20 +0800</lastBuildDate><atom:link href="https://jerrysmd.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Memories of 2021</title>
      <link>https://jerrysmd.github.io/post/20211218_2021_summary_video/article/</link>
      <pubDate>Sat, 18 Dec 2021 10:01:20 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20211218_2021_summary_video/article/</guid>
      <description>
        
          
            &lt;p&gt;Photos and videos recorded in 202&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Warehouse: Real-Time</title>
      <link>https://jerrysmd.github.io/post/20210308_dw_flink/article/</link>
      <pubDate>Mon, 08 Mar 2021 10:40:49 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20210308_dw_flink/article/</guid>
      <description>
        
          
            &lt;p&gt;Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Tensorflow Identify Simple Captcha</title>
      <link>https://jerrysmd.github.io/post/20210204_tensorflow_captcha/article/</link>
      <pubDate>Thu, 04 Feb 2021 07:10:16 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20210204_tensorflow_captcha/article/</guid>
      <description>
        
          
            &lt;p&gt;CAPTCHA stands for &#39;Completely Automated Public Turing test to tell Computers and Humans Apart&#39;. It’s already possible to solve it with the rise of deep learning and computer vision.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Data Warehouse: Offline</title>
      <link>https://jerrysmd.github.io/post/20210108_dw_behavior_collection/article/</link>
      <pubDate>Fri, 08 Jan 2021 09:43:01 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20210108_dw_behavior_collection/article/</guid>
      <description>
        
          
            &lt;p&gt;Data warehouse is a system that pulls together data derived from operational systems and external data sources within an organization for reporting and analysis. A data warehouse is a central repository of information that provides users with current and historical decision support information.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Parquet Format</title>
      <link>https://jerrysmd.github.io/post/20201215_parquet_format/parquet_format/</link>
      <pubDate>Tue, 15 Dec 2020 10:47:59 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201215_parquet_format/parquet_format/</guid>
      <description>
        
          
            &lt;p&gt;Apache Parquet is designed for efficient as well as performant flat columnar storage format of data compared to row based files like CSV or TSV files. Parquet uses the record shredding and assembly algorithm which is superior to simple flattening of nested namespaces. Parquet is optimized to work with complex data in bulk and features different ways for efficient data compression and encoding types. This approach is best especially for those queries that need to read certain columns from a large table. Parquet can only read the needed columns therefore greatly minimizing the IO.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Clickhouse Introduction</title>
      <link>https://jerrysmd.github.io/post/20201107_clickhouse_intro/clickhouse_intro/</link>
      <pubDate>Sat, 07 Nov 2020 14:55:59 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201107_clickhouse_intro/clickhouse_intro/</guid>
      <description>
        
          
            &lt;p&gt;A high performance columnar OLAP database management system for real-time analytics using SQL. ClickHouse can be customized with a new set of efficient columnar storage engines, and has realized rich functions such as data ordered storage, primary key indexing, sparse indexing, data sharding, data partitioning, TTL, and primary and backup replication.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hbase Optimize</title>
      <link>https://jerrysmd.github.io/post/20201026_hbase_optimize/article/</link>
      <pubDate>Mon, 26 Oct 2020 09:14:35 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201026_hbase_optimize/article/</guid>
      <description>
        
          
            &lt;p&gt;HBase is a high reliability, high performance, column-oriented, and scalable distributed database. However, the READ/write performance deteriorates when a large amount of concurrent data or existing data is generated. You can use the following methods to improve the HBase search speed.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hbase Rowkey Design</title>
      <link>https://jerrysmd.github.io/post/20201016hbase-rowkey-design/hbase-rowkey-design/</link>
      <pubDate>Fri, 16 Oct 2020 10:30:42 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20201016hbase-rowkey-design/hbase-rowkey-design/</guid>
      <description>
        
          
            &lt;p&gt;Rows in HBase are sorted lexicographically by row key. This design optimizes for scans, allowing you to store related rows, or rows that will be read together, near each other. However, poorly designed row keys are a common source of &lt;em&gt;hotspotting&lt;/em&gt;. Hotspotting occurs when a large amount of client traffic is directed at one node, or only a few nodes, of a cluster. This traffic may represent reads, writes, or other operations. The traffic overwhelms the single machine responsible for hosting that region, causing performance degradation and potentially leading to region unavailability. This can also have adverse effects on other regions hosted by the same region server as that host is unable to service the requested load. It is important to design data access patterns such that the cluster is fully and evenly utilized.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Elasticsearch Wildcard Search</title>
      <link>https://jerrysmd.github.io/post/20200911es-wildcard-search/es-wildcard-search/</link>
      <pubDate>Fri, 11 Sep 2020 10:57:33 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200911es-wildcard-search/es-wildcard-search/</guid>
      <description>
        
          
            &lt;p&gt;Elasticsearch is the distributed, RESTful search and analytics engine at the heart of the &lt;a href=&#34;https://www.elastic.co/products&#34;&gt;Elastic Stack&lt;/a&gt;. You can use Elasticsearch to store, search, and manage data for Logs，Metrics，A search backend，Application monitoring，Endpoint security.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark On Yarn: yarn-cluster, yarn-client</title>
      <link>https://jerrysmd.github.io/post/20200811yarn-clusteryarn-client/article/</link>
      <pubDate>Tue, 11 Aug 2020 10:42:19 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200811yarn-clusteryarn-client/article/</guid>
      <description>
        
          
            &lt;p&gt;YARN is a generic resource-management framework for distributed workloads; in other words, a cluster-level operating system. Although part of the Hadoop ecosystem, YARN can support a lot of varied compute-frameworks (such as Tez, and Spark) in addition to MapReduce.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅲ</title>
      <link>https://jerrysmd.github.io/post/20200803sparkguide3/sparkguide3/</link>
      <pubDate>Mon, 03 Aug 2020 16:10:33 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200803sparkguide3/sparkguide3/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅱ</title>
      <link>https://jerrysmd.github.io/post/20200707sparkguide2/sparkguide2/</link>
      <pubDate>Tue, 07 Jul 2020 11:14:54 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200707sparkguide2/sparkguide2/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hive Key Points</title>
      <link>https://jerrysmd.github.io/post/20200615hivekeypoint/hivekeypoint/</link>
      <pubDate>Mon, 15 Jun 2020 09:26:00 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200615hivekeypoint/hivekeypoint/</guid>
      <description>
        
          
            &lt;p&gt;Hive is a Hadoop-based data warehouse tool that maps structured data files into a database table and provides complete SQL query functionality that converts SQL statements into MapReduce tasks for execution. It is very suitable for statistical analysis of data warehouse.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Spark Guide, Part Ⅰ</title>
      <link>https://jerrysmd.github.io/post/20200527sparkguide1/sparkguide1/</link>
      <pubDate>Wed, 27 May 2020 17:20:52 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200527sparkguide1/sparkguide1/</guid>
      <description>
        
          
            &lt;p&gt;Apache Spark has its architectural foundation in the resilient distributed dataset (RDD), a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way. The Dataframe API was released as an abstraction on top of the RDD, followed by the Dataset API.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The Development of App Architecture</title>
      <link>https://jerrysmd.github.io/post/20200526apparchitecturedevelopment/article/</link>
      <pubDate>Tue, 26 May 2020 17:17:50 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200526apparchitecturedevelopment/article/</guid>
      <description>
        
          
            &lt;p&gt;The features of large applications: high availability, high concurrency and big data. High availability: system need to provide service without interruption. High concurrency: still stable under the big access. Big data: store and manage big data well.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Final 关键字: Final Key Word In Java</title>
      <link>https://jerrysmd.github.io/post/20200526finalkeywordinjava/finalkeywordinjava/</link>
      <pubDate>Tue, 26 May 2020 15:56:36 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200526finalkeywordinjava/finalkeywordinjava/</guid>
      <description>
        
          
            &lt;p&gt;Final key word no doubt be mentioned most time in Java language. There are some points about final key word. First of all, final key word can modify three objects: first is modify variables, second is modify way, third is modify class.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Key points of ZooKeeper</title>
      <link>https://jerrysmd.github.io/post/20200519zookeeper/zookeeper/</link>
      <pubDate>Tue, 19 May 2020 16:07:17 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200519zookeeper/zookeeper/</guid>
      <description>
        
          
            &lt;p&gt;ZooKeeper is an open source distributed coordination framework. It is positioned to provide consistent services for distributed applications and is the administrator of the entire big data system. ZooKeeper will encapsulate key services that are complex and error-prone, and provide users with efficient, stable, and easy-to-use services.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Kafka &amp; Message Queue</title>
      <link>https://jerrysmd.github.io/post/20200427kafka/kafka/</link>
      <pubDate>Mon, 27 Apr 2020 14:21:19 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200427kafka/kafka/</guid>
      <description>
        
          
            &lt;p&gt;Apache Kafka aims to provide a unified, high-throughput, low-latency platform for handling real-time data feeds. Kafka can connect to external systems (for data import/export) via Kafka Connect and provides Kafka Streams, a Java stream processing library. Kafka uses a binary TCP-based protocol that is optimized for efficiency and relies on a &amp;quot;message set&amp;quot; abstraction that naturally groups messages together to reduce the overhead of the network roundtrip. This &amp;quot;leads to larger network packets, larger sequential disk operations, contiguous memory blocks [...] which allows Kafka to turn a bursty stream of random message writes into linear writes.&amp;quot;&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>HDFS &amp; NFS</title>
      <link>https://jerrysmd.github.io/post/20200326hdfsnfs/hdfsnfs/</link>
      <pubDate>Thu, 26 Mar 2020 11:02:36 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200326hdfsnfs/hdfsnfs/</guid>
      <description>
        
          
            &lt;p&gt;The major difference between the two is Replication/Fault Tolerance. HDFS was designed to survive failures. NFS does not have any fault tolerance built in. Other than fault tolerance, HDFS does support multiple replicas of files. This eliminates (or eases) the common bottleneck of many clients accessing a single file. Since files have multiple replicas, on different physical disks, reading performance scales better than NFS.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Scala Introduction</title>
      <link>https://jerrysmd.github.io/post/20200208scalaintroduction/scalaintroduction/</link>
      <pubDate>Sat, 08 Feb 2020 10:16:17 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200208scalaintroduction/scalaintroduction/</guid>
      <description>
        
          
            &lt;p&gt;Scala combines object-oriented and functional programming in one concise, high-level language. Scala&#39;s static types help avoid bugs in complex applications, and its JVM and JavaScript runtimes let you build high-performance systems with easy access to huge ecosystems of libraries.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Static Keyword in Java</title>
      <link>https://jerrysmd.github.io/post/20200122javastatic/javastatic/</link>
      <pubDate>Wed, 22 Jan 2020 11:20:11 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200122javastatic/javastatic/</guid>
      <description>
        
          
            &lt;p&gt;The static keyword can be used for variables, methods, code blocks, and inner classes to indicate that a particular member belongs only to a class itself, and not to an object of that class.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Java Basic</title>
      <link>https://jerrysmd.github.io/post/20200101javainterview/javainterview/</link>
      <pubDate>Wed, 01 Jan 2020 11:18:47 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200101javainterview/javainterview/</guid>
      <description>
        
          
            &lt;p&gt;JVM, Garbage Collection, Multi - thread, Redis&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Leetcode</title>
      <link>https://jerrysmd.github.io/post/20200101leetcode/leetcode/</link>
      <pubDate>Wed, 01 Jan 2020 10:57:11 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20200101leetcode/leetcode/</guid>
      <description>
        
          
            &lt;p&gt;Algorithms are used in every part of computer science. They form the field&#39;s backbone. In computer science, an algorithm gives the computer a specific set of instructions, which allows the computer to do everything, be it running a calculator or running a rocket.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Shallow Copy and Deep Copy in JAVA</title>
      <link>https://jerrysmd.github.io/post/20191229copyinjava/copyinjava/</link>
      <pubDate>Sun, 29 Dec 2019 22:14:03 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191229copyinjava/copyinjava/</guid>
      <description>
        
          
            &lt;p&gt;Cloning is a process of creating an exact copy of an existing object in the memory. In java, clone() method of java.lang.Object class is used for cloning process. This method creates an exact copy of an object on which it is called through field-by-field assignment and returns the reference of that object. Not all the objects in java are eligible for cloning process. The objects which implement Cloneable interface are only eligible for cloning process. Cloneable interface is a marker interface which is used to provide the marker to cloning process. Click here to see more info on clone() method in java.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Memories of 2019</title>
      <link>https://jerrysmd.github.io/post/20191218_2019_summary_video/article/</link>
      <pubDate>Wed, 18 Dec 2019 10:01:12 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191218_2019_summary_video/article/</guid>
      <description>
        
          
            &lt;p&gt;Photos and videos recorded in 2019&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Git Contribution</title>
      <link>https://jerrysmd.github.io/post/20191126gitcontribution/gitcontribution/</link>
      <pubDate>Tue, 26 Nov 2019 11:22:47 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191126gitcontribution/gitcontribution/</guid>
      <description>
        
          
            &lt;p&gt;Contribution graph shows activity from public repositories. You can choose to show activity from both public and private repositories, with specific details of your activity in private repositories anonymized.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Various data types in C</title>
      <link>https://jerrysmd.github.io/post/20191007u32/u32/</link>
      <pubDate>Mon, 07 Oct 2019 09:11:47 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20191007u32/u32/</guid>
      <description>
        
          
            &lt;p&gt;C/C++ provides various data types that can be used in your programs.In general, you&#39;d commonly use: int for most variables and &amp;quot;countable&amp;quot; things (for loop counts, variables, events). char for characters and strings. float for general measurable things (seconds, distance, temperature). uint32 for bit manipulations, especially on 32-bit registers.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>The size of structure in C</title>
      <link>https://jerrysmd.github.io/post/20190825structsize/structsize/</link>
      <pubDate>Sun, 25 Aug 2019 14:38:34 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190825structsize/structsize/</guid>
      <description>
        
          
            &lt;p&gt;The sizeof for a struct is not always equal to the sum of sizeof of each individual member. This is because of the padding added by the compiler to avoid alignment issues. Padding is only added when a structure member is followed by a member with a larger size or at the end of the structure.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Typedef介绍: The descriptions of typedef</title>
      <link>https://jerrysmd.github.io/post/20190717typedef/typedef/</link>
      <pubDate>Wed, 17 Jul 2019 09:22:18 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190717typedef/typedef/</guid>
      <description>
        
          
            &lt;p&gt;typedef为C语言的关键字，作用是为一种数据类型定义一个新名字，这里的数据类型包括内部数据类型（int，char等）和自定义的数据类型（struct等）。typedef本身是一种存储类的关键字，与auto、extern、static、register等关键字不能出现在同一个表达式中。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>条件编译指令: Conditional compilation directives</title>
      <link>https://jerrysmd.github.io/post/20190611conditionalcompilation/conditionalcompilation/</link>
      <pubDate>Tue, 11 Jun 2019 10:42:49 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190611conditionalcompilation/conditionalcompilation/</guid>
      <description>
        
          
            &lt;p&gt;条件编译是根据实际定义宏（某类条件）进行代码静态编译的手段。可根据表达式的值或某个特定宏是否被定义来确定编译条件。c语言中条件编译相关的预编译指令，包括  #define、#undef、#ifdef、#ifndef、#if、#elif、#else、#endif、defined。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Python Introduction</title>
      <link>https://jerrysmd.github.io/post/20190510pythonintroduction/pythonintroduction/</link>
      <pubDate>Fri, 10 May 2019 09:34:11 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190510pythonintroduction/pythonintroduction/</guid>
      <description>
        
          
            &lt;p&gt;Python is dynamically-typed and garbage-collected. It supports multiple programming paradigms, including structured (particularly, procedural), object-oriented and functional programming. Python is often described as a &amp;quot;batteries included&amp;quot; language due to its comprehensive standard library.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>C Project performance optimization</title>
      <link>https://jerrysmd.github.io/post/20190409cperformanceoptimization/cperformanceoptimization/</link>
      <pubDate>Tue, 09 Apr 2019 20:04:00 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190409cperformanceoptimization/cperformanceoptimization/</guid>
      <description>
        
          
            &lt;p&gt;大型C项目的性能优化方法和思路。x86项目往低性能处理器上移植时遇到性能瓶颈的性能优化策略。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>GDB调试: GDB debug guide</title>
      <link>https://jerrysmd.github.io/post/20190304gdb_debug/gdb_debug/</link>
      <pubDate>Mon, 04 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190304gdb_debug/gdb_debug/</guid>
      <description>
        
          
            &lt;p&gt;GDB是一个由GNU开源组织发布的、UNIX/LINUX操作系统下的、基于命令行的、功能强大的程序调试工具。 对于一名Linux下工作的c/c++程序员，gdb是必不可少的工具；&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>大页内存: HugePages</title>
      <link>https://jerrysmd.github.io/post/20190203hugepages/hugepages/</link>
      <pubDate>Sun, 03 Feb 2019 13:04:09 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190203hugepages/hugepages/</guid>
      <description>
        
          
            &lt;p&gt;通过增大操作系统页的大小来减小页表，从而避免快表缺失。大页内存优化程序主要是针对其中的malloc机制的，意思就是分配大页，增加tlb的命中率。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>动态链接和静态链接: Dynamic link library and static link library</title>
      <link>https://jerrysmd.github.io/post/20190125dynamicstaticlinklibrary/dynamicstaticlinklibrary/</link>
      <pubDate>Fri, 25 Jan 2019 22:01:19 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20190125dynamicstaticlinklibrary/dynamicstaticlinklibrary/</guid>
      <description>
        
          
            &lt;p&gt;开发中，我们只需要知道lib是编译时需要的，dll是运行时需要的。如果要完成源代码的编译，有lib就够了。如果要使动态连接的程序运行起来，有dll就够了。本文会更加清晰的了解二者的区别、生成、使用。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>C文件读取的高级方法：Advanced method of reading files</title>
      <link>https://jerrysmd.github.io/post/20181220read_binary_file/read_binary_file/</link>
      <pubDate>Thu, 20 Dec 2018 18:39:37 -0700</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20181220read_binary_file/read_binary_file/</guid>
      <description>
        
          
            &lt;p&gt;读取二进制文件（任何文件都可以，本文以二进制为例），把二进制文件内容全部读取到char*字符串中。配合fseek()和fread()函数实现文件读取的高级方法。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>SQLcipher Guide</title>
      <link>https://jerrysmd.github.io/post/20181118sqlcipher/sqlcipher/</link>
      <pubDate>Sun, 18 Nov 2018 21:58:51 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20181118sqlcipher/sqlcipher/</guid>
      <description>
        
          
            &lt;p&gt;SQLCipher is based on SQLite, and thus, the majority of the accessible API is identical to the C/C++ interface for SQLite 3. However, SQLCipher does add a number of security specific extensions in the form of PRAGMAs, SQL Functions and C Functions.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>SQL Introduction</title>
      <link>https://jerrysmd.github.io/post/20181017sqlintroduction/sqlintroduction/</link>
      <pubDate>Wed, 17 Oct 2018 11:52:36 +0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20181017sqlintroduction/sqlintroduction/</guid>
      <description>
        
          
            &lt;p&gt;In computer programming, create, read, update, and delete (CRUD) are the four basic functions of persistent storage. Alternate words are sometimes used when defining the four basic functions of CRUD, such as retrieve instead of read, modify instead of update, or destroy instead of delete. CRUD is also sometimes used to describe user interface conventions that facilitate viewing, searching, and changing information, often using computer-based forms and reports.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Makefile Guide</title>
      <link>https://jerrysmd.github.io/post/20180915makefile_cmakelist/makefile_cmakelist/</link>
      <pubDate>Sat, 15 Sep 2018 21:59:46 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180915makefile_cmakelist/makefile_cmakelist/</guid>
      <description>
        
          
            &lt;p&gt;Makefile定义了一系列的规则来指定，哪些文件需要先编译，哪些文件需要后编译，哪些文件需要重新编译，甚至于进行更复杂的功能操作，因为makefile就像一个Shell脚本一样，其中也可以执行操作系统的命令。makefile带来的好处就是——“自动化编译”，整个工程完全自动编译，极大的提高了软件开发的效率。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>Hyperscan: high-performance multiple regex matching library</title>
      <link>https://jerrysmd.github.io/post/20180812hyperscan/hyperscan/</link>
      <pubDate>Sun, 12 Aug 2018 05:42:44 -0700</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180812hyperscan/hyperscan/</guid>
      <description>
        
          
            &lt;p&gt;Hyperscan是一款来自于Intel的高性能的正则表达式匹配库。它是基于X86平台以PCRE为原型而开发的。在支持PCRE的大部分语法的前提下，Hyperscan增加了特定的语法和工作模式来保证其在真实网络场景下的实用性。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>文件读取多读取一行空行问题: The problem about file reading one more line</title>
      <link>https://jerrysmd.github.io/post/20180610cgetline/cgetline/</link>
      <pubDate>Sun, 10 Jun 2018 21:58:13 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180610cgetline/cgetline/</guid>
      <description>
        
          
            &lt;p&gt;getline()/get()/read()会出现多读一行的现象。造成该原因可能文件本身问题和getline()函数的问题。可以在while处判断时先判断getline()/get()/read()，如果拿到数据再处理。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>MXML的使用: The description of mxml</title>
      <link>https://jerrysmd.github.io/post/20180507mxml/mxml/</link>
      <pubDate>Mon, 07 May 2018 21:57:23 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180507mxml/mxml/</guid>
      <description>
        
          
            &lt;p&gt;MXML (Minimal XML) is a small, fast and versatile library that reads a whole XML file and puts it in a DOM tree.&lt;/p&gt;
          
          
        
      </description>
    </item>
    
    <item>
      <title>C语言实现HashMap: C language realize HashMap</title>
      <link>https://jerrysmd.github.io/post/20180405hashcode/hashcode/</link>
      <pubDate>Thu, 05 Apr 2018 21:56:51 -0800</pubDate>
      
      <guid>https://jerrysmd.github.io/post/20180405hashcode/hashcode/</guid>
      <description>
        
          
            &lt;p&gt;哈希表是一种十分重要的数据结构，在很多应用场景下都有用到，本文会对哈希表原理进行简单的剖析，并使用C语言实现一个完整的HashMap。&lt;/p&gt;
          
          
        
      </description>
    </item>
    
  </channel>
</rss>
